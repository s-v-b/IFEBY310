---
title: Using with `pyspark` for data preprocessing
jupyter: python3
execute: 
  eval: true
---

We want to use pyspark to preprocess a potentially huge dataset used for web-marketing.

## Data description

The data is a `parquet` file which contains a dataframe with 8 columns:

- `xid`: unique user id
- `action`: type of action. 'C' is a click, 'O' or 'VSL' is a web-display
- `date`: date of the action
- `website_id`: unique id of the website
- `url`: url of the webpage
- `category_id`: id of the display
- `zipcode`: postal zipcode of the user
- `device`: type of device used by the user

## Q1. Some statistics / computations

Using `pyspark.sql` we want to do the following things:

1. Compute the total number of unique users
2. Construct a column containing the total number of actions per user
3. Construct a column containing the number of days since the last action of the user
4. Construct a column containing the number of actions of each user for each modality of device 

## Q2. Binary classification

Then, we want to construct a classifier to predict the click on the category 1204. 
Here is an agenda for this:

1. Construction of a features matrix for which each line corresponds to the information concerning a user.
2. In this matrix, we need to keep only the users that have been exposed to the display in category 1204
3. Using this training dataset, train a binary classifier, and evaluate your classifier using a precision / recall curve computed on test data.


# Download/read the data and a first look at the data

```{python}
import os
import sys
import requests, zipfile, io
from pathlib import Path


os.environ['PYSPARK_PYTHON'] = sys.executable
os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable
```

```{python}
#| ExecuteTime: {end_time: '2020-05-03T16:25:56.650024Z', start_time: '2020-05-03T16:25:52.400542Z'}
from pyspark.sql import SparkSession
```


```{python}
from pyspark.sql import Window
import pyspark.sql.functions as func
from pyspark.sql.types import *
from pyspark.sql.functions import col, lit
```

::: {.callout-note}

### Spark in standalone mode

So far, we used the `local` mode. 

To launch spark in standalone mode, assuming the current working directory is `$SPARK_HOME` 

Assume that `hadoop` is running. This can be checked by monitoring java processes running on the local machine 

```{.jps}
$ jps 
30033 Jps
>>> 18898 NodeManager
>>> 17735 NameNode
>>> 18216 SecondaryNameNode
>>> 17965 DataNode
>>> 18542 ResourceManager
```
`NameNode` `DataNode` and `SecondaryNameNode` have been launched by `start-dfs.sh`, while `NodeManager` and  `ResourceManager` have been 
launched by `start-yarn`.

We can monitor `NameNode` at `http://localhost:9870`

Then we may launch a `master`  and a `worker` process. 

```{.bash}
$ ./sbin/start-master.sh --ip localhost
>>> starting org.apache.spark.deploy.master.Master, logging to ...
$ ./sbin/start-worker.sh spark://localhost:7077
```

We may now launch the  instance of `SparkSession`, setting explicitly the `master` node and the port to communicate with the master.

SparkUI should be reachable at `localhost:4040`. 

The master can be monitored at `localhost:8080`

:::

```{python}
from pyspark import SparkConf, SparkContext
from pyspark.sql import SparkSession
from pyspark.sql import functions as fn
from pyspark.sql.functions import col
```

```{python}
#| eval: false
spark = (SparkSession
    .builder
    .appName("Taming Webdata")
    .getOrCreate()
)

sc = spark._sc
```

```{python}
#| eval: false

spark =  (
    SparkSession.builder 
            .appName("Spark webdata") 
            .master("spark://localhost:7077") 
            # .config("spark.driver.memory", "16G") 
            # .config("spark.serializer", "org.apache.spark.serializer.KryoSerializer") 
            # .config("spark.kryoserializer.buffer.max", "2000M") 
            # .config("spark.driver.maxResultSize", "0") 
#            .config("spark.jars.packages", "com.johnsnowlabs.nlp:spark-nlp_2.12:5.2.3") \
            .getOrCreate()
)
```

```{python}
#| ExecuteTime: {end_time: '2020-05-03T16:25:56.650024Z', start_time: '2020-05-03T16:25:52.400542Z'}
#spark = (SparkSession
#    .builder
#    .appName("Web data")         
#    .getOrCreate()
#)
```

```{python}
#| ExecuteTime: {end_time: '2020-05-03T16:28:38.406210Z', start_time: '2020-05-03T16:28:08.594803Z'}


path = Path('./data/webdata.parquet')

if not path.exists():
    url = "https://s-v-b.github.io/IFEBY310/data/webdata.parquet.zip"
    r = requests.get(url)
    z = zipfile.ZipFile(io.BytesIO(r.content))
    z.extractall(path='data/')
```

```{python}
#| ExecuteTime: {end_time: '2020-05-03T16:27:49.141388Z', start_time: '2020-05-03T16:27:48.989135Z'}
#| scrolled: true
input_path = Path('data/webdata.parquet').absolute()
file_path = 'file://' + str(input_path)

df = spark.read.parquet(file_path)
```

We can also give a try to [Pandas on Spark](https://spark.apache.org/pandas-on-spark/)

::: {.callout-note}

We can also give a try to `pyarrow.parquet` module to load the Parquet file in an Arrow table.

:::

```{python}
import pyarrow as pa
import comet as co
import pyarrow.parquet as pq

dfa = pq.read_table(file_path)

type(dfa)
```

```{python}
dfa.num_columns
```

```{python}
dfa.schema
```

::: {.callout-warning}

Let us go back to the spark data frame

:::

```{python}
#| ExecuteTime: {end_time: '2020-05-03T16:27:52.121782Z', start_time: '2020-05-03T16:27:50.752210Z'}
df.head(6)
```

```{python}
df.show(5)
```

```{python}
df.describe()
```

```{python}
df.printSchema()
```

```{python}
df.rdd.getNumPartitions()
```

::: {.callout-note  title="Question"}

Explain the partition size. 

:::

# Basic statistics

First we need to import some things:

- `Window` class
- SQL functions module
- Some very useful functions
- Spark types

```{python}
from pyspark.sql import Window
import pyspark.sql.functions as func
from pyspark.sql.types import *
from pyspark.sql.functions import col, lit
```

## Compute the total number of unique users


Compute the number of distinct values in column  `xid`
```{python}
( 
    df.select('xid')
      .distinct()
      .count()
)
```

Compute the number of distinct values in column `xid` for each partition of the dataframe. 

```{python}
def foo(x):
   c = len(set(x))
   print(c)
   return c
```

```{python}
foo([1, 1, 2])
```


```{python}
df.rdd.map(lambda x : x.xid).foreachPartition(foo)
```

```{python}
78120 + 78636 + 79090 + 78865 + 79296 + 79754
```

This might pump up some computational resources 

```{python}
( 
    df.select('xid')
      .distinct() 
      .explain()
)
```

::: {.callout-note}

The distinct values of `xid` seem to be evenly spread among the six files making the `parquet` directory.  

:::

::: {.callout-note}

Have a look at WebUI (usually `http://localhost:4040/`) and at the stages tab to visualize the corresponding DAG. 

Can you match the items (at least some of them) of the output of `explain()` with the items visualized in the DAG?  

:::


## Construct a column containing the total number of actions per user

```{python}
#| ExecuteTime: {end_time: '2020-05-03T15:20:52.163321Z', start_time: '2020-05-03T15:20:50.965856Z'}
xid_partition = Window.partitionBy('xid')

n_events = func.count(col('action')).over(xid_partition)

df = df.withColumn('n_events', n_events)

df.show(n=2)
```

```{python}
( 
  df
    .groupBy('xid')
    .agg(func.count('action'))
    .show(n=5)
)
```

Visualize the distribution of the number of users per number of actions.

```{python}
# One row per user with their n_events, then count users per n_events
users_per_n_events = (
    df.select('xid', 'n_events')
    .dropDuplicates()
    .groupBy('n_events')
    .agg(func.count('xid').alias('height'))
    .orderBy('n_events')
)
```

```{python}
#| label: fig-users-per-actions
#| fig-cap: "Number of users (xid) per number of actions (n_events)"
# Spark DataFrame plot (Plotly backend): n_events on X, number of users (height) on Y
fig = (
    users_per_n_events
        .where(col('n_events') <= 10)
        .plot.bar(
            x='n_events', 
            y='height', 
            title='Number of users per number of actions performed',
            labels={'n_events': 'Number of actions performed', 'height': 'Number of users'})
)

fig.update_layout(
    xaxis_title='Number of actions performed',
    yaxis_title='Number of users'
)
fig
```


::: {.callout-note title="Question"}

Construct a column containing the number of days since the last action of the user

:::



```{python}
#| ExecuteTime: {end_time: '2020-05-03T15:20:55.895918Z', start_time: '2020-05-03T15:20:54.925126Z'}
# xid_partition = Window.partitionBy('xid')

max_date = (
  func
    .max(col('date'))
    .over(xid_partition)
)

n_days_since_last_event = func.datediff(func.current_date(), max_date)

df = df.withColumn('n_days_since_last_event',
                    n_days_since_last_event)

df.show(n=2)
```

```{python}
df.printSchema()
```

## Construct a column containing the number of actions of each user for each modality of device

```{python}
#| ExecuteTime: {end_time: '2020-05-03T15:21:03.846417Z', start_time: '2020-05-03T15:21:03.027855Z'}
#| scrolled: true
xid_device_partition = xid_partition.partitionBy('device')

n_events_per_device = (
    func
        .count(col('action'))
        .over(xid_device_partition)
)

df = df.withColumn('n_events_per_device', n_events_per_device)

df.show(n=2)
```

## Number of devices per user {{< fa mug-hot >}}

::: {.callout-note}

We do not intend to just perform an aggregation here. We aim at adding a new column to the dataframe. Using window functions (appropriate ranking function) may prove more readable and possibly more efficient than  performing aggregation and joining the result of aggregation with the original dataframe. 

:::

```{python}
#| ExecuteTime: {end_time: '2020-05-03T15:21:06.411472Z', start_time: '2020-05-03T15:21:05.373879Z'}
xid_partition = Window.partitionBy('xid')

rank_device = (
  func
    .dense_rank()
    .over(xid_partition.orderBy('device'))
)

n_unique_device = (
    func
      .last(rank_device)
      .over(xid_partition)
)

df = df.withColumn('n_device', n_unique_device)

df.show(n=2)
```


```{python}
#| label: number-events-per-device
fig_2 = (
    df.select(['device','n_events_per_device'])
      .dropDuplicates()
      .orderBy('n_events_per_device', ascending=False)
      .plot.bar(
            x='device', 
            y='n_events_per_device')
)

fig_2.update_layout(
    title='Number of events per device',
    xaxis_title='Device',
    yaxis_title='Number of events',
    yaxis_type='log'
)

fig_2
```


```{python}
#| ExecuteTime: {end_time: '2020-05-03T15:21:13.863382Z', start_time: '2020-05-03T15:21:12.479688Z'}
#| scrolled: true
df\
    .where(col('n_device') > 1)\
    .select('xid', 'device', 'n_events',  'n_device', 'n_events_per_device')\
    .show(n=8)
```

```{python}
df\
    .where(col('n_device') > 1)\
    .select('xid', 'device', 'n_events',  'n_device', 'n_events_per_device')\
    .count()
```

# Let  us select the correct users and build a training dataset

We construct a ETL (Extract Transform Load) process on this data using the `pyspark.sql` API.


::: {.callout-note}

ETL (Extract Transform Load) and ELT (Extract Load Transform). Check that you understand these two acronyms. What do they stand for? What is the difference?

:::

## Extraction

Here *extraction* is just about reading the data.

::: {.callout-note}

What else coud *extraction* be about?

:::

```{python}
#| ExecuteTime: {end_time: '2020-05-03T15:21:36.683625Z', start_time: '2020-05-03T15:21:36.427615Z'}
#| scrolled: true

file_path = './data/webdata.parquet'
file_path = 'file://' + str(Path(file_path).absolute())
df = spark.read.parquet(file_path)
df.show(n=3)
```

```{python}
df = df.repartition(20, 'xid')
```

```{python}

spark._sc.setCheckpointDir("file://" + str(Path('.').absolute()))   

df.checkpoint()
```

## Transformation of the data

At this step we compute a lot of extra things from the data. The aim is to build *features* that describe users. The features will eventually be used by ML (Machine Learning) methods.

Each feature is built by feeding a *transformer* with a dataframe. Each 
transformer returns a dataframe with one more column.

The prospective list of transformers is:
```
hour_transformer
n_actions_per_category_id_transformer
n_days_since_last_action_transformer
n_days_since_last_event_transformer
n_events_per_category_id_transformer
n_events_per_device_transformer
n_events_per_hour_transformer
n_events_per_website_id_transformer
n_events_per_weekday_transformer
n_unique_day_transformer
n_unique_device_transformer
n_unique_hour_transformer
weekday_transformer
```

Each transformer is defined by 

- a `Window` object (optional, some are used several times), 
- a column function that defines the added column, and 
- a name  for the added column. 

The transformer is named after the name  of the added column. All transformers have the same signature. 

```{python}
#| ExecuteTime: {end_time: '2020-05-03T15:24:09.159215Z', start_time: '2020-05-03T15:24:09.136189Z'}
def n_events_transformer(df):
    xid_partition = Window.partitionBy('xid')
    n_events = func.count(col('action')).over(xid_partition)
    
    df = df.withColumn('n_events', n_events)

    return df
```

```{python}
def n_events_per_action_transformer(df):
    xid_action_partition = Window.partitionBy('xid', 'action')
    n_events_per_action = func.count(col('action')).over(xid_action_partition)

    df = df.withColumn('n_events_per_action', n_events_per_action)
    
    return df
```

```{python}
def hour_transformer(df):
    hour = func.hour(col('date'))
    df = df.withColumn('hour', hour)
    return df

def weekday_transformer(df):
    weekday = func.date_format(col('date'), 'EEEE')
    df = df.withColumn('weekday', weekday)
    return df

def n_events_per_hour_transformer(df):
    xid_hour_partition = Window.partitionBy('xid', 'hour')
    n_events_per_hour = func.count(col('action')).over(xid_hour_partition)
    df = df.withColumn('n_events_per_hour', n_events_per_hour)
    return df

def n_events_per_weekday_transformer(df):
    xid_weekday_partition = Window.partitionBy('xid', 'weekday')
    n_events_per_weekday = func.count(col('action')).over(xid_weekday_partition)
    df = df.withColumn('n_events_per_weekday', n_events_per_weekday)
    return df

def n_days_since_last_event_transformer(df):
    xid_partition = Window.partitionBy('xid')
    max_date = func.max(col('date')).over(xid_partition)
    n_days_since_last_event = func.datediff(func.current_date(), max_date)
    df = df.withColumn('n_days_since_last_event',
                       n_days_since_last_event + lit(0.1))
    return df

def n_days_since_last_action_transformer(df):
    xid_partition_action = Window.partitionBy('xid', 'action')
    max_date = func.max(col('date')).over(xid_partition_action)
    n_days_since_last_action = func.datediff(func.current_date(),
                                                        max_date)
    df = df.withColumn('n_days_since_last_action',
                       n_days_since_last_action + lit(0.1))
    return df

def n_unique_day_transformer(df):
    xid_partition = Window.partitionBy('xid')
    dayofyear = func.dayofyear(col('date'))
    rank_day = func.dense_rank().over(xid_partition.orderBy(dayofyear))
    n_unique_day = func.last(rank_day).over(xid_partition)
    df = df.withColumn('n_unique_day', n_unique_day)
    return df

def n_unique_hour_transformer(df):
    xid_partition = Window.partitionBy('xid')
    rank_hour = func.dense_rank().over(xid_partition.orderBy('hour'))
    n_unique_hour = func.last(rank_hour).over(xid_partition)
    df = df.withColumn('n_unique_hour', n_unique_hour)
    return df

def n_events_per_device_transformer(df):
    xid_device_partition = Window.partitionBy('xid', 'device')
    n_events_per_device = func.count(func.col('device')) \
        .over(xid_device_partition)
    df = df.withColumn('n_events_per_device', n_events_per_device)
    return df

def n_unique_device_transformer(df):
    xid_partition = Window.partitionBy('xid')
    rank_device = func.dense_rank().over(xid_partition.orderBy('device'))
    n_unique_device = func.last(rank_device).over(xid_partition)
    df = df.withColumn('n_device', n_unique_device)
    return df

def n_actions_per_category_id_transformer(df):
    xid_category_id_partition = Window.partitionBy('xid', 'category_id',
                                                   'action')
    n_actions_per_category_id = func.count(func.col('action')) \
        .over(xid_category_id_partition)
    df = df.withColumn('n_actions_per_category_id', n_actions_per_category_id)
    return df

def n_unique_category_id_transformer(df):
    xid_partition = Window.partitionBy('xid')
    rank_category_id = func.dense_rank().over(xid_partition\
                                              .orderBy('category_id'))
    n_unique_category_id = func.last(rank_category_id).over(xid_partition)
    df = df.withColumn('n_unique_category_id', n_unique_category_id)
    return df

def n_events_per_category_id_transformer(df):
    xid_category_id_partition = Window.partitionBy('xid', 'category_id')
    n_events_per_category_id = func.count(func.col('action')) \
        .over(xid_category_id_partition)
    df = df.withColumn('n_events_per_category_id', n_events_per_category_id)
    return df

def n_events_per_website_id_transformer(df):
    xid_website_id_partition = Window.partitionBy('xid', 'website_id')
    n_events_per_website_id = func.count(col('action'))\
        .over(xid_website_id_partition)
    df = df.withColumn('n_events_per_website_id', n_events_per_website_id)
    return df
```

```{python}
#| ExecuteTime: {end_time: '2020-05-03T15:24:33.042444Z', start_time: '2020-05-03T15:24:15.032735Z'}
transformers = [
    hour_transformer,
    weekday_transformer,
    n_events_per_hour_transformer,
    n_events_per_weekday_transformer,
    n_days_since_last_event_transformer,
    n_days_since_last_action_transformer,
    n_unique_day_transformer,
    n_unique_hour_transformer,
    n_events_per_device_transformer,
    n_unique_device_transformer,
    n_actions_per_category_id_transformer,
    n_events_per_category_id_transformer,
    n_events_per_website_id_transformer,
]
```

```{python}
N = 10000
```

```{python}
sample_df = df.sample(withReplacement=False, fraction=.05)
```

```{python}
sample_df.count()
```

```{python}
for transformer in transformers:
    sample_df = transformer(sample_df)

sample_df.show(n=1)
```

```{python}
sample_df.printSchema()
```


```{python}
#| label: apply-transformers
#| 
from time import perf_counter

t0 =  perf_counter()
for transformer in transformers:
    df = transformer(df)

( 
    df
        .sample(withReplacement=False, fraction=.05)
        .show(n=1)
)

print(perf_counter() -t0)
```

```{python}
df.write.parquet('file:///home/boucheron/Documents/IFEBY310/core/notebooks/data/webdata-transformed.parquet', mode='overwrite')
```

```{python}
#| ExecuteTime: {end_time: '2020-05-03T15:24:40.406910Z', start_time: '2020-05-03T15:24:40.393184Z'}
#| scrolled: true
sorted(df.columns)
```


::: {.callout-note}

- How DRY-compliant is this code? 
- Should we dry it?
- Why would we dry it (readability, maintainability, ...)? 


:::


## Load step

Here, we use all the previous computations (saved in the columns of the dataframe) to compute aggregated informations about each user.


::: {.callout-note}

All functions in the next chunk have one argument: a dataframe with schema

```
root
 |-- xid: string (nullable = true)
 |-- action: string (nullable = true)
 |-- date: timestamp (nullable = true)
 |-- website_id: string (nullable = true)
 |-- url: string (nullable = true)
 |-- category_id: float (nullable = true)
 |-- zipcode: string (nullable = true)
 |-- device: string (nullable = true)
 |-- hour: integer (nullable = true)
 |-- weekday: string (nullable = true)
 |-- n_events_per_hour: long (nullable = false)
 |-- n_events_per_weekday: long (nullable = false)
 |-- n_days_since_last_event: double (nullable = true)
 |-- n_days_since_last_action: double (nullable = true)
 |-- n_unique_day: integer (nullable = true)
 |-- n_unique_hour: integer (nullable = true)
 |-- n_events_per_device: long (nullable = false)
 |-- n_device: integer (nullable = true)
 |-- n_actions_per_category_id: long (nullable = false)
 |-- n_events_per_category_id: long (nullable = false)
 |-- n_events_per_website_id: long (nullable = false)
```

All functions in the next chunk return a dataframe with three columns: 
`xid`, `value`, `feature_name`.  Column `value` is obtained by renaming 
a column `n_...` (built by one of the previous transformers). Column `feature_name`   


:::

```{python}
#| ExecuteTime: {end_time: '2020-05-03T15:25:22.243433Z', start_time: '2020-05-03T15:25:22.217248Z'}
def n_events_per_hour_loader(df):
    csr = df\
        .select('xid', 'hour', 'n_events_per_hour')\
        .withColumnRenamed('n_events_per_hour', 'value')\
        .distinct()     # action
    feature_name = func.concat(lit('n_events_per_hour#'), col('hour'))
    csr = csr\
        .withColumn('feature_name', feature_name)\
        .drop('hour')
    return csr

def n_events_per_website_id_loader(df):
    csr = df.select('xid', 'website_id', 'n_events_per_website_id')\
        .withColumnRenamed('n_events_per_website_id', 'value')\
        .distinct()
    feature_name = func.concat(lit('n_events_per_website_id#'),
                               col('website_id'))
    csr = csr\
        .withColumn('feature_name', feature_name)\
        .drop('website_id')
    return csr

def n_events_per_hour_loader(df):
    csr = df\
        .select('xid', 'hour', 'n_events_per_hour')\
        .withColumnRenamed('n_events_per_hour', 'value')\
        .distinct()
    feature_name = func.concat(lit('n_events_per_hour#'), col('hour'))
    csr = csr\
        .withColumn('feature_name', feature_name)\
        .drop('hour')
    return csr

def n_events_per_weekday_loader(df):
    csr = df\
        .select('xid', 'weekday', 'n_events_per_weekday')\
        .withColumnRenamed('n_events_per_weekday', 'value')\
        .distinct()
    feature_name = func.concat(lit('n_events_per_weekday#'), col('weekday'))
    csr = csr\
        .withColumn('feature_name', feature_name)\
        .drop('weekday')
    return csr

def n_days_since_last_event_loader(df):
    csr = df.select('xid',  'n_days_since_last_event')\
        .withColumnRenamed('n_days_since_last_event#', 'value')\
        .distinct()
    feature_name = lit('n_days_since_last_event')
    csr = csr\
        .withColumn('feature_name', feature_name)
    return csr

def n_days_since_last_action_loader(df):
    csr = df.select('xid', 'action', 'n_days_since_last_action')\
        .withColumnRenamed('n_days_since_last_action', 'value')\
        .distinct()
    feature_name = func.concat(lit('n_days_since_last_action#'), col('action'))
    csr = csr\
        .withColumn('feature_name', feature_name)\
        .drop('action')
    return csr

def n_unique_day_loader(df):
    csr = df.select('xid', 'n_unique_day')\
        .withColumnRenamed('n_unique_day', 'value')\
        .distinct()
    feature_name = lit('n_unique_day')
    csr = csr\
        .withColumn('feature_name', feature_name)
    return csr

def n_unique_hour_loader(df):
    csr = df.select('xid', 'n_unique_hour')\
        .withColumnRenamed('n_unique_hour', 'value')\
        .distinct()
    feature_name = lit('n_unique_hour')
    csr = csr\
        .withColumn('feature_name', feature_name)
    return csr

def n_events_per_device_loader(df):
    csr = df\
        .select('xid', 'device', 'n_events_per_device')\
        .withColumnRenamed('n_events_per_device', 'value')\
        .distinct()
    feature_name = func.concat(lit('n_events_per_device#'), col('device'))
    csr = csr\
        .withColumn('feature_name', feature_name)\
        .drop('device')
    return csr

def n_unique_device_loader(df):
    csr = df.select('xid', 'n_device')\
        .withColumnRenamed('n_device', 'value')\
        .distinct()
    feature_name = lit('n_device')
    csr = csr\
        .withColumn('feature_name', feature_name)
    return csr

def n_events_per_category_id_loader(df):
    csr = df.select('xid', 'category_id', 'n_events_per_category_id')\
        .withColumnRenamed('n_events_per_category_id', 'value')\
        .distinct()
    feature_name = func.concat(lit('n_events_per_category_id#'),
                               col('category_id'))
    csr = csr\
        .withColumn('feature_name', feature_name)\
        .drop('category_id')
    return csr

def n_actions_per_category_id_loader(df):
    csr = df.select('xid', 'category_id', 'action', 'n_actions_per_category_id')\
        .withColumnRenamed('n_actions_per_category_id', 'value')\
        .distinct()
    feature_name = func.concat(lit('n_actions_per_category_id#'),
                               col('action'), lit('#'), 
                               col('category_id'))
    csr = csr\
        .withColumn('feature_name', feature_name)\
        .drop('category_id')\
        .drop('action')
    return csr

def n_events_per_website_id_loader(df):
    csr = df.select('xid', 'website_id', 'n_events_per_website_id')\
        .withColumnRenamed('n_events_per_website_id', 'value')\
        .distinct()
    feature_name = func.concat(lit('n_events_per_website_id#'),
                               col('website_id'))
    csr = csr\
        .withColumn('feature_name', feature_name)\
        .drop('website_id')
    return csr
```


::: {.callout-note}


- How DRY-compliant is this code (bis)? 
- Should we dry it?
- Why would we dry it (readability, maintainability, ...)? 


:::


```{python}
#| ExecuteTime: {end_time: '2020-05-03T15:25:48.149670Z', start_time: '2020-05-03T15:25:37.966657Z'}
#| scrolled: true
from functools import reduce
```

```{python}
loaders = [
    n_events_per_hour_loader,
    n_events_per_website_id_loader,
    n_events_per_hour_loader,
    n_events_per_weekday_loader,
    n_days_since_last_event_loader,
    n_days_since_last_action_loader,
    n_unique_day_loader,
    n_unique_hour_loader,
    n_events_per_device_loader,
    n_unique_device_loader,
    n_events_per_category_id_loader,
    n_actions_per_category_id_loader,
    n_events_per_website_id_loader,
]
```

```{python}
def union(df, other):
    return df.union(other)
```

::: {.callout-caution title="About DataFrame.union()"}

This method performs a SQL-style set union of the rows from both DataFrame objects, with no automatic deduplication of elements.

Use the distinct() method to perform deduplication of rows.

The method resolves columns by position (not by name), following the standard behavior in SQL.

:::

```{python}
spam = [loader(df) for loader in loaders]
```

```{python}
spam[0].printSchema()
```

```{python}
len(spam)
```

```{python}
csr = reduce(
    lambda df1, df2: df1.union(df2),
    spam
)

csr.show(n=3)
```

```{python}
#| ExecuteTime: {end_time: '2020-05-03T15:25:54.862814Z', start_time: '2020-05-03T15:25:54.857914Z'}
csr.columns
```

```{python}
#| ExecuteTime: {end_time: '2020-05-03T15:26:13.629146Z', start_time: '2020-05-03T15:25:55.683800Z'}
csr.show(5)
```

```{python}
csr.rdd.getNumPartitions()
```

```{python}
#| ExecuteTime: {end_time: '2020-05-03T15:30:20.643141Z', start_time: '2020-05-03T15:29:45.221790Z'}
# Replace features names and xid by a unique number
feature_name_partition = Window().orderBy('feature_name')
xid_partition = Window().orderBy('xid')

col_idx = func.dense_rank().over(feature_name_partition)
row_idx = func.dense_rank().over(xid_partition)
```

```{python}
#| ExecuteTime: {end_time: '2020-05-03T15:30:20.643141Z', start_time: '2020-05-03T15:29:45.221790Z'}
csr = csr.withColumn('col', col_idx)\
    .withColumn('row', row_idx)

csr = csr.na.drop('any')
```

```{python}
csr.head(n=5)
```


```{python}
#| ExecuteTime: {end_time: '2020-05-03T15:32:02.552364Z', start_time: '2020-05-03T15:31:14.990298Z'}
# Let's save the result of our hard work into a new parquet file (in the local filesystem)
output_path = Path('./data').absolute()
output_file = 'file://' + str(output_path / 'csr.parquet')
csr.write.parquet(output_file, mode='overwrite')
```

```{python}
# Save  the result  to hdfs
cs.write.parquet('csr.parquet', mode='overwrite')
```

# Preparation of the training dataset

```{python}
#| ExecuteTime: {end_time: '2020-05-03T15:32:56.421452Z', start_time: '2020-05-03T15:32:55.819071Z'}
#| scrolled: true
csr_path = './data'
csr_file = os.path.join(csr_path, 'csr.parquet')

df = spark.read.parquet(csr_file)
df.head(n=5)
```

```{python}
#| ExecuteTime: {end_time: '2020-05-03T15:33:17.229477Z', start_time: '2020-05-03T15:33:16.995048Z'}
#| scrolled: true
df.count()
```

```{python}
#| ExecuteTime: {end_time: '2020-05-03T15:33:20.881392Z', start_time: '2020-05-03T15:33:19.624525Z'}
#| scrolled: true
# What are the features related to campaign_id 1204 ?
features_names = \
    df.select('feature_name')\
    .distinct()\
    .toPandas()['feature_name']
```

```{python}
#| ExecuteTime: {end_time: '2020-05-03T15:33:21.818568Z', start_time: '2020-05-03T15:33:21.812810Z'}
features_names
```

```{python}
#| ExecuteTime: {end_time: '2020-05-03T15:33:27.083141Z', start_time: '2020-05-03T15:33:27.078374Z'}
#| scrolled: true
[feature_name for feature_name in features_names if '1204' in feature_name]
```

```{python}
#| ExecuteTime: {end_time: '2020-05-03T15:33:28.560631Z', start_time: '2020-05-03T15:33:27.903921Z'}
#| scrolled: true
# Look for the xid that have at least one exposure to campaign 1204
keep = func.when(
    (col('feature_name') == 'n_actions_per_category_id#C#1204.0') |
    (col('feature_name') == 'n_actions_per_category_id#O#1204.0'),
    1).otherwise(0)
df = df.withColumn('keep', keep)

df.where(col('keep') > 0).count()
```

```{python}
#| ExecuteTime: {end_time: '2020-05-03T15:33:31.274277Z', start_time: '2020-05-03T15:33:31.244066Z'}
# Sum of the keeps :)
xid_partition = Window.partitionBy('xid')
sum_keep = func.sum(col('keep')).over(xid_partition)
df = df.withColumn('sum_keep', sum_keep)
```

```{python}
#| ExecuteTime: {end_time: '2020-05-03T15:33:31.467139Z', start_time: '2020-05-03T15:33:31.404561Z'}
# Let's keep the xid exposed to 1204
df = df.where(col('sum_keep') > 0)
```

```{python}
#| ExecuteTime: {end_time: '2020-05-03T15:33:34.619928Z', start_time: '2020-05-03T15:33:31.572475Z'}
df.count()
```

```{python}
#| ExecuteTime: {end_time: '2020-05-03T15:33:37.918500Z', start_time: '2020-05-03T15:33:34.622711Z'}
#| scrolled: true
df.select('xid').distinct().count()
```

```{python}
#| ExecuteTime: {end_time: '2020-05-03T15:33:52.607777Z', start_time: '2020-05-03T15:33:40.110545Z'}
row_partition = Window().orderBy('row')
col_partition = Window().orderBy('col')
row_new = func.dense_rank().over(row_partition)
col_new = func.dense_rank().over(col_partition)
df = df.withColumn('row_new', row_new)
df = df.withColumn('col_new', col_new)
csr_data = df.select('row_new', 'col_new', 'value').toPandas()
```

```{python}
#| ExecuteTime: {end_time: '2020-05-03T15:33:52.617724Z', start_time: '2020-05-03T15:33:52.609488Z'}
#| scrolled: true
csr_data.head()
```

```{python}
#| ExecuteTime: {end_time: '2020-05-03T15:33:58.443120Z', start_time: '2020-05-03T15:33:52.619858Z'}
#| scrolled: true
features_names = df.select('feature_name', 'col_new').distinct()
features_names.where(col('feature_name') == 'n_actions_per_category_id#C#1204.0').head()
```

```{python}
#| ExecuteTime: {end_time: '2020-05-03T15:34:04.104342Z', start_time: '2020-05-03T15:33:58.445504Z'}
features_names.where(col('feature_name') == 'n_actions_per_category_id#O#1204.0').head()
```

```{python}
#| ExecuteTime: {end_time: '2020-05-03T15:34:11.510538Z', start_time: '2020-05-03T15:34:11.454802Z'}
from scipy.sparse import csr_matrix
import numpy as np

rows = csr_data['row_new'].values - 1
cols = csr_data['col_new'].values - 1
vals = csr_data['value'].values

X_csr = csr_matrix((vals, (rows, cols)))
```

```{python}
#| ExecuteTime: {end_time: '2020-05-03T15:34:11.977267Z', start_time: '2020-05-03T15:34:11.972602Z'}
X_csr.shape
```

```{python}
#| ExecuteTime: {end_time: '2020-05-03T15:34:28.207343Z', start_time: '2020-05-03T15:34:28.202443Z'}
X_csr.shape, X_csr.nnz
```

```{python}
#| ExecuteTime: {end_time: '2020-05-03T15:34:30.978599Z', start_time: '2020-05-03T15:34:30.972909Z'}
X_csr.nnz / (X_csr.shape[0]* X_csr.shape[1])   # 0152347 * 92)
```

```{python}
#| ExecuteTime: {end_time: '2020-05-03T15:34:32.871960Z', start_time: '2020-05-03T15:34:32.860482Z'}
# The label vector. Let's make it dense, flat and binary
y = np.array(X_csr[:, 1].todense()).ravel()
y = np.array(y > 0, dtype=np.int64)
```

```{python}
#| ExecuteTime: {end_time: '2020-05-03T15:34:33.348181Z', start_time: '2020-05-03T15:34:33.343110Z'}
#| scrolled: true
X_csr.shape
```

```{python}
#| ExecuteTime: {end_time: '2020-05-03T15:34:37.382059Z', start_time: '2020-05-03T15:34:37.371588Z'}
# We remove the second and fourth column. 
# It actually contain the label we'll want to predict.
kept_cols = list(range(X_csr.shape[1]))
kept_cols.pop(1)
kept_cols.pop(2)
X = X_csr[:, kept_cols]
```

```{python}
len(kept_cols)
```

```{python}
#| ExecuteTime: {end_time: '2020-05-03T15:34:38.375629Z', start_time: '2020-05-03T15:34:38.369734Z'}
X_csr.shape, X.shape
```

## Finally !!

Wow ! That was a lot of work. Now we have a features matrix $X$ and a vector of labels $y$.

```{python}
#| ExecuteTime: {end_time: '2020-05-03T15:34:40.526092Z', start_time: '2020-05-03T15:34:40.521420Z'}
#| scrolled: true
X.indices
```


```{python}
#| ExecuteTime: {end_time: '2020-05-03T15:34:40.750471Z', start_time: '2020-05-03T15:34:40.744670Z'}
#| scrolled: true
X.indptr
```

```{python}
#| ExecuteTime: {end_time: '2020-05-03T15:34:40.974359Z', start_time: '2020-05-03T15:34:40.969638Z'}
X.shape, X.nnz
```

```{python}
#| ExecuteTime: {end_time: '2020-05-03T15:34:41.220722Z', start_time: '2020-05-03T15:34:41.213466Z'}
y.shape, y.sum()
```

# Some learning for/from this data

```{python}
#| ExecuteTime: {end_time: '2020-05-03T15:51:21.964565Z', start_time: '2020-05-03T15:51:20.939544Z'}
from sklearn.preprocessing import MaxAbsScaler
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression

# Normalize the features
X = MaxAbsScaler().fit_transform(X)
X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.3)

clf = LogisticRegression(
    penalty='l2',
    C=1e3,
    solver='lbfgs',
    class_weight='balanced'
)

clf.fit(X_train, y_train)
```

```{python}
#| ExecuteTime: {end_time: '2020-05-03T15:51:22.820046Z', start_time: '2020-05-03T15:51:22.809009Z'}
#| scrolled: true
features_names = features_names.toPandas()['feature_name']
```

```{python}
features_names[range(6)]
```

```{python}
#| ExecuteTime: {end_time: '2020-05-03T15:51:25.078266Z', start_time: '2020-05-03T15:51:23.622795Z'}
import matplotlib.pyplot as plt
%matplotlib inline
```


```{python}
#| ExecuteTime: {end_time: '2020-05-03T15:51:25.078266Z', start_time: '2020-05-03T15:51:23.622795Z'}
plt.figure(figsize=(16, 5))
plt.stem(clf.coef_[0]) # , use_line_collection=True)
plt.title('Logistic regression coefficients', fontsize=18)
```

```{python}
clf.coef_[0].shape[0]
```

```{python}
len(features_names)
```

```{python}
#| ExecuteTime: {end_time: '2020-05-03T15:51:25.078266Z', start_time: '2020-05-03T15:51:23.622795Z'}
# We change the fontsize of minor ticks label
_ = plt.xticks(np.arange(clf.coef_[0].shape[0]), features_names, 
           rotation='vertical', fontsize=8)
```

```{python}
#| ExecuteTime: {end_time: '2020-05-03T15:51:25.078266Z', start_time: '2020-05-03T15:51:23.622795Z'}
_ = plt.yticks(fontsize=14)
```

```{python}
#| ExecuteTime: {end_time: '2020-05-03T15:51:25.280157Z', start_time: '2020-05-03T15:51:25.081464Z'}
from sklearn.metrics import precision_recall_curve, f1_score

precision, recall, _ = precision_recall_curve(y_test, clf.predict_proba(X_test)[:, 1])
    
plt.figure(figsize=(8, 6))
plt.plot(recall, precision, label='LR (F1=%.2f)' % f1_score(y_test, clf.predict(X_test)), lw=2)
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('Recall', fontsize=16)
plt.ylabel('Precision', fontsize=16)
plt.title('Precision/recall curve', fontsize=18)
plt.legend(loc="upper right", fontsize=14)
```

# Analyse the tables 

```{python}
query = """ANALYZE TABLE db_table COMPUTE STATISTICS
            FOR COLUMNS xid"""
```

```{python}
df.createOrReplaceTempView("db_table")
```

```{python}
df.columns
```

```{python}
spark.sql("cache table db_table")
```

```{python}
spark.sql(query)
```

```{python}
spark.sql("show tables")
```

