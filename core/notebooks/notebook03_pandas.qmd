---
title: Introduction to `pandas`
jupyter: python3
---


The `pandas` library (https://pandas.pydata.org) is one of the most used tool at the disposal of people working with data in `python` today.

- It allows to **crunch data** easily
- It mainly provides a `DataFrame` object (a **table of data**) with a huge set of functionalities


## Why ?

Through `pandas`, you get acquainted with your data by **analyzing** it 

- What's the average, median, max, or min of each column?
- Does column A correlate with column B?
- What does the distribution of data in column C look like?

## Why  (con't) ?

you get acquainted with your data by **cleaning** and  **transforming** it 

- Removing missing values, filter rows or columns using some criteria
- Store the cleaned, transformed data back into virtually any format or database
- Data visualization (when combined `matplotlib`, `seaborn`, `plotly` or others)

## Where ?

`pandas` is a central component of the `python` stack for data science

- `Pandas` is built on top of `NumPy`
- often used in conjunction with other libraries
- a `DataFrame` is often fed to plotting functions or machine learning algorithms (such as `scikit-learn`)
- Well-interfaced with `jupyter`, leading to a nice interactive environment for data exploration and modeling

## Core components of pandas

The two primary components of `Pandas` are the `Series` and `DataFrame`.

- A `Series` is essentially a column

- A `DataFrame` is a multi-dimensional table made up of a collection of `Series` with equal length

## Creating a `DataFrame` from scratch

```{python}
#| label: df_fruits
import pandas as pd

fruits = {
    "apples": [3, 2, 0, 1],
    "oranges": [0, 3, 7, 2]
}

df_fruits = pd.DataFrame(fruits)
df_fruits
```

```{python}
#| lavel: type_df
type(df_fruits)
```

```{python}
#| label: display_series_apple
#| scrolled: true
df_fruits["apples"]
```

```{python}
#| label: type_apples
type(df_fruits["apples"])
```

## Indexing

- By default, a `DataFrame` uses a contiguous index
- But what if we want to say **who** buys the fruits ?

```{python}
#| label: setting_up_index
df_fruits = pd.DataFrame(fruits, index=["Daniel", "Sean", "Pierce", "Roger"])
df_fruits
```

## `.loc` versus `.iloc`

- `.loc` **loc**ates by name
- `.iloc` **loc**ates by numerical **i**ndex

```{python}

df_fruits
```

We can pick rows 
```{python}
#| label: locating_information_by_name
#| scrolled: true
# What's in Sean's basket ?
df_fruits.loc['Sean']
```
Note that this returns a `Series`

We can pick slices of rows and columns
```{python}
#| 
# Who has oranges ?
df_fruits.loc[:, 'oranges']
```

```{python}
#| 
# How many apples in Pierce's basket ?
df_fruits.loc['Pierce', 'apples']
```

Note that the type of the result depends on the indexing information.


```{python}
#| 
#| scrolled: true
df_fruits
```

We may also pick information through positions using `iloc`. 
```{python}
#| 
df_fruits.iloc[2, 1]
```

Note that the DataFrame has two index:

```{python}
#| label: two-index-in-df
df_fruits.index 
df_fruits.columns
```


## Main attributes and methods of a `DataFrame`

A `DataFrame` has many **attributes**

```{python}
#| label: column-attribute
#| scrolled: true
df_fruits.columns
```

```{python}
#| label: ze-index-attribute
df_fruits.index
```

```{python}
#| label: dtypes-attribute
df_fruits.dtypes
```

A `DataFrame` has many **methods**


Method `info()` provides information on the table schema, name and type columns, whether the cells can contain missing values.
```{python}
#| label: info-method
df_fruits.info()
```

Method `describe()` provides with statistical summaries for columns  
```{python}
#| label: describe-method
#| 
df_fruits.describe()
```

## Missing values

What if we don't know how many apples are in Sean's basket ?

```{python}
#| label: Nulling some cells
#| scrolled: true
df_fruits.loc['Sean', 'apples'] = None
df_fruits
```

`None` is a Python keyword. `NaN` belongs to `Pandas`. 


```{python}
#| label: describe-with-nulls
#| scrolled: true
df_fruits.describe()
```

Note that `count` is **3** for apples now, since we have 1 missing value among the 4


::: {.callout-note}

To review the members of objects of class `pandas.DataFrame`, `dir()` and module `inspect` are convenient.

:::

```{python}
#| eval: false
#| label: list-non-dunder-attributes
[x for x in dir(df_fruits) if not x.startswith('_') and not callable(x)]
```

```{python}
import inspect

# Get a list of methods
membres = inspect.getmembers(df_fruits)

method_names = [m[0] for m in membres 
    if callable(m[1]) and not m[0].startswith('_')]

print(method_names)
```

DataFrames have more than $400$ *members*. Among them, almost $200$ are what we call *methods*. 
See [Dataframe documentation](https://pandas.pydata.org/docs/reference/frame.html)

Among non-callable members, we find genuine data attributes and properties. 
```{python}
others = [x for x in membres
    if not callable(x[1])]

[x[0] for x in others if not x[0].startswith('_')]
```

## Adding a column

Ooooops, we forgot about the bananas !

```{python}
#| 
df_fruits["bananas"] = [0, 2, 1, 6]
df_fruits
```

This amounts to add an entry in the *columns* index.

```{python}
df_fruits.columns
```

## Adding a column with the date

And we forgot the dates!

```{python}
#| 
#| scrolled: true
df_fruits['time'] = [
    "2020/10/08 12:13", "2020/10/07 11:37", 
    "2020/10/10 14:07", "2020/10/09 10:51"
]
df_fruits
```

```{python}
#| 
#| scrolled: true
#| 
df_fruits.dtypes
```

```{python}
#| 
#| scrolled: true
type(df_fruits.loc["Roger", "time"])
```

It is not a date but a string (`str`) ! So we convert this column to something called `datetime`  

```{python}
#| 
#| scrolled: true
df_fruits["time"] = pd.to_datetime(df_fruits["time"])
df_fruits
```

```{python}
#| 
df_fruits.dtypes
```

::: {.callout-note}

Every data science framework implements some `datetime` handling scheme. For Python see [Python official documentation on `datetime` module](https://docs.python.org/3/library/datetime.html#module-datetime)

Note that `datetime64[ns]`  parallels NumPy `datetime64`.  


:::

What if we want to keep only the baskets after (including) October, 9th ?

```{python}
#| 
#| scrolled: false
df_fruits.loc[df_fruits["time"] >= pd.Timestamp("2020/10/09")]
```

We can filter rows using a boolean mask and member `loc`. This does not work with `iloc`.

## Casting a Series to another type


In many circumstances, we have to cast columns to a different type. 
To convert a Pandas Series to a different data type, we may use 
the `.astype()` method:

```{python}
#| 
# Create a sample Series
s = pd.Series([1, 2, 3, 4, 5])
s
```

```{python}
#| 
# Check the current dtype
s.dtype
```
If we want to move to `float`:
```{python}
#| 
# Cast to float
s_float = s.astype('float64')
s_float
```
to strings:
```{python}
#| 
# Cast to string
s_str = s.astype('str')
s_str
```
to a categorical type:
```{python}
#| 
# Cast to category
s_cat = s.astype('category')
s_cat
```
This also works for `bool`. 

Sometimes, it may go wrong. 

### Handling errors during conversion

When converting types, you may encounter errors if the conversion is not possible:

```{python}
#| 
# This will raise an error if conversion fails
try:
    pd.Series(['1', '2', 'abc']).astype('int64')
except ValueError as e:
    print(f"Error: {e}")
```

Then method `astype()` may not be the best choice. 
For more robust conversion, use `pd.to_numeric()` with error handling:

```{python}
#| 
# Convert with error handling - invalid values become NaN
pd.to_numeric(pd.Series(['1', '2', 'abc']), errors='coerce')
```

For datetime conversions, `pd.to_datetime()` is usually preferred over `.astype('datetime64[ns]')` as it handles various date formats more robustly.




## Slices and subsets of rows or columns

```{python}
#| 
df_fruits
```

```{python}
#| 
df_fruits.loc[:, "oranges":"time"]
```

```{python}
#| 
df_fruits.loc["Daniel":"Sean", "apples":"bananas"]
```

If we want to project over a collection of columns, 
we have to  
```{python}
#| 
#| scrolled: true
df_fruits[["apples", "time"]]
```

```{python}
tropicals = ("apples", "oranges")

df_fruits[[*tropicals]]
```

We cannot write: 

```{python}
#| eval: false
df_fruits["apples", "time"]
```

Why?

## Write our data to a CSV file

What if we want to write the file ?

```{python}
df_fruits
```

```{python}
#| 
df_fruits.to_csv("fruits.csv")
```

```{python}
#| 
#| scrolled: false
# Use !dir on windows
!ls -alh | grep fru
```

```{python}
#| 
!head -n 5 fruits.csv
```

## Reading data and working with it



::: {.callout-note}

The `tips` dataset comes through [Kaggle](https://www.kaggle.com/code/sanjanabasu/tips-dataset/input)

> This dataset is a treasure trove of information from a collection of case studies for business statistics. Special thanks to Bryant and Smith for their diligent work:

> Bryant, P. G. and Smith, M (1995) Practical Data Analysis: Case Studies in Business Statistics. Homewood, IL: Richard D. Irwin Publishing.

> You can also access this dataset now through the Python package Seaborn.

:::

It contains data about a restaurant: the bill, tip and some informations about the customers.

::: {.callout-note}

### A toy extraction pattern

A data pipeline usually starts with Extraction, that is gathering data from some source, possibly in a galaxy far, far awy. Here follows a toy extraction pattern

- obtain the data from some `URL` using package `requests`
- save the data on the hard drive
- load the data using Pandas 


```{python}
#| scrolled: true
#| eval: false
import requests
import os

# The path containing your notebook
path_data = './'
# The name of the file
filename = 'tips.csv'

if os.path.exists(os.path.join(path_data, filename)):
    print(f'The file {os.path.join(path_data, filename)} already exists.')
else:
    url = 'https://raw.githubusercontent.com/mwaskom/seaborn-data/refs/heads/master/tips.csv'
    r = requests.get(url)
    with open(os.path.join(path_data, filename), 'wb') as f:
        f.write(r.content)
    print('Downloaded file %s.' % os.path.join(path_data, filename))
```

```{python}
#| eval: false
#| scrolled: true
df = pd.read_csv(
    "tips.csv", 
    delimiter=","
)
```

:::

The data can be obtained from package `seaborn`.

```{python}
import seaborn as sns

sns_ds = sns.get_dataset_names()

'tips' in sns_ds

df = sns.load_dataset('tips')
```

Note that the dataframe loaded from the `csv` file and the dataframe obtained from package `seaborn` differ. This can be checked by examining the representations and properties of column `smoker, sex` (check `df.sex.array` in both cases)

```{python}
# `.head()` shows the first rows of the dataframe
df.head(n=10)
```

```{python}
#| 
#| scrolled: true
df.info()
```

```{python}
#| 
#| scrolled: true
df.loc[42, "day"]
```

```{python}
#| 
#| scrolled: true
type(df.loc[42, "day"])
```

By default, columns that are non-numerical contain strings (`str` type)

## The `category` type

An important type in `pandas` is `category` for variables that are **non-numerical**

**Pro tip.** It's always a good idea to tell `pandas` which columns should be imported as **categorical**

So, let's read again the file specifying some `dtype`s to the `read_csv` function

```{python}
#| 
dtypes = {
    "sex": "category",
    "smoker": "category",
    "day": "category",
    "time": "category"
} 

df = pd.read_csv("tips.csv", dtype=dtypes)
```

Supplemented with this typing information, the dataframe loaded from the `csv` file is more like the dataframe obtained from `seaborn`. 


```{python}
#| 
#| scrolled: true
df.dtypes
```

## Computing statistics

```{python}
#| 
# The describe method only shows statistics for the numerical columns by default
df.describe()
```

```{python}
#| 
#| scrolled: true
# We use the include="all" option to see everything
df.describe(include="all")
```

```{python}
#| 
# Correlation between the numerical columns
df.corr(numeric_only = True)
```


In more general settings, to select only numerical columns from a DataFrame, use the `select_dtypes()` method:

```{python}
#| 
# Select only numerical columns (int, float, etc.)
(
    df
        .select_dtypes(include=['number'])
        .head()
)
```



::: {.callout-note}

The `select_dtypes()` method is very flexible:
- `include=['number']` selects all numeric types (int, float, etc.)
- `include=['int64', 'float64']` selects specific dtypes
- `exclude=['object']` excludes string columns
- You can combine `include` and `exclude` parameters

:::

```{python}
(
    df
        .select_dtypes(include='float64')
        .corr()
)
```

# Plotting backends for Pandas DataFrames

Pandas DataFrames have built-in plotting capabilities through the `.plot` accessor. By default, Pandas uses `matplotlib` as the plotting backend.

## Setting the plotting backend

You can set the plotting backend using `pd.options.plotting.backend`:

```{python}
#| eval: false
# Set the backend globally for all DataFrames
pd.options.plotting.backend = 'matplotlib'  # default
```

Available backends include:

- `'matplotlib'` (default) 
- `'plotly'` (requires `plotly` package)
- `'hvplot'` (requires `hvplot` package)

::: {.callout-caution}

Seaborn is **not** available as a plotting backend for `pd.options.plotting.backend`. Seaborn is a separate visualization library built on top of matplotlib that works directly with pandas DataFrames through its own API (e.g., `sns.scatterplot(data=df, ...)`). While seaborn integrates seamlessly with pandas DataFrames, it doesn't replace the `.plot` accessor's backend system.

:::

## Using different backends

### Matplotlib (default)

```{python}
#| eval: true
# Default matplotlib backend
pd.options.plotting.backend = 'matplotlib'
df.plot.scatter(x='total_bill', y='tip')
```

### Plotly backend

```{python}
#| eval: true
# Switch to plotly for interactive plots
pd.options.plotting.backend = 'plotly'
df.plot.scatter(x='total_bill', y='tip')  # Now creates an interactive plotly plot
```

### Per-plot backend selection

You can also specify the backend for a specific plot without changing the global setting:

```{python}
#| eval: true
# Use a specific backend for one plot only
df.plot(backend='plotly', kind='scatter', x='total_bill', y='tip')
```

::: {.callout-important}

- The backend must be installed separately (e.g., `pip install plotly` for plotly backend)
- Different backends support different plot types and options
- The `matplotlib` backend is always available and is the default
- Backend settings are session-wide until changed

:::

# Data visualization with `matplotlib` and `seaborn`

Let's show how we can use `matplotlib` and `seaborn` to visualize data contained in a `pandas` dataframe

```{python}
#| 
import matplotlib.pyplot as plt
```

## How do the tip depends on the total bill ?

```{python}
#| 
sns.jointplot(x="total_bill", y="tip", data=df)
```

A `jointplot` (as in seaborn) is an enriched `scatterplot` with histograms on both axes.

## When do customers go to this restaurant ?

```{python}
#| 
#| scrolled: false
sns.countplot(x='day', hue="time", data=df)
```

This is also called a `barplot`. 

## When do customers spend the most ?

```{python}
#| 
plt.figure(figsize=(7, 5))
sns.boxplot(x='day', y='total_bill', hue='time', data=df)
plt.legend(loc="upper left")
```

`boxplot` (box and whiskers plot)  are used to sketch empirical distributions and to display summary statistics (median and quartiles).

```{python}
#| 
plt.figure(figsize=(7, 5))
sns.violinplot(x='day', y='total_bill', hue='time', split=True, data=df)
plt.legend(loc="upper left")
```

`violinplot` are sketchy variants of kernel density estimates. 


## Who spends the most ?

```{python}
#| 
#| scrolled: true
sns.boxplot(x='sex', y='total_bill', hue='smoker', data=df)
```

## When should waiters want to work ?

```{python}
#| 
#| scrolled: true
sns.boxplot(x='day', y='tip', hue='time', data=df)
```

```{python}
#| 
sns.violinplot(x='day', y='tip', hue='time', data=df)
```

# Data processing with `pandas`

Let us read again the `tips.csv` file

```{python}
#| 
#| scrolled: true
import pandas as pd

dtypes = {
    "sex": "category",
    "smoker": "category",
    "day": "category",
    "time": "category"
} 

df = pd.read_csv("tips.csv", dtype=dtypes)
df.head()
```

## Computations using `pandas` : broadcasting

Let's add a column that contains the tip percentage. The content 
of this column is computed by performing elementwise operations between 
elements of two columns.  This works as if the columns were NumPy arrays (even though they are not). 

```{python}
#| 
#| scrolled: false
df["tip_percentage"] = df["tip"] / df["total_bill"]
df.head()
```

The computation

```{{python}}
df["tip"] / df["total_bill"]
```
uses a **broadcast** rule (see [NumPy notebook about broadcasting](/core/notebooks/notebook02-2_numpy.qmd)).

- We can multiply, add, subtract, etc. together `numpy` arrays, `Series` or `pandas` dataframes when the computation **makes sense** in view of their respective **shape**

This principle is called **broadcast** or **broadcasting**.

::: {.callout-note}

Broadcasting is a key feature of `numpy` `ndarray`, see 

- [Numpy User's guide](https://numpy.org/doc/stable/user/basics.broadcasting.html)
- [Pandas book](https://wesmckinney.com/book/advanced-numpy.html#numpy_broadcasting)
- [A toy example with broadcasting](/core/notebooks/notebook02-2_numpy.qmd)

:::

```{python}
#| 
#| scrolled: false
df["tip"].shape, df["total_bill"].shape
```

The `tip` and `total_bill`columns have the same `shape`, so broadcasting performs **pairwise division**.

This corresponds to the following "hand-crafted" approach with a `for` loop:

```{.python}
#| 
for i in range(df.shape[0]):
    df.loc[i, "tip_percentage"] = df.loc[i, "tip"] / df.loc[i, "total_bill"]
```

But using such a loop is: 

- longer to write
- less readable 
- prone to mistakes
- and *slower* :(

*NEVER* use `Python` for-loops unless you need to !

```{python}
#| 
%%timeit -n 10
for i in range(df.shape[0]):
    df.loc[i, "tip_percentage"] = df.loc[i, "tip"] / df.loc[i, "total_bill"]
```

```{python}
#| 
%%timeit -n 10
df["tip_percentage"] = df["tip"] / df["total_bill"]
```

The `for` loop is $\approx$ **100 times slower** ! (even worse on larger data)

### Pitfall. Changing values in a `DataFrame`

When you want to change a value in a `DataFrame`, never use

```{.python}
df["tip_percentage"].loc[i] = 42
```

but use

```{.python}
df.loc[i, "tip_percentage"] = 42
```

::: {.callout-caution}

Use a **single** `loc` or `iloc` statement. The first version **might not work**: it might modify a copy of the column and not the dataframe itself !

:::

Another example of broadcasting is:

```{python}
#| 
(100 * df[["tip_percentage"]]).head()
```

where we multiplied **each entry** of the `tip_percentage` column by 100.

::: {.callout-note}

### Remark 

Note the difference between


```{.python}
df[['tip_percentage']]
```

which returns a `DataFrame` containing only the `tip_percentage` column and

```{.python}
df['tip_percentage']
```

which returns a `Series` containing the data of the `tip_percentage` column

:::


## Some more plots

### How do the tip percentages relates to the total bill ?

```{python}
#| 
sns.jointplot(
    x="total_bill", 
    y="tip_percentage", 
    data=df
)
```

### Who tips best ?

```{python}
#| 
sns.boxplot(
    x='sex', 
    y='tip_percentage', 
    hue='smoker', 
    data=df
)
```

### Who tips best without the `tip_percentage` outliers ?

```{python}
#| 
sns.boxplot(
    x='sex', 
    y='tip_percentage', 
    hue='smoker', 
    data=df.loc[df["tip_percentage"] <= 0.3]
)
```

Object identity

```{python}
#| 
id(df)
```

## The all-mighty `groupby` and `aggregate`

Many computations can be formulated as a **groupby** followed by and **aggregation**.

### What is the mean `tip` and `tip percentage` each day ?

```{python}
#| 
df.head()
```

```{python}
#| 
#| scrolled: true

try:
    (
        df
            .groupby("day", observed=True)
            .mean()
    )
except TypeError:
    print('TypeError: category dtype does not support aggregation "mean"')
```

But we do not care about the `size` column here, so we can use instead

```{python}
(
    df[["total_bill", "tip", "tip_percentage", "day"]]
        .groupby("day")
        .mean()
)
```

If we want to be more precise, we can `groupby` using several columns

```{python}
#| 
(
    df[["total_bill", "tip", "tip_percentage", "day", "time"]]   # selection
        .groupby(["day","time"])                                # partition
        .mean()                                                  # aggregation
)
```

::: {.callout-note}

### Remarks 

- We obtain a `DataFrame` with a two-level indexing: on the `day` and the `time`
- Groups must be homogeneous: we have `NaN` values for empty groups (e.g. `Sat`, `Lunch`)

:::


### Pro tip

Sometimes, it is more convenient to get the groups as columns instead of a multi-level index.

For this, use `reset_index`:

```{python}
#| 
(
    df[["total_bill", "tip", "tip_percentage", "day", "time"]]   # selection
        .groupby(["day", "time"])                                # partition
        .mean() # aggregation
        .reset_index()   # ako ungroup
)
```

### Another pro tip: care about code readers

Computations with pandas can include many operations that are **pipelined** until the final computation.

Pipelining many operations is good practice and perfectly normal, but in order to make the code readable you can put it between parenthesis (`python` expression) as follows:

```{python}
#| 
#| scrolled: true
(
    df[["total_bill", "tip", "tip_percentage", "day", "time"]]
    .groupby(["day", "time"])
    .mean()
    .reset_index()
    # and on top of all this we sort the dataframe with respect 
    # to the tip_percentage
    .sort_values("tip_percentage")
)
```

## Displaying a `DataFrame` with `style`

Now, we can answer, with style, to the question: what are the average tip percentages along the week ?

```{python}
#| 
#| scrolled: true
(
    df[["tip_percentage", "day", "time"]]
    .groupby(["day", "time"])
    .mean()
    # At the end of the pipeline you can use .style
    .style
    # Print numerical values as percentages 
    .format("{:.2%}")
    .background_gradient()
)
```

## Removing the `NaN` values

But the `NaN` values are somewhat annoying. Let's remove them

```{python}
#| 
(
    df[["tip_percentage", "day", "time"]]
    .groupby(["day", "time"])
    .mean()
    # We just add this from the previous pipeline
    .dropna()
    .style
    .format("{:.2%}")
    .background_gradient()
)
```

Now, we see when `tip_percentage` is maximal. But what about the standard deviation?

- We used only `.mean()` for now, but we can use several aggregating function using `.agg()`

```{python}
#| 
#| scrolled: true
(
    df[["tip_percentage", "day", "time"]]
    .groupby(["day", "time"])
    .agg(["mean", "std"])   # we feed `agg`  with a list of names of callables 
    .dropna()
    .style
    .format("{:.2%}")
    .background_gradient()
)
```

And we can use also `.describe()` as aggregation function. Moreover we
- use the `subset` option to specify which column we want to style
- we use `("tip_percentage", "count")` to access multi-level index

```{python}
(
    df[["tip_percentage", "day", "time"]]
    .groupby(["day", "time"])
    .describe()    # all-purpose summarising function
)
```

```{python}
#| 
(
    df[["tip_percentage", "day", "time"]]
    .groupby(["day", "time"])
    .describe()
    .dropna()
    .style
    .bar(subset=[("tip_percentage", "count")])
    .background_gradient(subset=[("tip_percentage", "50%")])
)
```

## Supervised learning of `tip` based on the `total_bill` 

As an example of very simple **machine-learning** problem, let us try to understand how we can predict `tip` based on `total_bill`.

```{python}
#| 
import numpy as np

plt.scatter(df["total_bill"], df["tip"])
plt.xlabel("total_bill", fontsize=12)
plt.ylabel("tip", fontsize=12)
```

There's a rough **linear** dependence between the two. Let us try to find it by hand!<br>
Namely, we look for numbers $b$ and $w$ such that

```
tip ≈ b + w × total_bill
```

for all the examples of pairs of `(tip, total_bill)` we observe in the data.

In **machine learning**, we say that this is a very simple example of a **supervised learning** problem (here it is a regression problem), where `tip` is the **label** and where `total_bill` is the (only) **feature**, for which we intend to use a **linear predictor**.

```{python}
#| 
plt.scatter(df["total_bill"], df["tip"])
plt.xlabel("total_bill", fontsize=12)
plt.ylabel("tip", fontsize=12)

slope = 1.0
intercept = 0.0

x = np.linspace(0, 50, 1000)
plt.plot(x, intercept + slope * x, color="red")
```

### A more interactive way 

This might require


```{python}
#| 
!pip install ipympl
```

```{python}
#| 
#| scrolled: false
#| 
import ipywidgets as widgets
import matplotlib.pyplot as plt
import numpy as np

%matplotlib widget
%matplotlib inline

x = np.linspace(0, 50, 1000)

@widgets.interact(intercept=(-5, 5, 1.), slope=(0, 1, .05))
def update(intercept=0.0, slope=0.5):
    plt.scatter(df["total_bill"], df["tip"])
    plt.plot(x, intercept + slope * x, color="red")
    plt.xlim((0, 50))
    plt.ylim((0, 10))
    plt.xlabel("total_bill", fontsize=12)
    plt.ylabel("tip", fontsize=12)
```

This is kind of tedious to do this by hand... it would be nice to come up with an **automated** way of doing this. Moreover:

- We are using a **linear** function, while something more complicated (such as a polynomial) might be better
- More importantly, we use **only** the `total_bill` column to predict the `tip`, while we know about many other things

```{python}
#| 
df.head()
```

## One-hot encoding of categorical variables

We can't perform computations (products and sums) with columns containing **categorical** variables. So, we can't use them like this to predict the `tip`.
We need to **convert** them to numbers somehow.

The most classical approach for this is **one-hot encoding** (or "create dummies" or "binarize") of the categorical variables, which can be easily achieved with `pandas.get_dummies`

Why *one-hot* ? See [wikipedia](https://en.wikipedia.org/wiki/One-hot) for a plausible explanation

```{python}
#| 
#| scrolled: true
#| 
df_one_hot = pd.get_dummies(df, prefix_sep='#')
df_one_hot.head(5)
```

Only the categorical columns have been one-hot encoded. For instance, the `"day"` column is replaced by 4 columns named `"day#Thur"`, `"day#Fri"`, `"day#Sat"`, `"day#Sun"`, since `"day"` has 4 modalities (see next line).

```{python}
#| 
#| scrolled: true
df['day'].unique()
```

```{python}
#| 
#| scrolled: false
df_one_hot.dtypes
```

## Pitfall. Colinearities with one-hot encoding

Sums over dummies for `sex`, `smoker`, `day`, `time` and `size` are all equal to one (by constrution of the one-hot encoded vectors).

- Leads to **colinearities** in the matrix of features
- It is **much harder** to train a linear regressor when the columns of the features matrix has colinearities

```{python}
#| 
#| scrolled: true
day_cols = [col for col in df_one_hot.columns if col.startswith("day")]
df_one_hot[day_cols].head()
df_one_hot[day_cols].sum(axis=1)
```

```{python}
#| 
all(df_one_hot[day_cols].sum(axis=1) == 1)
```

The most standard solution is to remove a modality (i.e. remove a one-hot encoding vector). Simply achieved by specifying `drop_first=True` in the `get_dummies` function.

```{python}
#| 
df["day"].unique()
```

```{python}
#| 
pd.get_dummies(df, prefix_sep='#', drop_first=True).head()
```

Now, if a categorical feature has $K$ modalities/levels, we use only $K-1$ dummies.
For instance, there is no more `sex#Female` binary column. 

**Question.** So, a linear regression won't fit a weight for `sex#Female`. But, where do the model weights of the dropped binary columns go ?

**Answer.** They just "go" to the **intercept**: interpretation of the population bias depends on the "dropped" one-hot encodings.

So, we actually fit:

$$
\begin{array}{rl} \texttt{tip} \approx b & + w_1 \times \texttt{total\_bill} + w_2 \times \texttt{size} \\ & + w_3 \times \texttt{sex\#Male} + w_4 \times \texttt{smoker\#Yes} \\ & + w_5 \times \texttt{day\#Sat} + w_6 \times \texttt{day\#Sun} + w_7 \times \texttt{day\#Thur} \\ & + w_8 \times \texttt{time\#Lunch} \end{array}
$$

