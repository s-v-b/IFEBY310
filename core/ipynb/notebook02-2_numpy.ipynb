{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9739fb27-b77c-412e-9bf8-d28d9969da43",
   "metadata": {},
   "source": [
    "# Numpy : broadcasting\\`\n",
    "\n",
    "# Centering and scaling a data matrix\n",
    "\n",
    "In the sequel $X$ is a (data) numerical matrix, that is an element of\n",
    "$\\mathbb{R}^{n \\times p}$. The rows of $X$ (the individuals) are the\n",
    "sample points. Each sample point is a tuple of $p$ elements (the\n",
    "so-called variables).\n",
    "\n",
    "The *sample mean* is defined as \n",
    "\n",
    "$$\n",
    "\\overline{X}= \\begin{pmatrix} \\frac{1}{n}\\sum_{i=1}^n X_{i,j}\\end{pmatrix}_{j\\leq p}\n",
    "$$\n",
    "\n",
    "In linear algebra, $\\overline{X}$ is the result of vector matrix\n",
    "multiplication: \n",
    "\n",
    "$$\n",
    "\\overline{X} = \\frac{1}{n}\\begin{pmatrix} 1 & \\ldots & 1\\end{pmatrix} \\times X\n",
    "$$\n",
    "\n",
    "Here, we view $\\overline{X}_n$ as a row vector built from column\n",
    "averages.\n",
    "\n",
    "Centering $X$ consists in subtracting the column average from each\n",
    "matrix element.\n",
    "\n",
    "$$X - \\begin{pmatrix}1 \\\\ \\vdots \\\\ 1\\end{pmatrix} \\times \\overline{X}$$\n",
    "\n",
    "Note that centering consists on projecting the columns of $X$ on the\n",
    "$n-1$ dimensional subspace of $\\mathbb{R}^n$ that is orthogonal to\n",
    "$\\begin{pmatrix} 1 & \\ldots  &  1\\end{pmatrix}^\\top$:\n",
    "\n",
    "Let us call $Y$ the matrix obtained from centering the columns of $X$.\n",
    "\n",
    "Scaling $Y$ consists of dividing each coefficient of $Y$ by $1/\\sqrt{n}$\n",
    "times the (Euclidean) norm of its column.\n",
    "\n",
    "Let us call $\\sigma_j$ the Euclidean norm of column $j$ of $Y$\n",
    "($1\\leq j \\leq p$) divided by $1/\\sqrt{n}$: \n",
    "\n",
    "$$\n",
    "\\sigma_j^2 = \\frac{1}{n}\\sum_{i=1}^n Y_{i,j}^2 = \\frac{1}{n} \\sum_{i=1}^n \\left(X_{i,j} - \\overline{X}_j\\right)^2\n",
    "$$\n",
    "\n",
    "This is also the standard deviation of the $j^{\\text{n}}$ column of $X$.\n",
    "\n",
    "The standardized matrix $Z$ is obtained from the next multiplication \n",
    "\n",
    "$$\n",
    "Z = Y \\times \n",
    "  \\begin{pmatrix} \n",
    "    \\sigma_1 & 0        &  \\ldots      &     & 0 \\\\\n",
    "    0        & \\sigma_2 &   0    &     & \\vdots \\\\\n",
    "    \\vdots   & 0         & \\ddots &     & \\vdots  \\\\\n",
    "    0        & \\ldots   &        &     & \\sigma_d       \n",
    "  \\end{pmatrix}^{-1}\n",
    "$$\n",
    "\n",
    "Note that for $i\\leq n$, $j \\leq p$:\n",
    "\n",
    "$$Z_{i,j} = \\frac{X_{i,j} - \\overline{X}_j}{\\sigma_j}$$\n",
    "\n",
    "$Z$ is called the $z$-score matrix. It shows up in many statistical\n",
    "analyses.\n",
    "\n",
    "In the statistical computing environment `R`, a function called\n",
    "`scale()` computes the $z$-score matrix. On may ask whether NumPy offers\n",
    "such a function.\n",
    "\n",
    "# Scaling in NumPy (standardization)\n",
    "\n",
    "In NumPy, there is no single function equivalent to R’s `scale()`\n",
    "function. However, you can achieve the same result using *broadcasting*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72455962",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "056dc228-a7aa-468d-845e-6f5b74d05c80",
   "metadata": {},
   "source": [
    "Let us first generate a toy data matrix with random (Gaussian)\n",
    "coefficients. This is an opportunity to introduce `np.random`.\n",
    "\n",
    "We will work with $n=5$ and $p=3$.\n",
    "\n",
    "We first build a (positive definite) covariance matrix. We ensure\n",
    "positive definiteness starting from the Cholesky factorization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b6a4204",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "L = np.array([  \n",
    "  [1., 0., 0.], \n",
    "  [.5, 1., 0.], \n",
    "  [.5, .5, 1.]])\n",
    "C = L @ L.transpose()\n",
    "# L is the Cholesky factor of C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9da696d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a sample of 5 independent normal vectors with mean (1, 2, 3) and covariance C\n",
    "mu = np.array([1, 2, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60a53a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.random.randn(5,3) @ L.transpose() + mu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8244dbd-cf94-4832-a3fa-d39049318b26",
   "metadata": {},
   "source": [
    "In NumPy, function `mean` with well chosen optional `axis` argument\n",
    "returns a 1D array filled with column averages\n",
    "\n",
    "> **Note**\n",
    ">\n",
    "> We just did something strange: we added a matrix with shape $(5,3)$\n",
    "> and a vector with length $3$. In linear algebra, this is not\n",
    "> legitimate. We just used the device called *broadcasting*. See below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "321f96b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute column averages\n",
    "# That is compute arithmetic mean along axis `0`\n",
    "emp_mean = np.mean(X, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2004e9c-c885-4ac7-b20a-8cd5a4a97791",
   "metadata": {},
   "source": [
    "We can magically center the columns using broadcasting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a26a8c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_centered = X - emp_mean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3f1bce4-8fe9-43e4-b221-16f9acc2e7b0",
   "metadata": {},
   "source": [
    "If broadcasting were note possible, we could still achieve the result by\n",
    "resorting to NumPy implementation of matrix multiplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f78f426",
   "metadata": {},
   "outputs": [],
   "source": [
    "X - np.ones((5,1), dtype=np.float16) @ emp_mean.reshape(1,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7edf50f-7195-46a6-a5b7-62f5a4738378",
   "metadata": {},
   "source": [
    "is a centered version of `X`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d9ab302",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_centered - (X - np.ones((5,1), dtype=np.float16) @ emp_mean.reshape(1,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03e86599-6b0a-4ebf-87d4-148cadf49ca5",
   "metadata": {},
   "source": [
    "We compute now the column standard deviations using function `np.std`\n",
    "with `axis` argument set to `0`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30d78950",
   "metadata": {},
   "outputs": [],
   "source": [
    "emp_std = np.std(X, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dca2a61e-ab29-4ead-a42e-896b5ed1db0a",
   "metadata": {},
   "source": [
    "The $z$-score matrix is obtained using another broadcasting operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "839a7817",
   "metadata": {},
   "outputs": [],
   "source": [
    "Z = (X - emp_mean) / emp_std  # X_centered / emp_std"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d88dcb58-6856-403f-9b51-2da83777ab44",
   "metadata": {},
   "source": [
    "Finally, let us perform the sanity checks :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1117a29a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "np.mean(Z, axis=0)  # Z is column centered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81248b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.std(Z, axis=0)   # Z is standardized"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5ca0692-d800-4afd-beec-7e49b4573e29",
   "metadata": {},
   "source": [
    "> **Note**\n",
    ">\n",
    "> Alternatively, we can use `scipy.stats.zscore` which provides R’s\n",
    "> `scale()`-like functionality:\n",
    ">\n",
    "> ``` python\n",
    "> from scipy.stats import zscore\n",
    "> X_scaled_scipy = zscore(X, axis=0)\n",
    "> X_scaled_scipy\n",
    "> ```\n",
    ">\n",
    "> `scipy.stats.zscore` centers and scales the data by default\n",
    "> (equivalent to R’s `scale()` with default arguments).\n",
    ">\n",
    "> Centering and standardization are classical preprocessing steps before\n",
    "> Principal Component Analysis (and before many Machine Learning\n",
    "> procedures).\n",
    "\n",
    "# How does broadcasting work ?\n",
    "\n",
    "## Why broadcasting ? (from the documentation)\n",
    "\n",
    "> The term broadcasting describes how NumPy treats arrays with different\n",
    "> shapes during arithmetic operations. Subject to certain constraints,\n",
    "> the smaller array is “broadcast” across the larger array so that they\n",
    "> have compatible shapes. Broadcasting provides a means of vectorizing\n",
    "> array operations so that looping occurs in C instead of Python. It\n",
    "> does this without making needless copies of data and usually leads to\n",
    "> efficient algorithm implementations. There are, however, cases where\n",
    "> broadcasting is a bad idea because it leads to inefficient use of\n",
    "> memory that slows computation.\n",
    "\n",
    "## How ?\n",
    "\n",
    "> When operating on two arrays, NumPy compares their shapes\n",
    "> element-wise. It starts with the trailing (i.e. rightmost) dimension\n",
    "> and works its way left. Two dimensions are compatible when\n",
    ">\n",
    "> -   they are equal, or\n",
    "> -   one of them is 1.\n",
    "\n",
    "In our setting the shape of `X` and `emp_mean` are `(5,3)` and `(3)`.\n",
    "The rightmost dimensions are equal, hence compatible.\n",
    "\n",
    "> Input arrays do not need to have the same number of dimensions. The\n",
    "> resulting array will have the same number of dimensions as the input\n",
    "> array with the greatest number of dimensions, where the size of each\n",
    "> dimension is the largest size of the corresponding dimension among the\n",
    "> input arrays. Note that missing dimensions are assumed to have size\n",
    "> one.\n",
    "\n",
    "In our setting, `emp_mean` is (virtually) reshaped to `(1,3)` and the\n",
    "two arrays are fully compatible. To match the leading dimension of `X`,\n",
    "we can stack three copies of reshaped `emp_mean`, this is just like\n",
    "multiplying `emp_mean` by `np.array([1.], shape=(3,1))`.\n",
    "\n",
    "> When either of the dimensions compared is one, the other is used. In\n",
    "> other words, dimensions with size 1 are stretched or “copied” to match\n",
    "> the other.\n",
    "\n",
    "# References\n",
    "\n",
    "[Official Numpy\n",
    "documentation](https://numpy.org/devdocs/user/basics.broadcasting.html#basics-broadcasting)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
