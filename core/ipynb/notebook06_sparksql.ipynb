{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# `DataFrame`"
      ],
      "id": "2100034c-9fde-44ee-a590-a5c44fe0e0b8"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "\n",
        "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
        "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable"
      ],
      "id": "d8066114"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-08T20:23:05.867529Z",
          "start_time": "2022-02-08T20:23:01.418071Z"
        }
      },
      "outputs": [],
      "source": [
        "from pyspark import SparkConf, SparkContext\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "conf = SparkConf().setAppName(\"Spark SQL Course\")\n",
        "sc = SparkContext(conf=conf)  # no need for Spark 3...\n",
        "\n",
        "spark = (SparkSession\n",
        "    .builder\n",
        "    .appName(\"Spark SQL Course\")\n",
        "    .getOrCreate()\n",
        ")"
      ],
      "id": "f50c0ae3"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-01-26T10:58:46.926825Z",
          "start_time": "2022-01-26T10:58:46.920913Z"
        }
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import Row\n",
        "\n",
        "row1 = Row(name=\"John\", age=21)\n",
        "row2 = Row(name=\"James\", age=32)\n",
        "row3 = Row(name=\"Jane\", age=18)\n",
        "row1['name']"
      ],
      "id": "a1e760f2"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-01-26T10:58:57.185741Z",
          "start_time": "2022-01-26T10:58:57.155181Z"
        }
      },
      "outputs": [],
      "source": [
        "df = spark.createDataFrame([row1, row2, row3])"
      ],
      "id": "b83e5d96"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-01-26T10:59:13.491438Z",
          "start_time": "2022-01-26T10:59:13.486119Z"
        }
      },
      "outputs": [],
      "source": [
        "df.printSchema()"
      ],
      "id": "796f8978"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-01-26T10:59:17.760344Z",
          "start_time": "2022-01-26T10:59:17.597166Z"
        }
      },
      "outputs": [],
      "source": [
        "df.show()"
      ],
      "id": "48490060"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-01-26T10:59:25.889372Z",
          "start_time": "2022-01-26T10:59:25.866666Z"
        }
      },
      "outputs": [],
      "source": [
        "print(df.rdd.toDebugString().decode(\"utf-8\"))"
      ],
      "id": "c4258aeb"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-01-26T10:59:45.432264Z",
          "start_time": "2022-01-26T10:59:45.426727Z"
        }
      },
      "outputs": [],
      "source": [
        "df.rdd.getNumPartitions()"
      ],
      "id": "9ca3be4a"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Creating dataframes"
      ],
      "id": "c92ecceb-c783-47eb-b198-6b71419929cc"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-01-26T10:58:18.707591Z",
          "start_time": "2022-01-26T10:58:18.220608Z"
        }
      },
      "outputs": [],
      "source": [
        "rows = [\n",
        "    Row(name=\"John\", age=21, gender=\"male\"),\n",
        "    Row(name=\"James\", age=25, gender=\"female\"),\n",
        "    Row(name=\"Albert\", age=46, gender=\"male\")\n",
        "]\n",
        "\n",
        "df = spark.createDataFrame(rows)"
      ],
      "id": "f92d6e3e"
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-01-26T10:58:18.707591Z",
          "start_time": "2022-01-26T10:58:18.220608Z"
        }
      },
      "outputs": [],
      "source": [
        "df.show()"
      ],
      "id": "059dfb8e"
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "help(Row)"
      ],
      "id": "4bf64a09"
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-01-26T10:58:19.065539Z",
          "start_time": "2022-01-26T10:58:18.710711Z"
        }
      },
      "outputs": [],
      "source": [
        "column_names = [\"name\", \"age\", \"gender\"]\n",
        "rows = [\n",
        "    [\"John\", 21, \"male\"],\n",
        "    [\"James\", 25, \"female\"],\n",
        "    [\"Albert\", 46, \"male\"]\n",
        "]\n",
        "\n",
        "df = spark.createDataFrame(\n",
        "    rows, \n",
        "    column_names\n",
        ")\n",
        "\n",
        "df.show()"
      ],
      "id": "d4fda476"
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-01-26T10:58:19.074335Z",
          "start_time": "2022-01-26T10:58:19.068088Z"
        }
      },
      "outputs": [],
      "source": [
        "df.printSchema()"
      ],
      "id": "8ff919fe"
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-01-26T10:58:19.840178Z",
          "start_time": "2022-01-26T10:58:19.077057Z"
        }
      },
      "outputs": [],
      "source": [
        "# sc = SparkContext(conf=conf)  # no need for Spark 3...\n",
        "\n",
        "column_names = [\"name\", \"age\", \"gender\"]\n",
        "rdd = sc.parallelize([\n",
        "    (\"John\", 21, \"male\"),\n",
        "    (\"James\", 25, \"female\"),\n",
        "    (\"Albert\", 46, \"male\")\n",
        "])\n",
        "df = spark.createDataFrame(rdd, column_names)\n",
        "df.show()"
      ],
      "id": "9d5b4e9f"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Schema\n",
        "\n",
        "There is special type schemata. A object of class `StructType` is made\n",
        "of a list of objects of type `StructField`."
      ],
      "id": "80998f31-58d5-4e03-bc14-7f9d4f727988"
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-01-26T10:58:19.850578Z",
          "start_time": "2022-01-26T10:58:19.843835Z"
        }
      },
      "outputs": [],
      "source": [
        "df.schema"
      ],
      "id": "c0853036"
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-01-26T10:58:19.860631Z",
          "start_time": "2022-01-26T10:58:19.854012Z"
        }
      },
      "outputs": [],
      "source": [
        "type(df.schema)"
      ],
      "id": "cb1c81f8"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "A object of type `StructField` has a name, a PySpark type, an d a\n",
        "boolean parameter."
      ],
      "id": "624c4b7c-bb30-4e5f-9f25-4a99c3d5b938"
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-01-26T10:58:20.199419Z",
          "start_time": "2022-01-26T10:58:19.863528Z"
        }
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.types import *\n",
        "\n",
        "schema = StructType(\n",
        "    [\n",
        "        StructField(\"name\", StringType(), True),\n",
        "        StructField(\"age\", IntegerType(), True),\n",
        "        StructField(\"gender\", StringType(), True)\n",
        "    ]\n",
        ")\n",
        "\n",
        "rows = [(\"John\", 21, \"male\")]\n",
        "df = spark.createDataFrame(rows, schema)\n",
        "df.printSchema()\n",
        "df.show()"
      ],
      "id": "60a93b96"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Queries (single table $σ$, $π$)\n",
        "\n",
        "PySpark offers two ways to query a datafrane:\n",
        "\n",
        "-   An ad hod API with methods for the DataFrame class.\n",
        "-   The possibility to post SQL queries (provided a temporary view has\n",
        "    been created)."
      ],
      "id": "0293a22b-5cd9-43e9-8aa1-f22ccc06e3bd"
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-01-26T10:58:20.882311Z",
          "start_time": "2022-01-26T10:58:20.201993Z"
        }
      },
      "outputs": [],
      "source": [
        "column_names = [\"name\", \"age\", \"gender\"]\n",
        "rows = [\n",
        "    [\"John\", 21, \"male\"],\n",
        "    [\"Jane\", 25, \"female\"]\n",
        "]\n",
        "# \n",
        "df = spark.createDataFrame(rows, column_names)\n",
        "\n",
        "# Create a temporary view from the DataFrame\n",
        "df.createOrReplaceTempView(\"new_view\")\n",
        "\n",
        "# Apply the query\n",
        "query = \"\"\"\n",
        "    SELECT \n",
        "        name, age \n",
        "    FROM \n",
        "        new_view \n",
        "    WHERE \n",
        "        gender='male'\n",
        "\"\"\"\n",
        "\n",
        "men_df = spark.sql(query)\n",
        "men_df.show()"
      ],
      "id": "2225d878"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## `SELECT` (projection $π$)"
      ],
      "id": "a7b31a17-9de2-475c-9666-07be5d0e8b74"
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-01-26T10:58:21.162623Z",
          "start_time": "2022-01-26T10:58:20.884802Z"
        }
      },
      "outputs": [],
      "source": [
        "df.createOrReplaceTempView(\"table\")    \n",
        "\n",
        "query = \"\"\"\n",
        "    SELECT \n",
        "        name, age \n",
        "    FROM \n",
        "        table\n",
        "\"\"\"\n",
        "\n",
        "spark.sql(query).show()"
      ],
      "id": "a289080c"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Using the API:"
      ],
      "id": "c7bb41a7-82d2-4a62-aa73-915e49fe3df6"
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-01-26T10:58:21.388097Z",
          "start_time": "2022-01-26T10:58:21.164840Z"
        }
      },
      "outputs": [],
      "source": [
        "(\n",
        "    df\n",
        "        .select(\"name\", \"age\")\n",
        "        .show()\n",
        ")"
      ],
      "id": "eafeb2d9"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "`π(df, \"name\", \"age\")`\n",
        "\n",
        "## `WHERE` (filter, selection, $σ$)"
      ],
      "id": "8e6c3252-c879-486c-be01-09575258f6ed"
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-01-26T10:58:21.704402Z",
          "start_time": "2022-01-26T10:58:21.402155Z"
        }
      },
      "outputs": [],
      "source": [
        "df.createOrReplaceTempView(\"table\")\n",
        "\n",
        "query = \"\"\"\n",
        "    SELECT \n",
        "        * \n",
        "    FROM \n",
        "        table\n",
        "    WHERE \n",
        "        age > 21\n",
        "\"\"\"\n",
        "\n",
        "spark.sql(query).show()"
      ],
      "id": "27990e8c"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Using the API"
      ],
      "id": "5fcfd201-13cf-4d41-87d2-ae378a1e1686"
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-01-26T10:58:21.924501Z",
          "start_time": "2022-01-26T10:58:21.706741Z"
        }
      },
      "outputs": [],
      "source": [
        "( \n",
        "    df\n",
        "        .where(\"age > 21\")\n",
        "        .show()\n",
        ")"
      ],
      "id": "672c6d18"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This implements `σ(df, \"age > 21\")`"
      ],
      "id": "b1f904f2-dcfb-46f0-9e0a-670907b071c9"
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-01-26T10:58:22.377417Z",
          "start_time": "2022-01-26T10:58:21.926708Z"
        }
      },
      "outputs": [],
      "source": [
        "# Alternatively:\n",
        "( \n",
        "    df\n",
        "      .where(df['age'] > 21)\n",
        "      .show()\n",
        ")"
      ],
      "id": "1ab4f3be"
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-01-26T10:58:22.566385Z",
          "start_time": "2022-01-26T10:58:22.380036Z"
        }
      },
      "outputs": [],
      "source": [
        "( \n",
        "    df\n",
        "      .where(df.age > 21)\n",
        "      .show()\n",
        ")"
      ],
      "id": "015a3091"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Method chaining allows to construct complex queries"
      ],
      "id": "7e0f541a-ba66-4705-883f-a0633da00245"
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-01-26T10:58:22.837136Z",
          "start_time": "2022-01-26T10:58:22.569324Z"
        }
      },
      "outputs": [],
      "source": [
        "( \n",
        "    df\n",
        "      .where(\"age > 21\")\n",
        "      .select([\"name\", \"age\"])\n",
        "      .show()\n",
        ")"
      ],
      "id": "5e0e9dfc"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This implements\n",
        "\n",
        "        σ(df, \"age > 21\") |>\n",
        "        π([\"name\", \"age\"])\n",
        "\n",
        "## `LIMIT`"
      ],
      "id": "dfd1fb06-a0ba-4386-bf4a-c5576a42fec0"
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-01-26T10:58:23.315363Z",
          "start_time": "2022-01-26T10:58:22.842106Z"
        }
      },
      "outputs": [],
      "source": [
        "df.createOrReplaceTempView(\"table\")\n",
        "\n",
        "query = \"\"\"\n",
        "    SELECT \n",
        "        * \n",
        "    FROM \n",
        "        table \n",
        "    LIMIT 1\n",
        "\"\"\"\n",
        "\n",
        "spark.sql(query).show()"
      ],
      "id": "10fb91b9"
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-01-26T10:58:23.522646Z",
          "start_time": "2022-01-26T10:58:23.318694Z"
        }
      },
      "outputs": [],
      "source": [
        "df.limit(1).show()"
      ],
      "id": "74247fed"
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-01-26T10:58:23.778517Z",
          "start_time": "2022-01-26T10:58:23.525281Z"
        }
      },
      "outputs": [],
      "source": [
        "df.select(\"*\").limit(1).show()"
      ],
      "id": "e4501567"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## `ORDER BY`"
      ],
      "id": "acb43af1-29ec-4328-a770-1cff6d969011"
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-01-26T10:58:24.190838Z",
          "start_time": "2022-01-26T10:58:23.781166Z"
        }
      },
      "outputs": [],
      "source": [
        "df.createOrReplaceTempView(\"table\")\n",
        "\n",
        "query = \"\"\"\n",
        "    SELECT \n",
        "        * \n",
        "    FROM \n",
        "        table\n",
        "    ORDER BY \n",
        "        name ASC\n",
        "\"\"\"\n",
        "\n",
        "spark.sql(query).show()"
      ],
      "id": "89a04990"
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-01-26T10:58:24.368069Z",
          "start_time": "2022-01-26T10:58:24.193899Z"
        }
      },
      "outputs": [],
      "source": [
        "df.orderBy(df.name.asc()).show()"
      ],
      "id": "156278c2"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## `ALIAS` (rename)"
      ],
      "id": "f3a1dde9-9145-4063-bac0-98c9c81ce63d"
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-01-26T10:58:24.643668Z",
          "start_time": "2022-01-26T10:58:24.370758Z"
        }
      },
      "outputs": [],
      "source": [
        "df.createOrReplaceTempView(\"table\")\n",
        "query = \"SELECT name, age, gender AS sex FROM table\"\n",
        "spark.sql(query).show()"
      ],
      "id": "6d73d18c"
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [],
      "source": [
        "type(df.age)"
      ],
      "id": "9857b7fd"
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-01-26T10:58:24.858104Z",
          "start_time": "2022-01-26T10:58:24.646119Z"
        }
      },
      "outputs": [],
      "source": [
        "df.select(df.name, df.age, df.gender.alias('sex')).show()"
      ],
      "id": "f9e1120a"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## `CAST`"
      ],
      "id": "e3bd62c5-49a0-4183-9859-5e5acd52c524"
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-01-26T10:58:25.072286Z",
          "start_time": "2022-01-26T10:58:24.860474Z"
        }
      },
      "outputs": [],
      "source": [
        "df.createOrReplaceTempView(\"table\")\n",
        "query = \"SELECT name, cast(age AS float) AS age_f FROM table\"\n",
        "spark.sql(query).show()"
      ],
      "id": "443af1f3"
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-01-26T10:58:25.384433Z",
          "start_time": "2022-01-26T10:58:25.074523Z"
        }
      },
      "outputs": [],
      "source": [
        "df.select(df.name, df.age.cast(\"float\").alias(\"age_f\")).show()"
      ],
      "id": "f62d388b"
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-01-26T10:58:25.648155Z",
          "start_time": "2022-01-26T10:58:25.386952Z"
        }
      },
      "outputs": [],
      "source": [
        "new_age_col = df.age.cast(\"float\").alias(\"age_f\")\n",
        "type(new_age_col), type(df.age)"
      ],
      "id": "a75570c3"
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [],
      "source": [
        "df.select(df.name, new_age_col).show()"
      ],
      "id": "cac7536c"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Adding new columns"
      ],
      "id": "7d314410-35c5-4aba-8194-de84213ac800"
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-01-26T10:58:25.931495Z",
          "start_time": "2022-01-26T10:58:25.651283Z"
        }
      },
      "outputs": [],
      "source": [
        "df.createOrReplaceTempView(\"table\")\n",
        "\n",
        "query = \"\"\"\n",
        "    SELECT \n",
        "        *, \n",
        "        12*age AS age_months \n",
        "    FROM \n",
        "        table\n",
        "\"\"\"\n",
        "\n",
        "spark.sql(query).show()"
      ],
      "id": "e27fbe1c"
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-01-26T10:58:26.195480Z",
          "start_time": "2022-01-26T10:58:25.933620Z"
        }
      },
      "outputs": [],
      "source": [
        "( \n",
        "    df\n",
        "        .withColumn(\"age_months\", df.age * 12)\n",
        "        .show()\n",
        ")"
      ],
      "id": "65a90907"
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-01-26T10:58:26.422122Z",
          "start_time": "2022-01-26T10:58:26.197759Z"
        }
      },
      "outputs": [],
      "source": [
        "(\n",
        "    df\n",
        "        .select(\"*\", \n",
        "                (df.age * 12).alias(\"age_months\"))\n",
        "        .show()\n",
        ")"
      ],
      "id": "2718a303"
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [],
      "source": [
        "import datetime\n",
        "\n",
        "hui = datetime.date.today()\n",
        "\n",
        "hui = hui.replace(year=hui.year-21)\n",
        "\n",
        "str(hui)"
      ],
      "id": "7f8c38d6"
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [],
      "source": [
        "# df.select(\"*\", hui.replace(year=hui.year - df.age ).alias(\"yob\")).show()"
      ],
      "id": "7acdeca1"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Column functions\n",
        "\n",
        "## Numeric functions examples"
      ],
      "id": "8ef2ef9f-8d28-415a-a9b1-bcb23b9880ef"
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-01-26T10:58:26.748718Z",
          "start_time": "2022-01-26T10:58:26.425451Z"
        }
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import functions as fn\n",
        "\n",
        "columns = [\"brand\", \"cost\"]\n",
        "df = spark.createDataFrame([\n",
        "    (\"garnier\", 3.49),\n",
        "    (\"elseve\", 2.71)\n",
        "], columns)\n",
        "\n",
        "round_cost = fn.round(df.cost, 1)\n",
        "floor_cost = fn.floor(df.cost)\n",
        "ceil_cost = fn.ceil(df.cost)\n",
        "\n",
        "df.withColumn('round', round_cost)\\\n",
        "    .withColumn('floor', floor_cost)\\\n",
        "    .withColumn('ceil', ceil_cost)\\\n",
        "    .show()"
      ],
      "id": "9dd49744"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## String functions examples"
      ],
      "id": "3a083f56-6496-4cd5-bbf9-cf92d835cb4a"
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-01-26T10:58:27.055563Z",
          "start_time": "2022-01-26T10:58:26.751235Z"
        }
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import functions as fn\n",
        "\n",
        "columns = [\"first_name\", \"last_name\"]\n",
        "\n",
        "df = spark.createDataFrame([\n",
        "    (\"John\", \"Doe\"),\n",
        "    (\"Mary\", \"Jane\")\n",
        "], columns)\n",
        "\n",
        "last_name_initial = fn.substring(df.last_name, 0, 1)\n",
        "# last_name_initial_dotted = fn.concat(last_name_initial, \".\")\n",
        "\n",
        "name = fn.concat_ws(\" \", df.first_name, last_name_initial)\n",
        "df.withColumn(\"name\", name).show()"
      ],
      "id": "665208c8"
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {},
      "outputs": [],
      "source": [
        "( \n",
        "    df.selectExpr(\"*\", \"substring(last_name, 0, 1) as lni\")\n",
        "      .selectExpr(\"first_name\", \"last_name\", \"concat(first_name, ' ', lni, '.') as nname\")\n",
        "      .show()\n",
        ")"
      ],
      "id": "5eac1a4d"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Date functions examples"
      ],
      "id": "d04e148e-0f0b-4a9a-aae4-9c90729570de"
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-01-26T10:58:27.373396Z",
          "start_time": "2022-01-26T10:58:27.057938Z"
        }
      },
      "outputs": [],
      "source": [
        "from datetime import date\n",
        "from pyspark.sql import functions as fn\n",
        "\n",
        "df = spark.createDataFrame([\n",
        "    (date(2015, 1, 1), date(2015, 1, 15)),\n",
        "    (date(2015, 2, 21), date(2015, 3, 8)),\n",
        "], [\"start_date\", \"end_date\"])\n",
        "\n",
        "days_between = fn.datediff(df.end_date, df.start_date)\n",
        "start_month = fn.month(df.start_date)\n",
        "\n",
        "df.withColumn('days_between', days_between)\\\n",
        "    .withColumn('start_month', start_month)\\\n",
        "    .show()"
      ],
      "id": "9d04759b"
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {},
      "outputs": [],
      "source": [
        "str(date(2015, 1, 1) - date(2015, 1, 15))"
      ],
      "id": "0a1e8fbc"
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {},
      "outputs": [],
      "source": [
        "from datetime import timedelta\n",
        "\n",
        "date(2023, 2 , 14) + timedelta(days=3)"
      ],
      "id": "63ec6e15"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Conditional transformations"
      ],
      "id": "562270ad-da95-45b0-9ae6-a0b12fb877f8"
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-01-26T10:58:27.630822Z",
          "start_time": "2022-01-26T10:58:27.375855Z"
        }
      },
      "outputs": [],
      "source": [
        "df = spark.createDataFrame([\n",
        "    (\"John\", 21, \"male\"),\n",
        "    (\"Jane\", 25, \"female\"),\n",
        "    (\"Albert\", 46, \"male\"),\n",
        "    (\"Brad\", 49, \"super-hero\")\n",
        "], [\"name\", \"age\", \"gender\"])"
      ],
      "id": "41870f11"
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-01-26T10:58:27.630822Z",
          "start_time": "2022-01-26T10:58:27.375855Z"
        }
      },
      "outputs": [],
      "source": [
        "supervisor = ( \n",
        "    fn.when(df.gender == 'male', 'Mr. Smith')\n",
        "      .when(df.gender == 'female', 'Miss Jones')\n",
        "      .otherwise('NA')\n",
        ")\n",
        "\n",
        "type(supervisor), type(fn.when)"
      ],
      "id": "26f52925"
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-01-26T10:58:27.630822Z",
          "start_time": "2022-01-26T10:58:27.375855Z"
        }
      },
      "outputs": [],
      "source": [
        "df.withColumn(\"supervisor\", supervisor).show()"
      ],
      "id": "d50c1af9"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## User-defined functions"
      ],
      "id": "c917f535-42dc-419a-8986-72e239679257"
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-01-26T10:58:28.037428Z",
          "start_time": "2022-01-26T10:58:27.633093Z"
        }
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import functions as fn\n",
        "from pyspark.sql.types import StringType\n",
        "\n",
        "df = spark.createDataFrame([(1, 3), (4, 2)], [\"first\", \"second\"])\n",
        "\n",
        "def my_func(col_1, col_2):\n",
        "    if (col_1 > col_2):\n",
        "        return \"{} is bigger than {}\".format(col_1, col_2)\n",
        "    else:\n",
        "        return \"{} is bigger than {}\".format(col_2, col_1)\n",
        "\n",
        "my_udf = fn.udf(my_func, StringType())\n",
        "\n",
        "df.withColumn(\"udf\", my_udf(df['first'], df['second'])).show()"
      ],
      "id": "68538796"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Joins ($⋈$)\n",
        "\n",
        "## Using the `spark.sql` API"
      ],
      "id": "cc1da690-49e8-4681-83f5-5c036621ec97"
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-01-26T10:58:29.098957Z",
          "start_time": "2022-01-26T10:58:28.042691Z"
        }
      },
      "outputs": [],
      "source": [
        "from datetime import date\n",
        "\n",
        "products = spark.createDataFrame([\n",
        "    ('1', 'mouse', 'microsoft', 39.99),\n",
        "    ('2', 'keyboard', 'logitech', 59.99),\n",
        "], ['prod_id', 'prod_cat', 'prod_brand', 'prod_value'])\n",
        "\n",
        "purchases = spark.createDataFrame([\n",
        "    (date(2017, 11, 1), 2, '1'),\n",
        "    (date(2017, 11, 2), 1, '1'),\n",
        "    (date(2017, 11, 5), 1, '2'),\n",
        "], ['date', 'quantity', 'prod_id'])\n",
        "\n",
        "# The default join type is the \"INNER\" join\n",
        "purchases.join(products, 'prod_id').show()"
      ],
      "id": "6917902a"
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {},
      "outputs": [],
      "source": [
        "purchases.join(products, 'prod_id').explain()"
      ],
      "id": "414e22ce"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Using a `SQL` query"
      ],
      "id": "1ee7f902-34b7-4512-bc6b-f86657db09bc"
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-01-26T10:58:29.731271Z",
          "start_time": "2022-01-26T10:58:29.101559Z"
        }
      },
      "outputs": [],
      "source": [
        "products.createOrReplaceTempView(\"products\")\n",
        "purchases.createOrReplaceTempView(\"purchases\")\n",
        "\n",
        "query = \"\"\"\n",
        "    SELECT * \n",
        "    FROM purchases AS prc INNER JOIN \n",
        "        products AS prd \n",
        "    ON prc.prod_id = prd.prod_id\n",
        "\"\"\"\n",
        "spark.sql(query).show()"
      ],
      "id": "fb954f9b"
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {},
      "outputs": [],
      "source": [
        "spark.sql(query).explain()"
      ],
      "id": "6a4381a7"
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-01-26T10:58:30.660419Z",
          "start_time": "2022-01-26T10:58:29.734282Z"
        }
      },
      "outputs": [],
      "source": [
        "new_purchases = spark.createDataFrame([\n",
        "    (date(2017, 11, 1), 2, '1'),\n",
        "    (date(2017, 11, 2), 1, '3'),\n",
        "], ['date', 'quantity', 'prod_id_x'])\n",
        "\n",
        "# The default join type is the \"INNER\" join\n",
        "join_rule = new_purchases.prod_id_x == products.prod_id\n",
        "\n",
        "print(type(join_rule))\n",
        "\n",
        "new_purchases.join(products, join_rule, 'left').show()"
      ],
      "id": "9448092e"
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {},
      "outputs": [],
      "source": [
        "join_rule.info"
      ],
      "id": "8377407e"
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-01-26T10:58:31.319336Z",
          "start_time": "2022-01-26T10:58:30.663809Z"
        }
      },
      "outputs": [],
      "source": [
        "new_purchases = spark.createDataFrame([\n",
        "    (date(2017, 11, 1), 2, '1'),\n",
        "    (date(2017, 11, 2), 1, '3'),\n",
        "], ['date', 'quantity', 'prod_id_x'])\n",
        "\n",
        "# The default join type is the \"INNER\" join\n",
        "join_rule = new_purchases.prod_id_x == products.prod_id\n",
        "\n",
        "new_purchases.join(products, join_rule, 'left').show()"
      ],
      "id": "06db0f4f"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Various types of joins"
      ],
      "id": "c7443533-fd12-43ac-bd92-d22e35d1face"
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-01-26T10:58:31.376310Z",
          "start_time": "2022-01-26T10:58:31.323600Z"
        }
      },
      "outputs": [],
      "source": [
        "left = spark.createDataFrame([\n",
        "    (1, \"A1\"), (2, \"A2\"), (3, \"A3\"), (4, \"A4\")], \n",
        "    [\"id\", \"value\"])\n",
        "\n",
        "right = spark.createDataFrame([\n",
        "    (3, \"A3\"), (4, \"A4\"), (4, \"A4_1\"), (5, \"A5\"), (6, \"A6\")], \n",
        "    [\"id\", \"value\"])\n",
        "\n",
        "join_types = [\n",
        "    \"inner\", \"outer\", \"left\", \"right\",\n",
        "    \"leftsemi\", \"leftanti\"\n",
        "]"
      ],
      "id": "e8148789"
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-01-26T10:58:34.708236Z",
          "start_time": "2022-01-26T10:58:31.380091Z"
        }
      },
      "outputs": [],
      "source": [
        "for join_type in join_types:\n",
        "    print(join_type)\n",
        "    left.join(right, on=\"id\", how=join_type)\\\n",
        "        .orderBy(\"id\")\\\n",
        "        .show()"
      ],
      "id": "16562523"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Agregations (summarize)\n",
        "\n",
        "## Examples using the API"
      ],
      "id": "bdcb3d3a-65cd-4132-b969-512c7e838afb"
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-01-26T10:58:35.398306Z",
          "start_time": "2022-01-26T10:58:34.710552Z"
        }
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import functions as fn\n",
        "\n",
        "products = spark.createDataFrame([\n",
        "    ('1', 'mouse', 'microsoft', 39.99),\n",
        "    ('2', 'mouse', 'microsoft', 59.99),\n",
        "    ('3', 'keyboard', 'microsoft', 59.99),\n",
        "    ('4', 'keyboard', 'logitech', 59.99),\n",
        "    ('5', 'mouse', 'logitech', 29.99),\n",
        "], ['prod_id', 'prod_cat', 'prod_brand', 'prod_value'])\n",
        "\n",
        "( \n",
        "    products\n",
        "        .groupBy('prod_cat')\n",
        "        .avg('prod_value')\n",
        "        .show()\n",
        ")"
      ],
      "id": "fc80a8b4"
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-01-26T10:58:35.782623Z",
          "start_time": "2022-01-26T10:58:35.400724Z"
        }
      },
      "outputs": [],
      "source": [
        "(\n",
        "    products\n",
        "        .groupBy('prod_cat')\n",
        "        .agg(fn.avg('prod_value'))\n",
        "        .show()\n",
        ")"
      ],
      "id": "f6e7f905"
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {},
      "outputs": [],
      "source": [
        "(\n",
        "    products\n",
        "        .groupBy('prod_cat')\n",
        "        .agg(\n",
        "            fn.mean('prod_value'), \n",
        "            fn.stddev('prod_value')\n",
        "        )\n",
        "        .show()\n",
        ")"
      ],
      "id": "6886da58"
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-01-26T10:58:36.195471Z",
          "start_time": "2022-01-26T10:58:35.784780Z"
        }
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import functions as fn\n",
        "\n",
        "(\n",
        "    products\n",
        "        .groupBy('prod_brand', 'prod_cat')\\\n",
        "        .agg(\n",
        "            fn.avg('prod_value')\n",
        "        )\n",
        "        .show()\n",
        ")"
      ],
      "id": "4cea6fac"
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-01-26T10:58:36.650354Z",
          "start_time": "2022-01-26T10:58:36.207985Z"
        }
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import functions as fn\n",
        "\n",
        "(\n",
        "    products\n",
        "        .groupBy('prod_brand')\n",
        "        .agg(\n",
        "            fn.round(\n",
        "                fn.avg('prod_value'), 1)\n",
        "                .alias('average'),\n",
        "            fn.ceil(\n",
        "                fn.sum('prod_value'))\n",
        "                .alias('sum'),\n",
        "            fn.min('prod_value')\n",
        "                .alias('min')\n",
        "        )\n",
        "        .show()\n",
        ")"
      ],
      "id": "f22406d4"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Example using a query"
      ],
      "id": "e41f009a-2310-42fa-817f-2b09ca032553"
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-01-26T10:58:37.089099Z",
          "start_time": "2022-01-26T10:58:36.652842Z"
        }
      },
      "outputs": [],
      "source": [
        "products.createOrReplaceTempView(\"products\")"
      ],
      "id": "0381910c"
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {},
      "outputs": [],
      "source": [
        "query = \"\"\"\n",
        "SELECT\n",
        "    prod_brand,\n",
        "    round(avg(prod_value), 1) AS average,\n",
        "    min(prod_value) AS min\n",
        "FROM \n",
        "    products\n",
        "GROUP BY \n",
        "    prod_brand\n",
        "\"\"\"\n",
        "\n",
        "spark.sql(query).show()"
      ],
      "id": "9ca2d6fd"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Window functions\n",
        "\n",
        "## Numerical window functions"
      ],
      "id": "b719c5b0-22bf-453d-af92-2dbc17882eae"
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-01-26T10:58:37.751296Z",
          "start_time": "2022-01-26T10:58:37.092075Z"
        }
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import Window\n",
        "from pyspark.sql import functions as fn\n",
        "\n",
        "# First, we create the Window definition\n",
        "window = Window.partitionBy('prod_brand')\n",
        "\n",
        "print(type(window))"
      ],
      "id": "2e4da557"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Then, we can use `over` to aggregate on this window"
      ],
      "id": "a0b1bd20-ce25-4734-a11e-ed0f670940b9"
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {},
      "outputs": [],
      "source": [
        "avg = fn.avg('prod_value').over(window)\n",
        "\n",
        "# Finally, we can it as a classical column\n",
        "(\n",
        "    products\n",
        "        .withColumn('avg_brand_value', fn.round(avg, 2))\n",
        "        .show()\n",
        ")"
      ],
      "id": "4ca2924b"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "With SQL queries, using multiple windows is not a problem"
      ],
      "id": "26101105-1bfe-495b-8e7c-f0cffb87585d"
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {},
      "outputs": [],
      "source": [
        "query = \"\"\"\n",
        "    SELECT \n",
        "        *, \n",
        "        ROUND(AVG(prod_value) OVER w1, 2)  AS avg_brand_value,\n",
        "        ROUND(AVG(prod_value) OVER w2, 1)  AS avg_prod_value\n",
        "    FROM \n",
        "        products\n",
        "    WINDOW \n",
        "        w1 AS (PARTITION BY prod_brand),\n",
        "        w2 AS (PARTITION BY prod_cat)\n",
        "\"\"\"\n",
        "\n",
        "spark.sql(query).show()"
      ],
      "id": "da98590f"
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {},
      "outputs": [],
      "source": [
        "window2 = Window.partitionBy('prod_cat')\n",
        "\n",
        "avg2 = fn.avg('prod_value').over(window2)\n",
        "\n",
        "# Finally, we can it as a classical column\n",
        "( \n",
        "    products\n",
        "        .withColumn('avg_brand_value', fn.round(avg, 2))\n",
        "        .withColumn('avg_prod_value', fn.round(avg2, 1))\n",
        "        .show()\n",
        ")"
      ],
      "id": "7df40203"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we can compare the physical plans associated with the two jobs."
      ],
      "id": "db220e4a-9397-470d-a838-4dad244999e5"
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {},
      "outputs": [],
      "source": [
        "( \n",
        "    products\n",
        "        .withColumn('avg_brand_value', fn.round(avg, 2))\n",
        "        .withColumn('avg_prod_value', fn.round(avg2, 1))\n",
        "        .explain()\n",
        ")"
      ],
      "id": "6e775d6f"
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {},
      "outputs": [],
      "source": [
        "spark.sql(query).explain()"
      ],
      "id": "d5e2fc6d"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Windows can be defined on multiple columns"
      ],
      "id": "e0650953-f833-4dae-ad3a-9fd2136c2a21"
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-01-26T10:58:38.261379Z",
          "start_time": "2022-01-26T10:58:37.753256Z"
        }
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import Window\n",
        "from pyspark.sql import functions as fn\n",
        "\n",
        "window = Window.partitionBy('prod_brand', 'prod_cat')\n",
        "\n",
        "avg = fn.avg('prod_value').over(window)\n",
        "\n",
        "\n",
        "(\n",
        "    products    \n",
        "        .withColumn('avg_value', fn.round(avg, 2))\n",
        "        .show()\n",
        ")"
      ],
      "id": "4a955fd0"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Lag and Lead"
      ],
      "id": "af20d55e-acd3-4e44-819a-50ec2f2f6cac"
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-01-26T10:58:39.785452Z",
          "start_time": "2022-01-26T10:58:39.084502Z"
        }
      },
      "outputs": [],
      "source": [
        "purchases = spark.createDataFrame(\n",
        "    [\n",
        "        (date(2017, 11, 1), 'mouse'),\n",
        "        (date(2017, 11, 2), 'mouse'),\n",
        "        (date(2017, 11, 4), 'keyboard'),\n",
        "        (date(2017, 11, 6), 'keyboard'),\n",
        "        (date(2017, 11, 9), 'keyboard'),\n",
        "        (date(2017, 11, 12), 'mouse'),\n",
        "        (date(2017, 11, 18), 'keyboard')\n",
        "    ], \n",
        "    ['date', 'prod_cat']\n",
        ")\n",
        "\n",
        "purchases.show()\n",
        "\n",
        "window = Window.partitionBy('prod_cat').orderBy('date')\n",
        "\n",
        "prev_purch = fn.lag('date', 1).over(window)\n",
        "next_purch = fn.lead('date', 1).over(window)\n",
        "\n",
        "purchases\\\n",
        "    .withColumn('prev', prev_purch)\\\n",
        "    .withColumn('next', next_purch)\\\n",
        "    .orderBy('prod_cat', 'date')\\\n",
        "    .show()"
      ],
      "id": "0d7a1d67"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Rank, DenseRank and RowNumber"
      ],
      "id": "d60e5943-3594-4d1b-8fd4-902b71cf1dbd"
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-01-26T10:58:40.005845Z",
          "start_time": "2022-01-26T10:58:39.787433Z"
        }
      },
      "outputs": [],
      "source": [
        "contestants = spark.createDataFrame(\n",
        "    [   \n",
        "        ('veterans', 'John', 3000),\n",
        "        ('veterans', 'Bob', 3200),\n",
        "        ('veterans', 'Mary', 4000),\n",
        "        ('young', 'Jane', 4000),\n",
        "        ('young', 'April', 3100),\n",
        "        ('young', 'Alice', 3700),\n",
        "        ('young', 'Micheal', 4000),\n",
        "    ], \n",
        "    ['category', 'name', 'points']\n",
        ")\n",
        "\n",
        "contestants.show()"
      ],
      "id": "fc2ebaa0"
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-01-26T10:58:40.653650Z",
          "start_time": "2022-01-26T10:58:40.009618Z"
        }
      },
      "outputs": [],
      "source": [
        "window = (\n",
        "    Window\n",
        "        .partitionBy('category')\n",
        "        .orderBy(contestants.points.desc())\n",
        ")\n",
        "\n",
        "rank = fn.rank().over(window)\n",
        "dense_rank = fn.dense_rank().over(window)\n",
        "row_number = fn.row_number().over(window)\n",
        "\n",
        "contestants\\\n",
        "    .withColumn('rank', rank)\\\n",
        "    .withColumn('dense_rank', dense_rank)\\\n",
        "    .withColumn('row_number', row_number)\\\n",
        "    .orderBy('category', fn.col('points').desc())\\\n",
        "    .show()"
      ],
      "id": "1c127d34"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Connection to a database \n",
        "\n",
        "The postgres server runs locally on my laptop, it is equiped with a\n",
        "number of training schemata, including `nycflights` (see\n",
        "<https://s-v-b.github.io/MA15Y030/schemas/schema-nycflights.html>)"
      ],
      "id": "1815e665-20d5-4264-aa85-4f422f188097"
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_flights = spark.read \\\n",
        "    .format(\"jdbc\") \\\n",
        "    .option(\"url\", \"jdbc:postgresql://localhost:5434/bd_2023-24\") \\\n",
        "    .option(\"dbschema\", \"nycflights\")\\\n",
        "    .option(\"dbtable\", \"flights\") \\\n",
        "    .option(\"user\", \"postgres\") \\\n",
        "    .option(\"password\", \"postgres\") \\\n",
        "    .option(\"driver\", \"org.postgresql.Driver\") \\\n",
        "    .load()\n",
        "\n",
        "df_flights.printSchema()"
      ],
      "id": "b7cbaeb9"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To load the five tables, we avoid cut and paste, and abide to the DRY\n",
        "principle.\n",
        "\n",
        "We package the options in a dictionnary"
      ],
      "id": "521f5d8a-25fd-42de-9925-fdd9b2f6061c"
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {},
      "outputs": [],
      "source": [
        "db_con_settings = {\n",
        "    'url': \"jdbc:postgresql://localhost:5434/bd_2023-24\",\n",
        "    'dbschema':  \"nycflights\",\n",
        "    'user':  \"postgres\",\n",
        "    'password':  \"postgres\",\n",
        "    'driver':  \"org.postgresql.Driver\"\n",
        "}"
      ],
      "id": "897b5f69"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We prepare a Python object using dictionnary unpacking."
      ],
      "id": "5ca01f42-c32e-4e99-a292-1995c1d4e5e2"
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {},
      "outputs": [],
      "source": [
        "o  = spark.read \\\n",
        "    .format(\"jdbc\")\\\n",
        "    .options(**db_con_settings)"
      ],
      "id": "caa8cc27"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We use the object to load the different tables in a `for` loop."
      ],
      "id": "5a13e372-ef80-43cf-a575-02e5b564ba71"
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {},
      "outputs": [],
      "source": [
        "tbl_names = ['flights', 'airports', 'airlines', 'planes', 'weather']\n",
        "\n",
        "dic_df = {}\n",
        "\n",
        "for tn in tbl_names:\n",
        "    dic_df[tn] = o.option('dbtable', tn).load()"
      ],
      "id": "3a9b487e"
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {},
      "outputs": [],
      "source": [
        "for k, v in dic_df.items():\n",
        "    v.printSchema()"
      ],
      "id": "372cb305"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can now query the tables."
      ],
      "id": "41e0e77f-fd08-4c6e-aafe-31daa47a37c9"
    }
  ],
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "path": "/usr/share/jupyter/kernels/python3"
    },
    "language_info": {
      "name": "python",
      "codemirror_mode": {
        "name": "ipython",
        "version": "3"
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  }
}