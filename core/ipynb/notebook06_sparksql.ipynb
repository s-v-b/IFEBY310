{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# `DataFrame`"
      ],
      "id": "a47fa630-1974-4c1f-9f55-e037af068835"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "\n",
        "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
        "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable"
      ],
      "id": "d4115c44"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-08T20:23:05.867529Z",
          "start_time": "2022-02-08T20:23:01.418071Z"
        }
      },
      "outputs": [],
      "source": [
        "from pyspark import SparkConf, SparkContext\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "conf = SparkConf().setAppName(\"Spark SQL Course\")\n",
        "sc = SparkContext(conf=conf)  # no need for Spark 3...\n",
        "\n",
        "spark = (SparkSession\n",
        "    .builder\n",
        "    .appName(\"Spark SQL Course\")\n",
        "    .getOrCreate()\n",
        ")"
      ],
      "id": "ee087bed"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-01-26T10:58:46.926825Z",
          "start_time": "2022-01-26T10:58:46.920913Z"
        }
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import Row\n",
        "\n",
        "row1 = Row(name=\"John\", age=21)\n",
        "row2 = Row(name=\"James\", age=32)\n",
        "row3 = Row(name=\"Jane\", age=18)\n",
        "row1['name']"
      ],
      "id": "a9b282f3"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-01-26T10:58:57.185741Z",
          "start_time": "2022-01-26T10:58:57.155181Z"
        }
      },
      "outputs": [],
      "source": [
        "df = spark.createDataFrame([row1, row2, row3])"
      ],
      "id": "f9a8b895"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-01-26T10:59:13.491438Z",
          "start_time": "2022-01-26T10:59:13.486119Z"
        }
      },
      "outputs": [],
      "source": [
        "df.printSchema()"
      ],
      "id": "50650819"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-01-26T10:59:17.760344Z",
          "start_time": "2022-01-26T10:59:17.597166Z"
        }
      },
      "outputs": [],
      "source": [
        "df.show()"
      ],
      "id": "a083b359"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-01-26T10:59:25.889372Z",
          "start_time": "2022-01-26T10:59:25.866666Z"
        }
      },
      "outputs": [],
      "source": [
        "print(df.rdd.toDebugString().decode(\"utf-8\"))"
      ],
      "id": "a52778f4"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-01-26T10:59:45.432264Z",
          "start_time": "2022-01-26T10:59:45.426727Z"
        }
      },
      "outputs": [],
      "source": [
        "df.rdd.getNumPartitions()"
      ],
      "id": "1f2d3a12"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Creating dataframes"
      ],
      "id": "0552e76a-58eb-44c8-845f-72ef6e10d4a3"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-01-26T10:58:18.707591Z",
          "start_time": "2022-01-26T10:58:18.220608Z"
        }
      },
      "outputs": [],
      "source": [
        "rows = [\n",
        "    Row(name=\"John\", age=21, gender=\"male\"),\n",
        "    Row(name=\"James\", age=25, gender=\"female\"),\n",
        "    Row(name=\"Albert\", age=46, gender=\"male\")\n",
        "]\n",
        "\n",
        "df = spark.createDataFrame(rows)"
      ],
      "id": "cd0aaf3f"
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-01-26T10:58:18.707591Z",
          "start_time": "2022-01-26T10:58:18.220608Z"
        }
      },
      "outputs": [],
      "source": [
        "df.show()"
      ],
      "id": "eff6e7c1"
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "help(Row)"
      ],
      "id": "a819ba4b"
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-01-26T10:58:19.065539Z",
          "start_time": "2022-01-26T10:58:18.710711Z"
        }
      },
      "outputs": [],
      "source": [
        "column_names = [\"name\", \"age\", \"gender\"]\n",
        "rows = [\n",
        "    [\"John\", 21, \"male\"],\n",
        "    [\"James\", 25, \"female\"],\n",
        "    [\"Albert\", 46, \"male\"]\n",
        "]\n",
        "\n",
        "df = spark.createDataFrame(\n",
        "    rows, \n",
        "    column_names\n",
        ")\n",
        "\n",
        "df.show()"
      ],
      "id": "6c52d94b"
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-01-26T10:58:19.074335Z",
          "start_time": "2022-01-26T10:58:19.068088Z"
        }
      },
      "outputs": [],
      "source": [
        "df.printSchema()"
      ],
      "id": "43e781d3"
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-01-26T10:58:19.840178Z",
          "start_time": "2022-01-26T10:58:19.077057Z"
        }
      },
      "outputs": [],
      "source": [
        "# sc = SparkContext(conf=conf)  # no need for Spark 3...\n",
        "\n",
        "column_names = [\"name\", \"age\", \"gender\"]\n",
        "rdd = sc.parallelize([\n",
        "    (\"John\", 21, \"male\"),\n",
        "    (\"James\", 25, \"female\"),\n",
        "    (\"Albert\", 46, \"male\")\n",
        "])\n",
        "df = spark.createDataFrame(rdd, column_names)\n",
        "df.show()"
      ],
      "id": "ef788482"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Schema\n",
        "\n",
        "There is special type schemata. A object of class `StructType` is made\n",
        "of a list of objects of type `StructField`."
      ],
      "id": "f999863f-cda0-43f3-865b-febe9f0e8867"
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-01-26T10:58:19.850578Z",
          "start_time": "2022-01-26T10:58:19.843835Z"
        }
      },
      "outputs": [],
      "source": [
        "df.schema"
      ],
      "id": "5b240c91"
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-01-26T10:58:19.860631Z",
          "start_time": "2022-01-26T10:58:19.854012Z"
        }
      },
      "outputs": [],
      "source": [
        "type(df.schema)"
      ],
      "id": "518c7d89"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "A object of type `StructField` has a name, a PySpark type, an d a\n",
        "boolean parameter."
      ],
      "id": "3977fa3e-89ac-41dd-9c70-8118fa37990d"
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-01-26T10:58:20.199419Z",
          "start_time": "2022-01-26T10:58:19.863528Z"
        }
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.types import *\n",
        "\n",
        "schema = StructType(\n",
        "    [\n",
        "        StructField(\"name\", StringType(), True),\n",
        "        StructField(\"age\", IntegerType(), True),\n",
        "        StructField(\"gender\", StringType(), True)\n",
        "    ]\n",
        ")\n",
        "\n",
        "rows = [(\"John\", 21, \"male\")]\n",
        "df = spark.createDataFrame(rows, schema)\n",
        "df.printSchema()\n",
        "df.show()"
      ],
      "id": "5da86aac"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Queries (single table $σ$, $π$)\n",
        "\n",
        "PySpark offers two ways to query a datafrane:\n",
        "\n",
        "-   An ad hod API with methods for the DataFrame class.\n",
        "-   The possibility to post SQL queries (provided a temporary view has\n",
        "    been created)."
      ],
      "id": "11d47c5e-e243-45fa-8dc6-bf434b615b25"
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-01-26T10:58:20.882311Z",
          "start_time": "2022-01-26T10:58:20.201993Z"
        }
      },
      "outputs": [],
      "source": [
        "column_names = [\"name\", \"age\", \"gender\"]\n",
        "rows = [\n",
        "    [\"John\", 21, \"male\"],\n",
        "    [\"Jane\", 25, \"female\"]\n",
        "]\n",
        "# \n",
        "df = spark.createDataFrame(rows, column_names)\n",
        "\n",
        "# Create a temporary view from the DataFrame\n",
        "df.createOrReplaceTempView(\"new_view\")\n",
        "\n",
        "# Apply the query\n",
        "query = \"\"\"\n",
        "    SELECT \n",
        "        name, age \n",
        "    FROM \n",
        "        new_view \n",
        "    WHERE \n",
        "        gender='male'\n",
        "\"\"\"\n",
        "\n",
        "men_df = spark.sql(query)\n",
        "men_df.show()"
      ],
      "id": "51dfc0f6"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## `SELECT` (projection $π$)"
      ],
      "id": "03c79a90-8f8d-4d91-a25f-46ff561da3cc"
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-01-26T10:58:21.162623Z",
          "start_time": "2022-01-26T10:58:20.884802Z"
        }
      },
      "outputs": [],
      "source": [
        "df.createOrReplaceTempView(\"table\")    \n",
        "\n",
        "query = \"\"\"\n",
        "    SELECT \n",
        "        name, age \n",
        "    FROM \n",
        "        table\n",
        "\"\"\"\n",
        "\n",
        "spark.sql(query).show()"
      ],
      "id": "d278cbfa"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Using the API:"
      ],
      "id": "2e376aad-5a81-40e2-8f30-1fe7889610b5"
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-01-26T10:58:21.388097Z",
          "start_time": "2022-01-26T10:58:21.164840Z"
        }
      },
      "outputs": [],
      "source": [
        "(\n",
        "    df\n",
        "        .select(\"name\", \"age\")\n",
        "        .show()\n",
        ")"
      ],
      "id": "cce91e8b"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "`π(df, \"name\", \"age\")`\n",
        "\n",
        "## `WHERE` (filter, selection, $σ$)"
      ],
      "id": "e0e48635-5564-4724-9a37-7a099c463237"
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-01-26T10:58:21.704402Z",
          "start_time": "2022-01-26T10:58:21.402155Z"
        }
      },
      "outputs": [],
      "source": [
        "df.createOrReplaceTempView(\"table\")\n",
        "\n",
        "query = \"\"\"\n",
        "    SELECT \n",
        "        * \n",
        "    FROM \n",
        "        table\n",
        "    WHERE \n",
        "        age > 21\n",
        "\"\"\"\n",
        "\n",
        "spark.sql(query).show()"
      ],
      "id": "4439b4c5"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Using the API"
      ],
      "id": "39255835-582f-421e-a296-96ed4ad66996"
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-01-26T10:58:21.924501Z",
          "start_time": "2022-01-26T10:58:21.706741Z"
        }
      },
      "outputs": [],
      "source": [
        "( \n",
        "    df\n",
        "        .where(\"age > 21\")\n",
        "        .show()\n",
        ")"
      ],
      "id": "c09b7122"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This implements `σ(df, \"age > 21\")`"
      ],
      "id": "6918b0e4-c286-47e7-a693-38568700fd19"
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-01-26T10:58:22.377417Z",
          "start_time": "2022-01-26T10:58:21.926708Z"
        }
      },
      "outputs": [],
      "source": [
        "# Alternatively:\n",
        "( \n",
        "    df\n",
        "      .where(df['age'] > 21)\n",
        "      .show()\n",
        ")"
      ],
      "id": "09be6195"
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-01-26T10:58:22.566385Z",
          "start_time": "2022-01-26T10:58:22.380036Z"
        }
      },
      "outputs": [],
      "source": [
        "( \n",
        "    df\n",
        "      .where(df.age > 21)\n",
        "      .show()\n",
        ")"
      ],
      "id": "eaabaa28"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Method chaining allows to construct complex queries"
      ],
      "id": "cdf9bbfd-9d35-4581-94f8-7abeb12d3aab"
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-01-26T10:58:22.837136Z",
          "start_time": "2022-01-26T10:58:22.569324Z"
        }
      },
      "outputs": [],
      "source": [
        "( \n",
        "    df\n",
        "      .where(\"age > 21\")\n",
        "      .select([\"name\", \"age\"])\n",
        "      .show()\n",
        ")"
      ],
      "id": "6c49cad7"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This implements\n",
        "\n",
        "        σ(df, \"age > 21\") |>\n",
        "        π([\"name\", \"age\"])\n",
        "\n",
        "## `LIMIT`"
      ],
      "id": "7758b06e-72bc-4682-8411-224c8b40b3c5"
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-01-26T10:58:23.315363Z",
          "start_time": "2022-01-26T10:58:22.842106Z"
        }
      },
      "outputs": [],
      "source": [
        "df.createOrReplaceTempView(\"table\")\n",
        "\n",
        "query = \"\"\"\n",
        "    SELECT \n",
        "        * \n",
        "    FROM \n",
        "        table \n",
        "    LIMIT 1\n",
        "\"\"\"\n",
        "\n",
        "spark.sql(query).show()"
      ],
      "id": "98e3a5f8"
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-01-26T10:58:23.522646Z",
          "start_time": "2022-01-26T10:58:23.318694Z"
        }
      },
      "outputs": [],
      "source": [
        "df.limit(1).show()"
      ],
      "id": "471e3b8f"
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-01-26T10:58:23.778517Z",
          "start_time": "2022-01-26T10:58:23.525281Z"
        }
      },
      "outputs": [],
      "source": [
        "df.select(\"*\").limit(1).show()"
      ],
      "id": "d1681313"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## `ORDER BY`"
      ],
      "id": "92ce2e99-0532-48a0-8152-995491870ce3"
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-01-26T10:58:24.190838Z",
          "start_time": "2022-01-26T10:58:23.781166Z"
        }
      },
      "outputs": [],
      "source": [
        "df.createOrReplaceTempView(\"table\")\n",
        "\n",
        "query = \"\"\"\n",
        "    SELECT \n",
        "        * \n",
        "    FROM \n",
        "        table\n",
        "    ORDER BY \n",
        "        name ASC\n",
        "\"\"\"\n",
        "\n",
        "spark.sql(query).show()"
      ],
      "id": "856910a3"
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-01-26T10:58:24.368069Z",
          "start_time": "2022-01-26T10:58:24.193899Z"
        }
      },
      "outputs": [],
      "source": [
        "df.orderBy(df.name.asc()).show()"
      ],
      "id": "75c2cc60"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## `ALIAS` (rename)"
      ],
      "id": "ec007341-1e1c-4ef3-8736-7dbb7dd2bf37"
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-01-26T10:58:24.643668Z",
          "start_time": "2022-01-26T10:58:24.370758Z"
        }
      },
      "outputs": [],
      "source": [
        "df.createOrReplaceTempView(\"table\")\n",
        "query = \"SELECT name, age, gender AS sex FROM table\"\n",
        "spark.sql(query).show()"
      ],
      "id": "862eb8b9"
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [],
      "source": [
        "type(df.age)"
      ],
      "id": "574dd906"
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-01-26T10:58:24.858104Z",
          "start_time": "2022-01-26T10:58:24.646119Z"
        }
      },
      "outputs": [],
      "source": [
        "df.select(df.name, df.age, df.gender.alias('sex')).show()"
      ],
      "id": "cf02d93b"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## `CAST`"
      ],
      "id": "c9bcfa5c-ea4b-40c0-979f-af12f769897b"
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-01-26T10:58:25.072286Z",
          "start_time": "2022-01-26T10:58:24.860474Z"
        }
      },
      "outputs": [],
      "source": [
        "df.createOrReplaceTempView(\"table\")\n",
        "query = \"SELECT name, cast(age AS float) AS age_f FROM table\"\n",
        "spark.sql(query).show()"
      ],
      "id": "7eab8801"
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-01-26T10:58:25.384433Z",
          "start_time": "2022-01-26T10:58:25.074523Z"
        }
      },
      "outputs": [],
      "source": [
        "df.select(df.name, df.age.cast(\"float\").alias(\"age_f\")).show()"
      ],
      "id": "de0e7233"
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-01-26T10:58:25.648155Z",
          "start_time": "2022-01-26T10:58:25.386952Z"
        }
      },
      "outputs": [],
      "source": [
        "new_age_col = df.age.cast(\"float\").alias(\"age_f\")\n",
        "type(new_age_col), type(df.age)"
      ],
      "id": "cbac42f5"
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [],
      "source": [
        "df.select(df.name, new_age_col).show()"
      ],
      "id": "4eec5547"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Adding new columns"
      ],
      "id": "2fe02dfe-a949-40d6-b979-48c9ba0e541f"
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-01-26T10:58:25.931495Z",
          "start_time": "2022-01-26T10:58:25.651283Z"
        }
      },
      "outputs": [],
      "source": [
        "df.createOrReplaceTempView(\"table\")\n",
        "\n",
        "query = \"\"\"\n",
        "    SELECT \n",
        "        *, \n",
        "        12*age AS age_months \n",
        "    FROM \n",
        "        table\n",
        "\"\"\"\n",
        "\n",
        "spark.sql(query).show()"
      ],
      "id": "e2c1f139"
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-01-26T10:58:26.195480Z",
          "start_time": "2022-01-26T10:58:25.933620Z"
        }
      },
      "outputs": [],
      "source": [
        "( \n",
        "    df\n",
        "        .withColumn(\"age_months\", df.age * 12)\n",
        "        .show()\n",
        ")"
      ],
      "id": "f7b65119"
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-01-26T10:58:26.422122Z",
          "start_time": "2022-01-26T10:58:26.197759Z"
        }
      },
      "outputs": [],
      "source": [
        "(\n",
        "    df\n",
        "        .select(\"*\", \n",
        "                (df.age * 12).alias(\"age_months\"))\n",
        "        .show()\n",
        ")"
      ],
      "id": "1a6bc66b"
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [],
      "source": [
        "import datetime\n",
        "\n",
        "hui = datetime.date.today()\n",
        "\n",
        "hui = hui.replace(year=hui.year-21)\n",
        "\n",
        "str(hui)"
      ],
      "id": "4b921c79"
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [],
      "source": [
        "# df.select(\"*\", hui.replace(year=hui.year - df.age ).alias(\"yob\")).show()"
      ],
      "id": "50e23897"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Column functions\n",
        "\n",
        "## Numeric functions examples"
      ],
      "id": "bd3774eb-931a-4a40-88b9-5e3c7907e35c"
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-01-26T10:58:26.748718Z",
          "start_time": "2022-01-26T10:58:26.425451Z"
        }
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import functions as fn\n",
        "\n",
        "columns = [\"brand\", \"cost\"]\n",
        "df = spark.createDataFrame([\n",
        "    (\"garnier\", 3.49),\n",
        "    (\"elseve\", 2.71)\n",
        "], columns)\n",
        "\n",
        "round_cost = fn.round(df.cost, 1)\n",
        "floor_cost = fn.floor(df.cost)\n",
        "ceil_cost = fn.ceil(df.cost)\n",
        "\n",
        "df.withColumn('round', round_cost)\\\n",
        "    .withColumn('floor', floor_cost)\\\n",
        "    .withColumn('ceil', ceil_cost)\\\n",
        "    .show()"
      ],
      "id": "664d7aed"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## String functions examples"
      ],
      "id": "56affa7e-efcd-43f8-8da7-9eb2151f2948"
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-01-26T10:58:27.055563Z",
          "start_time": "2022-01-26T10:58:26.751235Z"
        }
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import functions as fn\n",
        "\n",
        "columns = [\"first_name\", \"last_name\"]\n",
        "\n",
        "df = spark.createDataFrame([\n",
        "    (\"John\", \"Doe\"),\n",
        "    (\"Mary\", \"Jane\")\n",
        "], columns)\n",
        "\n",
        "last_name_initial = fn.substring(df.last_name, 0, 1)\n",
        "# last_name_initial_dotted = fn.concat(last_name_initial, \".\")\n",
        "\n",
        "name = fn.concat_ws(\" \", df.first_name, last_name_initial)\n",
        "df.withColumn(\"name\", name).show()"
      ],
      "id": "a3a3892b"
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {},
      "outputs": [],
      "source": [
        "( \n",
        "    df.selectExpr(\"*\", \"substring(last_name, 0, 1) as lni\")\n",
        "      .selectExpr(\"first_name\", \"last_name\", \"concat(first_name, ' ', lni, '.') as nname\")\n",
        "      .show()\n",
        ")"
      ],
      "id": "4270ccd8"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Date functions examples"
      ],
      "id": "7f356dc1-9362-4c3e-90ab-6370e4d6abb4"
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-01-26T10:58:27.373396Z",
          "start_time": "2022-01-26T10:58:27.057938Z"
        }
      },
      "outputs": [],
      "source": [
        "from datetime import date\n",
        "from pyspark.sql import functions as fn\n",
        "\n",
        "df = spark.createDataFrame([\n",
        "    (date(2015, 1, 1), date(2015, 1, 15)),\n",
        "    (date(2015, 2, 21), date(2015, 3, 8)),\n",
        "], [\"start_date\", \"end_date\"])\n",
        "\n",
        "days_between = fn.datediff(df.end_date, df.start_date)\n",
        "start_month = fn.month(df.start_date)\n",
        "\n",
        "df.withColumn('days_between', days_between)\\\n",
        "    .withColumn('start_month', start_month)\\\n",
        "    .show()"
      ],
      "id": "a14a001c"
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {},
      "outputs": [],
      "source": [
        "str(date(2015, 1, 1) - date(2015, 1, 15))"
      ],
      "id": "638d3c0e"
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {},
      "outputs": [],
      "source": [
        "from datetime import timedelta\n",
        "\n",
        "date(2023, 2 , 14) + timedelta(days=3)"
      ],
      "id": "07ce83f2"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Conditional transformations"
      ],
      "id": "3c61d724-0545-429b-ab8e-39bb51977867"
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-01-26T10:58:27.630822Z",
          "start_time": "2022-01-26T10:58:27.375855Z"
        }
      },
      "outputs": [],
      "source": [
        "df = spark.createDataFrame([\n",
        "    (\"John\", 21, \"male\"),\n",
        "    (\"Jane\", 25, \"female\"),\n",
        "    (\"Albert\", 46, \"male\"),\n",
        "    (\"Brad\", 49, \"super-hero\")\n",
        "], [\"name\", \"age\", \"gender\"])"
      ],
      "id": "20f25e37"
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-01-26T10:58:27.630822Z",
          "start_time": "2022-01-26T10:58:27.375855Z"
        }
      },
      "outputs": [],
      "source": [
        "supervisor = ( \n",
        "    fn.when(df.gender == 'male', 'Mr. Smith')\n",
        "      .when(df.gender == 'female', 'Miss Jones')\n",
        "      .otherwise('NA')\n",
        ")\n",
        "\n",
        "type(supervisor), type(fn.when)"
      ],
      "id": "7ef58cae"
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-01-26T10:58:27.630822Z",
          "start_time": "2022-01-26T10:58:27.375855Z"
        }
      },
      "outputs": [],
      "source": [
        "df.withColumn(\"supervisor\", supervisor).show()"
      ],
      "id": "07fb054a"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## User-defined functions"
      ],
      "id": "7bbf1857-7750-48d8-a96b-02404146e081"
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-01-26T10:58:28.037428Z",
          "start_time": "2022-01-26T10:58:27.633093Z"
        }
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import functions as fn\n",
        "from pyspark.sql.types import StringType\n",
        "\n",
        "df = spark.createDataFrame([(1, 3), (4, 2)], [\"first\", \"second\"])\n",
        "\n",
        "def my_func(col_1, col_2):\n",
        "    if (col_1 > col_2):\n",
        "        return \"{} is bigger than {}\".format(col_1, col_2)\n",
        "    else:\n",
        "        return \"{} is bigger than {}\".format(col_2, col_1)\n",
        "\n",
        "my_udf = fn.udf(my_func, StringType())\n",
        "\n",
        "df.withColumn(\"udf\", my_udf(df['first'], df['second'])).show()"
      ],
      "id": "9b2f7e09"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Joins ($⋈$)\n",
        "\n",
        "## Using the `spark.sql` API"
      ],
      "id": "5c3f909e-7d0e-4b8c-8bab-7086e9249a3f"
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-01-26T10:58:29.098957Z",
          "start_time": "2022-01-26T10:58:28.042691Z"
        }
      },
      "outputs": [],
      "source": [
        "from datetime import date\n",
        "\n",
        "products = spark.createDataFrame([\n",
        "    ('1', 'mouse', 'microsoft', 39.99),\n",
        "    ('2', 'keyboard', 'logitech', 59.99),\n",
        "], ['prod_id', 'prod_cat', 'prod_brand', 'prod_value'])\n",
        "\n",
        "purchases = spark.createDataFrame([\n",
        "    (date(2017, 11, 1), 2, '1'),\n",
        "    (date(2017, 11, 2), 1, '1'),\n",
        "    (date(2017, 11, 5), 1, '2'),\n",
        "], ['date', 'quantity', 'prod_id'])\n",
        "\n",
        "# The default join type is the \"INNER\" join\n",
        "purchases.join(products, 'prod_id').show()"
      ],
      "id": "94ac5b61"
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {},
      "outputs": [],
      "source": [
        "purchases.join(products, 'prod_id').explain()"
      ],
      "id": "18fbb575"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Using a `SQL` query"
      ],
      "id": "5c4c9b1b-6ad8-42a7-abf6-8ed02c7fd6c5"
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-01-26T10:58:29.731271Z",
          "start_time": "2022-01-26T10:58:29.101559Z"
        }
      },
      "outputs": [],
      "source": [
        "products.createOrReplaceTempView(\"products\")\n",
        "purchases.createOrReplaceTempView(\"purchases\")\n",
        "\n",
        "query = \"\"\"\n",
        "    SELECT * \n",
        "    FROM purchases AS prc INNER JOIN \n",
        "        products AS prd \n",
        "    ON prc.prod_id = prd.prod_id\n",
        "\"\"\"\n",
        "spark.sql(query).show()"
      ],
      "id": "ddf9b8d6"
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {},
      "outputs": [],
      "source": [
        "spark.sql(query).explain()"
      ],
      "id": "30869a5c"
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-01-26T10:58:30.660419Z",
          "start_time": "2022-01-26T10:58:29.734282Z"
        }
      },
      "outputs": [],
      "source": [
        "new_purchases = spark.createDataFrame([\n",
        "    (date(2017, 11, 1), 2, '1'),\n",
        "    (date(2017, 11, 2), 1, '3'),\n",
        "], ['date', 'quantity', 'prod_id_x'])\n",
        "\n",
        "# The default join type is the \"INNER\" join\n",
        "join_rule = new_purchases.prod_id_x == products.prod_id\n",
        "\n",
        "print(type(join_rule))\n",
        "\n",
        "new_purchases.join(products, join_rule, 'left').show()"
      ],
      "id": "7e5008a3"
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {},
      "outputs": [],
      "source": [
        "join_rule.info"
      ],
      "id": "8cd627bf"
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-01-26T10:58:31.319336Z",
          "start_time": "2022-01-26T10:58:30.663809Z"
        }
      },
      "outputs": [],
      "source": [
        "new_purchases = spark.createDataFrame([\n",
        "    (date(2017, 11, 1), 2, '1'),\n",
        "    (date(2017, 11, 2), 1, '3'),\n",
        "], ['date', 'quantity', 'prod_id_x'])\n",
        "\n",
        "# The default join type is the \"INNER\" join\n",
        "join_rule = new_purchases.prod_id_x == products.prod_id\n",
        "\n",
        "new_purchases.join(products, join_rule, 'left').show()"
      ],
      "id": "4d9de3e1"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Various types of joins"
      ],
      "id": "dab68de3-81b2-4770-8d6e-4da782fbd9cd"
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-01-26T10:58:31.376310Z",
          "start_time": "2022-01-26T10:58:31.323600Z"
        }
      },
      "outputs": [],
      "source": [
        "left = spark.createDataFrame([\n",
        "    (1, \"A1\"), (2, \"A2\"), (3, \"A3\"), (4, \"A4\")], \n",
        "    [\"id\", \"value\"])\n",
        "\n",
        "right = spark.createDataFrame([\n",
        "    (3, \"A3\"), (4, \"A4\"), (4, \"A4_1\"), (5, \"A5\"), (6, \"A6\")], \n",
        "    [\"id\", \"value\"])\n",
        "\n",
        "join_types = [\n",
        "    \"inner\", \"outer\", \"left\", \"right\",\n",
        "    \"leftsemi\", \"leftanti\"\n",
        "]"
      ],
      "id": "fcfbba55"
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-01-26T10:58:34.708236Z",
          "start_time": "2022-01-26T10:58:31.380091Z"
        }
      },
      "outputs": [],
      "source": [
        "for join_type in join_types:\n",
        "    print(join_type)\n",
        "    left.join(right, on=\"id\", how=join_type)\\\n",
        "        .orderBy(\"id\")\\\n",
        "        .show()"
      ],
      "id": "b2e1b136"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Agregations (summarize)\n",
        "\n",
        "## Examples using the API"
      ],
      "id": "ecdeaba1-db07-4c4a-8307-f07b661d4ec9"
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-01-26T10:58:35.398306Z",
          "start_time": "2022-01-26T10:58:34.710552Z"
        }
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import functions as fn\n",
        "\n",
        "products = spark.createDataFrame([\n",
        "    ('1', 'mouse', 'microsoft', 39.99),\n",
        "    ('2', 'mouse', 'microsoft', 59.99),\n",
        "    ('3', 'keyboard', 'microsoft', 59.99),\n",
        "    ('4', 'keyboard', 'logitech', 59.99),\n",
        "    ('5', 'mouse', 'logitech', 29.99),\n",
        "], ['prod_id', 'prod_cat', 'prod_brand', 'prod_value'])\n",
        "\n",
        "( \n",
        "    products\n",
        "        .groupBy('prod_cat')\n",
        "        .avg('prod_value')\n",
        "        .show()\n",
        ")"
      ],
      "id": "6837d86b"
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-01-26T10:58:35.782623Z",
          "start_time": "2022-01-26T10:58:35.400724Z"
        }
      },
      "outputs": [],
      "source": [
        "(\n",
        "    products\n",
        "        .groupBy('prod_cat')\n",
        "        .agg(fn.avg('prod_value'))\n",
        "        .show()\n",
        ")"
      ],
      "id": "47143c86"
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {},
      "outputs": [],
      "source": [
        "(\n",
        "    products\n",
        "        .groupBy('prod_cat')\n",
        "        .agg(\n",
        "            fn.mean('prod_value'), \n",
        "            fn.stddev('prod_value')\n",
        "        )\n",
        "        .show()\n",
        ")"
      ],
      "id": "904586bb"
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-01-26T10:58:36.195471Z",
          "start_time": "2022-01-26T10:58:35.784780Z"
        }
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import functions as fn\n",
        "\n",
        "(\n",
        "    products\n",
        "        .groupBy('prod_brand', 'prod_cat')\\\n",
        "        .agg(\n",
        "            fn.avg('prod_value')\n",
        "        )\n",
        "        .show()\n",
        ")"
      ],
      "id": "37b2c8b3"
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-01-26T10:58:36.650354Z",
          "start_time": "2022-01-26T10:58:36.207985Z"
        }
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import functions as fn\n",
        "\n",
        "(\n",
        "    products\n",
        "        .groupBy('prod_brand')\n",
        "        .agg(\n",
        "            fn.round(\n",
        "                fn.avg('prod_value'), 1)\n",
        "                .alias('average'),\n",
        "            fn.ceil(\n",
        "                fn.sum('prod_value'))\n",
        "                .alias('sum'),\n",
        "            fn.min('prod_value')\n",
        "                .alias('min')\n",
        "        )\n",
        "        .show()\n",
        ")"
      ],
      "id": "6f3e3724"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Example using a query"
      ],
      "id": "0e81ce53-b6af-4578-b557-c69d5865084c"
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-01-26T10:58:37.089099Z",
          "start_time": "2022-01-26T10:58:36.652842Z"
        }
      },
      "outputs": [],
      "source": [
        "products.createOrReplaceTempView(\"products\")"
      ],
      "id": "e53cdb0d"
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {},
      "outputs": [],
      "source": [
        "query = \"\"\"\n",
        "SELECT\n",
        "    prod_brand,\n",
        "    round(avg(prod_value), 1) AS average,\n",
        "    min(prod_value) AS min\n",
        "FROM \n",
        "    products\n",
        "GROUP BY \n",
        "    prod_brand\n",
        "\"\"\"\n",
        "\n",
        "spark.sql(query).show()"
      ],
      "id": "2c6b6178"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Window functions\n",
        "\n",
        "## Numerical window functions"
      ],
      "id": "cf9b3922-5a69-45e2-8f61-c4fee377a83f"
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-01-26T10:58:37.751296Z",
          "start_time": "2022-01-26T10:58:37.092075Z"
        }
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import Window\n",
        "from pyspark.sql import functions as fn\n",
        "\n",
        "# First, we create the Window definition\n",
        "window = Window.partitionBy('prod_brand')\n",
        "\n",
        "print(type(window))"
      ],
      "id": "1cd9bacd"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Then, we can use `over` to aggregate on this window"
      ],
      "id": "356b6ee6-6964-48ed-b65b-aa62f0e02d03"
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {},
      "outputs": [],
      "source": [
        "avg = fn.avg('prod_value').over(window)\n",
        "\n",
        "# Finally, we can it as a classical column\n",
        "(\n",
        "    products\n",
        "        .withColumn('avg_brand_value', fn.round(avg, 2))\n",
        "        .show()\n",
        ")"
      ],
      "id": "3e1e67cf"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "With SQL queries, using multiple windows is not a problem"
      ],
      "id": "33d0d2d5-120f-40be-b28b-b14a78c8556e"
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {},
      "outputs": [],
      "source": [
        "query = \"\"\"\n",
        "    SELECT \n",
        "        *, \n",
        "        ROUND(AVG(prod_value) OVER w1, 2)  AS avg_brand_value,\n",
        "        ROUND(AVG(prod_value) OVER w2, 1)  AS avg_prod_value\n",
        "    FROM \n",
        "        products\n",
        "    WINDOW \n",
        "        w1 AS (PARTITION BY prod_brand),\n",
        "        w2 AS (PARTITION BY prod_cat)\n",
        "\"\"\"\n",
        "\n",
        "spark.sql(query).show()"
      ],
      "id": "a233d7a3"
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {},
      "outputs": [],
      "source": [
        "window2 = Window.partitionBy('prod_cat')\n",
        "\n",
        "avg2 = fn.avg('prod_value').over(window2)\n",
        "\n",
        "# Finally, we can it as a classical column\n",
        "( \n",
        "    products\n",
        "        .withColumn('avg_brand_value', fn.round(avg, 2))\n",
        "        .withColumn('avg_prod_value', fn.round(avg2, 1))\n",
        "        .show()\n",
        ")"
      ],
      "id": "3c0db3d0"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we can compare the physical plans associated with the two jobs."
      ],
      "id": "36019414-efac-4cb1-9c11-c0481970bf8a"
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {},
      "outputs": [],
      "source": [
        "( \n",
        "    products\n",
        "        .withColumn('avg_brand_value', fn.round(avg, 2))\n",
        "        .withColumn('avg_prod_value', fn.round(avg2, 1))\n",
        "        .explain()\n",
        ")"
      ],
      "id": "a589ea17"
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {},
      "outputs": [],
      "source": [
        "spark.sql(query).explain()"
      ],
      "id": "edc1ae84"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Windows can be defined on multiple columns"
      ],
      "id": "29eb8ed5-11b7-4239-8e65-4ee72509ce45"
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-01-26T10:58:38.261379Z",
          "start_time": "2022-01-26T10:58:37.753256Z"
        }
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import Window\n",
        "from pyspark.sql import functions as fn\n",
        "\n",
        "window = Window.partitionBy('prod_brand', 'prod_cat')\n",
        "\n",
        "avg = fn.avg('prod_value').over(window)\n",
        "\n",
        "\n",
        "(\n",
        "    products    \n",
        "        .withColumn('avg_value', fn.round(avg, 2))\n",
        "        .show()\n",
        ")"
      ],
      "id": "e821a670"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Lag and Lead"
      ],
      "id": "6607d5d3-2506-40cf-91d8-463445cf7965"
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-01-26T10:58:39.785452Z",
          "start_time": "2022-01-26T10:58:39.084502Z"
        }
      },
      "outputs": [],
      "source": [
        "purchases = spark.createDataFrame(\n",
        "    [\n",
        "        (date(2017, 11, 1), 'mouse'),\n",
        "        (date(2017, 11, 2), 'mouse'),\n",
        "        (date(2017, 11, 4), 'keyboard'),\n",
        "        (date(2017, 11, 6), 'keyboard'),\n",
        "        (date(2017, 11, 9), 'keyboard'),\n",
        "        (date(2017, 11, 12), 'mouse'),\n",
        "        (date(2017, 11, 18), 'keyboard')\n",
        "    ], \n",
        "    ['date', 'prod_cat']\n",
        ")\n",
        "\n",
        "purchases.show()\n",
        "\n",
        "window = Window.partitionBy('prod_cat').orderBy('date')\n",
        "\n",
        "prev_purch = fn.lag('date', 1).over(window)\n",
        "next_purch = fn.lead('date', 1).over(window)\n",
        "\n",
        "purchases\\\n",
        "    .withColumn('prev', prev_purch)\\\n",
        "    .withColumn('next', next_purch)\\\n",
        "    .orderBy('prod_cat', 'date')\\\n",
        "    .show()"
      ],
      "id": "44f413c3"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Rank, DenseRank and RowNumber"
      ],
      "id": "7f6b2689-6691-4647-b96a-3bc377e0597d"
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-01-26T10:58:40.005845Z",
          "start_time": "2022-01-26T10:58:39.787433Z"
        }
      },
      "outputs": [],
      "source": [
        "contestants = spark.createDataFrame(\n",
        "    [   \n",
        "        ('veterans', 'John', 3000),\n",
        "        ('veterans', 'Bob', 3200),\n",
        "        ('veterans', 'Mary', 4000),\n",
        "        ('young', 'Jane', 4000),\n",
        "        ('young', 'April', 3100),\n",
        "        ('young', 'Alice', 3700),\n",
        "        ('young', 'Micheal', 4000),\n",
        "    ], \n",
        "    ['category', 'name', 'points']\n",
        ")\n",
        "\n",
        "contestants.show()"
      ],
      "id": "9198bf60"
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-01-26T10:58:40.653650Z",
          "start_time": "2022-01-26T10:58:40.009618Z"
        }
      },
      "outputs": [],
      "source": [
        "window = (\n",
        "    Window\n",
        "        .partitionBy('category')\n",
        "        .orderBy(contestants.points.desc())\n",
        ")\n",
        "\n",
        "rank = fn.rank().over(window)\n",
        "dense_rank = fn.dense_rank().over(window)\n",
        "row_number = fn.row_number().over(window)\n",
        "\n",
        "contestants\\\n",
        "    .withColumn('rank', rank)\\\n",
        "    .withColumn('dense_rank', dense_rank)\\\n",
        "    .withColumn('row_number', row_number)\\\n",
        "    .orderBy('category', fn.col('points').desc())\\\n",
        "    .show()"
      ],
      "id": "0be70027"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Connection to a database \n",
        "\n",
        "The postgres server runs locally on my laptop, it is equiped with a\n",
        "number of training schemata, including `nycflights` (see\n",
        "<https://s-v-b.github.io/MA15Y030/schemas/schema-nycflights.html>)"
      ],
      "id": "9b68e47a-0281-4ca7-88e4-fc67f2e4fb53"
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_flights = spark.read \\\n",
        "    .format(\"jdbc\") \\\n",
        "    .option(\"url\", \"jdbc:postgresql://localhost:5434/bd_2023-24\") \\\n",
        "    .option(\"dbschema\", \"nycflights\")\\\n",
        "    .option(\"dbtable\", \"flights\") \\\n",
        "    .option(\"user\", \"postgres\") \\\n",
        "    .option(\"password\", \"postgres\") \\\n",
        "    .option(\"driver\", \"org.postgresql.Driver\") \\\n",
        "    .load()\n",
        "\n",
        "df_flights.printSchema()"
      ],
      "id": "14d91de7"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To load the five tables, we avoid cut and paste, and abide to the DRY\n",
        "principle.\n",
        "\n",
        "We package the options in a dictionnary"
      ],
      "id": "acb88dfb-c43f-42c0-9df2-1abfc798620f"
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {},
      "outputs": [],
      "source": [
        "db_con_settings = {\n",
        "    'url': \"jdbc:postgresql://localhost:5434/bd_2023-24\",\n",
        "    'dbschema':  \"nycflights\",\n",
        "    'user':  \"postgres\",\n",
        "    'password':  \"postgres\",\n",
        "    'driver':  \"org.postgresql.Driver\"\n",
        "}"
      ],
      "id": "a9e99ce4"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We prepare a Python object using dictionnary unpacking."
      ],
      "id": "fec3cdc2-c9aa-4073-90bd-f73b795f773c"
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {},
      "outputs": [],
      "source": [
        "o  = spark.read \\\n",
        "    .format(\"jdbc\")\\\n",
        "    .options(**db_con_settings)"
      ],
      "id": "58d03e4f"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We use the object to load the different tables in a `for` loop."
      ],
      "id": "12bef435-6acf-4ac8-9291-386290a7e086"
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {},
      "outputs": [],
      "source": [
        "tbl_names = ['flights', 'airports', 'airlines', 'planes', 'weather']\n",
        "\n",
        "dic_df = {}\n",
        "\n",
        "for tn in tbl_names:\n",
        "    dic_df[tn] = o.option('dbtable', tn).load()"
      ],
      "id": "2709bd0d"
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {},
      "outputs": [],
      "source": [
        "for k, v in dic_df.items():\n",
        "    v.printSchema()"
      ],
      "id": "1f587915"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can now query the tables."
      ],
      "id": "2cd001a3-df52-411f-9ec1-999e69e28ccf"
    }
  ],
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "path": "/usr/share/jupyter/kernels/python3"
    },
    "language_info": {
      "name": "python",
      "codemirror_mode": {
        "name": "ipython",
        "version": "3"
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  }
}