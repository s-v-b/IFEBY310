{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Diving deeer"
      ],
      "id": "bd6534cd-8a48-4e08-bc5b-acde0de63ce9"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "\n",
        "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
        "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable"
      ],
      "id": "67050681"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pyspark import SparkConf, SparkContext\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "conf = SparkConf().setAppName(\"Spark SQL Course\")\n",
        "sc = SparkContext(conf=conf)  # no need for Spark 3...\n",
        "\n",
        "spark = (SparkSession\n",
        "    .builder\n",
        "    .appName(\"Spark SQL Course\")\n",
        "    .getOrCreate()\n",
        ")"
      ],
      "id": "6adc399a"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "sc = spark._sc"
      ],
      "id": "e5f64270"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "rdd = sc.parallelize([(\"a\", 1), (\"b\", 1), (\"a\", 1)])"
      ],
      "id": "a82d39bb"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "rdd.reduceByKey(lambda a, b: a + b).collect()"
      ],
      "id": "a81b27c3"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "import requests, zipfile, io\n",
        "from pathlib import Path\n",
        "\n",
        "path = Path('webdata.parquet')\n",
        "if not path.exists():\n",
        "    url = \"https://stephanegaiffas.github.io/big_data_course/data/webdata.parquet.zip\"\n",
        "    r = requests.get(url)\n",
        "    z = zipfile.ZipFile(io.BytesIO(r.content))\n",
        "    z.extractall(path='./')"
      ],
      "id": "dd49a238"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "input_path = './'\n",
        "\n",
        "input_file = os.path.join(input_path, 'webdata.parquet')\n",
        "df = spark.read.parquet(input_file)"
      ],
      "id": "8b81d07f"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "df.head(6)"
      ],
      "id": "5546ab33"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "df.describe()"
      ],
      "id": "a3a0a431"
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "df.count()"
      ],
      "id": "4d5236fd"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Basic statistics\n",
        "\n",
        "First we need to import some things"
      ],
      "id": "e075efce-983e-47ce-86d5-eba9337d194b"
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pyspark.sql import Window\n",
        "import pyspark.sql.functions as func\n",
        "from pyspark.sql.types import *\n",
        "from pyspark.sql.functions import col, lit"
      ],
      "id": "7ceff9fa"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Compute the total number of unique users"
      ],
      "id": "db0c44e3-5faa-4e83-95bb-7e0f2a95e9fd"
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "df.select('xid').distinct().count()"
      ],
      "id": "695d377d"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Construct a column containing the total number of actions per user"
      ],
      "id": "157bd6e3-e3d1-407e-ad2e-74463829152c"
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "xid_partition = Window.partitionBy('xid')\n",
        "n_events = func.count(col('action')).over(xid_partition)\n",
        "df = df.withColumn('n_events', n_events)\n",
        "df.head(n=2)"
      ],
      "id": "2d909cb6"
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "df.groupBy('xid').agg(func.count('action')).head(5)"
      ],
      "id": "7856172f"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Construct a column containing the number of days since the last action of the user"
      ],
      "id": "9640048c-aacc-43c9-bd12-ad3a98d9dc2d"
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "xid_partition = Window.partitionBy('xid')\n",
        "max_date = func.max(col('date')).over(xid_partition)\n",
        "n_days_since_last_event = func.datediff(func.current_date(), max_date)\n",
        "df = df.withColumn('n_days_since_last_event',\n",
        "                   n_days_since_last_event)\n",
        "df.head(n=2)"
      ],
      "id": "6fac33e0"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Construct a column containing the number of actions of each user for each modality of device"
      ],
      "id": "874eadc6-556c-4be1-80f3-b59d2d7c88ea"
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "xid_device_partition = Window.partitionBy('xid', 'device')\n",
        "n_events_per_device = func.count(col('action')).over(xid_device_partition)\n",
        "df = df.withColumn('n_events_per_device', n_events_per_device)\n",
        "df.head(n=2)"
      ],
      "id": "465c73a6"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Number of device per user: some mental gymnastics"
      ],
      "id": "57f0c5e5-e869-4e2a-9bc9-550b6331cfd2"
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "xid_partition = Window.partitionBy('xid')\n",
        "rank_device = func.dense_rank().over(xid_partition.orderBy('device'))\n",
        "n_unique_device = func.last(rank_device).over(xid_partition)\n",
        "df = df.withColumn('n_device', n_unique_device)\n",
        "df.head(n=2)"
      ],
      "id": "106b3cb4"
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "df\\\n",
        "    .where(col('n_device') > 1)\\\n",
        "    .select('xid', 'device', 'n_events',  'n_device', 'n_events_per_device')\\\n",
        "    .head(n=8)"
      ],
      "id": "b1b2d39a"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Let’s select the correct users and build a training dataset\n",
        "\n",
        "We construct a ETL (Extract Transform Load) process on this data using\n",
        "the `pyspark.sql` API.\n",
        "\n",
        "## Extraction\n",
        "\n",
        "Extraction is easy here, it’s just about reading the data"
      ],
      "id": "e3160dea-bbff-4d5b-afd1-ab9e2389d98d"
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "df = spark.read.parquet(input_file)\n",
        "df.head(n=3)"
      ],
      "id": "13937004"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Transformation of the data\n",
        "\n",
        "At this step we compute a lot of extra things from the data. The aim is\n",
        "to build features that describe users."
      ],
      "id": "fefb5a6f-6734-4b1c-9914-6ea276df3f64"
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "def n_events_transformer(df):\n",
        "    xid_partition = Window.partitionBy('xid')\n",
        "    n_events = func.count(col('action')).over(xid_partition)\n",
        "    df = df.withColumn('n_events', n_events)\n",
        "    return df\n",
        "\n",
        "def n_events_per_action_transformer(df):\n",
        "    xid_action_partition = Window.partitionBy('xid', 'action')\n",
        "    n_events_per_action = func.count(col('action')).over(xid_action_partition)\n",
        "    df = df.withColumn('n_events_per_action', n_events_per_action)\n",
        "    return df\n",
        "\n",
        "def hour_transformer(df):\n",
        "    hour = func.hour(col('date'))\n",
        "    df = df.withColumn('hour', hour)\n",
        "    return df\n",
        "\n",
        "def weekday_transformer(df):\n",
        "    weekday = func.date_format(col('date'), 'EEEE')\n",
        "    df = df.withColumn('weekday', weekday)\n",
        "    return df\n",
        "\n",
        "def n_events_per_hour_transformer(df):\n",
        "    xid_hour_partition = Window.partitionBy('xid', 'hour')\n",
        "    n_events_per_hour = func.count(col('action')).over(xid_hour_partition)\n",
        "    df = df.withColumn('n_events_per_hour', n_events_per_hour)\n",
        "    return df\n",
        "\n",
        "def n_events_per_weekday_transformer(df):\n",
        "    xid_weekday_partition = Window.partitionBy('xid', 'weekday')\n",
        "    n_events_per_weekday = func.count(col('action')).over(xid_weekday_partition)\n",
        "    df = df.withColumn('n_events_per_weekday', n_events_per_weekday)\n",
        "    return df\n",
        "\n",
        "def n_days_since_last_event_transformer(df):\n",
        "    xid_partition = Window.partitionBy('xid')\n",
        "    max_date = func.max(col('date')).over(xid_partition)\n",
        "    n_days_since_last_event = func.datediff(func.current_date(), max_date)\n",
        "    df = df.withColumn('n_days_since_last_event',\n",
        "                       n_days_since_last_event + lit(0.1))\n",
        "    return df\n",
        "\n",
        "def n_days_since_last_action_transformer(df):\n",
        "    xid_partition_action = Window.partitionBy('xid', 'action')\n",
        "    max_date = func.max(col('date')).over(xid_partition_action)\n",
        "    n_days_since_last_action = func.datediff(func.current_date(),\n",
        "                                                        max_date)\n",
        "    df = df.withColumn('n_days_since_last_action',\n",
        "                       n_days_since_last_action + lit(0.1))\n",
        "    return df\n",
        "\n",
        "def n_unique_day_transformer(df):\n",
        "    xid_partition = Window.partitionBy('xid')\n",
        "    dayofyear = func.dayofyear(col('date'))\n",
        "    rank_day = func.dense_rank().over(xid_partition.orderBy(dayofyear))\n",
        "    n_unique_day = func.last(rank_day).over(xid_partition)\n",
        "    df = df.withColumn('n_unique_day', n_unique_day)\n",
        "    return df\n",
        "\n",
        "def n_unique_hour_transformer(df):\n",
        "    xid_partition = Window.partitionBy('xid')\n",
        "    rank_hour = func.dense_rank().over(xid_partition.orderBy('hour'))\n",
        "    n_unique_hour = func.last(rank_hour).over(xid_partition)\n",
        "    df = df.withColumn('n_unique_hour', n_unique_hour)\n",
        "    return df\n",
        "\n",
        "def n_events_per_device_transformer(df):\n",
        "    xid_device_partition = Window.partitionBy('xid', 'device')\n",
        "    n_events_per_device = func.count(func.col('device')) \\\n",
        "        .over(xid_device_partition)\n",
        "    df = df.withColumn('n_events_per_device', n_events_per_device)\n",
        "    return df\n",
        "\n",
        "def n_unique_device_transformer(df):\n",
        "    xid_partition = Window.partitionBy('xid')\n",
        "    rank_device = func.dense_rank().over(xid_partition.orderBy('device'))\n",
        "    n_unique_device = func.last(rank_device).over(xid_partition)\n",
        "    df = df.withColumn('n_device', n_unique_device)\n",
        "    return df\n",
        "\n",
        "def n_actions_per_category_id_transformer(df):\n",
        "    xid_category_id_partition = Window.partitionBy('xid', 'category_id',\n",
        "                                                   'action')\n",
        "    n_actions_per_category_id = func.count(func.col('action')) \\\n",
        "        .over(xid_category_id_partition)\n",
        "    df = df.withColumn('n_actions_per_category_id', n_actions_per_category_id)\n",
        "    return df\n",
        "\n",
        "def n_unique_category_id_transformer(df):\n",
        "    xid_partition = Window.partitionBy('xid')\n",
        "    rank_category_id = func.dense_rank().over(xid_partition\\\n",
        "                                              .orderBy('category_id'))\n",
        "    n_unique_category_id = func.last(rank_category_id).over(xid_partition)\n",
        "    df = df.withColumn('n_unique_category_id', n_unique_category_id)\n",
        "    return df\n",
        "\n",
        "def n_events_per_category_id_transformer(df):\n",
        "    xid_category_id_partition = Window.partitionBy('xid', 'category_id')\n",
        "    n_events_per_category_id = func.count(func.col('action')) \\\n",
        "        .over(xid_category_id_partition)\n",
        "    df = df.withColumn('n_events_per_category_id', n_events_per_category_id)\n",
        "    return df\n",
        "\n",
        "def n_events_per_website_id_transformer(df):\n",
        "    xid_website_id_partition = Window.partitionBy('xid', 'website_id')\n",
        "    n_events_per_website_id = func.count(col('action'))\\\n",
        "        .over(xid_website_id_partition)\n",
        "    df = df.withColumn('n_events_per_website_id', n_events_per_website_id)\n",
        "    return df"
      ],
      "id": "17281fd2"
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "transformers = [\n",
        "    hour_transformer,\n",
        "    weekday_transformer,\n",
        "    n_events_per_hour_transformer,\n",
        "    n_events_per_weekday_transformer,\n",
        "    n_days_since_last_event_transformer,\n",
        "    n_days_since_last_action_transformer,\n",
        "    n_unique_day_transformer,\n",
        "    n_unique_hour_transformer,\n",
        "    n_events_per_device_transformer,\n",
        "    n_unique_device_transformer,\n",
        "    n_actions_per_category_id_transformer,\n",
        "    n_events_per_category_id_transformer,\n",
        "    n_events_per_website_id_transformer,\n",
        "]\n",
        "\n",
        "for transformer in transformers:\n",
        "    df = transformer(df)\n",
        "\n",
        "df.head(n=1)"
      ],
      "id": "09a33774"
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "sorted(df.columns)"
      ],
      "id": "881402ac"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load step\n",
        "\n",
        "Here, we use all the previous computations (saved in the columns of the\n",
        "dataframe) to compute aggregated informations about each user."
      ],
      "id": "f71a0493-2620-40df-bedc-456fc454c554"
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "def n_events_per_hour_loader(df):\n",
        "    csr = df\\\n",
        "        .select('xid', 'hour', 'n_events_per_hour')\\\n",
        "        .withColumnRenamed('n_events_per_hour', 'value')\\\n",
        "        .distinct()\n",
        "    feature_name = func.concat(lit('n_events_per_hour#'), col('hour'))\n",
        "    csr = csr\\\n",
        "        .withColumn('feature_name', feature_name)\\\n",
        "        .drop('hour')\n",
        "    return csr\n",
        "\n",
        "def n_events_per_website_id_loader(df):\n",
        "    csr = df.select('xid', 'website_id', 'n_events_per_website_id')\\\n",
        "        .withColumnRenamed('n_events_per_hour', 'value')\\\n",
        "        .distinct()\n",
        "    feature_name = func.concat(lit('n_events_per_website_id#'),\n",
        "                               col('website_id'))\n",
        "    csr = csr\\\n",
        "        .withColumn('feature_name', feature_name)\\\n",
        "        .drop('website_id')\n",
        "    return csr\n",
        "\n",
        "def n_events_per_hour_loader(df):\n",
        "    csr = df\\\n",
        "        .select('xid', 'hour', 'n_events_per_hour')\\\n",
        "        .withColumnRenamed('n_events_per_hour', 'value')\\\n",
        "        .distinct()\n",
        "    feature_name = func.concat(lit('n_events_per_hour#'), col('hour'))\n",
        "    csr = csr\\\n",
        "        .withColumn('feature_name', feature_name)\\\n",
        "        .drop('hour')\n",
        "    return csr\n",
        "\n",
        "def n_events_per_weekday_loader(df):\n",
        "    csr = df\\\n",
        "        .select('xid', 'weekday', 'n_events_per_weekday')\\\n",
        "        .withColumnRenamed('n_events_per_weekday', 'value')\\\n",
        "        .distinct()\n",
        "    feature_name = func.concat(lit('n_events_per_weekday#'), col('weekday'))\n",
        "    csr = csr\\\n",
        "        .withColumn('feature_name', feature_name)\\\n",
        "        .drop('weekday')\n",
        "    return csr\n",
        "\n",
        "def n_days_since_last_event_loader(df):\n",
        "    csr = df.select('xid',  'n_days_since_last_event')\\\n",
        "        .withColumnRenamed('n_days_since_last_event#', 'value')\\\n",
        "        .distinct()\n",
        "    feature_name = lit('n_days_since_last_event')\n",
        "    csr = csr\\\n",
        "        .withColumn('feature_name', feature_name)\n",
        "    return csr\n",
        "\n",
        "def n_days_since_last_action_loader(df):\n",
        "    csr = df.select('xid', 'action', 'n_days_since_last_action')\\\n",
        "        .withColumnRenamed('n_days_since_last_action', 'value')\\\n",
        "        .distinct()\n",
        "    feature_name = func.concat(lit('n_days_since_last_action#'), col('action'))\n",
        "    csr = csr\\\n",
        "        .withColumn('feature_name', feature_name)\\\n",
        "        .drop('action')\n",
        "    return csr\n",
        "\n",
        "def n_unique_day_loader(df):\n",
        "    csr = df.select('xid', 'n_unique_day')\\\n",
        "        .withColumnRenamed('n_unique_day', 'value')\\\n",
        "        .distinct()\n",
        "    feature_name = lit('n_unique_day')\n",
        "    csr = csr\\\n",
        "        .withColumn('feature_name', feature_name)\n",
        "    return csr\n",
        "\n",
        "def n_unique_hour_loader(df):\n",
        "    csr = df.select('xid', 'n_unique_hour')\\\n",
        "        .withColumnRenamed('n_unique_hour', 'value')\\\n",
        "        .distinct()\n",
        "    feature_name = lit('n_unique_hour')\n",
        "    csr = csr\\\n",
        "        .withColumn('feature_name', feature_name)\n",
        "    return csr\n",
        "\n",
        "def n_events_per_device_loader(df):\n",
        "    csr = df\\\n",
        "        .select('xid', 'device', 'n_events_per_device')\\\n",
        "        .withColumnRenamed('n_events_per_device', 'value')\\\n",
        "        .distinct()\n",
        "    feature_name = func.concat(lit('n_events_per_device#'), col('device'))\n",
        "    csr = csr\\\n",
        "        .withColumn('feature_name', feature_name)\\\n",
        "        .drop('device')\n",
        "    return csr\n",
        "\n",
        "def n_unique_device_loader(df):\n",
        "    csr = df.select('xid', 'n_device')\\\n",
        "        .withColumnRenamed('n_device', 'value')\\\n",
        "        .distinct()\n",
        "    feature_name = lit('n_device')\n",
        "    csr = csr\\\n",
        "        .withColumn('feature_name', feature_name)\n",
        "    return csr\n",
        "\n",
        "def n_events_per_category_id_loader(df):\n",
        "    csr = df.select('xid', 'category_id', 'n_events_per_category_id')\\\n",
        "        .withColumnRenamed('n_events_per_category_id', 'value')\\\n",
        "        .distinct()\n",
        "    feature_name = func.concat(lit('n_events_per_category_id#'),\n",
        "                               col('category_id'))\n",
        "    csr = csr\\\n",
        "        .withColumn('feature_name', feature_name)\\\n",
        "        .drop('category_id')\n",
        "    return csr\n",
        "\n",
        "def n_actions_per_category_id_loader(df):\n",
        "    csr = df.select('xid', 'category_id', 'action', 'n_actions_per_category_id')\\\n",
        "        .withColumnRenamed('n_actions_per_category_id', 'value')\\\n",
        "        .distinct()\n",
        "    feature_name = func.concat(lit('n_actions_per_category_id#'),\n",
        "                               col('action'), lit('#'), \n",
        "                               col('category_id'))\n",
        "    csr = csr\\\n",
        "        .withColumn('feature_name', feature_name)\\\n",
        "        .drop('category_id')\\\n",
        "        .drop('action')\n",
        "    return csr\n",
        "\n",
        "def n_events_per_website_id_loader(df):\n",
        "    csr = df.select('xid', 'website_id', 'n_events_per_website_id')\\\n",
        "        .withColumnRenamed('n_events_per_website_id', 'value')\\\n",
        "        .distinct()\n",
        "    feature_name = func.concat(lit('n_events_per_website_id#'),\n",
        "                               col('website_id'))\n",
        "    csr = csr\\\n",
        "        .withColumn('feature_name', feature_name)\\\n",
        "        .drop('website_id')\n",
        "    return csr"
      ],
      "id": "0fa9d0e2"
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [],
      "source": [
        "from functools import reduce\n",
        "\n",
        "loaders = [\n",
        "    n_events_per_hour_loader,\n",
        "    n_events_per_website_id_loader,\n",
        "    n_events_per_hour_loader,\n",
        "    n_events_per_weekday_loader,\n",
        "    n_days_since_last_event_loader,\n",
        "    n_days_since_last_action_loader,\n",
        "    n_unique_day_loader,\n",
        "    n_unique_hour_loader,\n",
        "    n_events_per_device_loader,\n",
        "    n_unique_device_loader,\n",
        "    n_events_per_category_id_loader,\n",
        "    n_actions_per_category_id_loader,\n",
        "    n_events_per_website_id_loader,\n",
        "]\n",
        "\n",
        "def union(df, other):\n",
        "    return df.union(other)\n",
        "\n",
        "csr = reduce(\n",
        "    lambda df1, df2: df1.union(df2),\n",
        "    [loader(df) for loader in loaders]\n",
        ")\n",
        "\n",
        "csr.head(n=3)"
      ],
      "id": "bde3dede"
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [],
      "source": [
        "csr.columns"
      ],
      "id": "8c8c532c"
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [],
      "source": [
        "csr.count()"
      ],
      "id": "b50b778a"
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Replace features names and xid by a unique number\n",
        "feature_name_partition = Window().orderBy('feature_name')\n",
        "xid_partition = Window().orderBy('xid')\n",
        "\n",
        "col_idx = func.dense_rank().over(feature_name_partition)\n",
        "row_idx = func.dense_rank().over(xid_partition)\n",
        "\n",
        "csr = csr.withColumn('col', col_idx)\\\n",
        "    .withColumn('row', row_idx)\n",
        "\n",
        "csr = csr.na.drop('any')\n",
        "\n",
        "csr.head(n=5)"
      ],
      "id": "d89f0719"
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Let's save the result of our hard work into a new parquet file\n",
        "output_path = './'\n",
        "output_file = os.path.join(output_path, 'csr.parquet')\n",
        "csr.write.parquet(output_file, mode='overwrite')"
      ],
      "id": "e4ea40d6"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Preparation of the training dataset"
      ],
      "id": "a0af31c2-916c-4829-9597-6300a17b78f7"
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [],
      "source": [
        "csr_path = './'\n",
        "csr_file = os.path.join(csr_path, 'csr.parquet')\n",
        "\n",
        "df = spark.read.parquet(csr_file)\n",
        "df.head(n=5)"
      ],
      "id": "64df2c98"
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [],
      "source": [
        "df.count()"
      ],
      "id": "e36d514c"
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [],
      "source": [
        "# What are the features related to campaign_id 1204 ?\n",
        "features_names = \\\n",
        "    df.select('feature_name')\\\n",
        "    .distinct()\\\n",
        "    .toPandas()['feature_name']"
      ],
      "id": "11ce228c"
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [],
      "source": [
        "features_names"
      ],
      "id": "e3647d64"
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [],
      "source": [
        "[feature_name for feature_name in features_names if '1204' in feature_name]"
      ],
      "id": "2da9763b"
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Look for the xid that have at least one exposure to campaign 1204\n",
        "keep = func.when(\n",
        "    (col('feature_name') == 'n_actions_per_category_id#C#1204.0') |\n",
        "    (col('feature_name') == 'n_actions_per_category_id#O#1204.0'),\n",
        "    1).otherwise(0)\n",
        "df = df.withColumn('keep', keep)\n",
        "\n",
        "df.where(col('keep') > 0).count()"
      ],
      "id": "2b49cda6"
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Sum of the keeps :)\n",
        "xid_partition = Window.partitionBy('xid')\n",
        "sum_keep = func.sum(col('keep')).over(xid_partition)\n",
        "df = df.withColumn('sum_keep', sum_keep)"
      ],
      "id": "94d08cc0"
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Let's keep the xid exposed to 1204\n",
        "df = df.where(col('sum_keep') > 0)"
      ],
      "id": "1a14730d"
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [],
      "source": [
        "df.count()"
      ],
      "id": "35eda6cd"
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [],
      "source": [
        "df.select('xid').distinct().count()"
      ],
      "id": "96b8b970"
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [],
      "source": [
        "row_partition = Window().orderBy('row')\n",
        "col_partition = Window().orderBy('col')\n",
        "row_new = func.dense_rank().over(row_partition)\n",
        "col_new = func.dense_rank().over(col_partition)\n",
        "df = df.withColumn('row_new', row_new)\n",
        "df = df.withColumn('col_new', col_new)\n",
        "csr_data = df.select('row_new', 'col_new', 'value').toPandas()"
      ],
      "id": "5d12f76f"
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [],
      "source": [
        "csr_data.head()"
      ],
      "id": "24f81387"
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [],
      "source": [
        "features_names = df.select('feature_name', 'col_new').distinct()\n",
        "features_names.where(col('feature_name') == 'n_actions_per_category_id#C#1204.0').head()"
      ],
      "id": "d7ec2bad"
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [],
      "source": [
        "features_names.where(col('feature_name') == 'n_actions_per_category_id#O#1204.0').head()"
      ],
      "id": "dba73f68"
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [],
      "source": [
        "from scipy.sparse import csr_matrix\n",
        "import numpy as np\n",
        "\n",
        "rows = csr_data['row_new'].values - 1\n",
        "cols = csr_data['col_new'].values - 1\n",
        "vals = csr_data['value'].values\n",
        "\n",
        "X_csr = csr_matrix((vals, (rows, cols)))"
      ],
      "id": "10126386"
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_csr.shape"
      ],
      "id": "5bfb7272"
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_csr.shape, X_csr.nnz"
      ],
      "id": "74e2dd38"
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_csr.nnz / (152347 * 92)"
      ],
      "id": "d1932b32"
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {},
      "outputs": [],
      "source": [
        "# The label vector. Let's make it dense, flat and binary\n",
        "y = np.array(X_csr[:, 1].todense()).ravel()\n",
        "y = np.array(y > 0, dtype=np.int64)"
      ],
      "id": "3d278b1e"
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_csr.shape"
      ],
      "id": "67045821"
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {},
      "outputs": [],
      "source": [
        "# We remove the second and fourth column. \n",
        "# It actually contain the label we'll want to predict.\n",
        "kept_cols = list(range(92))\n",
        "kept_cols.pop(1)\n",
        "kept_cols.pop(2)\n",
        "X = X_csr[:, kept_cols]"
      ],
      "id": "466d70bc"
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_csr.shape"
      ],
      "id": "a64404ca"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Finally !!\n",
        "\n",
        "Wow ! That was a lot of work. Now we have a features matrix $X$ and a\n",
        "vector of labels $y$."
      ],
      "id": "6ea0715c-c510-458a-a558-b97fce9e6e14"
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {},
      "outputs": [],
      "source": [
        "X.indices"
      ],
      "id": "d0d5e084"
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {},
      "outputs": [],
      "source": [
        "X.indptr"
      ],
      "id": "6bdb0151"
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {},
      "outputs": [],
      "source": [
        "X.shape, X.nnz"
      ],
      "id": "b8df722b"
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {},
      "outputs": [],
      "source": [
        "y.shape, y.sum()"
      ],
      "id": "2d7871f4"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Some learning for this data"
      ],
      "id": "66f8915c-46f3-45f5-b957-ec1ef60cae6e"
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import MaxAbsScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Normalize the features\n",
        "X = MaxAbsScaler().fit_transform(X)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.3)\n",
        "\n",
        "clf = LogisticRegression(\n",
        "    penalty='l2',\n",
        "    C=1e3,\n",
        "    solver='lbfgs',\n",
        "    class_weight='balanced'\n",
        ")\n",
        "clf.fit(X_train, y_train)"
      ],
      "id": "432b408b"
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {},
      "outputs": [],
      "source": [
        "features_names = features_names.toPandas()['feature_name']"
      ],
      "id": "e9516e42"
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "plt.figure(figsize=(16, 5))\n",
        "plt.stem(clf.coef_[0], use_line_collection=True)\n",
        "plt.title('Logistic regression coefficients', fontsize=18)\n",
        "# We change the fontsize of minor ticks label\n",
        "_ = plt.xticks(np.arange(clf.coef_[0].shape[0]), features_names, \n",
        "           rotation='vertical', fontsize=8)\n",
        "_ = plt.yticks(fontsize=14)"
      ],
      "id": "4ad8e387"
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.metrics import precision_recall_curve, f1_score\n",
        "\n",
        "precision, recall, _ = precision_recall_curve(y_test, clf.predict_proba(X_test)[:, 1])\n",
        "    \n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(recall, precision, label='LR (F1=%.2f)' % f1_score(y_test, clf.predict(X_test)), lw=2)\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('Recall', fontsize=16)\n",
        "plt.ylabel('Precision', fontsize=16)\n",
        "plt.title('Precision/recall curve', fontsize=18)\n",
        "plt.legend(loc=\"upper right\", fontsize=14)"
      ],
      "id": "255eef79"
    }
  ],
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "path": "/usr/share/jupyter/kernels/python3"
    },
    "language_info": {
      "name": "python",
      "codemirror_mode": {
        "name": "ipython",
        "version": "3"
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  }
}