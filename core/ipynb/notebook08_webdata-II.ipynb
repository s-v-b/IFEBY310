{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Using with `pyspark` for data preprocessing\n",
        "\n",
        "## Data description\n",
        "\n",
        "The data is a `parquet` file which contains a dataframe with 8 columns:\n",
        "\n",
        "-   `xid`: unique user id\n",
        "-   `action`: type of action. ‘C’ is a click, ‘O’ or ‘VSL’ is a\n",
        "    web-display\n",
        "-   `date`: date of the action\n",
        "-   `website_id`: unique id of the website\n",
        "-   `url`: url of the webpage\n",
        "-   `category_id`: id of the display\n",
        "-   `zipcode`: postal zipcode of the user\n",
        "-   `device`: type of device used by the user\n",
        "\n",
        "## Q1. Some statistics / computations\n",
        "\n",
        "Using `pyspark.sql` we want to do the following things:\n",
        "\n",
        "1.  Compute the total number of unique users\n",
        "2.  Construct a column containing the total number of actions per user\n",
        "3.  Construct a column containing the number of days since the last\n",
        "    action of the user\n",
        "4.  Construct a column containing the number of actions of each user for\n",
        "    each modality of device\n",
        "\n",
        "## Q2. Feature engineering\n",
        "\n",
        "Then, we want to construct a classifier to predict the click on the\n",
        "category 1204. Here is an agenda for this:\n",
        "\n",
        "1.  Construction of a features matrix for which each line corresponds to\n",
        "    the information concerning a user.\n",
        "2.  In this matrix, we need to keep only the users that have been\n",
        "    exposed to the display in category 1204\n",
        "\n",
        "## Q3. Classification\n",
        "\n",
        "1.  Using this training dataset, train a binary classifier, and evaluate\n",
        "    your classifier using a precision / recall curve computed on test\n",
        "    data.\n",
        "\n",
        "# Download/read the data and a first look at the data"
      ],
      "id": "cf25524e-17e9-4c17-92cb-0ffc75b654e8"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "\n",
        "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
        "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable"
      ],
      "id": "83d04014"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> **Spark in local mode**\n",
        ">\n",
        "> ``` python\n",
        "> from pyspark import SparkConf, SparkContext\n",
        "> from pyspark.sql import SparkSession\n",
        ">\n",
        "> spark = (SparkSession\n",
        ">     .builder\n",
        ">     .appName(\"Spark Webdata\")\n",
        ">     .getOrCreate()\n",
        "> )\n",
        "> ```\n",
        ">\n",
        "> ``` python\n",
        "> import requests, zipfile, io\n",
        "> from pathlib import Path\n",
        ">\n",
        "> path = Path('webdata.parquet')\n",
        "> if not path.exists():\n",
        ">     url = \"https://s-v-b.github.io/IFEBY310/data/webdata.parquet.zip\"\n",
        ">     r = requests.get(url)\n",
        ">     z = zipfile.ZipFile(io.BytesIO(r.content))\n",
        ">     z.extractall(path='./')\n",
        "> ```\n",
        ">\n",
        "> ``` python\n",
        "> input_path = './'\n",
        ">\n",
        "> input_file = os.path.join(input_path, 'webdata.parquet')\n",
        ">\n",
        "> df = spark.read.parquet(input_file)\n",
        "> ```\n",
        "\n",
        "> **Note**\n",
        ">\n",
        "> We can also give a try to `pyarrow.parquet` module to load the Parquet\n",
        "> file in an Arrow table."
      ],
      "id": "e3fee08b-9106-496b-9c9f-67c28b5d4f41"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pyarrow as pa\n",
        "import comet    as co\n",
        "import pyarrow.parquet as pq\n",
        "\n",
        "dfa = pq.read_table(input_file)"
      ],
      "id": "d4e927bf"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "dfa.num_columns"
      ],
      "id": "059d8ede"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ":::\n",
        "\n",
        "> **Warning**\n",
        ">\n",
        "> Let us go back to the spark data frame"
      ],
      "id": "ee78f344-1400-4b0a-b76b-bbd8c1d4ee80"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "df.printSchema()"
      ],
      "id": "f86161ef"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "df.rdd.getNumPartitions()"
      ],
      "id": "ea371921"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> **Question**\n",
        ">\n",
        "> Explain the partition size."
      ],
      "id": "d394a6a6-5ed1-4b15-b390-fd129fc624cd"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "df.rdd.toDebugString()"
      ],
      "id": "f196520d"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Basic statistics\n",
        "\n",
        "First we need to import some things:\n",
        "\n",
        "-   `Window` class\n",
        "-   SQL functions module\n",
        "-   Some very useful functions\n",
        "-   Spark types"
      ],
      "id": "95237708-1dfe-4328-9214-d1924e2bd65c"
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pyspark.sql import Window\n",
        "import pyspark.sql.functions as func\n",
        "from pyspark.sql.types import *\n",
        "from pyspark.sql.functions import col, lit"
      ],
      "id": "bc6d236d"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Compute the total number of unique users"
      ],
      "id": "566bb359-346d-46d5-9928-769a8288a056"
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "( \n",
        "    df.select('xid')\n",
        "      .distinct()\n",
        "      .count()\n",
        ")"
      ],
      "id": "b6f16924"
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "def foo(x): yield len(set(x))"
      ],
      "id": "bc34e278"
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "( df.rdd\n",
        "    .map(lambda x : x.xid)\n",
        "    .mapPartitions(foo)\n",
        "    .collect()\n",
        ")"
      ],
      "id": "1454ce37"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This might pump up some computational resources"
      ],
      "id": "02744a4d-c8d8-406e-b6fa-37afafed9400"
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "( \n",
        "    df.select('xid')\n",
        "      .distinct() \n",
        "      .explain()\n",
        ")"
      ],
      "id": "497110fb"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> **Note**\n",
        ">\n",
        "> The distinct values of `xid` seem to be evenly spread among the six\n",
        "> files making the `parquet` directory. Note that the last six\n",
        "> partitions look empty.\n",
        "\n",
        "## Construct a column containing the total number of actions per user"
      ],
      "id": "4bc1735e-5241-4fff-9bd1-2c4d9ec80835"
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "xid_partition = Window.partitionBy('xid')\n",
        "\n",
        "n_events = func.count(col('action')).over(xid_partition)\n",
        "\n",
        "df = df.withColumn('n_events', n_events)\n",
        "\n",
        "df.head(n=2)"
      ],
      "id": "8ead6642"
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "( \n",
        "  df\n",
        "    .groupBy('xid')\n",
        "    .agg(func.count('action'))\n",
        "    .head(5)\n",
        ")"
      ],
      "id": "40efd60d"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Construct a column containing the number of days since the last action of the user"
      ],
      "id": "4e5cd5f9-aa1b-48c4-80ee-0b2634e4ce60"
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "max_date = (\n",
        "  func\n",
        "    .max(col('date'))\n",
        "    .over(xid_partition)\n",
        ")\n",
        "\n",
        "n_days_since_last_event = func.datediff(func.current_date(), max_date)\n",
        "\n",
        "df = df.withColumn('n_days_since_last_event',\n",
        "                   n_days_since_last_event)\n",
        "\n",
        "df.head(n=2)"
      ],
      "id": "8857c1d5"
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "df.printSchema()"
      ],
      "id": "996e0878"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Construct a column containing the number of actions of each user for each modality of device\n",
        "\n",
        "Does this `partitionBy` triggers shuffling?"
      ],
      "id": "9b514594-ee53-4055-912d-54dd0e108aff"
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "xid_device_partition = xid_partition.partitionBy('device')\n",
        "\n",
        "n_events_per_device = func.count(col('action')).over(xid_device_partition)\n",
        "\n",
        "df = df.withColumn('n_events_per_device', n_events_per_device)\n",
        "\n",
        "df.head(n=2)"
      ],
      "id": "4d92f8b2"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Number of devices per user"
      ],
      "id": "3c974503-9ca7-4e12-b83f-45ebe9536fe8"
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "# xid_partition = Window.partitionBy('xid')\n",
        "\n",
        "rank_device = (\n",
        "  func\n",
        "    .dense_rank()\n",
        "    .over(xid_partition.orderBy('device'))\n",
        ")\n",
        "\n",
        "n_unique_device = (\n",
        "    func\n",
        "      .last(rank_device)\n",
        "      .over(xid_partition)\n",
        ")\n",
        "\n",
        "df = df.withColumn('n_device', n_unique_device)\n",
        "\n",
        "df.head(n=2)"
      ],
      "id": "2dc60bd7"
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "df\\\n",
        "    .where(col('n_device') > 1)\\\n",
        "    .select('xid', 'device', 'n_events',  'n_device', 'n_events_per_device')\\\n",
        "    .head(n=8)"
      ],
      "id": "590ba6b6"
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "df\\\n",
        "    .where(col('n_device') > 1)\\\n",
        "    .select('xid', 'device', 'n_events',  'n_device', 'n_events_per_device')\\\n",
        "    .count()"
      ],
      "id": "87663b5a"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Let’s select the correct users and build a training dataset\n",
        "\n",
        "We construct a ETL (Extract Transform Load) process on this data using\n",
        "the `pyspark.sql` API.\n",
        "\n",
        "## Extraction\n",
        "\n",
        "Here extraction is just about reading the data"
      ],
      "id": "f24de07f-1f4b-4c76-9707-57daffa10907"
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "df = spark.read.parquet(input_file)\n",
        "df.head(n=3)"
      ],
      "id": "8bb89fab"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Transformation of the data\n",
        "\n",
        "At this step we compute a lot of extra things from the data. The aim is\n",
        "to build *features* that describe users."
      ],
      "id": "0196e0c6-422f-496f-a11a-75f6f08efe96"
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [],
      "source": [
        "def n_events_transformer(df):\n",
        "    xid_partition = Window.partitionBy('xid')\n",
        "    n_events = func.count(col('action')).over(xid_partition)\n",
        "    \n",
        "    df = df.withColumn('n_events', n_events)\n",
        "\n",
        "    return df"
      ],
      "id": "29f06018"
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [],
      "source": [
        "def n_events_per_action_transformer(df):\n",
        "    xid_action_partition = Window.partitionBy('xid', 'action')\n",
        "    n_events_per_action = func.count(col('action')).over(xid_action_partition)\n",
        "\n",
        "    df = df.withColumn('n_events_per_action', n_events_per_action)\n",
        "    \n",
        "    return df"
      ],
      "id": "1fa6f87d"
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [],
      "source": [
        "def hour_transformer(df):\n",
        "    hour = func.hour(col('date'))\n",
        "    df = df.withColumn('hour', hour)\n",
        "    return df\n",
        "\n",
        "def weekday_transformer(df):\n",
        "    weekday = func.date_format(col('date'), 'EEEE')\n",
        "    df = df.withColumn('weekday', weekday)\n",
        "    return df\n",
        "\n",
        "def n_events_per_hour_transformer(df):\n",
        "    xid_hour_partition = Window.partitionBy('xid', 'hour')\n",
        "    n_events_per_hour = func.count(col('action')).over(xid_hour_partition)\n",
        "    df = df.withColumn('n_events_per_hour', n_events_per_hour)\n",
        "    return df\n",
        "\n",
        "def n_events_per_weekday_transformer(df):\n",
        "    xid_weekday_partition = Window.partitionBy('xid', 'weekday')\n",
        "    n_events_per_weekday = func.count(col('action')).over(xid_weekday_partition)\n",
        "    df = df.withColumn('n_events_per_weekday', n_events_per_weekday)\n",
        "    return df\n",
        "\n",
        "def n_days_since_last_event_transformer(df):\n",
        "    xid_partition = Window.partitionBy('xid')\n",
        "    max_date = func.max(col('date')).over(xid_partition)\n",
        "    n_days_since_last_event = func.datediff(func.current_date(), max_date)\n",
        "    df = df.withColumn('n_days_since_last_event',\n",
        "                       n_days_since_last_event + lit(0.1))\n",
        "    return df\n",
        "\n",
        "def n_days_since_last_action_transformer(df):\n",
        "    xid_partition_action = Window.partitionBy('xid', 'action')\n",
        "    max_date = func.max(col('date')).over(xid_partition_action)\n",
        "    n_days_since_last_action = func.datediff(func.current_date(),\n",
        "                                                        max_date)\n",
        "    df = df.withColumn('n_days_since_last_action',\n",
        "                       n_days_since_last_action + lit(0.1))\n",
        "    return df\n",
        "\n",
        "def n_unique_day_transformer(df):\n",
        "    xid_partition = Window.partitionBy('xid')\n",
        "    dayofyear = func.dayofyear(col('date'))\n",
        "    rank_day = func.dense_rank().over(xid_partition.orderBy(dayofyear))\n",
        "    n_unique_day = func.last(rank_day).over(xid_partition)\n",
        "    df = df.withColumn('n_unique_day', n_unique_day)\n",
        "    return df\n",
        "\n",
        "def n_unique_hour_transformer(df):\n",
        "    xid_partition = Window.partitionBy('xid')\n",
        "    rank_hour = func.dense_rank().over(xid_partition.orderBy('hour'))\n",
        "    n_unique_hour = func.last(rank_hour).over(xid_partition)\n",
        "    df = df.withColumn('n_unique_hour', n_unique_hour)\n",
        "    return df\n",
        "\n",
        "def n_events_per_device_transformer(df):\n",
        "    xid_device_partition = Window.partitionBy('xid', 'device')\n",
        "    n_events_per_device = func.count(func.col('device')) \\\n",
        "        .over(xid_device_partition)\n",
        "    df = df.withColumn('n_events_per_device', n_events_per_device)\n",
        "    return df\n",
        "\n",
        "def n_unique_device_transformer(df):\n",
        "    xid_partition = Window.partitionBy('xid')\n",
        "    rank_device = func.dense_rank().over(xid_partition.orderBy('device'))\n",
        "    n_unique_device = func.last(rank_device).over(xid_partition)\n",
        "    df = df.withColumn('n_device', n_unique_device)\n",
        "    return df\n",
        "\n",
        "def n_actions_per_category_id_transformer(df):\n",
        "    xid_category_id_partition = Window.partitionBy('xid', 'category_id',\n",
        "                                                   'action')\n",
        "    n_actions_per_category_id = func.count(func.col('action')) \\\n",
        "        .over(xid_category_id_partition)\n",
        "    df = df.withColumn('n_actions_per_category_id', n_actions_per_category_id)\n",
        "    return df\n",
        "\n",
        "def n_unique_category_id_transformer(df):\n",
        "    xid_partition = Window.partitionBy('xid')\n",
        "    rank_category_id = func.dense_rank().over(xid_partition\\\n",
        "                                              .orderBy('category_id'))\n",
        "    n_unique_category_id = func.last(rank_category_id).over(xid_partition)\n",
        "    df = df.withColumn('n_unique_category_id', n_unique_category_id)\n",
        "    return df\n",
        "\n",
        "def n_events_per_category_id_transformer(df):\n",
        "    xid_category_id_partition = Window.partitionBy('xid', 'category_id')\n",
        "    n_events_per_category_id = func.count(func.col('action')) \\\n",
        "        .over(xid_category_id_partition)\n",
        "    df = df.withColumn('n_events_per_category_id', n_events_per_category_id)\n",
        "    return df\n",
        "\n",
        "def n_events_per_website_id_transformer(df):\n",
        "    xid_website_id_partition = Window.partitionBy('xid', 'website_id')\n",
        "    n_events_per_website_id = func.count(col('action'))\\\n",
        "        .over(xid_website_id_partition)\n",
        "    df = df.withColumn('n_events_per_website_id', n_events_per_website_id)\n",
        "    return df"
      ],
      "id": "7fb9d4ae"
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [],
      "source": [
        "transformers = [\n",
        "    hour_transformer,\n",
        "    weekday_transformer,\n",
        "    n_events_per_hour_transformer,\n",
        "    n_events_per_weekday_transformer,\n",
        "    n_days_since_last_event_transformer,\n",
        "    n_days_since_last_action_transformer,\n",
        "    n_unique_day_transformer,\n",
        "    n_unique_hour_transformer,\n",
        "    n_events_per_device_transformer,\n",
        "    n_unique_device_transformer,\n",
        "    n_actions_per_category_id_transformer,\n",
        "    n_events_per_category_id_transformer,\n",
        "    n_events_per_website_id_transformer,\n",
        "]"
      ],
      "id": "c842082c"
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [],
      "source": [
        "N = 10000"
      ],
      "id": "6b2c61e3"
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [],
      "source": [
        "sample_df = df.sample(withReplacement=False, fraction=.05)"
      ],
      "id": "fec8f049"
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [],
      "source": [
        "sample_df.count()"
      ],
      "id": "66d8ac01"
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [],
      "source": [
        "for transformer in transformers:\n",
        "    df = transformer(df)\n",
        "\n",
        "df.head(n=1)"
      ],
      "id": "9cbfd51e"
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [],
      "source": [
        "for transformer in transformers:\n",
        "    sample_df = transformer(sample_df)\n",
        "\n",
        "sample_df.head(n=1)"
      ],
      "id": "92470182"
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [],
      "source": [
        "df = sample_df"
      ],
      "id": "8009fdff"
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [],
      "source": [
        "sorted(df.columns)"
      ],
      "id": "92668c90"
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [],
      "source": [
        "df.explain()"
      ],
      "id": "9f6d0b94"
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [],
      "source": [
        "spark._sc.setCheckpointDir(\".\")   \n",
        "\n",
        "df.checkpoint()"
      ],
      "id": "16d82aed"
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [],
      "source": [
        "df.explain()"
      ],
      "id": "c8021f51"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load step\n",
        "\n",
        "Here, we use all the previous computations (saved in the columns of the\n",
        "dataframe) to compute aggregated informations about each user.\n",
        "\n",
        "> **Note**\n",
        ">\n",
        "> This should be DRYED"
      ],
      "id": "ba64ab05-ac48-4ab3-966e-380f91f291f9"
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [],
      "source": [
        "def n_events_per_hour_loader(df):\n",
        "    csr = df\\\n",
        "        .select('xid', 'hour', 'n_events_per_hour')\\\n",
        "        .withColumnRenamed('n_events_per_hour', 'value')\\\n",
        "        .distinct() \n",
        "            # action\n",
        "    feature_name = func.concat(lit('n_events_per_hour#'), col('hour'))\n",
        "\n",
        "    csr = csr\\\n",
        "        .withColumn('feature_name', feature_name)\\\n",
        "        .drop('hour')\n",
        "    return csr\n",
        "\n",
        "def n_events_per_website_id_loader(df):\n",
        "    csr = df.select('xid', 'website_id', 'n_events_per_website_id')\\\n",
        "        .withColumnRenamed('n_events_per_hour', 'value')\\\n",
        "        .distinct()\n",
        "\n",
        "    feature_name = func.concat(lit('n_events_per_website_id#'),\n",
        "                               col('website_id'))\n",
        "    \n",
        "    csr = csr\\\n",
        "        .withColumn('feature_name', feature_name)\\\n",
        "        .drop('website_id')\n",
        "    return csr\n",
        "\n",
        "def n_events_per_hour_loader(df):\n",
        "    csr = df\\\n",
        "        .select('xid', 'hour', 'n_events_per_hour')\\\n",
        "        .withColumnRenamed('n_events_per_hour', 'value')\\\n",
        "        .distinct()\n",
        "\n",
        "    feature_name = func.concat(lit('n_events_per_hour#'), col('hour'))\n",
        "    \n",
        "    csr = csr\\\n",
        "        .withColumn('feature_name', feature_name)\\\n",
        "        .drop('hour')\n",
        "    return csr\n",
        "\n",
        "def n_events_per_weekday_loader(df):\n",
        "    csr = df\\\n",
        "        .select('xid', 'weekday', 'n_events_per_weekday')\\\n",
        "        .withColumnRenamed('n_events_per_weekday', 'value')\\\n",
        "        .distinct()\n",
        "\n",
        "    feature_name = func.concat(lit('n_events_per_weekday#'), col('weekday'))\n",
        "    \n",
        "    csr = csr\\\n",
        "        .withColumn('feature_name', feature_name)\\\n",
        "        .drop('weekday')\n",
        "\n",
        "    return csr\n",
        "\n",
        "def n_days_since_last_event_loader(df):\n",
        "    csr = df.select('xid',  'n_days_since_last_event')\\\n",
        "        .withColumnRenamed('n_days_since_last_event', 'value')\\\n",
        "        .distinct()\n",
        "    feature_name = lit('n_days_since_last_event')\n",
        "    csr = csr\\\n",
        "        .withColumn('feature_name', feature_name)\n",
        "    return csr\n",
        "\n",
        "def n_days_since_last_action_loader(df):\n",
        "    csr = df.select('xid', 'action', 'n_days_since_last_action')\\\n",
        "        .withColumnRenamed('n_days_since_last_action', 'value')\\\n",
        "        .distinct()\n",
        "    feature_name = func.concat(lit('n_days_since_last_action#'), col('action'))\n",
        "    csr = csr\\\n",
        "        .withColumn('feature_name', feature_name)\\\n",
        "        .drop('action')\n",
        "    return csr\n",
        "\n",
        "def n_unique_day_loader(df):\n",
        "    csr = df.select('xid', 'n_unique_day')\\\n",
        "        .withColumnRenamed('n_unique_day', 'value')\\\n",
        "        .distinct()\n",
        "    feature_name = lit('n_unique_day')\n",
        "    csr = csr\\\n",
        "        .withColumn('feature_name', feature_name)\n",
        "    return csr\n",
        "\n",
        "def n_unique_hour_loader(df):\n",
        "    csr = df.select('xid', 'n_unique_hour')\\\n",
        "        .withColumnRenamed('n_unique_hour', 'value')\\\n",
        "        .distinct()\n",
        "    feature_name = lit('n_unique_hour')\n",
        "    csr = csr\\\n",
        "        .withColumn('feature_name', feature_name)\n",
        "    return csr\n",
        "\n",
        "def n_events_per_device_loader(df):\n",
        "    csr = df\\\n",
        "        .select('xid', 'device', 'n_events_per_device')\\\n",
        "        .withColumnRenamed('n_events_per_device', 'value')\\\n",
        "        .distinct()\n",
        "    feature_name = func.concat(lit('n_events_per_device#'), col('device'))\n",
        "    csr = csr\\\n",
        "        .withColumn('feature_name', feature_name)\\\n",
        "        .drop('device')\n",
        "    return csr\n",
        "\n",
        "def n_unique_device_loader(df):\n",
        "    csr = df.select('xid', 'n_device')\\\n",
        "        .withColumnRenamed('n_device', 'value')\\\n",
        "        .distinct()\n",
        "    feature_name = lit('n_device')\n",
        "    csr = csr\\\n",
        "        .withColumn('feature_name', feature_name)\n",
        "    return csr\n",
        "\n",
        "def n_events_per_category_id_loader(df):\n",
        "    csr = df.select('xid', 'category_id', 'n_events_per_category_id')\\\n",
        "        .withColumnRenamed('n_events_per_category_id', 'value')\\\n",
        "        .distinct()\n",
        "    feature_name = func.concat(lit('n_events_per_category_id#'),\n",
        "                               col('category_id'))\n",
        "    csr = csr\\\n",
        "        .withColumn('feature_name', feature_name)\\\n",
        "        .drop('category_id')\n",
        "    return csr\n",
        "\n",
        "def n_actions_per_category_id_loader(df):\n",
        "    csr = df.select('xid', 'category_id', 'action', 'n_actions_per_category_id')\\\n",
        "        .withColumnRenamed('n_actions_per_category_id', 'value')\\\n",
        "        .distinct()\n",
        "    feature_name = func.concat(lit('n_actions_per_category_id#'),\n",
        "                               col('action'), lit('#'), \n",
        "                               col('category_id'))\n",
        "    csr = csr\\\n",
        "        .withColumn('feature_name', feature_name)\\\n",
        "        .drop('category_id')\\\n",
        "        .drop('action')\n",
        "    return csr\n",
        "\n",
        "def n_events_per_website_id_loader(df):\n",
        "    csr = df.select('xid', 'website_id', 'n_events_per_website_id')\\\n",
        "        .withColumnRenamed('n_events_per_website_id', 'value')\\\n",
        "        .distinct()\n",
        "    feature_name = func.concat(lit('n_events_per_website_id#'),\n",
        "                               col('website_id'))\n",
        "    csr = csr\\\n",
        "        .withColumn('feature_name', feature_name)\\\n",
        "        .drop('website_id')\n",
        "    return csr"
      ],
      "id": "83282bc7"
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [],
      "source": [
        "from functools import reduce"
      ],
      "id": "24a91d78"
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [],
      "source": [
        "loaders = [\n",
        "    n_events_per_hour_loader,\n",
        "    n_events_per_website_id_loader,\n",
        "    n_events_per_hour_loader,\n",
        "    n_events_per_weekday_loader,\n",
        "    n_days_since_last_event_loader,\n",
        "    n_days_since_last_action_loader,\n",
        "    n_unique_day_loader,\n",
        "    n_unique_hour_loader,\n",
        "    n_events_per_device_loader,\n",
        "    n_unique_device_loader,\n",
        "    n_events_per_category_id_loader,\n",
        "    n_actions_per_category_id_loader,\n",
        "    n_events_per_website_id_loader,\n",
        "]"
      ],
      "id": "e24ae6a2"
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [],
      "source": [
        "def union(df, other):\n",
        "    return df.union(other)"
      ],
      "id": "8e35a2f4"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> **About DataFrame.union()**\n",
        ">\n",
        "> This method performs a SQL-style set union of the rows from both\n",
        "> DataFrame objects, with no automatic deduplication of elements.\n",
        ">\n",
        "> Use the distinct() method to perform deduplication of rows.\n",
        ">\n",
        "> The method resolves columns by position (not by name), following the\n",
        "> standard behavior in SQL."
      ],
      "id": "8f5a0471-4ea1-4eea-a0c4-b25517517539"
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [],
      "source": [
        "spam = [loader(df) for loader in loaders]"
      ],
      "id": "3dd51587"
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [],
      "source": [
        "spam[0].printSchema()"
      ],
      "id": "9388e742"
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {},
      "outputs": [],
      "source": [
        "all(spam[0].columns == it.columns for it in spam[1:])"
      ],
      "id": "3f0b338e"
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {},
      "outputs": [],
      "source": [
        "len(spam)"
      ],
      "id": "513385e5"
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {},
      "outputs": [],
      "source": [
        "csr = reduce(\n",
        "    lambda df1, df2: df1.union(df2),\n",
        "    spam\n",
        ")\n",
        "\n",
        "csr.head(n=3)"
      ],
      "id": "8de5d43a"
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "quarto-private-1": {
          "key": "ExecuteTime",
          "value": {
            "end_time": "2020-05-03T15:25:54.862814Z",
            "start_time": "2020-05-03T15:25:54.857914Z"
          }
        }
      },
      "outputs": [],
      "source": [
        "csr.columns"
      ],
      "id": "9ac739e3"
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "quarto-private-1": {
          "key": "ExecuteTime",
          "value": {
            "end_time": "2020-05-03T15:26:13.629146Z",
            "start_time": "2020-05-03T15:25:55.683800Z"
          }
        }
      },
      "outputs": [],
      "source": [
        "csr.show(5)"
      ],
      "id": "66bca32f"
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {},
      "outputs": [],
      "source": [
        "csr.rdd.getNumPartitions()"
      ],
      "id": "7ed032d5"
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "quarto-private-1": {
          "key": "ExecuteTime",
          "value": {
            "end_time": "2020-05-03T15:30:20.643141Z",
            "start_time": "2020-05-03T15:29:45.221790Z"
          }
        }
      },
      "outputs": [],
      "source": [
        "# Replace features names and xid by a unique number\n",
        "feature_name_partition = Window().orderBy('feature_name')\n",
        "\n",
        "xid_partition = Window().orderBy('xid')\n",
        "\n",
        "col_idx = func.dense_rank().over(feature_name_partition)\n",
        "row_idx = func.dense_rank().over(xid_partition)"
      ],
      "id": "6f804b78"
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "quarto-private-1": {
          "key": "ExecuteTime",
          "value": {
            "end_time": "2020-05-03T15:30:20.643141Z",
            "start_time": "2020-05-03T15:29:45.221790Z"
          }
        }
      },
      "outputs": [],
      "source": [
        "csr = csr.withColumn('col', col_idx)\\\n",
        "    .withColumn('row', row_idx)\n",
        "\n",
        "csr = csr.na.drop('any')\n",
        "\n",
        "csr.head(n=5)"
      ],
      "id": "0f57efe4"
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "quarto-private-1": {
          "key": "ExecuteTime",
          "value": {
            "end_time": "2020-05-03T15:32:02.552364Z",
            "start_time": "2020-05-03T15:31:14.990298Z"
          }
        }
      },
      "outputs": [],
      "source": [
        "# Let's save the result of our hard work into a new parquet file\n",
        "output_path = './'\n",
        "output_file = os.path.join(output_path, 'csr.parquet')\n",
        "csr.write.parquet(output_file, mode='overwrite')"
      ],
      "id": "4be27c1d"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Preparation of the training dataset"
      ],
      "id": "050fcf3d-94a8-47f8-8cb8-8a9c143c9a1e"
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {},
      "outputs": [],
      "source": [
        "csr_path = './'\n",
        "csr_file = os.path.join(csr_path, 'csr.parquet')\n",
        "\n",
        "df = spark.read.parquet(csr_file)\n",
        "df.head(n=5)"
      ],
      "id": "ea6f4387"
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "quarto-private-1": {
          "key": "ExecuteTime",
          "value": {
            "end_time": "2020-05-03T15:33:17.229477Z",
            "start_time": "2020-05-03T15:33:16.995048Z"
          }
        }
      },
      "outputs": [],
      "source": [
        "df.count()"
      ],
      "id": "5312b10e"
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "quarto-private-1": {
          "key": "ExecuteTime",
          "value": {
            "end_time": "2020-05-03T15:33:20.881392Z",
            "start_time": "2020-05-03T15:33:19.624525Z"
          }
        }
      },
      "outputs": [],
      "source": [
        "# What are the features related to campaign_id 1204 ?\n",
        "features_names = \\\n",
        "    df.select('feature_name')\\\n",
        "    .distinct()\\\n",
        "    .toPandas()['feature_name']"
      ],
      "id": "33b71225"
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "quarto-private-1": {
          "key": "ExecuteTime",
          "value": {
            "end_time": "2020-05-03T15:33:21.818568Z",
            "start_time": "2020-05-03T15:33:21.812810Z"
          }
        }
      },
      "outputs": [],
      "source": [
        "features_names"
      ],
      "id": "679fb79a"
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "quarto-private-1": {
          "key": "ExecuteTime",
          "value": {
            "end_time": "2020-05-03T15:33:27.083141Z",
            "start_time": "2020-05-03T15:33:27.078374Z"
          }
        }
      },
      "outputs": [],
      "source": [
        "[feature_name for feature_name in features_names if '1204' in feature_name]"
      ],
      "id": "e4269d26"
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "quarto-private-1": {
          "key": "ExecuteTime",
          "value": {
            "end_time": "2020-05-03T15:33:28.560631Z",
            "start_time": "2020-05-03T15:33:27.903921Z"
          }
        }
      },
      "outputs": [],
      "source": [
        "# Look for the xid that have at least one exposure to campaign 1204\n",
        "keep = func.when(\n",
        "    (col('feature_name') == 'n_actions_per_category_id#C#1204.0') |\n",
        "    (col('feature_name') == 'n_actions_per_category_id#O#1204.0'),\n",
        "    1).otherwise(0)\n",
        "df = df.withColumn('keep', keep)\n",
        "\n",
        "df.where(col('keep') > 0).count()"
      ],
      "id": "bc3a6f63"
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Sum of the keeps :)\n",
        "xid_partition = Window.partitionBy('xid')\n",
        "sum_keep = func.sum(col('keep')).over(xid_partition)\n",
        "df = df.withColumn('sum_keep', sum_keep)"
      ],
      "id": "8b0b5aa6"
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Let's keep the xid exposed to 1204\n",
        "df = df.where(col('sum_keep') > 0)"
      ],
      "id": "8a5acb60"
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {},
      "outputs": [],
      "source": [
        "df.count()"
      ],
      "id": "16d7d11a"
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {},
      "outputs": [],
      "source": [
        "df.select('xid').distinct().count()"
      ],
      "id": "2d0efad4"
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {},
      "outputs": [],
      "source": [
        "row_partition = Window().orderBy('row')\n",
        "col_partition = Window().orderBy('col')\n",
        "\n",
        "row_new = func.dense_rank().over(row_partition)\n",
        "col_new = func.dense_rank().over(col_partition)\n",
        "\n",
        "df = df.withColumn('row_new', row_new)\n",
        "df = df.withColumn('col_new', col_new)\n",
        "\n",
        "csr_data = df.select('row_new', 'col_new', 'value').toPandas()"
      ],
      "id": "3fe46e86"
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "quarto-private-1": {
          "key": "ExecuteTime",
          "value": {
            "end_time": "2020-05-03T15:33:52.617724Z",
            "start_time": "2020-05-03T15:33:52.609488Z"
          }
        }
      },
      "outputs": [],
      "source": [
        "csr_data.head()"
      ],
      "id": "e82404a1"
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {},
      "outputs": [],
      "source": [
        "features_names = df.select('feature_name', 'col_new').distinct()\n",
        "features_names.where(col('feature_name') == 'n_actions_per_category_id#C#1204.0').head()"
      ],
      "id": "f05d2f4f"
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {},
      "outputs": [],
      "source": [
        "features_names.where(col('feature_name') == 'n_actions_per_category_id#O#1204.0').head()"
      ],
      "id": "faebbf4c"
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "quarto-private-1": {
          "key": "ExecuteTime",
          "value": {
            "end_time": "2020-05-03T15:34:11.510538Z",
            "start_time": "2020-05-03T15:34:11.454802Z"
          }
        }
      },
      "outputs": [],
      "source": [
        "from scipy.sparse import csr_matrix\n",
        "import numpy as np\n",
        "\n",
        "rows = csr_data['row_new'].values - 1\n",
        "cols = csr_data['col_new'].values - 1\n",
        "vals = csr_data['value'].values\n",
        "\n",
        "X_csr = csr_matrix((vals, (rows, cols)))"
      ],
      "id": "97a19f7a"
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "quarto-private-1": {
          "key": "ExecuteTime",
          "value": {
            "end_time": "2020-05-03T15:34:11.977267Z",
            "start_time": "2020-05-03T15:34:11.972602Z"
          }
        }
      },
      "outputs": [],
      "source": [
        "X_csr.shape"
      ],
      "id": "9a289e8a"
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_csr.shape, X_csr.nnz"
      ],
      "id": "8592d939"
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_csr.nnz / (X_csr.shape[0]* X_csr.shape[1])   # 0152347 * 92)"
      ],
      "id": "b6696a0a"
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {},
      "outputs": [],
      "source": [
        "# The label vector. Let's make it dense, flat and binary\n",
        "y = np.array(X_csr[:, 1].todense()).ravel()\n",
        "y = np.array(y > 0, dtype=np.int64)"
      ],
      "id": "8dc06041"
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_csr.shape"
      ],
      "id": "fba4f078"
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {},
      "outputs": [],
      "source": [
        "# We remove the second and fourth column. \n",
        "# It actually contain the label we'll want to predict.\n",
        "kept_cols = list(range(X_csr.shape[1]))\n",
        "kept_cols.pop(1)\n",
        "kept_cols.pop(2)\n",
        "X = X_csr[:, kept_cols]"
      ],
      "id": "31b2bd0e"
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {},
      "outputs": [],
      "source": [
        "len(kept_cols)"
      ],
      "id": "be2be0cb"
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_csr.shape, X.shape"
      ],
      "id": "16d03065"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Finally !!\n",
        "\n",
        "Wow ! That was a lot of work. Now we have a features matrix $X$ and a\n",
        "vector of labels $y$."
      ],
      "id": "c6915533-3f3a-4d7b-ab71-97f85b73ce40"
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {},
      "outputs": [],
      "source": [
        "X.indices"
      ],
      "id": "837db322"
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {},
      "outputs": [],
      "source": [
        "X.indptr"
      ],
      "id": "9815abb2"
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {},
      "outputs": [],
      "source": [
        "X.shape, X.nnz"
      ],
      "id": "475fa585"
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {},
      "outputs": [],
      "source": [
        "y.shape, y.sum()"
      ],
      "id": "0ee0d280"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Some learning for/from this data"
      ],
      "id": "9af628df-be69-4127-845b-15b2a9d261a7"
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import MaxAbsScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Normalize the features\n",
        "X = MaxAbsScaler().fit_transform(X)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.3)\n",
        "\n",
        "clf = LogisticRegression(\n",
        "    penalty='l2',\n",
        "    C=1e3,\n",
        "    solver='lbfgs',\n",
        "    class_weight='balanced'\n",
        ")\n",
        "\n",
        "clf.fit(X_train, y_train)"
      ],
      "id": "b9f127b9"
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {},
      "outputs": [],
      "source": [
        "features_names = features_names.toPandas()['feature_name']"
      ],
      "id": "bc79e1a9"
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {},
      "outputs": [],
      "source": [
        "features_names[range(6)]"
      ],
      "id": "0d260c82"
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "id": "3ebd7ed8"
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.figure(figsize=(16, 5))\n",
        "plt.stem(clf.coef_[0]) # , use_line_collection=True)\n",
        "plt.title('Logistic regression coefficients', fontsize=18)"
      ],
      "id": "0ffc200f"
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {},
      "outputs": [],
      "source": [
        "clf.coef_[0].shape[0]"
      ],
      "id": "3bd1c5dd"
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {},
      "outputs": [],
      "source": [
        "len(features_names)"
      ],
      "id": "af5ef7f9"
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {},
      "outputs": [],
      "source": [
        "# We change the fontsize of minor ticks label\n",
        "_ = plt.xticks(np.arange(clf.coef_[0].shape[0]), features_names, \n",
        "           rotation='vertical', fontsize=8)"
      ],
      "id": "7e87ef4e"
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {},
      "outputs": [],
      "source": [
        "_ = plt.yticks(fontsize=14)"
      ],
      "id": "b0928a42"
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {
        "quarto-private-1": {
          "key": "ExecuteTime",
          "value": {
            "end_time": "2020-05-03T15:51:25.280157Z",
            "start_time": "2020-05-03T15:51:25.081464Z"
          }
        }
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import precision_recall_curve, f1_score\n",
        "\n",
        "precision, recall, _ = precision_recall_curve(y_test, clf.predict_proba(X_test)[:, 1])\n",
        "    \n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(recall, precision, label='LR (F1=%.2f)' % f1_score(y_test, clf.predict(X_test)), lw=2)\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('Recall', fontsize=16)\n",
        "plt.ylabel('Precision', fontsize=16)\n",
        "plt.title('Precision/recall curve', fontsize=18)\n",
        "plt.legend(loc=\"upper right\", fontsize=14)"
      ],
      "id": "b858fad9"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Analyse the tables"
      ],
      "id": "911fe969-20f6-4221-8ba6-e5715a1dd036"
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {},
      "outputs": [],
      "source": [
        "query = \"\"\"ANALYZE TABLE db_table COMPUTE STATISTICS\n",
        "            FOR COLUMNS xid\"\"\""
      ],
      "id": "ec13fe9b"
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {},
      "outputs": [],
      "source": [
        "df.createOrReplaceTempView(\"db_table\")"
      ],
      "id": "df56f084"
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {},
      "outputs": [],
      "source": [
        "df.columns"
      ],
      "id": "7db91a11"
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {},
      "outputs": [],
      "source": [
        "spark.sql(\"cache table db_table\")"
      ],
      "id": "ba32d27e"
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {},
      "outputs": [],
      "source": [
        "spark.sql(query)"
      ],
      "id": "eb3d91b0"
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {},
      "outputs": [],
      "source": [
        "spark.sql(\"show tables\")"
      ],
      "id": "0151d1be"
    }
  ],
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "path": "/usr/share/jupyter/kernels/python3"
    },
    "language_info": {
      "name": "python",
      "codemirror_mode": {
        "name": "ipython",
        "version": "3"
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  }
}