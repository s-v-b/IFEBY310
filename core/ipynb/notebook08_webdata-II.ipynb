{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Using with `pyspark` for data preprocessing\n",
        "\n",
        "## Data description\n",
        "\n",
        "The data is a `parquet` file which contains a dataframe with 8 columns:\n",
        "\n",
        "-   `xid`: unique user id\n",
        "-   `action`: type of action. ‘C’ is a click, ‘O’ or ‘VSL’ is a\n",
        "    web-display\n",
        "-   `date`: date of the action\n",
        "-   `website_id`: unique id of the website\n",
        "-   `url`: url of the webpage\n",
        "-   `category_id`: id of the display\n",
        "-   `zipcode`: postal zipcode of the user\n",
        "-   `device`: type of device used by the user\n",
        "\n",
        "## Q1. Some statistics / computations\n",
        "\n",
        "Using `pyspark.sql` we want to do the following things:\n",
        "\n",
        "1.  Compute the total number of unique users\n",
        "2.  Construct a column containing the total number of actions per user\n",
        "3.  Construct a column containing the number of days since the last\n",
        "    action of the user\n",
        "4.  Construct a column containing the number of actions of each user for\n",
        "    each modality of device\n",
        "\n",
        "## Q2. Binary classification\n",
        "\n",
        "Then, we want to construct a classifier to predict the click on the\n",
        "category 1204. Here is an agenda for this:\n",
        "\n",
        "1.  Construction of a features matrix for which each line corresponds to\n",
        "    the information concerning a user.\n",
        "2.  In this matrix, we need to keep only the users that have been\n",
        "    exposed to the display in category 1204\n",
        "3.  Using this training dataset, train a binary classifier, and evaluate\n",
        "    your classifier using a precision / recall curve computed on test\n",
        "    data.\n",
        "\n",
        "# Download/read the data and a first look at the data"
      ],
      "id": "5544b639-a13d-461e-83b5-b8596634949e"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "\n",
        "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
        "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable"
      ],
      "id": "f1e5c035"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> **Spark in local mode**\n",
        ">\n",
        "> ``` python\n",
        "> from pyspark import SparkConf, SparkContext\n",
        "> from pyspark.sql import SparkSession\n",
        ">\n",
        "> spark = (SparkSession\n",
        ">     .builder\n",
        ">     .appName(\"Spark Webdata\")\n",
        ">     .getOrCreate()\n",
        "> )\n",
        "> ```\n",
        ">\n",
        "> ``` python\n",
        "> import requests, zipfile, io\n",
        "> from pathlib import Path\n",
        ">\n",
        "> path = Path('webdata.parquet')\n",
        "> if not path.exists():\n",
        ">     url = \"https://s-v-b.github.io/IFEBY310/data/webdata.parquet.zip\"\n",
        ">     r = requests.get(url)\n",
        ">     z = zipfile.ZipFile(io.BytesIO(r.content))\n",
        ">     z.extractall(path='./')\n",
        "> ```\n",
        ">\n",
        "> ``` python\n",
        "> input_path = './'\n",
        ">\n",
        "> input_file = os.path.join(input_path, 'webdata.parquet')\n",
        ">\n",
        "> df = spark.read.parquet(input_file)\n",
        "> ```\n",
        ">\n",
        "> > **Note**\n",
        "> >\n",
        "> > We can also give a try to `pyarrow.parquet` module to load the\n",
        "> > Parquet file in an Arrow table.\n",
        ">\n",
        "> ``` python\n",
        "> import pyarrow as pa\n",
        "> import comet    as co\n",
        "> import pyarrow.parquet as pq\n",
        ">\n",
        "> dfa = pq.read_table(input_file)\n",
        "> ```\n",
        ">\n",
        "> ``` python\n",
        "> dfa.num_columns\n",
        "> ```\n",
        ">\n",
        "> > **Warning**\n",
        "> >\n",
        "> > Let us go back to the spark data frame\n",
        ">\n",
        "> ``` python\n",
        "> df.printSchema()\n",
        "> ```\n",
        ">\n",
        "> ``` python\n",
        "> df.rdd.getNumPartitions()\n",
        "> ```\n",
        ">\n",
        "> > **Question**\n",
        "> >\n",
        "> > Explain the partition size.\n",
        ">\n",
        "> ``` python\n",
        "> df.rdd.toDebugString()\n",
        "> ```\n",
        ">\n",
        "> # Basic statistics\n",
        ">\n",
        "> First we need to import some things:\n",
        ">\n",
        "> -   `Window` class\n",
        "> -   SQL functions module\n",
        "> -   Some very useful functions\n",
        "> -   Spark types\n",
        ">\n",
        "> ``` python\n",
        "> from pyspark.sql import Window\n",
        "> import pyspark.sql.functions as func\n",
        "> from pyspark.sql.types import *\n",
        "> from pyspark.sql.functions import col, lit\n",
        "> ```\n",
        ">\n",
        "> ## Compute the total number of unique users\n",
        ">\n",
        "> ``` python\n",
        "> ( \n",
        ">     df.select('xid')\n",
        ">       .distinct()\n",
        ">       .count()\n",
        "> )\n",
        "> ```\n",
        ">\n",
        "> ``` python\n",
        "> def foo(x): yield len(set(x))\n",
        "> ```\n",
        ">\n",
        "> ``` python\n",
        "> ( df.rdd\n",
        ">     .map(lambda x : x.xid)\n",
        ">     .mapPartitions(foo)\n",
        ">     .collect()\n",
        "> )\n",
        "> ```\n",
        ">\n",
        "> This might pump up some computational resources\n",
        ">\n",
        "> ``` python\n",
        "> ( \n",
        ">     df.select('xid')\n",
        ">       .distinct() \n",
        ">       .explain()\n",
        "> )\n",
        "> ```\n",
        ">\n",
        "> > **Note**\n",
        "> >\n",
        "> > The distinct values of `xid` seem to be evenly spread among the six\n",
        "> > files making the `parquet` directory. Note that the last six\n",
        "> > partitions look empty.\n",
        ">\n",
        "> ## Construct a column containing the total number of actions per user\n",
        ">\n",
        "> ``` python\n",
        "> xid_partition = Window.partitionBy('xid')\n",
        ">\n",
        "> n_events = func.count(col('action')).over(xid_partition)\n",
        ">\n",
        "> df = df.withColumn('n_events', n_events)\n",
        ">\n",
        "> df.head(n=2)\n",
        "> ```\n",
        ">\n",
        "> ``` python\n",
        "> ( \n",
        ">   df\n",
        ">     .groupBy('xid')\n",
        ">     .agg(func.count('action'))\n",
        ">     .head(5)\n",
        "> )\n",
        "> ```\n",
        ">\n",
        "> ## Construct a column containing the number of days since the last action of the user\n",
        ">\n",
        "> ``` python\n",
        "> max_date = (\n",
        ">   func\n",
        ">     .max(col('date'))\n",
        ">     .over(xid_partition)\n",
        "> )\n",
        ">\n",
        "> n_days_since_last_event = func.datediff(func.current_date(), max_date)\n",
        ">\n",
        "> df = df.withColumn('n_days_since_last_event',\n",
        ">                    n_days_since_last_event)\n",
        ">\n",
        "> df.head(n=2)\n",
        "> ```\n",
        ">\n",
        "> ``` python\n",
        "> df.printSchema()\n",
        "> ```\n",
        ">\n",
        "> ## Construct a column containing the number of actions of each user for each modality of device\n",
        ">\n",
        "> Does this `partitionBy` triggers shuffling?\n",
        ">\n",
        "> ``` python\n",
        "> xid_device_partition = xid_partition.partitionBy('device')\n",
        ">\n",
        "> n_events_per_device = func.count(col('action')).over(xid_device_partition)\n",
        ">\n",
        "> df = df.withColumn('n_events_per_device', n_events_per_device)\n",
        ">\n",
        "> df.head(n=2)\n",
        "> ```\n",
        ">\n",
        "> ## Number of devices per user \n",
        ">\n",
        "> ``` python\n",
        "> # xid_partition = Window.partitionBy('xid')\n",
        ">\n",
        "> rank_device = (\n",
        ">   func\n",
        ">     .dense_rank()\n",
        ">     .over(xid_partition.orderBy('device'))\n",
        "> )\n",
        ">\n",
        "> n_unique_device = (\n",
        ">     func\n",
        ">       .last(rank_device)\n",
        ">       .over(xid_partition)\n",
        "> )\n",
        ">\n",
        "> df = df.withColumn('n_device', n_unique_device)\n",
        ">\n",
        "> df.head(n=2)\n",
        "> ```\n",
        ">\n",
        "> ``` python\n",
        "> df\\\n",
        ">     .where(col('n_device') > 1)\\\n",
        ">     .select('xid', 'device', 'n_events',  'n_device', 'n_events_per_device')\\\n",
        ">     .head(n=8)\n",
        "> ```\n",
        ">\n",
        "> ``` python\n",
        "> df\\\n",
        ">     .where(col('n_device') > 1)\\\n",
        ">     .select('xid', 'device', 'n_events',  'n_device', 'n_events_per_device')\\\n",
        ">     .count()\n",
        "> ```\n",
        ">\n",
        "> # Let’s select the correct users and build a training dataset\n",
        ">\n",
        "> We construct a ETL (Extract Transform Load) process on this data using\n",
        "> the `pyspark.sql` API.\n",
        ">\n",
        "> ## Extraction\n",
        ">\n",
        "> Here extraction is just about reading the data\n",
        ">\n",
        "> ``` python\n",
        "> df = spark.read.parquet(input_file)\n",
        "> df.head(n=3)\n",
        "> ```\n",
        ">\n",
        "> ## Transformation of the data\n",
        ">\n",
        "> At this step we compute a lot of extra things from the data. The aim\n",
        "> is to build *features* that describe users.\n",
        ">\n",
        "> ``` python\n",
        "> def n_events_transformer(df):\n",
        ">     xid_partition = Window.partitionBy('xid')\n",
        ">     n_events = func.count(col('action')).over(xid_partition)\n",
        ">     \n",
        ">     df = df.withColumn('n_events', n_events)\n",
        ">\n",
        ">     return df\n",
        "> ```\n",
        ">\n",
        "> ``` python\n",
        "> def n_events_per_action_transformer(df):\n",
        ">     xid_action_partition = Window.partitionBy('xid', 'action')\n",
        ">     n_events_per_action = func.count(col('action')).over(xid_action_partition)\n",
        ">\n",
        ">     df = df.withColumn('n_events_per_action', n_events_per_action)\n",
        ">     \n",
        ">     return df\n",
        "> ```\n",
        ">\n",
        "> ``` python\n",
        "> def hour_transformer(df):\n",
        ">     hour = func.hour(col('date'))\n",
        ">     df = df.withColumn('hour', hour)\n",
        ">     return df\n",
        ">\n",
        "> def weekday_transformer(df):\n",
        ">     weekday = func.date_format(col('date'), 'EEEE')\n",
        ">     df = df.withColumn('weekday', weekday)\n",
        ">     return df\n",
        ">\n",
        "> def n_events_per_hour_transformer(df):\n",
        ">     xid_hour_partition = Window.partitionBy('xid', 'hour')\n",
        ">     n_events_per_hour = func.count(col('action')).over(xid_hour_partition)\n",
        ">     df = df.withColumn('n_events_per_hour', n_events_per_hour)\n",
        ">     return df\n",
        ">\n",
        "> def n_events_per_weekday_transformer(df):\n",
        ">     xid_weekday_partition = Window.partitionBy('xid', 'weekday')\n",
        ">     n_events_per_weekday = func.count(col('action')).over(xid_weekday_partition)\n",
        ">     df = df.withColumn('n_events_per_weekday', n_events_per_weekday)\n",
        ">     return df\n",
        ">\n",
        "> def n_days_since_last_event_transformer(df):\n",
        ">     xid_partition = Window.partitionBy('xid')\n",
        ">     max_date = func.max(col('date')).over(xid_partition)\n",
        ">     n_days_since_last_event = func.datediff(func.current_date(), max_date)\n",
        ">     df = df.withColumn('n_days_since_last_event',\n",
        ">                        n_days_since_last_event + lit(0.1))\n",
        ">     return df\n",
        ">\n",
        "> def n_days_since_last_action_transformer(df):\n",
        ">     xid_partition_action = Window.partitionBy('xid', 'action')\n",
        ">     max_date = func.max(col('date')).over(xid_partition_action)\n",
        ">     n_days_since_last_action = func.datediff(func.current_date(),\n",
        ">                                                         max_date)\n",
        ">     df = df.withColumn('n_days_since_last_action',\n",
        ">                        n_days_since_last_action + lit(0.1))\n",
        ">     return df\n",
        ">\n",
        "> def n_unique_day_transformer(df):\n",
        ">     xid_partition = Window.partitionBy('xid')\n",
        ">     dayofyear = func.dayofyear(col('date'))\n",
        ">     rank_day = func.dense_rank().over(xid_partition.orderBy(dayofyear))\n",
        ">     n_unique_day = func.last(rank_day).over(xid_partition)\n",
        ">     df = df.withColumn('n_unique_day', n_unique_day)\n",
        ">     return df\n",
        ">\n",
        "> def n_unique_hour_transformer(df):\n",
        ">     xid_partition = Window.partitionBy('xid')\n",
        ">     rank_hour = func.dense_rank().over(xid_partition.orderBy('hour'))\n",
        ">     n_unique_hour = func.last(rank_hour).over(xid_partition)\n",
        ">     df = df.withColumn('n_unique_hour', n_unique_hour)\n",
        ">     return df\n",
        ">\n",
        "> def n_events_per_device_transformer(df):\n",
        ">     xid_device_partition = Window.partitionBy('xid', 'device')\n",
        ">     n_events_per_device = func.count(func.col('device')) \\\n",
        ">         .over(xid_device_partition)\n",
        ">     df = df.withColumn('n_events_per_device', n_events_per_device)\n",
        ">     return df\n",
        ">\n",
        "> def n_unique_device_transformer(df):\n",
        ">     xid_partition = Window.partitionBy('xid')\n",
        ">     rank_device = func.dense_rank().over(xid_partition.orderBy('device'))\n",
        ">     n_unique_device = func.last(rank_device).over(xid_partition)\n",
        ">     df = df.withColumn('n_device', n_unique_device)\n",
        ">     return df\n",
        ">\n",
        "> def n_actions_per_category_id_transformer(df):\n",
        ">     xid_category_id_partition = Window.partitionBy('xid', 'category_id',\n",
        ">                                                    'action')\n",
        ">     n_actions_per_category_id = func.count(func.col('action')) \\\n",
        ">         .over(xid_category_id_partition)\n",
        ">     df = df.withColumn('n_actions_per_category_id', n_actions_per_category_id)\n",
        ">     return df\n",
        ">\n",
        "> def n_unique_category_id_transformer(df):\n",
        ">     xid_partition = Window.partitionBy('xid')\n",
        ">     rank_category_id = func.dense_rank().over(xid_partition\\\n",
        ">                                               .orderBy('category_id'))\n",
        ">     n_unique_category_id = func.last(rank_category_id).over(xid_partition)\n",
        ">     df = df.withColumn('n_unique_category_id', n_unique_category_id)\n",
        ">     return df\n",
        ">\n",
        "> def n_events_per_category_id_transformer(df):\n",
        ">     xid_category_id_partition = Window.partitionBy('xid', 'category_id')\n",
        ">     n_events_per_category_id = func.count(func.col('action')) \\\n",
        ">         .over(xid_category_id_partition)\n",
        ">     df = df.withColumn('n_events_per_category_id', n_events_per_category_id)\n",
        ">     return df\n",
        ">\n",
        "> def n_events_per_website_id_transformer(df):\n",
        ">     xid_website_id_partition = Window.partitionBy('xid', 'website_id')\n",
        ">     n_events_per_website_id = func.count(col('action'))\\\n",
        ">         .over(xid_website_id_partition)\n",
        ">     df = df.withColumn('n_events_per_website_id', n_events_per_website_id)\n",
        ">     return df\n",
        "> ```\n",
        ">\n",
        "> ``` python\n",
        "> transformers = [\n",
        ">     hour_transformer,\n",
        ">     weekday_transformer,\n",
        ">     n_events_per_hour_transformer,\n",
        ">     n_events_per_weekday_transformer,\n",
        ">     n_days_since_last_event_transformer,\n",
        ">     n_days_since_last_action_transformer,\n",
        ">     n_unique_day_transformer,\n",
        ">     n_unique_hour_transformer,\n",
        ">     n_events_per_device_transformer,\n",
        ">     n_unique_device_transformer,\n",
        ">     n_actions_per_category_id_transformer,\n",
        ">     n_events_per_category_id_transformer,\n",
        ">     n_events_per_website_id_transformer,\n",
        "> ]\n",
        "> ```\n",
        ">\n",
        "> ``` python\n",
        "> N = 10000\n",
        "> ```\n",
        ">\n",
        "> ``` python\n",
        "> sample_df = df.sample(withReplacement=False, fraction=.05)\n",
        "> ```\n",
        ">\n",
        "> ``` python\n",
        "> sample_df.count()\n",
        "> ```\n",
        ">\n",
        "> ``` python\n",
        "> for transformer in transformers:\n",
        ">     df = transformer(df)\n",
        ">\n",
        "> df.head(n=1)\n",
        "> ```\n",
        ">\n",
        "> ``` python\n",
        "> for transformer in transformers:\n",
        ">     sample_df = transformer(sample_df)\n",
        ">\n",
        "> sample_df.head(n=1)\n",
        "> ```\n",
        ">\n",
        "> ``` python\n",
        "> df = sample_df\n",
        "> ```\n",
        ">\n",
        "> ``` python\n",
        "> sorted(df.columns)\n",
        "> ```\n",
        ">\n",
        "> ``` python\n",
        "> df.explain()\n",
        "> ```\n",
        ">\n",
        "> ``` python\n",
        "> spark._sc.setCheckpointDir(\".\")   \n",
        ">\n",
        "> df.checkpoint()\n",
        "> ```\n",
        ">\n",
        "> ``` python\n",
        "> df.explain()\n",
        "> ```\n",
        ">\n",
        "> ## Load step\n",
        ">\n",
        "> Here, we use all the previous computations (saved in the columns of\n",
        "> the dataframe) to compute aggregated informations about each user.\n",
        ">\n",
        "> > **Note**\n",
        "> >\n",
        "> > This should be DRYED\n",
        ">\n",
        "> ``` python\n",
        "> def n_events_per_hour_loader(df):\n",
        ">     csr = df\\\n",
        ">         .select('xid', 'hour', 'n_events_per_hour')\\\n",
        ">         .withColumnRenamed('n_events_per_hour', 'value')\\\n",
        ">         .distinct() \n",
        ">             # action\n",
        ">     feature_name = func.concat(lit('n_events_per_hour#'), col('hour'))\n",
        ">\n",
        ">     csr = csr\\\n",
        ">         .withColumn('feature_name', feature_name)\\\n",
        ">         .drop('hour')\n",
        ">     return csr\n",
        ">\n",
        "> def n_events_per_website_id_loader(df):\n",
        ">     csr = df.select('xid', 'website_id', 'n_events_per_website_id')\\\n",
        ">         .withColumnRenamed('n_events_per_hour', 'value')\\\n",
        ">         .distinct()\n",
        ">\n",
        ">     feature_name = func.concat(lit('n_events_per_website_id#'),\n",
        ">                                col('website_id'))\n",
        ">     \n",
        ">     csr = csr\\\n",
        ">         .withColumn('feature_name', feature_name)\\\n",
        ">         .drop('website_id')\n",
        ">     return csr\n",
        ">\n",
        "> def n_events_per_hour_loader(df):\n",
        ">     csr = df\\\n",
        ">         .select('xid', 'hour', 'n_events_per_hour')\\\n",
        ">         .withColumnRenamed('n_events_per_hour', 'value')\\\n",
        ">         .distinct()\n",
        ">\n",
        ">     feature_name = func.concat(lit('n_events_per_hour#'), col('hour'))\n",
        ">     \n",
        ">     csr = csr\\\n",
        ">         .withColumn('feature_name', feature_name)\\\n",
        ">         .drop('hour')\n",
        ">     return csr\n",
        ">\n",
        "> def n_events_per_weekday_loader(df):\n",
        ">     csr = df\\\n",
        ">         .select('xid', 'weekday', 'n_events_per_weekday')\\\n",
        ">         .withColumnRenamed('n_events_per_weekday', 'value')\\\n",
        ">         .distinct()\n",
        ">\n",
        ">     feature_name = func.concat(lit('n_events_per_weekday#'), col('weekday'))\n",
        ">     \n",
        ">     csr = csr\\\n",
        ">         .withColumn('feature_name', feature_name)\\\n",
        ">         .drop('weekday')\n",
        ">\n",
        ">     return csr\n",
        ">\n",
        "> def n_days_since_last_event_loader(df):\n",
        ">     csr = df.select('xid',  'n_days_since_last_event')\\\n",
        ">         .withColumnRenamed('n_days_since_last_event', 'value')\\\n",
        ">         .distinct()\n",
        ">     feature_name = lit('n_days_since_last_event')\n",
        ">     csr = csr\\\n",
        ">         .withColumn('feature_name', feature_name)\n",
        ">     return csr\n",
        ">\n",
        "> def n_days_since_last_action_loader(df):\n",
        ">     csr = df.select('xid', 'action', 'n_days_since_last_action')\\\n",
        ">         .withColumnRenamed('n_days_since_last_action', 'value')\\\n",
        ">         .distinct()\n",
        ">     feature_name = func.concat(lit('n_days_since_last_action#'), col('action'))\n",
        ">     csr = csr\\\n",
        ">         .withColumn('feature_name', feature_name)\\\n",
        ">         .drop('action')\n",
        ">     return csr\n",
        ">\n",
        "> def n_unique_day_loader(df):\n",
        ">     csr = df.select('xid', 'n_unique_day')\\\n",
        ">         .withColumnRenamed('n_unique_day', 'value')\\\n",
        ">         .distinct()\n",
        ">     feature_name = lit('n_unique_day')\n",
        ">     csr = csr\\\n",
        ">         .withColumn('feature_name', feature_name)\n",
        ">     return csr\n",
        ">\n",
        "> def n_unique_hour_loader(df):\n",
        ">     csr = df.select('xid', 'n_unique_hour')\\\n",
        ">         .withColumnRenamed('n_unique_hour', 'value')\\\n",
        ">         .distinct()\n",
        ">     feature_name = lit('n_unique_hour')\n",
        ">     csr = csr\\\n",
        ">         .withColumn('feature_name', feature_name)\n",
        ">     return csr\n",
        ">\n",
        "> def n_events_per_device_loader(df):\n",
        ">     csr = df\\\n",
        ">         .select('xid', 'device', 'n_events_per_device')\\\n",
        ">         .withColumnRenamed('n_events_per_device', 'value')\\\n",
        ">         .distinct()\n",
        ">     feature_name = func.concat(lit('n_events_per_device#'), col('device'))\n",
        ">     csr = csr\\\n",
        ">         .withColumn('feature_name', feature_name)\\\n",
        ">         .drop('device')\n",
        ">     return csr\n",
        ">\n",
        "> def n_unique_device_loader(df):\n",
        ">     csr = df.select('xid', 'n_device')\\\n",
        ">         .withColumnRenamed('n_device', 'value')\\\n",
        ">         .distinct()\n",
        ">     feature_name = lit('n_device')\n",
        ">     csr = csr\\\n",
        ">         .withColumn('feature_name', feature_name)\n",
        ">     return csr\n",
        ">\n",
        "> def n_events_per_category_id_loader(df):\n",
        ">     csr = df.select('xid', 'category_id', 'n_events_per_category_id')\\\n",
        ">         .withColumnRenamed('n_events_per_category_id', 'value')\\\n",
        ">         .distinct()\n",
        ">     feature_name = func.concat(lit('n_events_per_category_id#'),\n",
        ">                                col('category_id'))\n",
        ">     csr = csr\\\n",
        ">         .withColumn('feature_name', feature_name)\\\n",
        ">         .drop('category_id')\n",
        ">     return csr\n",
        ">\n",
        "> def n_actions_per_category_id_loader(df):\n",
        ">     csr = df.select('xid', 'category_id', 'action', 'n_actions_per_category_id')\\\n",
        ">         .withColumnRenamed('n_actions_per_category_id', 'value')\\\n",
        ">         .distinct()\n",
        ">     feature_name = func.concat(lit('n_actions_per_category_id#'),\n",
        ">                                col('action'), lit('#'), \n",
        ">                                col('category_id'))\n",
        ">     csr = csr\\\n",
        ">         .withColumn('feature_name', feature_name)\\\n",
        ">         .drop('category_id')\\\n",
        ">         .drop('action')\n",
        ">     return csr\n",
        ">\n",
        "> def n_events_per_website_id_loader(df):\n",
        ">     csr = df.select('xid', 'website_id', 'n_events_per_website_id')\\\n",
        ">         .withColumnRenamed('n_events_per_website_id', 'value')\\\n",
        ">         .distinct()\n",
        ">     feature_name = func.concat(lit('n_events_per_website_id#'),\n",
        ">                                col('website_id'))\n",
        ">     csr = csr\\\n",
        ">         .withColumn('feature_name', feature_name)\\\n",
        ">         .drop('website_id')\n",
        ">     return csr\n",
        "> ```\n",
        ">\n",
        "> ``` python\n",
        "> from functools import reduce\n",
        "> ```\n",
        ">\n",
        "> ``` python\n",
        "> loaders = [\n",
        ">     n_events_per_hour_loader,\n",
        ">     n_events_per_website_id_loader,\n",
        ">     n_events_per_hour_loader,\n",
        ">     n_events_per_weekday_loader,\n",
        ">     n_days_since_last_event_loader,\n",
        ">     n_days_since_last_action_loader,\n",
        ">     n_unique_day_loader,\n",
        ">     n_unique_hour_loader,\n",
        ">     n_events_per_device_loader,\n",
        ">     n_unique_device_loader,\n",
        ">     n_events_per_category_id_loader,\n",
        ">     n_actions_per_category_id_loader,\n",
        ">     n_events_per_website_id_loader,\n",
        "> ]\n",
        "> ```\n",
        ">\n",
        "> ``` python\n",
        "> def union(df, other):\n",
        ">     return df.union(other)\n",
        "> ```\n",
        ">\n",
        "> > **About DataFrame.union()**\n",
        "> >\n",
        "> > This method performs a SQL-style set union of the rows from both\n",
        "> > DataFrame objects, with no automatic deduplication of elements.\n",
        "> >\n",
        "> > Use the distinct() method to perform deduplication of rows.\n",
        "> >\n",
        "> > The method resolves columns by position (not by name), following the\n",
        "> > standard behavior in SQL.\n",
        ">\n",
        "> ``` python\n",
        "> spam = [loader(df) for loader in loaders]\n",
        "> ```\n",
        ">\n",
        "> ``` python\n",
        "> spam[0].printSchema()\n",
        "> ```\n",
        ">\n",
        "> ``` python\n",
        "> all(spam[0].columns == it.columns for it in spam[1:])\n",
        "> ```\n",
        ">\n",
        "> ``` python\n",
        "> len(spam)\n",
        "> ```\n",
        ">\n",
        "> ``` python\n",
        "> csr = reduce(\n",
        ">     lambda df1, df2: df1.union(df2),\n",
        ">     spam\n",
        "> )\n",
        ">\n",
        "> csr.head(n=3)\n",
        "> ```\n",
        ">\n",
        "> ``` python\n",
        "> csr.columns\n",
        "> ```\n",
        ">\n",
        "> ``` python\n",
        "> csr.show(5)\n",
        "> ```\n",
        ">\n",
        "> ``` python\n",
        "> csr.rdd.getNumPartitions()\n",
        "> ```\n",
        ">\n",
        "> ``` python\n",
        "> # Replace features names and xid by a unique number\n",
        "> feature_name_partition = Window().orderBy('feature_name')\n",
        ">\n",
        "> xid_partition = Window().orderBy('xid')\n",
        ">\n",
        "> col_idx = func.dense_rank().over(feature_name_partition)\n",
        "> row_idx = func.dense_rank().over(xid_partition)\n",
        "> ```\n",
        ">\n",
        "> ``` python\n",
        "> csr = csr.withColumn('col', col_idx)\\\n",
        ">     .withColumn('row', row_idx)\n",
        ">\n",
        "> csr = csr.na.drop('any')\n",
        ">\n",
        "> csr.head(n=5)\n",
        "> ```\n",
        ">\n",
        "> ``` python\n",
        "> # Let's save the result of our hard work into a new parquet file\n",
        "> output_path = './'\n",
        "> output_file = os.path.join(output_path, 'csr.parquet')\n",
        "> csr.write.parquet(output_file, mode='overwrite')\n",
        "> ```\n",
        ">\n",
        "> # Preparation of the training dataset\n",
        ">\n",
        "> ``` python\n",
        "> csr_path = './'\n",
        "> csr_file = os.path.join(csr_path, 'csr.parquet')\n",
        ">\n",
        "> df = spark.read.parquet(csr_file)\n",
        "> df.head(n=5)\n",
        "> ```\n",
        ">\n",
        "> ``` python\n",
        "> df.count()\n",
        "> ```\n",
        ">\n",
        "> ``` python\n",
        "> # What are the features related to campaign_id 1204 ?\n",
        "> features_names = \\\n",
        ">     df.select('feature_name')\\\n",
        ">     .distinct()\\\n",
        ">     .toPandas()['feature_name']\n",
        "> ```\n",
        ">\n",
        "> ``` python\n",
        "> features_names\n",
        "> ```\n",
        ">\n",
        "> ``` python\n",
        "> [feature_name for feature_name in features_names if '1204' in feature_name]\n",
        "> ```\n",
        ">\n",
        "> ``` python\n",
        "> # Look for the xid that have at least one exposure to campaign 1204\n",
        "> keep = func.when(\n",
        ">     (col('feature_name') == 'n_actions_per_category_id#C#1204.0') |\n",
        ">     (col('feature_name') == 'n_actions_per_category_id#O#1204.0'),\n",
        ">     1).otherwise(0)\n",
        "> df = df.withColumn('keep', keep)\n",
        ">\n",
        "> df.where(col('keep') > 0).count()\n",
        "> ```\n",
        ">\n",
        "> ``` python\n",
        "> # Sum of the keeps :)\n",
        "> xid_partition = Window.partitionBy('xid')\n",
        "> sum_keep = func.sum(col('keep')).over(xid_partition)\n",
        "> df = df.withColumn('sum_keep', sum_keep)\n",
        "> ```\n",
        ">\n",
        "> ``` python\n",
        "> # Let's keep the xid exposed to 1204\n",
        "> df = df.where(col('sum_keep') > 0)\n",
        "> ```\n",
        ">\n",
        "> ``` python\n",
        "> df.count()\n",
        "> ```\n",
        ">\n",
        "> ``` python\n",
        "> df.select('xid').distinct().count()\n",
        "> ```\n",
        ">\n",
        "> ``` python\n",
        "> row_partition = Window().orderBy('row')\n",
        "> col_partition = Window().orderBy('col')\n",
        ">\n",
        "> row_new = func.dense_rank().over(row_partition)\n",
        "> col_new = func.dense_rank().over(col_partition)\n",
        ">\n",
        "> df = df.withColumn('row_new', row_new)\n",
        "> df = df.withColumn('col_new', col_new)\n",
        ">\n",
        "> csr_data = df.select('row_new', 'col_new', 'value').toPandas()\n",
        "> ```\n",
        ">\n",
        "> ``` python\n",
        "> csr_data.head()\n",
        "> ```\n",
        ">\n",
        "> ``` python\n",
        "> features_names = df.select('feature_name', 'col_new').distinct()\n",
        "> features_names.where(col('feature_name') == 'n_actions_per_category_id#C#1204.0').head()\n",
        "> ```\n",
        ">\n",
        "> ``` python\n",
        "> features_names.where(col('feature_name') == 'n_actions_per_category_id#O#1204.0').head()\n",
        "> ```\n",
        ">\n",
        "> ``` python\n",
        "> from scipy.sparse import csr_matrix\n",
        "> import numpy as np\n",
        ">\n",
        "> rows = csr_data['row_new'].values - 1\n",
        "> cols = csr_data['col_new'].values - 1\n",
        "> vals = csr_data['value'].values\n",
        ">\n",
        "> X_csr = csr_matrix((vals, (rows, cols)))\n",
        "> ```\n",
        ">\n",
        "> ``` python\n",
        "> X_csr.shape\n",
        "> ```\n",
        ">\n",
        "> ``` python\n",
        "> X_csr.shape, X_csr.nnz\n",
        "> ```\n",
        ">\n",
        "> ``` python\n",
        "> X_csr.nnz / (X_csr.shape[0]* X_csr.shape[1])   # 0152347 * 92)\n",
        "> ```\n",
        ">\n",
        "> ``` python\n",
        "> # The label vector. Let's make it dense, flat and binary\n",
        "> y = np.array(X_csr[:, 1].todense()).ravel()\n",
        "> y = np.array(y > 0, dtype=np.int64)\n",
        "> ```\n",
        ">\n",
        "> ``` python\n",
        "> X_csr.shape\n",
        "> ```\n",
        ">\n",
        "> ``` python\n",
        "> # We remove the second and fourth column. \n",
        "> # It actually contain the label we'll want to predict.\n",
        "> kept_cols = list(range(X_csr.shape[1]))\n",
        "> kept_cols.pop(1)\n",
        "> kept_cols.pop(2)\n",
        "> X = X_csr[:, kept_cols]\n",
        "> ```\n",
        ">\n",
        "> ``` python\n",
        "> len(kept_cols)\n",
        "> ```\n",
        ">\n",
        "> ``` python\n",
        "> X_csr.shape, X.shape\n",
        "> ```\n",
        ">\n",
        "> ## Finally !!\n",
        ">\n",
        "> Wow ! That was a lot of work. Now we have a features matrix $X$ and a\n",
        "> vector of labels $y$.\n",
        ">\n",
        "> ``` python\n",
        "> X.indices\n",
        "> ```\n",
        ">\n",
        "> ``` python\n",
        "> X.indptr\n",
        "> ```\n",
        ">\n",
        "> ``` python\n",
        "> X.shape, X.nnz\n",
        "> ```\n",
        ">\n",
        "> ``` python\n",
        "> y.shape, y.sum()\n",
        "> ```\n",
        ">\n",
        "> # Some learning for/from this data\n",
        ">\n",
        "> ``` python\n",
        "> from sklearn.preprocessing import MaxAbsScaler\n",
        "> from sklearn.model_selection import train_test_split\n",
        "> from sklearn.linear_model import LogisticRegression\n",
        ">\n",
        "> # Normalize the features\n",
        "> X = MaxAbsScaler().fit_transform(X)\n",
        "> X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.3)\n",
        ">\n",
        "> clf = LogisticRegression(\n",
        ">     penalty='l2',\n",
        ">     C=1e3,\n",
        ">     solver='lbfgs',\n",
        ">     class_weight='balanced'\n",
        "> )\n",
        ">\n",
        "> clf.fit(X_train, y_train)\n",
        "> ```\n",
        ">\n",
        "> ``` python\n",
        "> features_names = features_names.toPandas()['feature_name']\n",
        "> ```\n",
        ">\n",
        "> ``` python\n",
        "> features_names[range(6)]\n",
        "> ```\n",
        ">\n",
        "> ``` python\n",
        "> import matplotlib.pyplot as plt\n",
        "> %matplotlib inline\n",
        "> ```\n",
        ">\n",
        "> ``` python\n",
        "> plt.figure(figsize=(16, 5))\n",
        "> plt.stem(clf.coef_[0]) # , use_line_collection=True)\n",
        "> plt.title('Logistic regression coefficients', fontsize=18)\n",
        "> ```\n",
        ">\n",
        "> ``` python\n",
        "> clf.coef_[0].shape[0]\n",
        "> ```\n",
        ">\n",
        "> ``` python\n",
        "> len(features_names)\n",
        "> ```\n",
        ">\n",
        "> ``` python\n",
        "> # We change the fontsize of minor ticks label\n",
        "> _ = plt.xticks(np.arange(clf.coef_[0].shape[0]), features_names, \n",
        ">            rotation='vertical', fontsize=8)\n",
        "> ```\n",
        ">\n",
        "> ``` python\n",
        "> _ = plt.yticks(fontsize=14)\n",
        "> ```\n",
        ">\n",
        "> ``` python\n",
        "> from sklearn.metrics import precision_recall_curve, f1_score\n",
        ">\n",
        "> precision, recall, _ = precision_recall_curve(y_test, clf.predict_proba(X_test)[:, 1])\n",
        ">     \n",
        "> plt.figure(figsize=(8, 6))\n",
        "> plt.plot(recall, precision, label='LR (F1=%.2f)' % f1_score(y_test, clf.predict(X_test)), lw=2)\n",
        "> plt.xlim([0.0, 1.0])\n",
        "> plt.ylim([0.0, 1.05])\n",
        "> plt.xlabel('Recall', fontsize=16)\n",
        "> plt.ylabel('Precision', fontsize=16)\n",
        "> plt.title('Precision/recall curve', fontsize=18)\n",
        "> plt.legend(loc=\"upper right\", fontsize=14)\n",
        "> ```\n",
        ">\n",
        "> # Analyse the tables\n",
        ">\n",
        "> ``` python\n",
        "> query = \"\"\"ANALYZE TABLE db_table COMPUTE STATISTICS\n",
        ">             FOR COLUMNS xid\"\"\"\n",
        "> ```\n",
        ">\n",
        "> ``` python\n",
        "> df.createOrReplaceTempView(\"db_table\")\n",
        "> ```\n",
        ">\n",
        "> ``` python\n",
        "> df.columns\n",
        "> ```\n",
        ">\n",
        "> ``` python\n",
        "> spark.sql(\"cache table db_table\")\n",
        "> ```\n",
        ">\n",
        "> ``` python\n",
        "> spark.sql(query)\n",
        "> ```\n",
        ">\n",
        "> ``` python\n",
        "> spark.sql(\"show tables\")\n",
        "> ```"
      ],
      "id": "ab7a3e38-f38c-4351-9f13-20819327034a"
    }
  ],
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "path": "/usr/share/jupyter/kernels/python3"
    },
    "language_info": {
      "name": "python",
      "codemirror_mode": {
        "name": "ipython",
        "version": "3"
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  }
}