{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Using with `pyspark` for data preprocessing\n",
        "\n",
        "## Data description\n",
        "\n",
        "The data is a `parquet` file which contains a dataframe with 8 columns:\n",
        "\n",
        "-   `xid`: unique user id\n",
        "-   `action`: type of action. ‘C’ is a click, ‘O’ or ‘VSL’ is a\n",
        "    web-display\n",
        "-   `date`: date of the action\n",
        "-   `website_id`: unique id of the website\n",
        "-   `url`: url of the webpage\n",
        "-   `category_id`: id of the display\n",
        "-   `zipcode`: postal zipcode of the user\n",
        "-   `device`: type of device used by the user\n",
        "\n",
        "## Q1. Some statistics / computations\n",
        "\n",
        "Using `pyspark.sql` we want to do the following things:\n",
        "\n",
        "1.  Compute the total number of unique users\n",
        "2.  Construct a column containing the total number of actions per user\n",
        "3.  Construct a column containing the number of days since the last\n",
        "    action of the user\n",
        "4.  Construct a column containing the number of actions of each user for\n",
        "    each modality of device\n",
        "\n",
        "## Q2. Feature engineering\n",
        "\n",
        "Then, we want to construct a classifier to predict the click on the\n",
        "category 1204. Here is an agenda for this:\n",
        "\n",
        "1.  Construction of a features matrix for which each line corresponds to\n",
        "    the information concerning a user.\n",
        "2.  In this matrix, we need to keep only the users that have been\n",
        "    exposed to the display in category 1204\n",
        "\n",
        "## Q3. Classification\n",
        "\n",
        "1.  Using this training dataset, train a binary classifier, and evaluate\n",
        "    your classifier using a precision / recall curve computed on test\n",
        "    data.\n",
        "\n",
        "# Download/read the data and a first look at the data"
      ],
      "id": "bea43c76-2681-4f09-ab8b-ca49791b3d13"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "\n",
        "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
        "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable"
      ],
      "id": "cf5c507b"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> **Spark in local mode**\n",
        ">\n",
        "> ``` python\n",
        "> from pyspark import SparkConf, SparkContext\n",
        "> from pyspark.sql import SparkSession\n",
        ">\n",
        "> spark = (SparkSession\n",
        ">     .builder\n",
        ">     .appName(\"Spark Webdata\")\n",
        ">     .getOrCreate()\n",
        "> )\n",
        "> ```\n",
        ">\n",
        "> ``` python\n",
        "> import requests, zipfile, io\n",
        "> from pathlib import Path\n",
        ">\n",
        "> path = Path('webdata.parquet')\n",
        "> if not path.exists():\n",
        ">     url = \"https://s-v-b.github.io/IFEBY310/data/webdata.parquet.zip\"\n",
        ">     r = requests.get(url)\n",
        ">     z = zipfile.ZipFile(io.BytesIO(r.content))\n",
        ">     z.extractall(path='./')\n",
        "> ```\n",
        ">\n",
        "> ``` python\n",
        "> input_path = './'\n",
        ">\n",
        "> input_file = os.path.join(input_path, 'webdata.parquet')\n",
        ">\n",
        "> df = spark.read.parquet(input_file)\n",
        "> ```\n",
        "\n",
        "> **Note**\n",
        ">\n",
        "> We can also give a try to `pyarrow.parquet` module to load the Parquet\n",
        "> file in an Arrow table."
      ],
      "id": "6a02ade1-adcd-4687-8469-52c3416bcb67"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pyarrow as pa\n",
        "import comet    as co\n",
        "import pyarrow.parquet as pq\n",
        "\n",
        "dfa = pq.read_table(input_file)"
      ],
      "id": "9affce5e"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "dfa.num_columns"
      ],
      "id": "a687edc0"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ":::\n",
        "\n",
        "> **Warning**\n",
        ">\n",
        "> Let us go back to the spark data frame"
      ],
      "id": "fa7398a4-092b-4677-b094-86db9c3d0e67"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "df.printSchema()"
      ],
      "id": "61df13eb"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "df.rdd.getNumPartitions()"
      ],
      "id": "cd77ca5b"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> **Question**\n",
        ">\n",
        "> Explain the partition size."
      ],
      "id": "bcd7dfdd-1599-41ce-85e6-6d05439a3ebe"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "df.rdd.toDebugString()"
      ],
      "id": "122dd99a"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Basic statistics\n",
        "\n",
        "First we need to import some things:\n",
        "\n",
        "-   `Window` class\n",
        "-   SQL functions module\n",
        "-   Some very useful functions\n",
        "-   Spark types"
      ],
      "id": "a184423b-d196-4726-beda-035724be95ca"
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pyspark.sql import Window\n",
        "import pyspark.sql.functions as func\n",
        "from pyspark.sql.types import *\n",
        "from pyspark.sql.functions import col, lit"
      ],
      "id": "e096f00c"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Compute the total number of unique users"
      ],
      "id": "58371d9c-f972-484a-891d-ba6695c2ac1e"
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "( \n",
        "    df.select('xid')\n",
        "      .distinct()\n",
        "      .count()\n",
        ")"
      ],
      "id": "4e2b5995"
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "def foo(x): yield len(set(x))"
      ],
      "id": "7f53937a"
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "( df.rdd\n",
        "    .map(lambda x : x.xid)\n",
        "    .mapPartitions(foo)\n",
        "    .collect()\n",
        ")"
      ],
      "id": "7690dafd"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This might pump up some computational resources"
      ],
      "id": "2bcbf254-d5d7-4013-96d5-d0c212f563b6"
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "( \n",
        "    df.select('xid')\n",
        "      .distinct() \n",
        "      .explain()\n",
        ")"
      ],
      "id": "f36ffbb2"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> **Note**\n",
        ">\n",
        "> The distinct values of `xid` seem to be evenly spread among the six\n",
        "> files making the `parquet` directory. Note that the last six\n",
        "> partitions look empty.\n",
        "\n",
        "## Construct a column containing the total number of actions per user"
      ],
      "id": "8b6f0e42-0200-45b8-9a1a-3c63d9b89fa1"
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "xid_partition = Window.partitionBy('xid')\n",
        "\n",
        "n_events = func.count(col('action')).over(xid_partition)\n",
        "\n",
        "df = df.withColumn('n_events', n_events)\n",
        "\n",
        "df.head(n=2)"
      ],
      "id": "ef2e9296"
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "( \n",
        "  df\n",
        "    .groupBy('xid')\n",
        "    .agg(func.count('action'))\n",
        "    .head(5)\n",
        ")"
      ],
      "id": "e14fa55d"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Construct a column containing the number of days since the last action of the user"
      ],
      "id": "8ce141e3-7446-4686-acd7-e7e5241e9bfa"
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "max_date = (\n",
        "  func\n",
        "    .max(col('date'))\n",
        "    .over(xid_partition)\n",
        ")\n",
        "\n",
        "n_days_since_last_event = func.datediff(func.current_date(), max_date)\n",
        "\n",
        "df = df.withColumn('n_days_since_last_event',\n",
        "                   n_days_since_last_event)\n",
        "\n",
        "df.head(n=2)"
      ],
      "id": "8a06d76b"
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "df.printSchema()"
      ],
      "id": "811f5328"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Construct a column containing the number of actions of each user for each modality of device\n",
        "\n",
        "Does this `partitionBy` triggers shuffling?"
      ],
      "id": "0030db6f-856b-4283-9dd7-d21b1b597df5"
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "xid_device_partition = xid_partition.partitionBy('device')\n",
        "\n",
        "n_events_per_device = func.count(col('action')).over(xid_device_partition)\n",
        "\n",
        "df = df.withColumn('n_events_per_device', n_events_per_device)\n",
        "\n",
        "df.head(n=2)"
      ],
      "id": "c6322032"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Number of devices per user"
      ],
      "id": "b6676ff3-62b6-4bf6-8b50-194c986b23e1"
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "# xid_partition = Window.partitionBy('xid')\n",
        "\n",
        "rank_device = (\n",
        "  func\n",
        "    .dense_rank()\n",
        "    .over(xid_partition.orderBy('device'))\n",
        ")\n",
        "\n",
        "n_unique_device = (\n",
        "    func\n",
        "      .last(rank_device)\n",
        "      .over(xid_partition)\n",
        ")\n",
        "\n",
        "df = df.withColumn('n_device', n_unique_device)\n",
        "\n",
        "df.head(n=2)"
      ],
      "id": "5cd6d049"
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "df\\\n",
        "    .where(col('n_device') > 1)\\\n",
        "    .select('xid', 'device', 'n_events',  'n_device', 'n_events_per_device')\\\n",
        "    .head(n=8)"
      ],
      "id": "77a94d17"
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "df\\\n",
        "    .where(col('n_device') > 1)\\\n",
        "    .select('xid', 'device', 'n_events',  'n_device', 'n_events_per_device')\\\n",
        "    .count()"
      ],
      "id": "7127993b"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Let’s select the correct users and build a training dataset\n",
        "\n",
        "We construct a ETL (Extract Transform Load) process on this data using\n",
        "the `pyspark.sql` API.\n",
        "\n",
        "## Extraction\n",
        "\n",
        "Here extraction is just about reading the data"
      ],
      "id": "5f3f0919-ed64-4b1d-aa3e-3a68d07b5f7e"
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "df = spark.read.parquet(input_file)\n",
        "df.head(n=3)"
      ],
      "id": "88cd9f07"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Transformation of the data\n",
        "\n",
        "At this step we compute a lot of extra things from the data. The aim is\n",
        "to build *features* that describe users."
      ],
      "id": "799a4f65-37ed-473d-af67-a1e12c35d729"
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [],
      "source": [
        "def n_events_transformer(df):\n",
        "    xid_partition = Window.partitionBy('xid')\n",
        "    n_events = func.count(col('action')).over(xid_partition)\n",
        "    \n",
        "    df = df.withColumn('n_events', n_events)\n",
        "\n",
        "    return df"
      ],
      "id": "7b446be0"
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [],
      "source": [
        "def n_events_per_action_transformer(df):\n",
        "    xid_action_partition = Window.partitionBy('xid', 'action')\n",
        "    n_events_per_action = func.count(col('action')).over(xid_action_partition)\n",
        "\n",
        "    df = df.withColumn('n_events_per_action', n_events_per_action)\n",
        "    \n",
        "    return df"
      ],
      "id": "b2d08e97"
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [],
      "source": [
        "def hour_transformer(df):\n",
        "    hour = func.hour(col('date'))\n",
        "    df = df.withColumn('hour', hour)\n",
        "    return df\n",
        "\n",
        "def weekday_transformer(df):\n",
        "    weekday = func.date_format(col('date'), 'EEEE')\n",
        "    df = df.withColumn('weekday', weekday)\n",
        "    return df\n",
        "\n",
        "def n_events_per_hour_transformer(df):\n",
        "    xid_hour_partition = Window.partitionBy('xid', 'hour')\n",
        "    n_events_per_hour = func.count(col('action')).over(xid_hour_partition)\n",
        "    df = df.withColumn('n_events_per_hour', n_events_per_hour)\n",
        "    return df\n",
        "\n",
        "def n_events_per_weekday_transformer(df):\n",
        "    xid_weekday_partition = Window.partitionBy('xid', 'weekday')\n",
        "    n_events_per_weekday = func.count(col('action')).over(xid_weekday_partition)\n",
        "    df = df.withColumn('n_events_per_weekday', n_events_per_weekday)\n",
        "    return df\n",
        "\n",
        "def n_days_since_last_event_transformer(df):\n",
        "    xid_partition = Window.partitionBy('xid')\n",
        "    max_date = func.max(col('date')).over(xid_partition)\n",
        "    n_days_since_last_event = func.datediff(func.current_date(), max_date)\n",
        "    df = df.withColumn('n_days_since_last_event',\n",
        "                       n_days_since_last_event + lit(0.1))\n",
        "    return df\n",
        "\n",
        "def n_days_since_last_action_transformer(df):\n",
        "    xid_partition_action = Window.partitionBy('xid', 'action')\n",
        "    max_date = func.max(col('date')).over(xid_partition_action)\n",
        "    n_days_since_last_action = func.datediff(func.current_date(),\n",
        "                                                        max_date)\n",
        "    df = df.withColumn('n_days_since_last_action',\n",
        "                       n_days_since_last_action + lit(0.1))\n",
        "    return df\n",
        "\n",
        "def n_unique_day_transformer(df):\n",
        "    xid_partition = Window.partitionBy('xid')\n",
        "    dayofyear = func.dayofyear(col('date'))\n",
        "    rank_day = func.dense_rank().over(xid_partition.orderBy(dayofyear))\n",
        "    n_unique_day = func.last(rank_day).over(xid_partition)\n",
        "    df = df.withColumn('n_unique_day', n_unique_day)\n",
        "    return df\n",
        "\n",
        "def n_unique_hour_transformer(df):\n",
        "    xid_partition = Window.partitionBy('xid')\n",
        "    rank_hour = func.dense_rank().over(xid_partition.orderBy('hour'))\n",
        "    n_unique_hour = func.last(rank_hour).over(xid_partition)\n",
        "    df = df.withColumn('n_unique_hour', n_unique_hour)\n",
        "    return df\n",
        "\n",
        "def n_events_per_device_transformer(df):\n",
        "    xid_device_partition = Window.partitionBy('xid', 'device')\n",
        "    n_events_per_device = func.count(func.col('device')) \\\n",
        "        .over(xid_device_partition)\n",
        "    df = df.withColumn('n_events_per_device', n_events_per_device)\n",
        "    return df\n",
        "\n",
        "def n_unique_device_transformer(df):\n",
        "    xid_partition = Window.partitionBy('xid')\n",
        "    rank_device = func.dense_rank().over(xid_partition.orderBy('device'))\n",
        "    n_unique_device = func.last(rank_device).over(xid_partition)\n",
        "    df = df.withColumn('n_device', n_unique_device)\n",
        "    return df\n",
        "\n",
        "def n_actions_per_category_id_transformer(df):\n",
        "    xid_category_id_partition = Window.partitionBy('xid', 'category_id',\n",
        "                                                   'action')\n",
        "    n_actions_per_category_id = func.count(func.col('action')) \\\n",
        "        .over(xid_category_id_partition)\n",
        "    df = df.withColumn('n_actions_per_category_id', n_actions_per_category_id)\n",
        "    return df\n",
        "\n",
        "def n_unique_category_id_transformer(df):\n",
        "    xid_partition = Window.partitionBy('xid')\n",
        "    rank_category_id = func.dense_rank().over(xid_partition\\\n",
        "                                              .orderBy('category_id'))\n",
        "    n_unique_category_id = func.last(rank_category_id).over(xid_partition)\n",
        "    df = df.withColumn('n_unique_category_id', n_unique_category_id)\n",
        "    return df\n",
        "\n",
        "def n_events_per_category_id_transformer(df):\n",
        "    xid_category_id_partition = Window.partitionBy('xid', 'category_id')\n",
        "    n_events_per_category_id = func.count(func.col('action')) \\\n",
        "        .over(xid_category_id_partition)\n",
        "    df = df.withColumn('n_events_per_category_id', n_events_per_category_id)\n",
        "    return df\n",
        "\n",
        "def n_events_per_website_id_transformer(df):\n",
        "    xid_website_id_partition = Window.partitionBy('xid', 'website_id')\n",
        "    n_events_per_website_id = func.count(col('action'))\\\n",
        "        .over(xid_website_id_partition)\n",
        "    df = df.withColumn('n_events_per_website_id', n_events_per_website_id)\n",
        "    return df"
      ],
      "id": "672edd2b"
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [],
      "source": [
        "transformers = [\n",
        "    hour_transformer,\n",
        "    weekday_transformer,\n",
        "    n_events_per_hour_transformer,\n",
        "    n_events_per_weekday_transformer,\n",
        "    n_days_since_last_event_transformer,\n",
        "    n_days_since_last_action_transformer,\n",
        "    n_unique_day_transformer,\n",
        "    n_unique_hour_transformer,\n",
        "    n_events_per_device_transformer,\n",
        "    n_unique_device_transformer,\n",
        "    n_actions_per_category_id_transformer,\n",
        "    n_events_per_category_id_transformer,\n",
        "    n_events_per_website_id_transformer,\n",
        "]"
      ],
      "id": "60c1753e"
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [],
      "source": [
        "N = 10000"
      ],
      "id": "8c8927d3"
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [],
      "source": [
        "sample_df = df.sample(withReplacement=False, fraction=.05)"
      ],
      "id": "bcd33728"
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [],
      "source": [
        "sample_df.count()"
      ],
      "id": "5434dc9e"
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [],
      "source": [
        "for transformer in transformers:\n",
        "    df = transformer(df)\n",
        "\n",
        "df.head(n=1)"
      ],
      "id": "206929d3"
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [],
      "source": [
        "for transformer in transformers:\n",
        "    sample_df = transformer(sample_df)\n",
        "\n",
        "sample_df.head(n=1)"
      ],
      "id": "df2b5b79"
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [],
      "source": [
        "df = sample_df"
      ],
      "id": "f1164e90"
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [],
      "source": [
        "sorted(df.columns)"
      ],
      "id": "b22a4e7a"
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [],
      "source": [
        "df.explain()"
      ],
      "id": "5705379b"
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [],
      "source": [
        "spark._sc.setCheckpointDir(\".\")   \n",
        "\n",
        "df.checkpoint()"
      ],
      "id": "81df4617"
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [],
      "source": [
        "df.explain()"
      ],
      "id": "a5ace83b"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load step\n",
        "\n",
        "Here, we use all the previous computations (saved in the columns of the\n",
        "dataframe) to compute aggregated informations about each user.\n",
        "\n",
        "> **Note**\n",
        ">\n",
        "> This should be DRYED"
      ],
      "id": "dbe1c418-f86b-4ac6-a725-632a445481f3"
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [],
      "source": [
        "def n_events_per_hour_loader(df):\n",
        "    csr = df\\\n",
        "        .select('xid', 'hour', 'n_events_per_hour')\\\n",
        "        .withColumnRenamed('n_events_per_hour', 'value')\\\n",
        "        .distinct() \n",
        "            # action\n",
        "    feature_name = func.concat(lit('n_events_per_hour#'), col('hour'))\n",
        "\n",
        "    csr = csr\\\n",
        "        .withColumn('feature_name', feature_name)\\\n",
        "        .drop('hour')\n",
        "    return csr\n",
        "\n",
        "def n_events_per_website_id_loader(df):\n",
        "    csr = df.select('xid', 'website_id', 'n_events_per_website_id')\\\n",
        "        .withColumnRenamed('n_events_per_hour', 'value')\\\n",
        "        .distinct()\n",
        "\n",
        "    feature_name = func.concat(lit('n_events_per_website_id#'),\n",
        "                               col('website_id'))\n",
        "    \n",
        "    csr = csr\\\n",
        "        .withColumn('feature_name', feature_name)\\\n",
        "        .drop('website_id')\n",
        "    return csr\n",
        "\n",
        "def n_events_per_hour_loader(df):\n",
        "    csr = df\\\n",
        "        .select('xid', 'hour', 'n_events_per_hour')\\\n",
        "        .withColumnRenamed('n_events_per_hour', 'value')\\\n",
        "        .distinct()\n",
        "\n",
        "    feature_name = func.concat(lit('n_events_per_hour#'), col('hour'))\n",
        "    \n",
        "    csr = csr\\\n",
        "        .withColumn('feature_name', feature_name)\\\n",
        "        .drop('hour')\n",
        "    return csr\n",
        "\n",
        "def n_events_per_weekday_loader(df):\n",
        "    csr = df\\\n",
        "        .select('xid', 'weekday', 'n_events_per_weekday')\\\n",
        "        .withColumnRenamed('n_events_per_weekday', 'value')\\\n",
        "        .distinct()\n",
        "\n",
        "    feature_name = func.concat(lit('n_events_per_weekday#'), col('weekday'))\n",
        "    \n",
        "    csr = csr\\\n",
        "        .withColumn('feature_name', feature_name)\\\n",
        "        .drop('weekday')\n",
        "\n",
        "    return csr\n",
        "\n",
        "def n_days_since_last_event_loader(df):\n",
        "    csr = df.select('xid',  'n_days_since_last_event')\\\n",
        "        .withColumnRenamed('n_days_since_last_event', 'value')\\\n",
        "        .distinct()\n",
        "    feature_name = lit('n_days_since_last_event')\n",
        "    csr = csr\\\n",
        "        .withColumn('feature_name', feature_name)\n",
        "    return csr\n",
        "\n",
        "def n_days_since_last_action_loader(df):\n",
        "    csr = df.select('xid', 'action', 'n_days_since_last_action')\\\n",
        "        .withColumnRenamed('n_days_since_last_action', 'value')\\\n",
        "        .distinct()\n",
        "    feature_name = func.concat(lit('n_days_since_last_action#'), col('action'))\n",
        "    csr = csr\\\n",
        "        .withColumn('feature_name', feature_name)\\\n",
        "        .drop('action')\n",
        "    return csr\n",
        "\n",
        "def n_unique_day_loader(df):\n",
        "    csr = df.select('xid', 'n_unique_day')\\\n",
        "        .withColumnRenamed('n_unique_day', 'value')\\\n",
        "        .distinct()\n",
        "    feature_name = lit('n_unique_day')\n",
        "    csr = csr\\\n",
        "        .withColumn('feature_name', feature_name)\n",
        "    return csr\n",
        "\n",
        "def n_unique_hour_loader(df):\n",
        "    csr = df.select('xid', 'n_unique_hour')\\\n",
        "        .withColumnRenamed('n_unique_hour', 'value')\\\n",
        "        .distinct()\n",
        "    feature_name = lit('n_unique_hour')\n",
        "    csr = csr\\\n",
        "        .withColumn('feature_name', feature_name)\n",
        "    return csr\n",
        "\n",
        "def n_events_per_device_loader(df):\n",
        "    csr = df\\\n",
        "        .select('xid', 'device', 'n_events_per_device')\\\n",
        "        .withColumnRenamed('n_events_per_device', 'value')\\\n",
        "        .distinct()\n",
        "    feature_name = func.concat(lit('n_events_per_device#'), col('device'))\n",
        "    csr = csr\\\n",
        "        .withColumn('feature_name', feature_name)\\\n",
        "        .drop('device')\n",
        "    return csr\n",
        "\n",
        "def n_unique_device_loader(df):\n",
        "    csr = df.select('xid', 'n_device')\\\n",
        "        .withColumnRenamed('n_device', 'value')\\\n",
        "        .distinct()\n",
        "    feature_name = lit('n_device')\n",
        "    csr = csr\\\n",
        "        .withColumn('feature_name', feature_name)\n",
        "    return csr\n",
        "\n",
        "def n_events_per_category_id_loader(df):\n",
        "    csr = df.select('xid', 'category_id', 'n_events_per_category_id')\\\n",
        "        .withColumnRenamed('n_events_per_category_id', 'value')\\\n",
        "        .distinct()\n",
        "    feature_name = func.concat(lit('n_events_per_category_id#'),\n",
        "                               col('category_id'))\n",
        "    csr = csr\\\n",
        "        .withColumn('feature_name', feature_name)\\\n",
        "        .drop('category_id')\n",
        "    return csr\n",
        "\n",
        "def n_actions_per_category_id_loader(df):\n",
        "    csr = df.select('xid', 'category_id', 'action', 'n_actions_per_category_id')\\\n",
        "        .withColumnRenamed('n_actions_per_category_id', 'value')\\\n",
        "        .distinct()\n",
        "    feature_name = func.concat(lit('n_actions_per_category_id#'),\n",
        "                               col('action'), lit('#'), \n",
        "                               col('category_id'))\n",
        "    csr = csr\\\n",
        "        .withColumn('feature_name', feature_name)\\\n",
        "        .drop('category_id')\\\n",
        "        .drop('action')\n",
        "    return csr\n",
        "\n",
        "def n_events_per_website_id_loader(df):\n",
        "    csr = df.select('xid', 'website_id', 'n_events_per_website_id')\\\n",
        "        .withColumnRenamed('n_events_per_website_id', 'value')\\\n",
        "        .distinct()\n",
        "    feature_name = func.concat(lit('n_events_per_website_id#'),\n",
        "                               col('website_id'))\n",
        "    csr = csr\\\n",
        "        .withColumn('feature_name', feature_name)\\\n",
        "        .drop('website_id')\n",
        "    return csr"
      ],
      "id": "ba2d216b"
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [],
      "source": [
        "from functools import reduce"
      ],
      "id": "347e6208"
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [],
      "source": [
        "loaders = [\n",
        "    n_events_per_hour_loader,\n",
        "    n_events_per_website_id_loader,\n",
        "    n_events_per_hour_loader,\n",
        "    n_events_per_weekday_loader,\n",
        "    n_days_since_last_event_loader,\n",
        "    n_days_since_last_action_loader,\n",
        "    n_unique_day_loader,\n",
        "    n_unique_hour_loader,\n",
        "    n_events_per_device_loader,\n",
        "    n_unique_device_loader,\n",
        "    n_events_per_category_id_loader,\n",
        "    n_actions_per_category_id_loader,\n",
        "    n_events_per_website_id_loader,\n",
        "]"
      ],
      "id": "5d71bdc0"
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [],
      "source": [
        "def union(df, other):\n",
        "    return df.union(other)"
      ],
      "id": "c70542de"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> **About DataFrame.union()**\n",
        ">\n",
        "> This method performs a SQL-style set union of the rows from both\n",
        "> DataFrame objects, with no automatic deduplication of elements.\n",
        ">\n",
        "> Use the distinct() method to perform deduplication of rows.\n",
        ">\n",
        "> The method resolves columns by position (not by name), following the\n",
        "> standard behavior in SQL."
      ],
      "id": "5bc58027-70c5-4dba-a57e-69987ea6bf6d"
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [],
      "source": [
        "spam = [loader(df) for loader in loaders]"
      ],
      "id": "96a459d8"
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [],
      "source": [
        "spam[0].printSchema()"
      ],
      "id": "916723ce"
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {},
      "outputs": [],
      "source": [
        "all(spam[0].columns == it.columns for it in spam[1:])"
      ],
      "id": "bc76c15e"
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {},
      "outputs": [],
      "source": [
        "len(spam)"
      ],
      "id": "e9f513ae"
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {},
      "outputs": [],
      "source": [
        "csr = reduce(\n",
        "    lambda df1, df2: df1.union(df2),\n",
        "    spam\n",
        ")\n",
        "\n",
        "csr.head(n=3)"
      ],
      "id": "0647a437"
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-05-03T15:25:54.862814Z",
          "start_time": "2020-05-03T15:25:54.857914Z"
        }
      },
      "outputs": [],
      "source": [
        "csr.columns"
      ],
      "id": "e718055b"
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-05-03T15:26:13.629146Z",
          "start_time": "2020-05-03T15:25:55.683800Z"
        }
      },
      "outputs": [],
      "source": [
        "csr.show(5)"
      ],
      "id": "67db5464"
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {},
      "outputs": [],
      "source": [
        "csr.rdd.getNumPartitions()"
      ],
      "id": "490e8690"
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-05-03T15:30:20.643141Z",
          "start_time": "2020-05-03T15:29:45.221790Z"
        }
      },
      "outputs": [],
      "source": [
        "# Replace features names and xid by a unique number\n",
        "feature_name_partition = Window().orderBy('feature_name')\n",
        "\n",
        "xid_partition = Window().orderBy('xid')\n",
        "\n",
        "col_idx = func.dense_rank().over(feature_name_partition)\n",
        "row_idx = func.dense_rank().over(xid_partition)"
      ],
      "id": "eac1f68f"
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-05-03T15:30:20.643141Z",
          "start_time": "2020-05-03T15:29:45.221790Z"
        }
      },
      "outputs": [],
      "source": [
        "csr = csr.withColumn('col', col_idx)\\\n",
        "    .withColumn('row', row_idx)\n",
        "\n",
        "csr = csr.na.drop('any')\n",
        "\n",
        "csr.head(n=5)"
      ],
      "id": "28b781fa"
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-05-03T15:32:02.552364Z",
          "start_time": "2020-05-03T15:31:14.990298Z"
        }
      },
      "outputs": [],
      "source": [
        "# Let's save the result of our hard work into a new parquet file\n",
        "output_path = './'\n",
        "output_file = os.path.join(output_path, 'csr.parquet')\n",
        "csr.write.parquet(output_file, mode='overwrite')"
      ],
      "id": "2e67f48b"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Preparation of the training dataset"
      ],
      "id": "588beee9-2633-467b-a7e8-befa7cffa54c"
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {},
      "outputs": [],
      "source": [
        "csr_path = './'\n",
        "csr_file = os.path.join(csr_path, 'csr.parquet')\n",
        "\n",
        "df = spark.read.parquet(csr_file)\n",
        "df.head(n=5)"
      ],
      "id": "49da4ba2"
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-05-03T15:33:17.229477Z",
          "start_time": "2020-05-03T15:33:16.995048Z"
        }
      },
      "outputs": [],
      "source": [
        "df.count()"
      ],
      "id": "771ab4ea"
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-05-03T15:33:20.881392Z",
          "start_time": "2020-05-03T15:33:19.624525Z"
        }
      },
      "outputs": [],
      "source": [
        "# What are the features related to campaign_id 1204 ?\n",
        "features_names = \\\n",
        "    df.select('feature_name')\\\n",
        "    .distinct()\\\n",
        "    .toPandas()['feature_name']"
      ],
      "id": "9b9926ca"
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-05-03T15:33:21.818568Z",
          "start_time": "2020-05-03T15:33:21.812810Z"
        }
      },
      "outputs": [],
      "source": [
        "features_names"
      ],
      "id": "99660614"
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-05-03T15:33:27.083141Z",
          "start_time": "2020-05-03T15:33:27.078374Z"
        }
      },
      "outputs": [],
      "source": [
        "[feature_name for feature_name in features_names if '1204' in feature_name]"
      ],
      "id": "f032fc63"
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-05-03T15:33:28.560631Z",
          "start_time": "2020-05-03T15:33:27.903921Z"
        }
      },
      "outputs": [],
      "source": [
        "# Look for the xid that have at least one exposure to campaign 1204\n",
        "keep = func.when(\n",
        "    (col('feature_name') == 'n_actions_per_category_id#C#1204.0') |\n",
        "    (col('feature_name') == 'n_actions_per_category_id#O#1204.0'),\n",
        "    1).otherwise(0)\n",
        "df = df.withColumn('keep', keep)\n",
        "\n",
        "df.where(col('keep') > 0).count()"
      ],
      "id": "f166bcad"
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Sum of the keeps :)\n",
        "xid_partition = Window.partitionBy('xid')\n",
        "sum_keep = func.sum(col('keep')).over(xid_partition)\n",
        "df = df.withColumn('sum_keep', sum_keep)"
      ],
      "id": "4b3338b9"
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Let's keep the xid exposed to 1204\n",
        "df = df.where(col('sum_keep') > 0)"
      ],
      "id": "3f63b47d"
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {},
      "outputs": [],
      "source": [
        "df.count()"
      ],
      "id": "0bfb2dac"
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {},
      "outputs": [],
      "source": [
        "df.select('xid').distinct().count()"
      ],
      "id": "1b2f882d"
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {},
      "outputs": [],
      "source": [
        "row_partition = Window().orderBy('row')\n",
        "col_partition = Window().orderBy('col')\n",
        "\n",
        "row_new = func.dense_rank().over(row_partition)\n",
        "col_new = func.dense_rank().over(col_partition)\n",
        "\n",
        "df = df.withColumn('row_new', row_new)\n",
        "df = df.withColumn('col_new', col_new)\n",
        "\n",
        "csr_data = df.select('row_new', 'col_new', 'value').toPandas()"
      ],
      "id": "c5ab0cf2"
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-05-03T15:33:52.617724Z",
          "start_time": "2020-05-03T15:33:52.609488Z"
        }
      },
      "outputs": [],
      "source": [
        "csr_data.head()"
      ],
      "id": "cacd093c"
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {},
      "outputs": [],
      "source": [
        "features_names = df.select('feature_name', 'col_new').distinct()\n",
        "features_names.where(col('feature_name') == 'n_actions_per_category_id#C#1204.0').head()"
      ],
      "id": "c13eb352"
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {},
      "outputs": [],
      "source": [
        "features_names.where(col('feature_name') == 'n_actions_per_category_id#O#1204.0').head()"
      ],
      "id": "9a16d677"
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-05-03T15:34:11.510538Z",
          "start_time": "2020-05-03T15:34:11.454802Z"
        }
      },
      "outputs": [],
      "source": [
        "from scipy.sparse import csr_matrix\n",
        "import numpy as np\n",
        "\n",
        "rows = csr_data['row_new'].values - 1\n",
        "cols = csr_data['col_new'].values - 1\n",
        "vals = csr_data['value'].values\n",
        "\n",
        "X_csr = csr_matrix((vals, (rows, cols)))"
      ],
      "id": "8f09006f"
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-05-03T15:34:11.977267Z",
          "start_time": "2020-05-03T15:34:11.972602Z"
        }
      },
      "outputs": [],
      "source": [
        "X_csr.shape"
      ],
      "id": "058e90fa"
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_csr.shape, X_csr.nnz"
      ],
      "id": "d1c15d50"
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_csr.nnz / (X_csr.shape[0]* X_csr.shape[1])   # 0152347 * 92)"
      ],
      "id": "6025957a"
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {},
      "outputs": [],
      "source": [
        "# The label vector. Let's make it dense, flat and binary\n",
        "y = np.array(X_csr[:, 1].todense()).ravel()\n",
        "y = np.array(y > 0, dtype=np.int64)"
      ],
      "id": "e5e24a1d"
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_csr.shape"
      ],
      "id": "3d92d068"
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {},
      "outputs": [],
      "source": [
        "# We remove the second and fourth column. \n",
        "# It actually contain the label we'll want to predict.\n",
        "kept_cols = list(range(X_csr.shape[1]))\n",
        "kept_cols.pop(1)\n",
        "kept_cols.pop(2)\n",
        "X = X_csr[:, kept_cols]"
      ],
      "id": "6f03194c"
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {},
      "outputs": [],
      "source": [
        "len(kept_cols)"
      ],
      "id": "9587f3bd"
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_csr.shape, X.shape"
      ],
      "id": "364d972c"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Finally !!\n",
        "\n",
        "Wow ! That was a lot of work. Now we have a features matrix $X$ and a\n",
        "vector of labels $y$."
      ],
      "id": "7865317f-fe47-47ec-bb03-d21da42bf861"
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {},
      "outputs": [],
      "source": [
        "X.indices"
      ],
      "id": "9307e51c"
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {},
      "outputs": [],
      "source": [
        "X.indptr"
      ],
      "id": "598e52c2"
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {},
      "outputs": [],
      "source": [
        "X.shape, X.nnz"
      ],
      "id": "81891a9b"
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {},
      "outputs": [],
      "source": [
        "y.shape, y.sum()"
      ],
      "id": "acbbe365"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Some learning for/from this data"
      ],
      "id": "f07a58a3-b101-41f9-87ba-524f9d6e3cbf"
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import MaxAbsScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Normalize the features\n",
        "X = MaxAbsScaler().fit_transform(X)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.3)\n",
        "\n",
        "clf = LogisticRegression(\n",
        "    penalty='l2',\n",
        "    C=1e3,\n",
        "    solver='lbfgs',\n",
        "    class_weight='balanced'\n",
        ")\n",
        "\n",
        "clf.fit(X_train, y_train)"
      ],
      "id": "b9023330"
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {},
      "outputs": [],
      "source": [
        "features_names = features_names.toPandas()['feature_name']"
      ],
      "id": "010202df"
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {},
      "outputs": [],
      "source": [
        "features_names[range(6)]"
      ],
      "id": "92def0c6"
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "id": "5fff01c6"
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.figure(figsize=(16, 5))\n",
        "plt.stem(clf.coef_[0]) # , use_line_collection=True)\n",
        "plt.title('Logistic regression coefficients', fontsize=18)"
      ],
      "id": "9eebc966"
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {},
      "outputs": [],
      "source": [
        "clf.coef_[0].shape[0]"
      ],
      "id": "f3396510"
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {},
      "outputs": [],
      "source": [
        "len(features_names)"
      ],
      "id": "ddec1aa7"
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {},
      "outputs": [],
      "source": [
        "# We change the fontsize of minor ticks label\n",
        "_ = plt.xticks(np.arange(clf.coef_[0].shape[0]), features_names, \n",
        "           rotation='vertical', fontsize=8)"
      ],
      "id": "a7706cde"
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {},
      "outputs": [],
      "source": [
        "_ = plt.yticks(fontsize=14)"
      ],
      "id": "c8609d2e"
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-05-03T15:51:25.280157Z",
          "start_time": "2020-05-03T15:51:25.081464Z"
        }
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import precision_recall_curve, f1_score\n",
        "\n",
        "precision, recall, _ = precision_recall_curve(y_test, clf.predict_proba(X_test)[:, 1])\n",
        "    \n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(recall, precision, label='LR (F1=%.2f)' % f1_score(y_test, clf.predict(X_test)), lw=2)\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('Recall', fontsize=16)\n",
        "plt.ylabel('Precision', fontsize=16)\n",
        "plt.title('Precision/recall curve', fontsize=18)\n",
        "plt.legend(loc=\"upper right\", fontsize=14)"
      ],
      "id": "f534e3d7"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Analyse the tables"
      ],
      "id": "ffd02fde-9bd9-4f10-9401-eacdd884a8e5"
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {},
      "outputs": [],
      "source": [
        "query = \"\"\"ANALYZE TABLE db_table COMPUTE STATISTICS\n",
        "            FOR COLUMNS xid\"\"\""
      ],
      "id": "03c15bc6"
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {},
      "outputs": [],
      "source": [
        "df.createOrReplaceTempView(\"db_table\")"
      ],
      "id": "cb68c9de"
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {},
      "outputs": [],
      "source": [
        "df.columns"
      ],
      "id": "2302baa8"
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {},
      "outputs": [],
      "source": [
        "spark.sql(\"cache table db_table\")"
      ],
      "id": "cd3683f6"
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {},
      "outputs": [],
      "source": [
        "spark.sql(query)"
      ],
      "id": "d670a103"
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {},
      "outputs": [],
      "source": [
        "spark.sql(\"show tables\")"
      ],
      "id": "ead4eae8"
    }
  ],
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "path": "/usr/share/jupyter/kernels/python3"
    },
    "language_info": {
      "name": "python",
      "codemirror_mode": {
        "name": "ipython",
        "version": "3"
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  }
}