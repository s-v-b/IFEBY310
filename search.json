[
  {
    "objectID": "weeks/week-7.html",
    "href": "weeks/week-7.html",
    "title": "Weeks 7",
    "section": "",
    "text": "Important\n\n\n\n1 Session\n\nFriday 7 March 2025 Sophie Germain 1005 13h30-15h30\n Calendar"
  },
  {
    "objectID": "weeks/week-7.html#lecture-slides",
    "href": "weeks/week-7.html#lecture-slides",
    "title": "Weeks 7",
    "section": "Lecture : slides",
    "text": "Lecture : slides\n\n Json format\n\nWe may come back to several parts of\n\n Spark high level APIs: SQL\n Spark low level APIs: RDD\n Python Data Science Stack\n Dask"
  },
  {
    "objectID": "weeks/week-7.html#notebooks",
    "href": "weeks/week-7.html#notebooks",
    "title": "Weeks 7",
    "section": "Notebooks",
    "text": "Notebooks\nWe shall spend most of the lectures on\n\n Jupyter notebook VII : JSON\n html: JSON\n\nYou shall have gone through (on your own)\n\n Jupyter notebook VI : Spark SQL\n Jupyter notebook V : Spark RDD\n html: Spark RDD\n\n Jupyter notebook I : tour of Python\n html: tour of Python\n\n Jupyter notebook II : tour of numpy\n html: tour of numpy\n\n Jupyter notebook III: tour of pandas\n html: tour of Pandas"
  },
  {
    "objectID": "weeks/week-7.html#references",
    "href": "weeks/week-7.html#references",
    "title": "Weeks 7",
    "section": "References",
    "text": "References\n\n\n\nSpark Definitive Guide\nSpark project\nSpark documentation\nData pipelines\nNext Generation Databases NoSQLand Big Data, Guy Harrison\nMastering Spark on R\n\n\n\nParquet\n‘Arrow’\nDask Docs\nDask Examples\nDask Code\nDask Blog\nPandas cheatsheet"
  },
  {
    "objectID": "weeks/week-7.html#logistics",
    "href": "weeks/week-7.html#logistics",
    "title": "Weeks 7",
    "section": "Logistics",
    "text": "Logistics\n pyspark\n To work the jupyter notebooks, install python 3, and modules related to jupyter: jupyter-cache, jupyter_client, jupyter_core, jupyterlab_widgets (this induces the installation of dependencies).\nhttps://jupyter.org\nDownload the jupyter notebooks from notebooks listings.\n If you do not already have an ENT account, follow instructions on Moodle to get one. You shall need this account to connect to PostGres cluster.\n\n\nBack to Agenda ⏎"
  },
  {
    "objectID": "weeks/week-3.html",
    "href": "weeks/week-3.html",
    "title": "Week 3",
    "section": "",
    "text": "Important\n\n\n\n\nRoom: Sophie Germain 0014\nFriday 31 January 2025 15h45-17h45\n Calendar"
  },
  {
    "objectID": "weeks/week-3.html#lecture-slides",
    "href": "weeks/week-3.html#lecture-slides",
    "title": "Week 3",
    "section": "Lecture : slides",
    "text": "Lecture : slides\n\n Dask\n\nWe shall briefly come back to several parts of\n\n Python Data Science Stack\n\nData processing\nVisualization"
  },
  {
    "objectID": "weeks/week-3.html#notebooks",
    "href": "weeks/week-3.html#notebooks",
    "title": "Week 3",
    "section": "Notebooks",
    "text": "Notebooks\nWe shall spend most of the lecture on\nDask Tutorial\nYou shall have gone through (on your own)\n\n Jupyter notebook I : tour of Python\n html: tour of Python\n\n Jupyter notebook II : tour of numpy\n html: tour of numpy\n\n Jupyter notebook III: tour of pandas\n html: tour of Pandas"
  },
  {
    "objectID": "weeks/week-3.html#references",
    "href": "weeks/week-3.html#references",
    "title": "Week 3",
    "section": "References",
    "text": "References\n\nDask Docs\nDask Examples\nDask Code\nDask Blog\nPandas cheatsheet"
  },
  {
    "objectID": "weeks/week-3.html#logistics",
    "href": "weeks/week-3.html#logistics",
    "title": "Week 3",
    "section": "Logistics",
    "text": "Logistics\n To work the jupyter notebooks, install python 3, and modules related to jupyter: jupyter-cache, jupyter_client, jupyter_core, jupyterlab_widgets (this induces the installation of dependencies).\nhttps://jupyter.org\nDownload the jupyter notebooks from notebooks listings.\n If you do not already have an ENT account, follow instructions on Moodle to get one. You shall need this account to connect to PostGres cluster.\n\n\nBack to Agenda ⏎"
  },
  {
    "objectID": "weeks/week-1.html",
    "href": "weeks/week-1.html",
    "title": "Week 1",
    "section": "",
    "text": "Important\n\n\n\n\nRoom: Sophie Germain 0014\nFriday 17 January 2025 15h45-17h45\n Calendar"
  },
  {
    "objectID": "weeks/week-1.html#lecture-slides",
    "href": "weeks/week-1.html#lecture-slides",
    "title": "Week 1",
    "section": "Lecture : slides",
    "text": "Lecture : slides\nWe went (rapidly) through the two decks of slides :\n\n Introduction to Big Data\n Python Data Science Stack\n\nWe shall come back to the description of Spark in a couple of weeks\nDuring the next two weeks, we shall explore the Python Data Stack."
  },
  {
    "objectID": "weeks/week-1.html#notebooks",
    "href": "weeks/week-1.html#notebooks",
    "title": "Week 1",
    "section": "Notebooks",
    "text": "Notebooks\n\n Jupyter notebook I : tour of Python\n html: tour of Python\n Jupyter notebook II : tour of numpy\n html: tour of numpy"
  },
  {
    "objectID": "weeks/week-1.html#references",
    "href": "weeks/week-1.html#references",
    "title": "Week 1",
    "section": "References",
    "text": "References\nYou can watch the Introduction to NumPy conference presented at EuroSciPy 2023"
  },
  {
    "objectID": "weeks/week-1.html#logistics",
    "href": "weeks/week-1.html#logistics",
    "title": "Week 1",
    "section": "Logistics",
    "text": "Logistics\n To work the jupyter notebooks, install python 3, and modules related to jupyter: jupyter-cache, jupyter_client, jupyter_core, jupyterlab_widgets (this induces the installation of dependencies).\nhttps://jupyter.org\nDownload the jupyter notebooks from notebooks listings.\n If you do not already have an ENT account, follow instructions on Moodle to get one. You shall need this account to connect to PostGres cluster.\n\n\nBack to Agenda ⏎"
  },
  {
    "objectID": "slides-listings.html",
    "href": "slides-listings.html",
    "title": "Slides",
    "section": "",
    "text": "Slides provide the framework for the course. You can use them to familiarise yourself with the material for the next lesson, and/or to review previous lessons.\nIcon  points to chalkboard arguments.\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Titre\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Description\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\nDate\n\n\nTitre\n\n\nDescription\n\n\n\n\n\n\nJan 17, 2025\n\n\nIntroduction to Big Data\n\n\n\n\n\n\n\nJan 17, 2025\n\n\nPython data stack : Numpy et Scipy\n\n\n\n\n\n\n\nJan 24, 2025\n\n\nPython data stack : Pandas \n\n\n\n\n\n\n\nJan 31, 2025\n\n\nPython data stack : Dask\n\n\n\n\n\n\n\nFeb 7, 2025\n\n\nSpark : RDDs\n\n\n\n\n\n\n\nFeb 20, 2025\n\n\nSpark : SQL\n\n\n\n\n\n\n\nFeb 21, 2025\n\n\nPandas on Spark and sparklyr\n\n\n\n\n\n\n\nMar 7, 2025\n\n\nJSON\n\n\n\n\n\n\n\nMar 14, 2025\n\n\nFile formats for Big Data\n\n\n\n\n\n\n\nMar 21, 2025\n\n\nSpark: a deeper dive\n\n\n\n\n\n\n\nMar 28, 2025\n\n\nSpark: tips\n\n\n\n\n\n\n\nApr 4, 2025\n\n\nSpark: Applications\n\n\n\n\n\n\n\nApr 11, 2025\n\n\nSpark: Applications\n\n\n\n\n\n\n\n\nNo matching items\n\n\n\n\n\n\n\n\nMode d’emploi\n\n\n\nSlides use revealjs de . They are best viewed using a browser.\nKeyboard shortcut  takes you to the Help page.",
    "crumbs": [
      "Slides"
    ]
  },
  {
    "objectID": "projects-listings.html",
    "href": "projects-listings.html",
    "title": "Projects",
    "section": "",
    "text": "Modus operandi\n\n\n\nCourse evaluation is based on Projects \n\n Find a friend : all work done by pairs of students\n Create a single private GitHub repository for each project and each pair of students.\n Grant me access to these repositories\n All work is transmitted through your private repository and nowhere else\n No emails for project submission\n All projects deliverables consist of Quarto files\n The report should be rendered at least in HTML format, and possibly also in PDF format\n Do not forget to disclose your name and first name in the report\n\n\n\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Description\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\nDate\n\n\nTitle\n\n\nDescription\n\n\n\n\n\n\nMar 10, 2025\n\n\nTable wrangling, visualization\n\n\nPython, NLP, Spacy, Dask\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "Homeworks"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "IFEBY310: Big Data Technologies",
    "section": "",
    "text": "moodle\n  \n  \n    \n     Github\n  \n\n  \n  \n\n\nCourse IFEBY310: Big Data Technologies\nfor Master MIDS (M1) at Université Paris Cité introduces a collection of software technologies dedicated to Big Data management. This course is designed for students with dual background in Mathematics and Computer Science.\n\n\n\nDuring this course, you shall learn to:\n\nHandle middlesize data using Python Data Stack: Numpy/Scipy/Pandas\nScale up and down with Dask\nHandle Big Data with Spark (PySpark)\nManage and store data using dedicated columnar formats (Parquet, ORC, Avro, Arrow)\n\n\n\n\nWe will work on Spark using Docker images. It is a good idea to install Python and the Python data stack on your laptop.\n\n\n\n\n\nCourse Syllabus\nCurrent and past teams\nSoftware tips\nSlides\nNotebooks\nProjects",
    "crumbs": [
      "Information",
      "Glimpse"
    ]
  },
  {
    "objectID": "cours-syllabus.html",
    "href": "cours-syllabus.html",
    "title": "IFEBY310 Syllabus",
    "section": "",
    "text": "Organization\n\n\n\nWe will have one weeky lecture. Each lecture is organized around Slides and Notebooks. We will switch from blackboard to laptop and back. You are invited to bring your laptop to the lectures.\n\n\n\n\n\n\n\n\n\n\n\nDay\nHour\nRoom\nStart\n\n\n\n\nLecture\nFriday\n15:45 - 17:46\nSophie Germain 014\n2025-01-17 –2025-02-21\n\n\nLecture\nFriday\n13:30 - 15:30\nSophie Germain 1005\n2025-03-07-…\n\n\n\n\n\n We will not attempt to complete the notebooks during the sessions. You are expected to complete the noteboks on your own time. Solutions (at least partial solutions) are available on the course website.\nYou can fork the course repository and post issues, comments, and corrections.\n\n\n\n\n\n\n\n\n\nObjectives\n\n\n\nDuring this course, you shall learn to:\n\nHandle middlesize data using Python Data Stack: Numpy/Scipy/Pandas\nScale up and down with Dask\nHandle Big Data with Spark (PySpark)\nManage and store data using dedicated columnar formats (Parquet, ORC, Avro, Arrow)\n\n\n\n\n\n\n\n\n\nCommunication \n\n\n\nCourse material: s-v-b.github.io/IFEBY310 Fork the repo, use github issues to send feedback (no email please)\nAlerts are spread through Moodle\nRegister at Moodle portal to be updated\n\n\n\n\n\n\n\n\nComputing environment \n\n\n\n We use the PostGreSQL server from UFR de Mathématiques. To obtain an ENT account  follow procedure Moodle\n You may install and use\n\nPython\nJupyter\nPandas\nNumpy\nScipy\nPlotly\nAltair\nDask\nSpark\nParquet\n‘Arrow’\nDocker\nQuarto\nR\npsql\nVS Code\n\n\n\n\n\n\n\n\n\nRéférences \n\n\n\n\nPandas Book\nPython Data Science Handbook\nDask\nSpark\nData pipelines\nData pipelines\nAlice\nDocumentation PostGres\nNext Generation Databases NoSQLand Big Data, Guy Harrison\nGuy Harrison Blog\nDatabases trends and applications\nUpcoming book “Principles of Databases”, by Marcelo Arenas, Pablo Barcelo, Leonid Libkin, Wim Martens, and Andreas Pieris.\n\n\n\n\n\n\n\n\n\n\nCourses \n\n\n\nslides\nNotebooks (html and ipynb)\n\n\n\n\n\n\n\n\n\nEvaluation \n\n\n\n\nTwo homeworks/projects\nGrading\n\n\n\n\n\n\n\n\n\n Trucs\n\n\n\n\nHave a look at slides before the course\nDon’t jump to corrections\nUse online help (StackOverflow, ChatGPT, copilot, …)\nRead error messages\n\n\n\n\n\n\n\n\n\nCode of conduct\n\n\n\nTL;DR: No cheating!\n\n\n\n\n\n\n\n\nSave the dates ! \n\n\n\n\nJanuary 17: Course kick-off\nFebruary 14: No session\nFebruary 28: Winter Holidays\nMarch 7: Session 6\nApril 4: Room 2017\nApril 18: Eastern Holidays\nApril 25: Eastern Holidays\nMay 2: Session 12\n\nUniversité Paris Cité calendar\nM1 MIDS calendar\n\n\n\n\n\n\n\n\nUniversité Paris Cité\n\n\n\nUseful links:\n\nCharte Université Paris Cité\nDémarches et accessibilité\nCentre de contact\nRelais handicap",
    "crumbs": [
      "Information",
      "Syllabus"
    ]
  },
  {
    "objectID": "core/notebooks/xcitibike_spark.html",
    "href": "core/notebooks/xcitibike_spark.html",
    "title": "Building parquet dataset from extracted csv files",
    "section": "",
    "text": "Code\nimport os\nimport sys\nimport re \nimport pandas as pd\nimport datetime\nfrom tqdm import tqdm\n\nos.environ['PYSPARK_PYTHON'] = sys.executable\nos.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable\nCode\ndata_dir = \"../data\"\n# os.path.exists(data_dir)\n\nextract_dir = os.path.join(data_dir, \"xcitibike\")\nif not os.path.exists(extract_dir):\n    os.mkdir(extract_dir)\n\nparquet_dir = os.path.join(data_dir, \"pq_citibike\")\nif not os.path.exists(parquet_dir):\n    os.mkdir(parquet_dir)\n\ncheckpoint_dir = os.path.join(data_dir, \"citibike_charlie\")\nif not os.path.exists(checkpoint_dir):\n    os.mkdir(checkpoint_dir)\n\n\n\n---------------------------------------------------------------------------\nFileNotFoundError                         Traceback (most recent call last)\nCell In[2], line 6\n      4 extract_dir = os.path.join(data_dir, \"xcitibike\")\n      5 if not os.path.exists(extract_dir):\n----&gt; 6     os.mkdir(extract_dir)\n      8 parquet_dir = os.path.join(data_dir, \"pq_citibike\")\n      9 if not os.path.exists(parquet_dir):\n\nFileNotFoundError: [Errno 2] No such file or directory: '../data/xcitibike'\nCode\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql import functions as fn\nfrom pyspark.sql.functions import pandas_udf\nfrom pyspark.sql.types import BooleanType\nfrom pyspark.sql.functions import PandasUDFType\nCode\nspark = (SparkSession\n    .builder\n    .appName(\"Spark building citibike parquet file\")\n    .getOrCreate()\n)\n\n\n25/01/15 06:09:07 WARN Utils: Your hostname, boucheron-Precision-5480 resolves to a loopback address: 127.0.1.1; using 192.168.10.120 instead (on interface wlp0s20f3)\n25/01/15 06:09:07 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\nSetting default log level to \"WARN\".\nTo adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n25/01/15 06:09:08 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\nCode\nspark.sparkContext.setCheckpointDir(checkpoint_dir)\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[5], line 1\n----&gt; 1 spark.sparkContext.setCheckpointDir(checkpoint_dir)\n\nNameError: name 'checkpoint_dir' is not defined\nCode\n@pandas_udf(BooleanType())\ndef detect_non_ISO(s: pd.Series) -&gt; bool:\n    r = s.str.match(r\"\\d+/\\d+/\\d+\").any()\n    return r\n\n@pandas_udf(\"string\")\ndef make_iso(s: pd.Series) -&gt; pd.Series:\n    t = s.str.split(' ', expand=True)\n    u = t[0].str.split('/')\n    v = u.map(lambda x  : [x[2], x[0], x[1]]).str.join('-')\n    w = v.combine(t[1], lambda x, y : ' '.join([x, y]))\n    return w\nCode\ndicts_rename = {\n    1: {\n 'tripduration': 'trip_duration',\n 'starttime': 'started_at',\n 'stoptime': 'ended_at',\n 'bikeid': 'bike_id',\n 'usertype': 'user_type',\n 'start station latitude': 'start_lat',\n 'start station longitude': 'start_lng',\n 'end station latitude': 'end_lat',\n 'end station longitude': 'end_lng'\n} ,\n  2:  {\n 'Trip Duration': 'trip_duration',\n  'Start Time': 'started_at',\n  'Stop Time': 'ended_at',\n  'Start Station Latitude': 'start_lat',\n  'Start Station Longitude': 'start_lng',\n  'End Station Latitude': 'end_lat',\n  'End Station Longitude': 'end_lng'    \n}\n}\nCode\nfor (root, dirs ,files) in tqdm(os.walk(extract_dir, topdown=True)):\n    if dirs:\n        continue\n\n    for flnm in files:\n        if not flnm.endswith('.csv'):  \n            continue\n\n        fpath = os.path.join(root, flnm)\n        df = spark.read.option(\"header\",\"true\").csv(fpath)\n\n        df = (\n            df.withColumnsRenamed(dicts_rename[1])\n            .withColumnsRenamed(dicts_rename[2])\n        )\n\n        df = df.toDF(*[c.replace(' ','_').lower() for c in df.columns])\n\n        if re.match(r\"\\d+/\\d+/\\d+\", df.select(\"started_at\").first()[0]):\n            df = (\n                   df\n                    .withColumn('started_at', make_iso(fn.col(\"started_at\")))\n                    .withColumn('ended_at', make_iso(fn.col(\"ended_at\")))\n            ) \n\n        df = df.withColumns(\n                {\n                'started_at': fn.to_timestamp(fn.col(\"started_at\")),\n                'ended_at': fn.to_timestamp(fn.col(\"ended_at\"))\n                }\n            )   \n\n        df = df.withColumns(\n                {\n                    'start_year': fn.year(fn.col('started_at')),\n                    'start_month': fn.month(fn.col('ended_at'))\n                }\n            )\n\n        df.checkpoint(eager=True)\n\n        # df.printSchema()\n\n        df.write.parquet(\n            parquet_dir, \n            partitionBy=['start_year', 'start_month'], \n            mode=\"append\"\n        )\n\n\n0it [00:00, ?it/s]0it [00:00, ?it/s]\nCode\n# spark.stop()"
  },
  {
    "objectID": "core/notebooks/xcitibike_spark.html#references",
    "href": "core/notebooks/xcitibike_spark.html#references",
    "title": "Building parquet dataset from extracted csv files",
    "section": "References",
    "text": "References\nPython vectorized string computations"
  },
  {
    "objectID": "core/notebooks/notebookxx_pg_pandas_spark.html",
    "href": "core/notebooks/notebookxx_pg_pandas_spark.html",
    "title": "PostgreSQL and Spark",
    "section": "",
    "text": "Reading and sriting Spark Dataframes from and to Databases\nCode\nimport pyspark\nfrom pyspark import SparkConf\nfrom pyspark.sql import SparkSession\n\nimport os\nimport getpass"
  },
  {
    "objectID": "core/notebooks/notebookxx_pg_pandas_spark.html#connect-to-pg-server",
    "href": "core/notebooks/notebookxx_pg_pandas_spark.html#connect-to-pg-server",
    "title": "PostgreSQL and Spark",
    "section": "Connect to Pg server",
    "text": "Connect to Pg server\n\n\nCode\nulogin = getpass.getuser()\npw = getpass.getpass()\n\n\n\n---------------------------------------------------------------------------\nStdinNotImplementedError                  Traceback (most recent call last)\nCell In[2], line 2\n      1 ulogin = getpass.getuser()\n----&gt; 2 pw = getpass.getpass()\n\nFile ~/Documents/IFEBY310/.venv/lib/python3.12/site-packages/ipykernel/kernelbase.py:1256, in Kernel.getpass(self, prompt, stream)\n   1254 if not self._allow_stdin:\n   1255     msg = \"getpass was called, but this frontend does not support input requests.\"\n-&gt; 1256     raise StdinNotImplementedError(msg)\n   1257 if stream is not None:\n   1258     import warnings\n\nStdinNotImplementedError: getpass was called, but this frontend does not support input requests.\n\n\n\nSpark jdbc readers and writers rely on a collection of options. Some options are used repeatedly. In order to avoid cut and paste, we pack them in a dictionary.\n\n\nCode\ndico_jdbc_pg = {\n    \"url\":  \"jdbc:postgresql://localhost:5434/bd_2023-24\",  \n    \"user\":  ulogin, \n    \"password\":  pw, \n    \"driver\":  \"org.postgresql.Driver\"\n}\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[3], line 4\n      1 dico_jdbc_pg = {\n      2     \"url\":  \"jdbc:postgresql://localhost:5434/bd_2023-24\",  \n      3     \"user\":  ulogin, \n----&gt; 4     \"password\":  pw, \n      5     \"driver\":  \"org.postgresql.Driver\"\n      6 }\n\nNameError: name 'pw' is not defined\n\n\n\n\n\nCode\ndbschema = 'nycflights'"
  },
  {
    "objectID": "core/notebooks/notebookxx_pg_pandas_spark.html#reading-spark-dataframes-from-a-postgresql-database",
    "href": "core/notebooks/notebookxx_pg_pandas_spark.html#reading-spark-dataframes-from-a-postgresql-database",
    "title": "PostgreSQL and Spark",
    "section": "Reading Spark Dataframes from a PostgreSQL database",
    "text": "Reading Spark Dataframes from a PostgreSQL database\n\n\nCode\nspark_home = \"/home/boucheron/.local/share/spark-3.5.0-bin-hadoop3\"\n\n\n\nTo get started you will need to include the JDBC driver for your particular database on the spark classpath.\n\n\n\nCode\nspark = (\n  SparkSession \n    .builder \n    .appName(\"Python Spark SQL basic example\") \n    .config(\"spark.jars\", \n            spark_home + \"/jars/\" + \"postgresql-42.7.2.jar\") \n    .getOrCreate()\n)\n\n\n25/01/14 22:39:39 WARN Utils: Your hostname, boucheron-Precision-5480 resolves to a loopback address: 127.0.1.1; using 192.168.10.120 instead (on interface wlp0s20f3)\n25/01/14 22:39:39 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n25/01/14 22:39:39 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\nSetting default log level to \"WARN\".\nTo adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel)."
  },
  {
    "objectID": "core/notebooks/notebookxx_pg_pandas_spark.html#downloading-a-table-to-spark",
    "href": "core/notebooks/notebookxx_pg_pandas_spark.html#downloading-a-table-to-spark",
    "title": "PostgreSQL and Spark",
    "section": "Downloading a table to Spark",
    "text": "Downloading a table to Spark\nWe rely on dictionary union and dictionary unpacking to set the options.\n\n\nCode\ndf_airlines = (\n  spark\n    .read\n    .format(\"jdbc\")\n    .options(**(dico_jdbc_pg | {'dbtable': 'nycflights.airlines'}))\n    .load()\n)\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[7], line 5\n      1 df_airlines = (\n      2   spark\n      3     .read\n      4     .format(\"jdbc\")\n----&gt; 5     .options(**(dico_jdbc_pg | {'dbtable': 'nycflights.airlines'}))\n      6     .load()\n      7 )\n\nNameError: name 'dico_jdbc_pg' is not defined\n\n\n\n\n\nCode\ndf_airlines.show(5)\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[8], line 1\n----&gt; 1 df_airlines.show(5)\n\nNameError: name 'df_airlines' is not defined"
  },
  {
    "objectID": "core/notebooks/notebookxx_pg_pandas_spark.html#querying-the-database",
    "href": "core/notebooks/notebookxx_pg_pandas_spark.html#querying-the-database",
    "title": "PostgreSQL and Spark",
    "section": "Querying the database",
    "text": "Querying the database\n\n\nCode\nquery = \"\"\"\n    SELECT DISTINCT fl.carrier, al.name, fl.origin, fl.dest\n    FROM nycflights.airlines al JOIN \n        nycflights.flights fl ON (fl.carrier=al.carrier)\n\"\"\"\n\n\n\n\nCode\ndf_query = (\n  spark\n    .read\n    .format(\"jdbc\")\n    .options(**(dico_jdbc_pg | {'query': query}))\n    .load()\n)\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[10], line 5\n      1 df_query = (\n      2   spark\n      3     .read\n      4     .format(\"jdbc\")\n----&gt; 5     .options(**(dico_jdbc_pg | {'query': query}))\n      6     .load()\n      7 )\n\nNameError: name 'dico_jdbc_pg' is not defined\n\n\n\n\n\nCode\ndf_query.show()\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[11], line 1\n----&gt; 1 df_query.show()\n\nNameError: name 'df_query' is not defined"
  },
  {
    "objectID": "core/notebooks/notebookxx_pg_pandas_spark.html#the-end",
    "href": "core/notebooks/notebookxx_pg_pandas_spark.html#the-end",
    "title": "PostgreSQL and Spark",
    "section": "The end",
    "text": "The end\n\n\nCode\nspark.stop()"
  },
  {
    "objectID": "core/notebooks/notebook14.html",
    "href": "core/notebooks/notebook14.html",
    "title": "Using with pyspark for data preprocessing",
    "section": "",
    "text": "Code\nimport os\nimport sys\nimport pyarrow as pa\nimport comet as co\nimport pyarrow.parquet as pq\n\n\n\n---------------------------------------------------------------------------\nModuleNotFoundError                       Traceback (most recent call last)\nCell In[1], line 4\n      2 import sys\n      3 import pyarrow as pa\n----&gt; 4 import comet as co\n      5 import pyarrow.parquet as pq\n\nModuleNotFoundError: No module named 'comet'\n\n\n\n\n\nCode\nfrom pyspark.sql import Window\nimport pyspark.sql.functions as func\nfrom pyspark.sql.types import *\nfrom pyspark.sql.functions import col, lit\n\n\n\n\nCode\nfrom functools import reduce\n\n\n\n\nCode\nos.environ['PYSPARK_PYTHON'] = sys.executable\nos.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable\n\n\n\n\nCode\nfrom pyspark.sql import SparkSession\n\nspark = (SparkSession\n    .builder\n    .appName(\"Web data\")         \n    .getOrCreate()\n)\n\n\n25/01/15 06:08:46 WARN Utils: Your hostname, boucheron-Precision-5480 resolves to a loopback address: 127.0.1.1; using 192.168.10.120 instead (on interface wlp0s20f3)\n25/01/15 06:08:47 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\nSetting default log level to \"WARN\".\nTo adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n25/01/15 06:08:47 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n\n\n\n\nCode\n%timeit\ninput_path = './'\n\ninput_file = os.path.join(input_path, 'webdata.parquet')\n\ndf = spark.read.parquet(input_file)\n\n\n\n---------------------------------------------------------------------------\nAnalysisException                         Traceback (most recent call last)\nCell In[6], line 6\n      2 input_path = './'\n      4 input_file = os.path.join(input_path, 'webdata.parquet')\n----&gt; 6 df = spark.read.parquet(input_file)\n\nFile ~/Documents/IFEBY310/.venv/lib/python3.12/site-packages/pyspark/sql/readwriter.py:544, in DataFrameReader.parquet(self, *paths, **options)\n    533 int96RebaseMode = options.get(\"int96RebaseMode\", None)\n    534 self._set_opts(\n    535     mergeSchema=mergeSchema,\n    536     pathGlobFilter=pathGlobFilter,\n   (...)\n    541     int96RebaseMode=int96RebaseMode,\n    542 )\n--&gt; 544 return self._df(self._jreader.parquet(_to_seq(self._spark._sc, paths)))\n\nFile ~/Documents/IFEBY310/.venv/lib/python3.12/site-packages/py4j/java_gateway.py:1322, in JavaMember.__call__(self, *args)\n   1316 command = proto.CALL_COMMAND_NAME +\\\n   1317     self.command_header +\\\n   1318     args_command +\\\n   1319     proto.END_COMMAND_PART\n   1321 answer = self.gateway_client.send_command(command)\n-&gt; 1322 return_value = get_return_value(\n   1323     answer, self.gateway_client, self.target_id, self.name)\n   1325 for temp_arg in temp_args:\n   1326     if hasattr(temp_arg, \"_detach\"):\n\nFile ~/Documents/IFEBY310/.venv/lib/python3.12/site-packages/pyspark/errors/exceptions/captured.py:185, in capture_sql_exception.&lt;locals&gt;.deco(*a, **kw)\n    181 converted = convert_exception(e.java_exception)\n    182 if not isinstance(converted, UnknownException):\n    183     # Hide where the exception came from that shows a non-Pythonic\n    184     # JVM exception message.\n--&gt; 185     raise converted from None\n    186 else:\n    187     raise\n\nAnalysisException: [PATH_NOT_FOUND] Path does not exist: file:/home/boucheron/Documents/IFEBY310/core/notebooks/webdata.parquet.\n\n\n\n\n\nCode\ndf.printSchema()\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[7], line 1\n----&gt; 1 df.printSchema()\n\nNameError: name 'df' is not defined\n\n\n\n\n\nCode\ndf.rdd.getNumPartitions()\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[8], line 1\n----&gt; 1 df.rdd.getNumPartitions()\n\nNameError: name 'df' is not defined\n\n\n\n\n\nCode\n%timeit\ndfa = pq.read_table(input_file)\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[9], line 2\n      1 get_ipython().run_line_magic('timeit', '')\n----&gt; 2 dfa = pq.read_table(input_file)\n\nNameError: name 'pq' is not defined\n\n\n\n\n\nCode\nprint(dfa.schema)\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[10], line 1\n----&gt; 1 print(dfa.schema)\n\nNameError: name 'dfa' is not defined\n\n\n\n\n\nCode\n( \n    df.select('xid')\n      .distinct()\n      .explain()\n)\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[11], line 2\n      1 ( \n----&gt; 2     df.select('xid')\n      3       .distinct()\n      4       .explain()\n      5 )\n\nNameError: name 'df' is not defined\n\n\n\n\n\nCode\n# \n( \n    df.select('xid')\n      .distinct()\n      .count()\n)\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[12], line 3\n      1 # \n      2 ( \n----&gt; 3     df.select('xid')\n      4       .distinct()\n      5       .count()\n      6 )\n\nNameError: name 'df' is not defined\n\n\n\n\n\nCode\nxid_partition = Window.partitionBy('xid')\nn_events = func.count(col('action')).over(xid_partition)\ndf = df.withColumn('n_events', n_events)\ndf.head(n=2)\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[13], line 3\n      1 xid_partition = Window.partitionBy('xid')\n      2 n_events = func.count(col('action')).over(xid_partition)\n----&gt; 3 df = df.withColumn('n_events', n_events)\n      4 df.head(n=2)\n\nNameError: name 'df' is not defined\n\n\n\n\n\nCode\n( \n  df\n    .groupBy('xid')\n    .agg(func.count('action'))\n    .head(5)\n)\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[14], line 2\n      1 ( \n----&gt; 2   df\n      3     .groupBy('xid')\n      4     .agg(func.count('action'))\n      5     .head(5)\n      6 )\n\nNameError: name 'df' is not defined\n\n\n\n\n\nCode\ndf = df.repartitionByRange(20, 'xid')\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[15], line 1\n----&gt; 1 df = df.repartitionByRange(20, 'xid')\n\nNameError: name 'df' is not defined\n\n\n\n\n\nCode\ndf.persist()\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[16], line 1\n----&gt; 1 df.persist()\n\nNameError: name 'df' is not defined\n\n\n\n\n\nCode\nxid_partition = Window.partitionBy('xid')\n\nmax_date = (\n  func\n    .max(col('date'))\n    .over(xid_partition)\n)\n\nn_days_since_last_event = func.datediff(func.current_date(), max_date)\n\ndf = df.withColumn('n_days_since_last_event',\n                   n_days_since_last_event)\n\ndf.head(n=2)\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[17], line 11\n      3 max_date = (\n      4   func\n      5     .max(col('date'))\n      6     .over(xid_partition)\n      7 )\n      9 n_days_since_last_event = func.datediff(func.current_date(), max_date)\n---&gt; 11 df = df.withColumn('n_days_since_last_event',\n     12                    n_days_since_last_event)\n     14 df.head(n=2)\n\nNameError: name 'df' is not defined\n\n\n\n\n\nCode\nn_events = func.count(col('action')).over(xid_partition)\n\ndf = df.withColumn('n_events', n_events)\n\ndf.head(n=2)\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[18], line 3\n      1 n_events = func.count(col('action')).over(xid_partition)\n----&gt; 3 df = df.withColumn('n_events', n_events)\n      5 df.head(n=2)\n\nNameError: name 'df' is not defined\n\n\n\n\n\nCode\nxid_device_partition = xid_partition.partitionBy('device')\n\nn_events_per_device = (\n    func.count(col('action'))\n        .over(xid_device_partition)\n)\n\ndf = df.withColumn('n_events_per_device', n_events_per_device)\ndf.head(n=2)\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[19], line 8\n      1 xid_device_partition = xid_partition.partitionBy('device')\n      3 n_events_per_device = (\n      4     func.count(col('action'))\n      5         .over(xid_device_partition)\n      6 )\n----&gt; 8 df = df.withColumn('n_events_per_device', n_events_per_device)\n      9 df.head(n=2)\n\nNameError: name 'df' is not defined\n\n\n\n\n\nCode\nrank_device = (\n  func\n    .dense_rank()\n    .over(xid_partition.orderBy('device'))\n)\n\nn_unique_device = (\n    func\n      .last(rank_device)\n      .over(xid_partition)\n)\n\ndf = df.withColumn('n_device', n_unique_device)\n\ndf.head(n=2)\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[20], line 13\n      1 rank_device = (\n      2   func\n      3     .dense_rank()\n      4     .over(xid_partition.orderBy('device'))\n      5 )\n      7 n_unique_device = (\n      8     func\n      9       .last(rank_device)\n     10       .over(xid_partition)\n     11 )\n---&gt; 13 df = df.withColumn('n_device', n_unique_device)\n     15 df.head(n=2)\n\nNameError: name 'df' is not defined\n\n\n\n\n\nCode\ndf.printSchema()\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[21], line 1\n----&gt; 1 df.printSchema()\n\nNameError: name 'df' is not defined\n\n\n\n\n\nCode\n(\n  df\n    .where(col('n_device') &gt; 1)\n    .select('xid', \n            'device', \n            'n_events', \n            'n_device', \n            'n_events_per_device')\n    .head(n=8)\n)\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[22], line 2\n      1 (\n----&gt; 2   df\n      3     .where(col('n_device') &gt; 1)\n      4     .select('xid', \n      5             'device', \n      6             'n_events', \n      7             'n_device', \n      8             'n_events_per_device')\n      9     .head(n=8)\n     10 )\n\nNameError: name 'df' is not defined\n\n\n\n\n\nCode\ndef n_events_transformer(df):\n    xid_partition = Window.partitionBy('xid')\n    n_events = func.count(col('action')).over(xid_partition)\n    \n    df = df.withColumn('n_events', n_events)\n\n    return df\n\n\n\n\nCode\ndef my_count_window_transform(df, input_col, output_col, part_col):\n    w = Window.partitionBy(part_col)\n    out_col = func.count(col(input_col)).over(w)\n\n    return df.withColumn(output_col, out_col)\n\n\n\n\nCode\ndef hour_transformer(df):\n    hour = func.hour(col('date'))\n    df = df.withColumn('hour', hour)\n    return df\n\ndef weekday_transformer(df):\n    weekday = func.date_format(col('date'), 'EEEE')\n    df = df.withColumn('weekday', weekday)\n    return df\n\n\n\n\nCode\ndef n_events_per_hour_transformer(df):\n    xid_hour_partition = Window.partitionBy('xid', 'hour')\n    n_events_per_hour = func.count(col('action')).over(xid_hour_partition)\n    df = df.withColumn('n_events_per_hour', n_events_per_hour)\n    return df\n\n\n\n\nCode\ndef n_events_per_weekday_transformer(df):\n    xid_weekday_partition = Window.partitionBy('xid', 'weekday')\n    n_events_per_weekday = func.count(col('action')).over(xid_weekday_partition)\n    df = df.withColumn('n_events_per_weekday', n_events_per_weekday)\n    return df\n\ndef n_days_since_last_event_transformer(df):\n    xid_partition = Window.partitionBy('xid')\n    max_date = func.max(col('date')).over(xid_partition)\n    n_days_since_last_event = func.datediff(func.current_date(), max_date)\n    df = df.withColumn('n_days_since_last_event',\n                       n_days_since_last_event + lit(0.1))\n    return df\n\ndef n_days_since_last_action_transformer(df):\n    xid_partition_action = Window.partitionBy('xid', 'action')\n    max_date = func.max(col('date')).over(xid_partition_action)\n    n_days_since_last_action = func.datediff(func.current_date(),\n                                                        max_date)\n    df = df.withColumn('n_days_since_last_action',\n                       n_days_since_last_action + lit(0.1))\n    return df\n\ndef n_unique_day_transformer(df):\n    xid_partition = Window.partitionBy('xid')\n    dayofyear = func.dayofyear(col('date'))\n    rank_day = func.dense_rank().over(xid_partition.orderBy(dayofyear))\n    n_unique_day = func.last(rank_day).over(xid_partition)\n    df = df.withColumn('n_unique_day', n_unique_day)\n    return df\n\ndef n_unique_hour_transformer(df):\n    xid_partition = Window.partitionBy('xid')\n    rank_hour = func.dense_rank().over(xid_partition.orderBy('hour'))\n    n_unique_hour = func.last(rank_hour).over(xid_partition)\n    df = df.withColumn('n_unique_hour', n_unique_hour)\n    return df\n\ndef n_events_per_device_transformer(df):\n    xid_device_partition = Window.partitionBy('xid', 'device')\n    n_events_per_device = func.count(func.col('device')) \\\n        .over(xid_device_partition)\n    df = df.withColumn('n_events_per_device', n_events_per_device)\n    return df\n\ndef n_unique_device_transformer(df):\n    xid_partition = Window.partitionBy('xid')\n    rank_device = func.dense_rank().over(xid_partition.orderBy('device'))\n    n_unique_device = func.last(rank_device).over(xid_partition)\n    df = df.withColumn('n_device', n_unique_device)\n    return df\n\ndef n_actions_per_category_id_transformer(df):\n    xid_category_id_partition = Window.partitionBy('xid', 'category_id',\n                                                   'action')\n    n_actions_per_category_id = func.count(func.col('action')) \\\n        .over(xid_category_id_partition)\n    df = df.withColumn('n_actions_per_category_id', n_actions_per_category_id)\n    return df\n\ndef n_unique_category_id_transformer(df):\n    xid_partition = Window.partitionBy('xid')\n    rank_category_id = func.dense_rank().over(xid_partition\\\n                                              .orderBy('category_id'))\n    n_unique_category_id = func.last(rank_category_id).over(xid_partition)\n    df = df.withColumn('n_unique_category_id', n_unique_category_id)\n    return df\n\ndef n_events_per_category_id_transformer(df):\n    xid_category_id_partition = Window.partitionBy('xid', 'category_id')\n    n_events_per_category_id = func.count(func.col('action')) \\\n        .over(xid_category_id_partition)\n    df = df.withColumn('n_events_per_category_id', n_events_per_category_id)\n    return df\n\ndef n_events_per_website_id_transformer(df):\n    xid_website_id_partition = Window.partitionBy('xid', 'website_id')\n    n_events_per_website_id = func.count(col('action'))\\\n        .over(xid_website_id_partition)\n    df = df.withColumn('n_events_per_website_id', n_events_per_website_id)\n    return df"
  },
  {
    "objectID": "core/notebooks/notebook10_graphx.html",
    "href": "core/notebooks/notebook10_graphx.html",
    "title": "",
    "section": "",
    "text": "CodeShow All CodeHide All CodeView Source\n\n\n\n\n\n\nCode\nfrom graphframes import GraphFrame\nfrom pyspark import SparkConf, SparkContext\nfrom pyspark.sql import SparkSession\n\nconf = SparkConf().setAppName(\"Spark graphx Course\")\nsc = SparkContext(conf=conf)  # no need for Spark 3...\n\nspark = (SparkSession\n         .builder\n         .appName(\"Spark graphx Course\")\n         .getOrCreate()\n         )\n\nspark._sc is sc\n\n\n\n---------------------------------------------------------------------------\nModuleNotFoundError                       Traceback (most recent call last)\nCell In[1], line 1\n----&gt; 1 from graphframes import GraphFrame\n      2 from pyspark import SparkConf, SparkContext\n      3 from pyspark.sql import SparkSession\n\nModuleNotFoundError: No module named 'graphframes'\n\n\n\n\n\nCode\nv = spark.createDataFrame([\n    (\"a\", \"Alice\", 34),\n    (\"b\", \"Bob\", 36),\n    (\"c\", \"Charlie\", 30),\n], [\"id\", \"name\", \"age\"])\n# Create an Edge DataFrame with \"src\" and \"dst\" columns\ne = spark.createDataFrame([\n    (\"a\", \"b\", \"friend\"),\n    (\"b\", \"c\", \"follow\"),\n    (\"c\", \"b\", \"follow\"),\n], [\"src\", \"dst\", \"relationship\"])\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[2], line 1\n----&gt; 1 v = spark.createDataFrame([\n      2     (\"a\", \"Alice\", 34),\n      3     (\"b\", \"Bob\", 36),\n      4     (\"c\", \"Charlie\", 30),\n      5 ], [\"id\", \"name\", \"age\"])\n      6 # Create an Edge DataFrame with \"src\" and \"dst\" columns\n      7 e = spark.createDataFrame([\n      8     (\"a\", \"b\", \"friend\"),\n      9     (\"b\", \"c\", \"follow\"),\n     10     (\"c\", \"b\", \"follow\"),\n     11 ], [\"src\", \"dst\", \"relationship\"])\n\nNameError: name 'spark' is not defined\n\n\n\n\n\nCode\ng = GraphFrame(v, e)\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[3], line 1\n----&gt; 1 g = GraphFrame(v, e)\n\nNameError: name 'GraphFrame' is not defined"
  },
  {
    "objectID": "core/notebooks/notebook08_webdata-II.html",
    "href": "core/notebooks/notebook08_webdata-II.html",
    "title": "Using with pyspark for data preprocessing",
    "section": "",
    "text": "The data is a parquet file which contains a dataframe with 8 columns:\n\nxid: unique user id\naction: type of action. ‘C’ is a click, ‘O’ or ‘VSL’ is a web-display\ndate: date of the action\nwebsite_id: unique id of the website\nurl: url of the webpage\ncategory_id: id of the display\nzipcode: postal zipcode of the user\ndevice: type of device used by the user"
  },
  {
    "objectID": "core/notebooks/notebook08_webdata-II.html#data-description",
    "href": "core/notebooks/notebook08_webdata-II.html#data-description",
    "title": "Using with pyspark for data preprocessing",
    "section": "",
    "text": "The data is a parquet file which contains a dataframe with 8 columns:\n\nxid: unique user id\naction: type of action. ‘C’ is a click, ‘O’ or ‘VSL’ is a web-display\ndate: date of the action\nwebsite_id: unique id of the website\nurl: url of the webpage\ncategory_id: id of the display\nzipcode: postal zipcode of the user\ndevice: type of device used by the user"
  },
  {
    "objectID": "core/notebooks/notebook08_webdata-II.html#q1.-some-statistics-computations",
    "href": "core/notebooks/notebook08_webdata-II.html#q1.-some-statistics-computations",
    "title": "Using with pyspark for data preprocessing",
    "section": "Q1. Some statistics / computations",
    "text": "Q1. Some statistics / computations\nUsing pyspark.sql we want to do the following things:\n\nCompute the total number of unique users\nConstruct a column containing the total number of actions per user\nConstruct a column containing the number of days since the last action of the user\nConstruct a column containing the number of actions of each user for each modality of device"
  },
  {
    "objectID": "core/notebooks/notebook08_webdata-II.html#q2.-binary-classification",
    "href": "core/notebooks/notebook08_webdata-II.html#q2.-binary-classification",
    "title": "Using with pyspark for data preprocessing",
    "section": "Q2. Binary classification",
    "text": "Q2. Binary classification\nThen, we want to construct a classifier to predict the click on the category 1204. Here is an agenda for this:\n\nConstruction of a features matrix for which each line corresponds to the information concerning a user.\nIn this matrix, we need to keep only the users that have been exposed to the display in category 1204\nUsing this training dataset, train a binary classifier, and evaluate your classifier using a precision / recall curve computed on test data."
  },
  {
    "objectID": "core/notebooks/notebook08_webdata-II.html#compute-the-total-number-of-unique-users",
    "href": "core/notebooks/notebook08_webdata-II.html#compute-the-total-number-of-unique-users",
    "title": "Using with pyspark for data preprocessing",
    "section": "Compute the total number of unique users",
    "text": "Compute the total number of unique users\n\n\nCode\n( \n    df.select('xid')\n      .distinct()\n      .count()\n)\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[11], line 2\n      1 ( \n----&gt; 2     df.select('xid')\n      3       .distinct()\n      4       .count()\n      5 )\n\nNameError: name 'df' is not defined\n\n\n\n\n\nCode\ndef foo(x): yield len(set(x))\n\n\n\n\nCode\n( df.rdd\n    .map(lambda x : x.xid)\n    .mapPartitions(foo)\n    .collect()\n)\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[13], line 1\n----&gt; 1 ( df.rdd\n      2     .map(lambda x : x.xid)\n      3     .mapPartitions(foo)\n      4     .collect()\n      5 )\n\nNameError: name 'df' is not defined\n\n\n\nThis might pump up some computational resources\n\n\nCode\n( \n    df.select('xid')\n      .distinct() \n      .explain()\n)\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[14], line 2\n      1 ( \n----&gt; 2     df.select('xid')\n      3       .distinct() \n      4       .explain()\n      5 )\n\nNameError: name 'df' is not defined\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe distinct values of xid seem to be evenly spread among the six files making the parquet directory. Note that the last six partitions look empty."
  },
  {
    "objectID": "core/notebooks/notebook08_webdata-II.html#construct-a-column-containing-the-total-number-of-actions-per-user",
    "href": "core/notebooks/notebook08_webdata-II.html#construct-a-column-containing-the-total-number-of-actions-per-user",
    "title": "Using with pyspark for data preprocessing",
    "section": "Construct a column containing the total number of actions per user",
    "text": "Construct a column containing the total number of actions per user\n\n\nCode\nxid_partition = Window.partitionBy('xid')\n\nn_events = func.count(col('action')).over(xid_partition)\n\ndf = df.withColumn('n_events', n_events)\n\ndf.head(n=2)\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[15], line 5\n      1 xid_partition = Window.partitionBy('xid')\n      3 n_events = func.count(col('action')).over(xid_partition)\n----&gt; 5 df = df.withColumn('n_events', n_events)\n      7 df.head(n=2)\n\nNameError: name 'df' is not defined\n\n\n\n\n\nCode\n( \n  df\n    .groupBy('xid')\n    .agg(func.count('action'))\n    .head(5)\n)\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[16], line 2\n      1 ( \n----&gt; 2   df\n      3     .groupBy('xid')\n      4     .agg(func.count('action'))\n      5     .head(5)\n      6 )\n\nNameError: name 'df' is not defined"
  },
  {
    "objectID": "core/notebooks/notebook08_webdata-II.html#construct-a-column-containing-the-number-of-days-since-the-last-action-of-the-user",
    "href": "core/notebooks/notebook08_webdata-II.html#construct-a-column-containing-the-number-of-days-since-the-last-action-of-the-user",
    "title": "Using with pyspark for data preprocessing",
    "section": "Construct a column containing the number of days since the last action of the user",
    "text": "Construct a column containing the number of days since the last action of the user\n\n\nCode\nmax_date = (\n  func\n    .max(col('date'))\n    .over(xid_partition)\n)\n\nn_days_since_last_event = func.datediff(func.current_date(), max_date)\n\ndf = df.withColumn('n_days_since_last_event',\n                   n_days_since_last_event)\n\ndf.head(n=2)\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[17], line 9\n      1 max_date = (\n      2   func\n      3     .max(col('date'))\n      4     .over(xid_partition)\n      5 )\n      7 n_days_since_last_event = func.datediff(func.current_date(), max_date)\n----&gt; 9 df = df.withColumn('n_days_since_last_event',\n     10                    n_days_since_last_event)\n     12 df.head(n=2)\n\nNameError: name 'df' is not defined\n\n\n\n\n\nCode\ndf.printSchema()\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[18], line 1\n----&gt; 1 df.printSchema()\n\nNameError: name 'df' is not defined"
  },
  {
    "objectID": "core/notebooks/notebook08_webdata-II.html#construct-a-column-containing-the-number-of-actions-of-each-user-for-each-modality-of-device",
    "href": "core/notebooks/notebook08_webdata-II.html#construct-a-column-containing-the-number-of-actions-of-each-user-for-each-modality-of-device",
    "title": "Using with pyspark for data preprocessing",
    "section": "Construct a column containing the number of actions of each user for each modality of device",
    "text": "Construct a column containing the number of actions of each user for each modality of device\nDoes this partitionBy triggers shuffling?\n\n\nCode\nxid_device_partition = xid_partition.partitionBy('device')\n\nn_events_per_device = func.count(col('action')).over(xid_device_partition)\n\ndf = df.withColumn('n_events_per_device', n_events_per_device)\n\ndf.head(n=2)\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[19], line 5\n      1 xid_device_partition = xid_partition.partitionBy('device')\n      3 n_events_per_device = func.count(col('action')).over(xid_device_partition)\n----&gt; 5 df = df.withColumn('n_events_per_device', n_events_per_device)\n      7 df.head(n=2)\n\nNameError: name 'df' is not defined"
  },
  {
    "objectID": "core/notebooks/notebook08_webdata-II.html#number-of-devices-per-user",
    "href": "core/notebooks/notebook08_webdata-II.html#number-of-devices-per-user",
    "title": "Using with pyspark for data preprocessing",
    "section": "Number of devices per user ",
    "text": "Number of devices per user \n\n\nCode\n# xid_partition = Window.partitionBy('xid')\n\nrank_device = (\n  func\n    .dense_rank()\n    .over(xid_partition.orderBy('device'))\n)\n\nn_unique_device = (\n    func\n      .last(rank_device)\n      .over(xid_partition)\n)\n\ndf = df.withColumn('n_device', n_unique_device)\n\ndf.head(n=2)\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[20], line 15\n      3 rank_device = (\n      4   func\n      5     .dense_rank()\n      6     .over(xid_partition.orderBy('device'))\n      7 )\n      9 n_unique_device = (\n     10     func\n     11       .last(rank_device)\n     12       .over(xid_partition)\n     13 )\n---&gt; 15 df = df.withColumn('n_device', n_unique_device)\n     17 df.head(n=2)\n\nNameError: name 'df' is not defined\n\n\n\n\n\nCode\ndf\\\n    .where(col('n_device') &gt; 1)\\\n    .select('xid', 'device', 'n_events',  'n_device', 'n_events_per_device')\\\n    .head(n=8)\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[21], line 1\n----&gt; 1 df\\\n      2     .where(col('n_device') &gt; 1)\\\n      3     .select('xid', 'device', 'n_events',  'n_device', 'n_events_per_device')\\\n      4     .head(n=8)\n\nNameError: name 'df' is not defined\n\n\n\n\n\nCode\ndf\\\n    .where(col('n_device') &gt; 1)\\\n    .select('xid', 'device', 'n_events',  'n_device', 'n_events_per_device')\\\n    .count()\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[22], line 1\n----&gt; 1 df\\\n      2     .where(col('n_device') &gt; 1)\\\n      3     .select('xid', 'device', 'n_events',  'n_device', 'n_events_per_device')\\\n      4     .count()\n\nNameError: name 'df' is not defined"
  },
  {
    "objectID": "core/notebooks/notebook08_webdata-II.html#extraction",
    "href": "core/notebooks/notebook08_webdata-II.html#extraction",
    "title": "Using with pyspark for data preprocessing",
    "section": "Extraction",
    "text": "Extraction\nHere extraction is just about reading the data\n\n\nCode\ndf = spark.read.parquet(input_file)\ndf.head(n=3)\n\n\n\n---------------------------------------------------------------------------\nAnalysisException                         Traceback (most recent call last)\nCell In[23], line 1\n----&gt; 1 df = spark.read.parquet(input_file)\n      2 df.head(n=3)\n\nFile ~/Documents/IFEBY310/.venv/lib/python3.12/site-packages/pyspark/sql/readwriter.py:544, in DataFrameReader.parquet(self, *paths, **options)\n    533 int96RebaseMode = options.get(\"int96RebaseMode\", None)\n    534 self._set_opts(\n    535     mergeSchema=mergeSchema,\n    536     pathGlobFilter=pathGlobFilter,\n   (...)\n    541     int96RebaseMode=int96RebaseMode,\n    542 )\n--&gt; 544 return self._df(self._jreader.parquet(_to_seq(self._spark._sc, paths)))\n\nFile ~/Documents/IFEBY310/.venv/lib/python3.12/site-packages/py4j/java_gateway.py:1322, in JavaMember.__call__(self, *args)\n   1316 command = proto.CALL_COMMAND_NAME +\\\n   1317     self.command_header +\\\n   1318     args_command +\\\n   1319     proto.END_COMMAND_PART\n   1321 answer = self.gateway_client.send_command(command)\n-&gt; 1322 return_value = get_return_value(\n   1323     answer, self.gateway_client, self.target_id, self.name)\n   1325 for temp_arg in temp_args:\n   1326     if hasattr(temp_arg, \"_detach\"):\n\nFile ~/Documents/IFEBY310/.venv/lib/python3.12/site-packages/pyspark/errors/exceptions/captured.py:185, in capture_sql_exception.&lt;locals&gt;.deco(*a, **kw)\n    181 converted = convert_exception(e.java_exception)\n    182 if not isinstance(converted, UnknownException):\n    183     # Hide where the exception came from that shows a non-Pythonic\n    184     # JVM exception message.\n--&gt; 185     raise converted from None\n    186 else:\n    187     raise\n\nAnalysisException: [PATH_NOT_FOUND] Path does not exist: file:/home/boucheron/Documents/IFEBY310/core/notebooks/webdata.parquet."
  },
  {
    "objectID": "core/notebooks/notebook08_webdata-II.html#transformation-of-the-data",
    "href": "core/notebooks/notebook08_webdata-II.html#transformation-of-the-data",
    "title": "Using with pyspark for data preprocessing",
    "section": "Transformation of the data",
    "text": "Transformation of the data\nAt this step we compute a lot of extra things from the data. The aim is to build features that describe users.\n\n\nCode\ndef n_events_transformer(df):\n    xid_partition = Window.partitionBy('xid')\n    n_events = func.count(col('action')).over(xid_partition)\n    \n    df = df.withColumn('n_events', n_events)\n\n    return df\n\n\n\n\nCode\ndef n_events_per_action_transformer(df):\n    xid_action_partition = Window.partitionBy('xid', 'action')\n    n_events_per_action = func.count(col('action')).over(xid_action_partition)\n\n    df = df.withColumn('n_events_per_action', n_events_per_action)\n    \n    return df\n\n\n\n\nCode\ndef hour_transformer(df):\n    hour = func.hour(col('date'))\n    df = df.withColumn('hour', hour)\n    return df\n\ndef weekday_transformer(df):\n    weekday = func.date_format(col('date'), 'EEEE')\n    df = df.withColumn('weekday', weekday)\n    return df\n\ndef n_events_per_hour_transformer(df):\n    xid_hour_partition = Window.partitionBy('xid', 'hour')\n    n_events_per_hour = func.count(col('action')).over(xid_hour_partition)\n    df = df.withColumn('n_events_per_hour', n_events_per_hour)\n    return df\n\ndef n_events_per_weekday_transformer(df):\n    xid_weekday_partition = Window.partitionBy('xid', 'weekday')\n    n_events_per_weekday = func.count(col('action')).over(xid_weekday_partition)\n    df = df.withColumn('n_events_per_weekday', n_events_per_weekday)\n    return df\n\ndef n_days_since_last_event_transformer(df):\n    xid_partition = Window.partitionBy('xid')\n    max_date = func.max(col('date')).over(xid_partition)\n    n_days_since_last_event = func.datediff(func.current_date(), max_date)\n    df = df.withColumn('n_days_since_last_event',\n                       n_days_since_last_event + lit(0.1))\n    return df\n\ndef n_days_since_last_action_transformer(df):\n    xid_partition_action = Window.partitionBy('xid', 'action')\n    max_date = func.max(col('date')).over(xid_partition_action)\n    n_days_since_last_action = func.datediff(func.current_date(),\n                                                        max_date)\n    df = df.withColumn('n_days_since_last_action',\n                       n_days_since_last_action + lit(0.1))\n    return df\n\ndef n_unique_day_transformer(df):\n    xid_partition = Window.partitionBy('xid')\n    dayofyear = func.dayofyear(col('date'))\n    rank_day = func.dense_rank().over(xid_partition.orderBy(dayofyear))\n    n_unique_day = func.last(rank_day).over(xid_partition)\n    df = df.withColumn('n_unique_day', n_unique_day)\n    return df\n\ndef n_unique_hour_transformer(df):\n    xid_partition = Window.partitionBy('xid')\n    rank_hour = func.dense_rank().over(xid_partition.orderBy('hour'))\n    n_unique_hour = func.last(rank_hour).over(xid_partition)\n    df = df.withColumn('n_unique_hour', n_unique_hour)\n    return df\n\ndef n_events_per_device_transformer(df):\n    xid_device_partition = Window.partitionBy('xid', 'device')\n    n_events_per_device = func.count(func.col('device')) \\\n        .over(xid_device_partition)\n    df = df.withColumn('n_events_per_device', n_events_per_device)\n    return df\n\ndef n_unique_device_transformer(df):\n    xid_partition = Window.partitionBy('xid')\n    rank_device = func.dense_rank().over(xid_partition.orderBy('device'))\n    n_unique_device = func.last(rank_device).over(xid_partition)\n    df = df.withColumn('n_device', n_unique_device)\n    return df\n\ndef n_actions_per_category_id_transformer(df):\n    xid_category_id_partition = Window.partitionBy('xid', 'category_id',\n                                                   'action')\n    n_actions_per_category_id = func.count(func.col('action')) \\\n        .over(xid_category_id_partition)\n    df = df.withColumn('n_actions_per_category_id', n_actions_per_category_id)\n    return df\n\ndef n_unique_category_id_transformer(df):\n    xid_partition = Window.partitionBy('xid')\n    rank_category_id = func.dense_rank().over(xid_partition\\\n                                              .orderBy('category_id'))\n    n_unique_category_id = func.last(rank_category_id).over(xid_partition)\n    df = df.withColumn('n_unique_category_id', n_unique_category_id)\n    return df\n\ndef n_events_per_category_id_transformer(df):\n    xid_category_id_partition = Window.partitionBy('xid', 'category_id')\n    n_events_per_category_id = func.count(func.col('action')) \\\n        .over(xid_category_id_partition)\n    df = df.withColumn('n_events_per_category_id', n_events_per_category_id)\n    return df\n\ndef n_events_per_website_id_transformer(df):\n    xid_website_id_partition = Window.partitionBy('xid', 'website_id')\n    n_events_per_website_id = func.count(col('action'))\\\n        .over(xid_website_id_partition)\n    df = df.withColumn('n_events_per_website_id', n_events_per_website_id)\n    return df\n\n\n\n\nCode\ntransformers = [\n    hour_transformer,\n    weekday_transformer,\n    n_events_per_hour_transformer,\n    n_events_per_weekday_transformer,\n    n_days_since_last_event_transformer,\n    n_days_since_last_action_transformer,\n    n_unique_day_transformer,\n    n_unique_hour_transformer,\n    n_events_per_device_transformer,\n    n_unique_device_transformer,\n    n_actions_per_category_id_transformer,\n    n_events_per_category_id_transformer,\n    n_events_per_website_id_transformer,\n]\n\n\n\n\nCode\nN = 10000\n\n\n\n\nCode\nsample_df = df.sample(withReplacement=False, fraction=.05)\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[29], line 1\n----&gt; 1 sample_df = df.sample(withReplacement=False, fraction=.05)\n\nNameError: name 'df' is not defined\n\n\n\n\n\nCode\nsample_df.count()\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[30], line 1\n----&gt; 1 sample_df.count()\n\nNameError: name 'sample_df' is not defined\n\n\n\n\n\nCode\nfor transformer in transformers:\n    df = transformer(df)\n\ndf.head(n=1)\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[31], line 2\n      1 for transformer in transformers:\n----&gt; 2     df = transformer(df)\n      4 df.head(n=1)\n\nNameError: name 'df' is not defined\n\n\n\n\n\nCode\nfor transformer in transformers:\n    sample_df = transformer(sample_df)\n\nsample_df.head(n=1)\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[32], line 2\n      1 for transformer in transformers:\n----&gt; 2     sample_df = transformer(sample_df)\n      4 sample_df.head(n=1)\n\nNameError: name 'sample_df' is not defined\n\n\n\n\n\nCode\ndf = sample_df\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[33], line 1\n----&gt; 1 df = sample_df\n\nNameError: name 'sample_df' is not defined\n\n\n\n\n\nCode\nsorted(df.columns)\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[34], line 1\n----&gt; 1 sorted(df.columns)\n\nNameError: name 'df' is not defined\n\n\n\n\n\nCode\ndf.explain()\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[35], line 1\n----&gt; 1 df.explain()\n\nNameError: name 'df' is not defined\n\n\n\n\n\nCode\nspark._sc.setCheckpointDir(\".\")   \n\ndf.checkpoint()\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[36], line 3\n      1 spark._sc.setCheckpointDir(\".\")   \n----&gt; 3 df.checkpoint()\n\nNameError: name 'df' is not defined\n\n\n\n\n\nCode\ndf.explain()\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[37], line 1\n----&gt; 1 df.explain()\n\nNameError: name 'df' is not defined"
  },
  {
    "objectID": "core/notebooks/notebook08_webdata-II.html#load-step",
    "href": "core/notebooks/notebook08_webdata-II.html#load-step",
    "title": "Using with pyspark for data preprocessing",
    "section": "Load step",
    "text": "Load step\nHere, we use all the previous computations (saved in the columns of the dataframe) to compute aggregated informations about each user.\n\n\n\n\n\n\nNote\n\n\n\nThis should be DRYED\n\n\n\n\nCode\ndef n_events_per_hour_loader(df):\n    csr = df\\\n        .select('xid', 'hour', 'n_events_per_hour')\\\n        .withColumnRenamed('n_events_per_hour', 'value')\\\n        .distinct() \n            # action\n    feature_name = func.concat(lit('n_events_per_hour#'), col('hour'))\n\n    csr = csr\\\n        .withColumn('feature_name', feature_name)\\\n        .drop('hour')\n    return csr\n\ndef n_events_per_website_id_loader(df):\n    csr = df.select('xid', 'website_id', 'n_events_per_website_id')\\\n        .withColumnRenamed('n_events_per_hour', 'value')\\\n        .distinct()\n\n    feature_name = func.concat(lit('n_events_per_website_id#'),\n                               col('website_id'))\n    \n    csr = csr\\\n        .withColumn('feature_name', feature_name)\\\n        .drop('website_id')\n    return csr\n\ndef n_events_per_hour_loader(df):\n    csr = df\\\n        .select('xid', 'hour', 'n_events_per_hour')\\\n        .withColumnRenamed('n_events_per_hour', 'value')\\\n        .distinct()\n\n    feature_name = func.concat(lit('n_events_per_hour#'), col('hour'))\n    \n    csr = csr\\\n        .withColumn('feature_name', feature_name)\\\n        .drop('hour')\n    return csr\n\ndef n_events_per_weekday_loader(df):\n    csr = df\\\n        .select('xid', 'weekday', 'n_events_per_weekday')\\\n        .withColumnRenamed('n_events_per_weekday', 'value')\\\n        .distinct()\n\n    feature_name = func.concat(lit('n_events_per_weekday#'), col('weekday'))\n    \n    csr = csr\\\n        .withColumn('feature_name', feature_name)\\\n        .drop('weekday')\n\n    return csr\n\ndef n_days_since_last_event_loader(df):\n    csr = df.select('xid',  'n_days_since_last_event')\\\n        .withColumnRenamed('n_days_since_last_event', 'value')\\\n        .distinct()\n    feature_name = lit('n_days_since_last_event')\n    csr = csr\\\n        .withColumn('feature_name', feature_name)\n    return csr\n\ndef n_days_since_last_action_loader(df):\n    csr = df.select('xid', 'action', 'n_days_since_last_action')\\\n        .withColumnRenamed('n_days_since_last_action', 'value')\\\n        .distinct()\n    feature_name = func.concat(lit('n_days_since_last_action#'), col('action'))\n    csr = csr\\\n        .withColumn('feature_name', feature_name)\\\n        .drop('action')\n    return csr\n\ndef n_unique_day_loader(df):\n    csr = df.select('xid', 'n_unique_day')\\\n        .withColumnRenamed('n_unique_day', 'value')\\\n        .distinct()\n    feature_name = lit('n_unique_day')\n    csr = csr\\\n        .withColumn('feature_name', feature_name)\n    return csr\n\ndef n_unique_hour_loader(df):\n    csr = df.select('xid', 'n_unique_hour')\\\n        .withColumnRenamed('n_unique_hour', 'value')\\\n        .distinct()\n    feature_name = lit('n_unique_hour')\n    csr = csr\\\n        .withColumn('feature_name', feature_name)\n    return csr\n\ndef n_events_per_device_loader(df):\n    csr = df\\\n        .select('xid', 'device', 'n_events_per_device')\\\n        .withColumnRenamed('n_events_per_device', 'value')\\\n        .distinct()\n    feature_name = func.concat(lit('n_events_per_device#'), col('device'))\n    csr = csr\\\n        .withColumn('feature_name', feature_name)\\\n        .drop('device')\n    return csr\n\ndef n_unique_device_loader(df):\n    csr = df.select('xid', 'n_device')\\\n        .withColumnRenamed('n_device', 'value')\\\n        .distinct()\n    feature_name = lit('n_device')\n    csr = csr\\\n        .withColumn('feature_name', feature_name)\n    return csr\n\ndef n_events_per_category_id_loader(df):\n    csr = df.select('xid', 'category_id', 'n_events_per_category_id')\\\n        .withColumnRenamed('n_events_per_category_id', 'value')\\\n        .distinct()\n    feature_name = func.concat(lit('n_events_per_category_id#'),\n                               col('category_id'))\n    csr = csr\\\n        .withColumn('feature_name', feature_name)\\\n        .drop('category_id')\n    return csr\n\ndef n_actions_per_category_id_loader(df):\n    csr = df.select('xid', 'category_id', 'action', 'n_actions_per_category_id')\\\n        .withColumnRenamed('n_actions_per_category_id', 'value')\\\n        .distinct()\n    feature_name = func.concat(lit('n_actions_per_category_id#'),\n                               col('action'), lit('#'), \n                               col('category_id'))\n    csr = csr\\\n        .withColumn('feature_name', feature_name)\\\n        .drop('category_id')\\\n        .drop('action')\n    return csr\n\ndef n_events_per_website_id_loader(df):\n    csr = df.select('xid', 'website_id', 'n_events_per_website_id')\\\n        .withColumnRenamed('n_events_per_website_id', 'value')\\\n        .distinct()\n    feature_name = func.concat(lit('n_events_per_website_id#'),\n                               col('website_id'))\n    csr = csr\\\n        .withColumn('feature_name', feature_name)\\\n        .drop('website_id')\n    return csr\n\n\n\n\nCode\nfrom functools import reduce\n\n\n\n\nCode\nloaders = [\n    n_events_per_hour_loader,\n    n_events_per_website_id_loader,\n    n_events_per_hour_loader,\n    n_events_per_weekday_loader,\n    n_days_since_last_event_loader,\n    n_days_since_last_action_loader,\n    n_unique_day_loader,\n    n_unique_hour_loader,\n    n_events_per_device_loader,\n    n_unique_device_loader,\n    n_events_per_category_id_loader,\n    n_actions_per_category_id_loader,\n    n_events_per_website_id_loader,\n]\n\n\n\n\nCode\ndef union(df, other):\n    return df.union(other)\n\n\n\n\n\n\n\n\nAbout DataFrame.union()\n\n\n\nThis method performs a SQL-style set union of the rows from both DataFrame objects, with no automatic deduplication of elements.\nUse the distinct() method to perform deduplication of rows.\nThe method resolves columns by position (not by name), following the standard behavior in SQL.\n\n\n\n\nCode\nspam = [loader(df) for loader in loaders]\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[42], line 1\n----&gt; 1 spam = [loader(df) for loader in loaders]\n\nNameError: name 'df' is not defined\n\n\n\n\n\nCode\nspam[0].printSchema()\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[43], line 1\n----&gt; 1 spam[0].printSchema()\n\nNameError: name 'spam' is not defined\n\n\n\n\n\nCode\nall(spam[0].columns == it.columns for it in spam[1:])\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[44], line 1\n----&gt; 1 all(spam[0].columns == it.columns for it in spam[1:])\n\nNameError: name 'spam' is not defined\n\n\n\n\n\nCode\nlen(spam)\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[45], line 1\n----&gt; 1 len(spam)\n\nNameError: name 'spam' is not defined\n\n\n\n\n\nCode\ncsr = reduce(\n    lambda df1, df2: df1.union(df2),\n    spam\n)\n\ncsr.head(n=3)\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[46], line 3\n      1 csr = reduce(\n      2     lambda df1, df2: df1.union(df2),\n----&gt; 3     spam\n      4 )\n      6 csr.head(n=3)\n\nNameError: name 'spam' is not defined\n\n\n\n\n\nCode\ncsr.columns\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[47], line 1\n----&gt; 1 csr.columns\n\nNameError: name 'csr' is not defined\n\n\n\n\n\nCode\ncsr.show(5)\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[48], line 1\n----&gt; 1 csr.show(5)\n\nNameError: name 'csr' is not defined\n\n\n\n\n\nCode\ncsr.rdd.getNumPartitions()\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[49], line 1\n----&gt; 1 csr.rdd.getNumPartitions()\n\nNameError: name 'csr' is not defined\n\n\n\n\n\nCode\n# Replace features names and xid by a unique number\nfeature_name_partition = Window().orderBy('feature_name')\n\nxid_partition = Window().orderBy('xid')\n\ncol_idx = func.dense_rank().over(feature_name_partition)\nrow_idx = func.dense_rank().over(xid_partition)\n\n\n\n\nCode\ncsr = csr.withColumn('col', col_idx)\\\n    .withColumn('row', row_idx)\n\ncsr = csr.na.drop('any')\n\ncsr.head(n=5)\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[51], line 1\n----&gt; 1 csr = csr.withColumn('col', col_idx)\\\n      2     .withColumn('row', row_idx)\n      4 csr = csr.na.drop('any')\n      6 csr.head(n=5)\n\nNameError: name 'csr' is not defined\n\n\n\n\n\nCode\n# Let's save the result of our hard work into a new parquet file\noutput_path = './'\noutput_file = os.path.join(output_path, 'csr.parquet')\ncsr.write.parquet(output_file, mode='overwrite')\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[52], line 4\n      2 output_path = './'\n      3 output_file = os.path.join(output_path, 'csr.parquet')\n----&gt; 4 csr.write.parquet(output_file, mode='overwrite')\n\nNameError: name 'csr' is not defined"
  },
  {
    "objectID": "core/notebooks/notebook08_webdata-II.html#finally",
    "href": "core/notebooks/notebook08_webdata-II.html#finally",
    "title": "Using with pyspark for data preprocessing",
    "section": "Finally !!",
    "text": "Finally !!\nWow ! That was a lot of work. Now we have a features matrix \\(X\\) and a vector of labels \\(y\\).\n\n\nCode\nX.indices\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[76], line 1\n----&gt; 1 X.indices\n\nNameError: name 'X' is not defined\n\n\n\n\n\nCode\nX.indptr\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[77], line 1\n----&gt; 1 X.indptr\n\nNameError: name 'X' is not defined\n\n\n\n\n\nCode\nX.shape, X.nnz\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[78], line 1\n----&gt; 1 X.shape, X.nnz\n\nNameError: name 'X' is not defined\n\n\n\n\n\nCode\ny.shape, y.sum()\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[79], line 1\n----&gt; 1 y.shape, y.sum()\n\nNameError: name 'y' is not defined"
  },
  {
    "objectID": "core/notebooks/notebook06_sparksql.html",
    "href": "core/notebooks/notebook06_sparksql.html",
    "title": "DataFrame",
    "section": "",
    "text": "Code\nimport os\nimport sys\n\nos.environ['PYSPARK_PYTHON'] = sys.executable\nos.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable\nCode\nfrom pyspark import SparkConf, SparkContext\nfrom pyspark.sql import SparkSession\n\nconf = SparkConf().setAppName(\"Spark SQL Course\")\nsc = SparkContext(conf=conf)  # no need for Spark 3...\n\nspark = (SparkSession\n    .builder\n    .appName(\"Spark SQL Course\")\n    .getOrCreate()\n)\n\n\n25/03/10 16:55:05 WARN Utils: Your hostname, boucheron-Precision-5480 resolves to a loopback address: 127.0.1.1; using 172.23.32.10 instead (on interface enxac91a1bd3e89)\n25/03/10 16:55:05 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\nSetting default log level to \"WARN\".\nTo adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n25/03/10 16:55:05 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n25/03/10 16:55:05 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\nCode\nfrom pyspark.sql import Row\n\nrow1 = Row(name=\"John\", age=21)\nrow2 = Row(name=\"James\", age=32)\nrow3 = Row(name=\"Jane\", age=18)\nrow1['name']\n\n\n'John'\nCode\ndf = spark.createDataFrame([row1, row2, row3])\nCode\ndf.printSchema()\n\n\nroot\n |-- name: string (nullable = true)\n |-- age: long (nullable = true)\nCode\ndf.show()\n\n\n[Stage 0:&gt;                                                          (0 + 1) / 1]                                                                                \n\n\n+-----+---+\n| name|age|\n+-----+---+\n| John| 21|\n|James| 32|\n| Jane| 18|\n+-----+---+\nCode\nprint(df.rdd.toDebugString().decode(\"utf-8\"))\n\n\n(20) MapPartitionsRDD[10] at javaToPython at NativeMethodAccessorImpl.java:0 []\n |   MapPartitionsRDD[9] at javaToPython at NativeMethodAccessorImpl.java:0 []\n |   SQLExecutionRDD[8] at javaToPython at NativeMethodAccessorImpl.java:0 []\n |   MapPartitionsRDD[7] at javaToPython at NativeMethodAccessorImpl.java:0 []\n |   MapPartitionsRDD[4] at applySchemaToPythonRDD at NativeMethodAccessorImpl.java:0 []\n |   MapPartitionsRDD[3] at map at SerDeUtil.scala:69 []\n |   MapPartitionsRDD[2] at mapPartitions at SerDeUtil.scala:117 []\n |   PythonRDD[1] at RDD at PythonRDD.scala:53 []\n |   ParallelCollectionRDD[0] at readRDDFromFile at PythonRDD.scala:289 []\nCode\ndf.rdd.getNumPartitions()\n\n\n20"
  },
  {
    "objectID": "core/notebooks/notebook06_sparksql.html#creating-dataframes",
    "href": "core/notebooks/notebook06_sparksql.html#creating-dataframes",
    "title": "DataFrame",
    "section": "Creating dataframes",
    "text": "Creating dataframes\n\n\nCode\nrows = [\n    Row(name=\"John\", age=21, gender=\"male\"),\n    Row(name=\"James\", age=25, gender=\"female\"),\n    Row(name=\"Albert\", age=46, gender=\"male\")\n]\n\ndf = spark.createDataFrame(rows)\n\n\n\n\nCode\ndf.show()\n\n\n+------+---+------+\n|  name|age|gender|\n+------+---+------+\n|  John| 21|  male|\n| James| 25|female|\n|Albert| 46|  male|\n+------+---+------+\n\n\n\n\n\nCode\nhelp(Row)\n\n\nHelp on class Row in module pyspark.sql.types:\n\nclass Row(builtins.tuple)\n |  Row(*args: Optional[str], **kwargs: Optional[Any]) -&gt; 'Row'\n |\n |  A row in :class:`DataFrame`.\n |  The fields in it can be accessed:\n |\n |  * like attributes (``row.key``)\n |  * like dictionary values (``row[key]``)\n |\n |  ``key in row`` will search through row keys.\n |\n |  Row can be used to create a row object by using named arguments.\n |  It is not allowed to omit a named argument to represent that the value is\n |  None or missing. This should be explicitly set to None in this case.\n |\n |  .. versionchanged:: 3.0.0\n |      Rows created from named arguments no longer have\n |      field names sorted alphabetically and will be ordered in the position as\n |      entered.\n |\n |  Examples\n |  --------\n |  &gt;&gt;&gt; from pyspark.sql import Row\n |  &gt;&gt;&gt; row = Row(name=\"Alice\", age=11)\n |  &gt;&gt;&gt; row\n |  Row(name='Alice', age=11)\n |  &gt;&gt;&gt; row['name'], row['age']\n |  ('Alice', 11)\n |  &gt;&gt;&gt; row.name, row.age\n |  ('Alice', 11)\n |  &gt;&gt;&gt; 'name' in row\n |  True\n |  &gt;&gt;&gt; 'wrong_key' in row\n |  False\n |\n |  Row also can be used to create another Row like class, then it\n |  could be used to create Row objects, such as\n |\n |  &gt;&gt;&gt; Person = Row(\"name\", \"age\")\n |  &gt;&gt;&gt; Person\n |  &lt;Row('name', 'age')&gt;\n |  &gt;&gt;&gt; 'name' in Person\n |  True\n |  &gt;&gt;&gt; 'wrong_key' in Person\n |  False\n |  &gt;&gt;&gt; Person(\"Alice\", 11)\n |  Row(name='Alice', age=11)\n |\n |  This form can also be used to create rows as tuple values, i.e. with unnamed\n |  fields.\n |\n |  &gt;&gt;&gt; row1 = Row(\"Alice\", 11)\n |  &gt;&gt;&gt; row2 = Row(name=\"Alice\", age=11)\n |  &gt;&gt;&gt; row1 == row2\n |  True\n |\n |  Method resolution order:\n |      Row\n |      builtins.tuple\n |      builtins.object\n |\n |  Methods defined here:\n |\n |  __call__(self, *args: Any) -&gt; 'Row'\n |      create new Row object\n |\n |  __contains__(self, item: Any) -&gt; bool\n |      Return bool(key in self).\n |\n |  __getattr__(self, item: str) -&gt; Any\n |\n |  __getitem__(self, item: Any) -&gt; Any\n |      Return self[key].\n |\n |  __reduce__(self) -&gt; Union[str, Tuple[Any, ...]]\n |      Returns a tuple so Python knows how to pickle Row.\n |\n |  __repr__(self) -&gt; str\n |      Printable representation of Row used in Python REPL.\n |\n |  __setattr__(self, key: Any, value: Any) -&gt; None\n |      Implement setattr(self, name, value).\n |\n |  asDict(self, recursive: bool = False) -&gt; Dict[str, Any]\n |      Return as a dict\n |\n |      Parameters\n |      ----------\n |      recursive : bool, optional\n |          turns the nested Rows to dict (default: False).\n |\n |      Notes\n |      -----\n |      If a row contains duplicate field names, e.g., the rows of a join\n |      between two :class:`DataFrame` that both have the fields of same names,\n |      one of the duplicate fields will be selected by ``asDict``. ``__getitem__``\n |      will also return one of the duplicate fields, however returned value might\n |      be different to ``asDict``.\n |\n |      Examples\n |      --------\n |      &gt;&gt;&gt; from pyspark.sql import Row\n |      &gt;&gt;&gt; Row(name=\"Alice\", age=11).asDict() == {'name': 'Alice', 'age': 11}\n |      True\n |      &gt;&gt;&gt; row = Row(key=1, value=Row(name='a', age=2))\n |      &gt;&gt;&gt; row.asDict() == {'key': 1, 'value': Row(name='a', age=2)}\n |      True\n |      &gt;&gt;&gt; row.asDict(True) == {'key': 1, 'value': {'name': 'a', 'age': 2}}\n |      True\n |\n |  ----------------------------------------------------------------------\n |  Static methods defined here:\n |\n |  __new__(cls, *args: Optional[str], **kwargs: Optional[Any]) -&gt; 'Row'\n |      Create and return a new object.  See help(type) for accurate signature.\n |\n |  ----------------------------------------------------------------------\n |  Data descriptors defined here:\n |\n |  __dict__\n |      dictionary for instance variables\n |\n |  ----------------------------------------------------------------------\n |  Methods inherited from builtins.tuple:\n |\n |  __add__(self, value, /)\n |      Return self+value.\n |\n |  __eq__(self, value, /)\n |      Return self==value.\n |\n |  __ge__(self, value, /)\n |      Return self&gt;=value.\n |\n |  __getattribute__(self, name, /)\n |      Return getattr(self, name).\n |\n |  __getnewargs__(self, /)\n |\n |  __gt__(self, value, /)\n |      Return self&gt;value.\n |\n |  __hash__(self, /)\n |      Return hash(self).\n |\n |  __iter__(self, /)\n |      Implement iter(self).\n |\n |  __le__(self, value, /)\n |      Return self&lt;=value.\n |\n |  __len__(self, /)\n |      Return len(self).\n |\n |  __lt__(self, value, /)\n |      Return self&lt;value.\n |\n |  __mul__(self, value, /)\n |      Return self*value.\n |\n |  __ne__(self, value, /)\n |      Return self!=value.\n |\n |  __rmul__(self, value, /)\n |      Return value*self.\n |\n |  count(self, value, /)\n |      Return number of occurrences of value.\n |\n |  index(self, value, start=0, stop=9223372036854775807, /)\n |      Return first index of value.\n |\n |      Raises ValueError if the value is not present.\n |\n |  ----------------------------------------------------------------------\n |  Class methods inherited from builtins.tuple:\n |\n |  __class_getitem__(...)\n |      See PEP 585\n\n\n\n\n\nCode\ncolumn_names = [\"name\", \"age\", \"gender\"]\nrows = [\n    [\"John\", 21, \"male\"],\n    [\"James\", 25, \"female\"],\n    [\"Albert\", 46, \"male\"]\n]\n\ndf = spark.createDataFrame(\n    rows, \n    column_names\n)\n\ndf.show()\n\n\n+------+---+------+\n|  name|age|gender|\n+------+---+------+\n|  John| 21|  male|\n| James| 25|female|\n|Albert| 46|  male|\n+------+---+------+\n\n\n\n\n\nCode\ndf.printSchema()\n\n\nroot\n |-- name: string (nullable = true)\n |-- age: long (nullable = true)\n |-- gender: string (nullable = true)\n\n\n\n\n\nCode\n# sc = SparkContext(conf=conf)  # no need for Spark 3...\n\ncolumn_names = [\"name\", \"age\", \"gender\"]\nrdd = sc.parallelize([\n    (\"John\", 21, \"male\"),\n    (\"James\", 25, \"female\"),\n    (\"Albert\", 46, \"male\")\n])\ndf = spark.createDataFrame(rdd, column_names)\ndf.show()\n\n\n+------+---+------+\n|  name|age|gender|\n+------+---+------+\n|  John| 21|  male|\n| James| 25|female|\n|Albert| 46|  male|\n+------+---+------+"
  },
  {
    "objectID": "core/notebooks/notebook06_sparksql.html#schema",
    "href": "core/notebooks/notebook06_sparksql.html#schema",
    "title": "DataFrame",
    "section": "Schema",
    "text": "Schema\nThere is special type schemata. A object of class StructType is made of a list of objects of type StructField.\n\n\nCode\ndf.schema\n\n\nStructType([StructField('name', StringType(), True), StructField('age', LongType(), True), StructField('gender', StringType(), True)])\n\n\n\n\nCode\ntype(df.schema)\n\n\npyspark.sql.types.StructType\n\n\nA object of type StructField has a name, a PySpark type, an d a boolean parameter.\n\n\nCode\nfrom pyspark.sql.types import *\n\nschema = StructType(\n    [\n        StructField(\"name\", StringType(), True),\n        StructField(\"age\", IntegerType(), True),\n        StructField(\"gender\", StringType(), True)\n    ]\n)\n\nrows = [(\"John\", 21, \"male\")]\ndf = spark.createDataFrame(rows, schema)\ndf.printSchema()\ndf.show()\n\n\nroot\n |-- name: string (nullable = true)\n |-- age: integer (nullable = true)\n |-- gender: string (nullable = true)\n\n+----+---+------+\n|name|age|gender|\n+----+---+------+\n|John| 21|  male|\n+----+---+------+"
  },
  {
    "objectID": "core/notebooks/notebook06_sparksql.html#select-projection-π",
    "href": "core/notebooks/notebook06_sparksql.html#select-projection-π",
    "title": "DataFrame",
    "section": "SELECT (projection \\(π\\))",
    "text": "SELECT (projection \\(π\\))\n\n\nCode\ndf.createOrReplaceTempView(\"table\")    \n\nquery = \"\"\"\n    SELECT \n        name, age \n    FROM \n        table\n\"\"\"\n\nspark.sql(query).show()\n\n\n+----+---+\n|name|age|\n+----+---+\n|John| 21|\n|Jane| 25|\n+----+---+\n\n\n\nUsing the API:\n\n\nCode\n(\n    df\n        .select(\"name\", \"age\")\n        .show()\n)\n\n\n+----+---+\n|name|age|\n+----+---+\n|John| 21|\n|Jane| 25|\n+----+---+\n\n\n\nπ(df, \"name\", \"age\")"
  },
  {
    "objectID": "core/notebooks/notebook06_sparksql.html#where-filter-selection-σ",
    "href": "core/notebooks/notebook06_sparksql.html#where-filter-selection-σ",
    "title": "DataFrame",
    "section": "WHERE (filter, selection, \\(σ\\))",
    "text": "WHERE (filter, selection, \\(σ\\))\n\n\nCode\ndf.createOrReplaceTempView(\"table\")\n\nquery = \"\"\"\n    SELECT \n        * \n    FROM \n        table\n    WHERE \n        age &gt; 21\n\"\"\"\n\nspark.sql(query).show()\n\n\n+----+---+------+\n|name|age|gender|\n+----+---+------+\n|Jane| 25|female|\n+----+---+------+\n\n\n\nUsing the API\n\n\nCode\n( \n    df\n        .where(\"age &gt; 21\")\n        .show()\n)\n\n\n+----+---+------+\n|name|age|gender|\n+----+---+------+\n|Jane| 25|female|\n+----+---+------+\n\n\n\nThis implements σ(df, \"age &gt; 21\")\n\n\nCode\n# Alternatively:\n( \n    df\n      .where(df['age'] &gt; 21)\n      .show()\n)\n\n\n+----+---+------+\n|name|age|gender|\n+----+---+------+\n|Jane| 25|female|\n+----+---+------+\n\n\n\n\n\nCode\n( \n    df\n      .where(df.age &gt; 21)\n      .show()\n)\n\n\n+----+---+------+\n|name|age|gender|\n+----+---+------+\n|Jane| 25|female|\n+----+---+------+\n\n\n\nMethod chaining allows to construct complex queries\n\n\nCode\n( \n    df\n      .where(\"age &gt; 21\")\n      .select([\"name\", \"age\"])\n      .show()\n)\n\n\n+----+---+\n|name|age|\n+----+---+\n|Jane| 25|\n+----+---+\n\n\n\nThis implements\n    σ(df, \"age &gt; 21\") |&gt;\n    π([\"name\", \"age\"])"
  },
  {
    "objectID": "core/notebooks/notebook06_sparksql.html#limit",
    "href": "core/notebooks/notebook06_sparksql.html#limit",
    "title": "DataFrame",
    "section": "LIMIT",
    "text": "LIMIT\n\n\nCode\ndf.createOrReplaceTempView(\"table\")\n\nquery = \"\"\"\n    SELECT \n        * \n    FROM \n        table \n    LIMIT 1\n\"\"\"\n\nspark.sql(query).show()\n\n\n+----+---+------+\n|name|age|gender|\n+----+---+------+\n|John| 21|  male|\n+----+---+------+\n\n\n\n\n\nCode\ndf.limit(1).show()\n\n\n+----+---+------+\n|name|age|gender|\n+----+---+------+\n|John| 21|  male|\n+----+---+------+\n\n\n\n\n\nCode\ndf.select(\"*\").limit(1).show()\n\n\n+----+---+------+\n|name|age|gender|\n+----+---+------+\n|John| 21|  male|\n+----+---+------+"
  },
  {
    "objectID": "core/notebooks/notebook06_sparksql.html#order-by",
    "href": "core/notebooks/notebook06_sparksql.html#order-by",
    "title": "DataFrame",
    "section": "ORDER BY",
    "text": "ORDER BY\n\n\nCode\ndf.createOrReplaceTempView(\"table\")\n\nquery = \"\"\"\n    SELECT \n        * \n    FROM \n        table\n    ORDER BY \n        name ASC\n\"\"\"\n\nspark.sql(query).show()\n\n\n+----+---+------+\n|name|age|gender|\n+----+---+------+\n|Jane| 25|female|\n|John| 21|  male|\n+----+---+------+\n\n\n\n\n\nCode\ndf.orderBy(df.name.asc()).show()\n\n\n+----+---+------+\n|name|age|gender|\n+----+---+------+\n|Jane| 25|female|\n|John| 21|  male|\n+----+---+------+"
  },
  {
    "objectID": "core/notebooks/notebook06_sparksql.html#alias-rename",
    "href": "core/notebooks/notebook06_sparksql.html#alias-rename",
    "title": "DataFrame",
    "section": "ALIAS (rename)",
    "text": "ALIAS (rename)\n\n\nCode\ndf.createOrReplaceTempView(\"table\")\nquery = \"SELECT name, age, gender AS sex FROM table\"\nspark.sql(query).show()\n\n\n+----+---+------+\n|name|age|   sex|\n+----+---+------+\n|John| 21|  male|\n|Jane| 25|female|\n+----+---+------+\n\n\n\n\n\nCode\ntype(df.age)\n\n\npyspark.sql.column.Column\n\n\n\n\nCode\ndf.select(df.name, df.age, df.gender.alias('sex')).show()\n\n\n+----+---+------+\n|name|age|   sex|\n+----+---+------+\n|John| 21|  male|\n|Jane| 25|female|\n+----+---+------+"
  },
  {
    "objectID": "core/notebooks/notebook06_sparksql.html#cast",
    "href": "core/notebooks/notebook06_sparksql.html#cast",
    "title": "DataFrame",
    "section": "CAST",
    "text": "CAST\n\n\nCode\ndf.createOrReplaceTempView(\"table\")\nquery = \"SELECT name, cast(age AS float) AS age_f FROM table\"\nspark.sql(query).show()\n\n\n+----+-----+\n|name|age_f|\n+----+-----+\n|John| 21.0|\n|Jane| 25.0|\n+----+-----+\n\n\n\n\n\nCode\ndf.select(df.name, df.age.cast(\"float\").alias(\"age_f\")).show()\n\n\n+----+-----+\n|name|age_f|\n+----+-----+\n|John| 21.0|\n|Jane| 25.0|\n+----+-----+\n\n\n\n\n\nCode\nnew_age_col = df.age.cast(\"float\").alias(\"age_f\")\ntype(new_age_col), type(df.age)\n\n\n(pyspark.sql.column.Column, pyspark.sql.column.Column)\n\n\n\n\nCode\ndf.select(df.name, new_age_col).show()\n\n\n+----+-----+\n|name|age_f|\n+----+-----+\n|John| 21.0|\n|Jane| 25.0|\n+----+-----+"
  },
  {
    "objectID": "core/notebooks/notebook06_sparksql.html#adding-new-columns",
    "href": "core/notebooks/notebook06_sparksql.html#adding-new-columns",
    "title": "DataFrame",
    "section": "Adding new columns",
    "text": "Adding new columns\n\n\nCode\ndf.createOrReplaceTempView(\"table\")\n\nquery = \"\"\"\n    SELECT \n        *, \n        12*age AS age_months \n    FROM \n        table\n\"\"\"\n\nspark.sql(query).show()\n\n\n+----+---+------+----------+\n|name|age|gender|age_months|\n+----+---+------+----------+\n|John| 21|  male|       252|\n|Jane| 25|female|       300|\n+----+---+------+----------+\n\n\n\n\n\nCode\n( \n    df\n        .withColumn(\"age_months\", df.age * 12)\n        .show()\n)\n\n\n+----+---+------+----------+\n|name|age|gender|age_months|\n+----+---+------+----------+\n|John| 21|  male|       252|\n|Jane| 25|female|       300|\n+----+---+------+----------+\n\n\n\n\n\nCode\n(\n    df\n        .select(\"*\", \n                (df.age * 12).alias(\"age_months\"))\n        .show()\n)\n\n\n+----+---+------+----------+\n|name|age|gender|age_months|\n+----+---+------+----------+\n|John| 21|  male|       252|\n|Jane| 25|female|       300|\n+----+---+------+----------+\n\n\n\n\n\nCode\nimport datetime\n\nhui = datetime.date.today()\n\nhui = hui.replace(year=hui.year-21)\n\nstr(hui)\n\n\n'2004-03-10'\n\n\n\n\nCode\n# df.select(\"*\", hui.replace(year=hui.year - df.age ).alias(\"yob\")).show()"
  },
  {
    "objectID": "core/notebooks/notebook06_sparksql.html#numeric-functions-examples",
    "href": "core/notebooks/notebook06_sparksql.html#numeric-functions-examples",
    "title": "DataFrame",
    "section": "Numeric functions examples",
    "text": "Numeric functions examples\n\n\nCode\nfrom pyspark.sql import functions as fn\n\ncolumns = [\"brand\", \"cost\"]\ndf = spark.createDataFrame([\n    (\"garnier\", 3.49),\n    (\"elseve\", 2.71)\n], columns)\n\nround_cost = fn.round(df.cost, 1)\nfloor_cost = fn.floor(df.cost)\nceil_cost = fn.ceil(df.cost)\n\ndf.withColumn('round', round_cost)\\\n    .withColumn('floor', floor_cost)\\\n    .withColumn('ceil', ceil_cost)\\\n    .show()\n\n\n+-------+----+-----+-----+----+\n|  brand|cost|round|floor|ceil|\n+-------+----+-----+-----+----+\n|garnier|3.49|  3.5|    3|   4|\n| elseve|2.71|  2.7|    2|   3|\n+-------+----+-----+-----+----+"
  },
  {
    "objectID": "core/notebooks/notebook06_sparksql.html#string-functions-examples",
    "href": "core/notebooks/notebook06_sparksql.html#string-functions-examples",
    "title": "DataFrame",
    "section": "String functions examples",
    "text": "String functions examples\n\n\nCode\nfrom pyspark.sql import functions as fn\n\ncolumns = [\"first_name\", \"last_name\"]\n\ndf = spark.createDataFrame([\n    (\"John\", \"Doe\"),\n    (\"Mary\", \"Jane\")\n], columns)\n\nlast_name_initial = fn.substring(df.last_name, 0, 1)\n# last_name_initial_dotted = fn.concat(last_name_initial, \".\")\n\nname = fn.concat_ws(\" \", df.first_name, last_name_initial)\ndf.withColumn(\"name\", name).show()\n\n\n+----------+---------+------+\n|first_name|last_name|  name|\n+----------+---------+------+\n|      John|      Doe|John D|\n|      Mary|     Jane|Mary J|\n+----------+---------+------+\n\n\n\n\n\nCode\n( \n    df.selectExpr(\"*\", \"substring(last_name, 0, 1) as lni\")\n      .selectExpr(\"first_name\", \"last_name\", \"concat(first_name, ' ', lni, '.') as nname\")\n      .show()\n)\n\n\n+----------+---------+-------+\n|first_name|last_name|  nname|\n+----------+---------+-------+\n|      John|      Doe|John D.|\n|      Mary|     Jane|Mary J.|\n+----------+---------+-------+"
  },
  {
    "objectID": "core/notebooks/notebook06_sparksql.html#date-functions-examples",
    "href": "core/notebooks/notebook06_sparksql.html#date-functions-examples",
    "title": "DataFrame",
    "section": "Date functions examples",
    "text": "Date functions examples\n\n\nCode\nfrom datetime import date\nfrom pyspark.sql import functions as fn\n\ndf = spark.createDataFrame([\n    (date(2015, 1, 1), date(2015, 1, 15)),\n    (date(2015, 2, 21), date(2015, 3, 8)),\n], [\"start_date\", \"end_date\"])\n\ndays_between = fn.datediff(df.end_date, df.start_date)\nstart_month = fn.month(df.start_date)\n\ndf.withColumn('days_between', days_between)\\\n    .withColumn('start_month', start_month)\\\n    .show()\n\n\n+----------+----------+------------+-----------+\n|start_date|  end_date|days_between|start_month|\n+----------+----------+------------+-----------+\n|2015-01-01|2015-01-15|          14|          1|\n|2015-02-21|2015-03-08|          15|          2|\n+----------+----------+------------+-----------+\n\n\n\n\n\nCode\nstr(date(2015, 1, 1) - date(2015, 1, 15))\n\n\n'-14 days, 0:00:00'\n\n\n\n\nCode\nfrom datetime import timedelta\n\ndate(2023, 2 , 14) + timedelta(days=3)\n\n\ndatetime.date(2023, 2, 17)"
  },
  {
    "objectID": "core/notebooks/notebook06_sparksql.html#conditional-transformations",
    "href": "core/notebooks/notebook06_sparksql.html#conditional-transformations",
    "title": "DataFrame",
    "section": "Conditional transformations",
    "text": "Conditional transformations\n\n\nCode\ndf = spark.createDataFrame([\n    (\"John\", 21, \"male\"),\n    (\"Jane\", 25, \"female\"),\n    (\"Albert\", 46, \"male\"),\n    (\"Brad\", 49, \"super-hero\")\n], [\"name\", \"age\", \"gender\"])\n\n\n\n\nCode\nsupervisor = ( \n    fn.when(df.gender == 'male', 'Mr. Smith')\n      .when(df.gender == 'female', 'Miss Jones')\n      .otherwise('NA')\n)\n\ntype(supervisor), type(fn.when)\n\n\n(pyspark.sql.column.Column, function)\n\n\n\n\nCode\ndf.withColumn(\"supervisor\", supervisor).show()\n\n\n+------+---+----------+----------+\n|  name|age|    gender|supervisor|\n+------+---+----------+----------+\n|  John| 21|      male| Mr. Smith|\n|  Jane| 25|    female|Miss Jones|\n|Albert| 46|      male| Mr. Smith|\n|  Brad| 49|super-hero|        NA|\n+------+---+----------+----------+"
  },
  {
    "objectID": "core/notebooks/notebook06_sparksql.html#user-defined-functions",
    "href": "core/notebooks/notebook06_sparksql.html#user-defined-functions",
    "title": "DataFrame",
    "section": "User-defined functions",
    "text": "User-defined functions\n\n\nCode\nfrom pyspark.sql import functions as fn\nfrom pyspark.sql.types import StringType\n\ndf = spark.createDataFrame([(1, 3), (4, 2)], [\"first\", \"second\"])\n\ndef my_func(col_1, col_2):\n    if (col_1 &gt; col_2):\n        return \"{} is bigger than {}\".format(col_1, col_2)\n    else:\n        return \"{} is bigger than {}\".format(col_2, col_1)\n\nmy_udf = fn.udf(my_func, StringType())\n\ndf.withColumn(\"udf\", my_udf(df['first'], df['second'])).show()\n\n\n+-----+------+------------------+\n|first|second|               udf|\n+-----+------+------------------+\n|    1|     3|3 is bigger than 1|\n|    4|     2|4 is bigger than 2|\n+-----+------+------------------+"
  },
  {
    "objectID": "core/notebooks/notebook06_sparksql.html#using-the-spark.sql-api",
    "href": "core/notebooks/notebook06_sparksql.html#using-the-spark.sql-api",
    "title": "DataFrame",
    "section": "Using the spark.sql API",
    "text": "Using the spark.sql API\n\n\nCode\nfrom datetime import date\n\nproducts = spark.createDataFrame([\n    ('1', 'mouse', 'microsoft', 39.99),\n    ('2', 'keyboard', 'logitech', 59.99),\n], ['prod_id', 'prod_cat', 'prod_brand', 'prod_value'])\n\npurchases = spark.createDataFrame([\n    (date(2017, 11, 1), 2, '1'),\n    (date(2017, 11, 2), 1, '1'),\n    (date(2017, 11, 5), 1, '2'),\n], ['date', 'quantity', 'prod_id'])\n\n# The default join type is the \"INNER\" join\npurchases.join(products, 'prod_id').show()\n\n\n+-------+----------+--------+--------+----------+----------+\n|prod_id|      date|quantity|prod_cat|prod_brand|prod_value|\n+-------+----------+--------+--------+----------+----------+\n|      1|2017-11-01|       2|   mouse| microsoft|     39.99|\n|      1|2017-11-02|       1|   mouse| microsoft|     39.99|\n|      2|2017-11-05|       1|keyboard|  logitech|     59.99|\n+-------+----------+--------+--------+----------+----------+\n\n\n\n\n\nCode\npurchases.join(products, 'prod_id').explain()\n\n\n== Physical Plan ==\nAdaptiveSparkPlan isFinalPlan=false\n+- Project [prod_id#577, date#575, quantity#576L, prod_cat#568, prod_brand#569, prod_value#570]\n   +- SortMergeJoin [prod_id#577], [prod_id#567], Inner\n      :- Sort [prod_id#577 ASC NULLS FIRST], false, 0\n      :  +- Exchange hashpartitioning(prod_id#577, 200), ENSURE_REQUIREMENTS, [plan_id=575]\n      :     +- Filter isnotnull(prod_id#577)\n      :        +- Scan ExistingRDD[date#575,quantity#576L,prod_id#577]\n      +- Sort [prod_id#567 ASC NULLS FIRST], false, 0\n         +- Exchange hashpartitioning(prod_id#567, 200), ENSURE_REQUIREMENTS, [plan_id=576]\n            +- Filter isnotnull(prod_id#567)\n               +- Scan ExistingRDD[prod_id#567,prod_cat#568,prod_brand#569,prod_value#570]"
  },
  {
    "objectID": "core/notebooks/notebook06_sparksql.html#using-a-sql-query",
    "href": "core/notebooks/notebook06_sparksql.html#using-a-sql-query",
    "title": "DataFrame",
    "section": "Using a SQL query",
    "text": "Using a SQL query\n\n\nCode\nproducts.createOrReplaceTempView(\"products\")\npurchases.createOrReplaceTempView(\"purchases\")\n\nquery = \"\"\"\n    SELECT * \n    FROM purchases AS prc INNER JOIN \n        products AS prd \n    ON prc.prod_id = prd.prod_id\n\"\"\"\nspark.sql(query).show()\n\n\n+----------+--------+-------+-------+--------+----------+----------+\n|      date|quantity|prod_id|prod_id|prod_cat|prod_brand|prod_value|\n+----------+--------+-------+-------+--------+----------+----------+\n|2017-11-01|       2|      1|      1|   mouse| microsoft|     39.99|\n|2017-11-02|       1|      1|      1|   mouse| microsoft|     39.99|\n|2017-11-05|       1|      2|      2|keyboard|  logitech|     59.99|\n+----------+--------+-------+-------+--------+----------+----------+\n\n\n\n\n\nCode\nspark.sql(query).explain()\n\n\n== Physical Plan ==\nAdaptiveSparkPlan isFinalPlan=false\n+- SortMergeJoin [prod_id#577], [prod_id#567], Inner\n   :- Sort [prod_id#577 ASC NULLS FIRST], false, 0\n   :  +- Exchange hashpartitioning(prod_id#577, 200), ENSURE_REQUIREMENTS, [plan_id=718]\n   :     +- Filter isnotnull(prod_id#577)\n   :        +- Scan ExistingRDD[date#575,quantity#576L,prod_id#577]\n   +- Sort [prod_id#567 ASC NULLS FIRST], false, 0\n      +- Exchange hashpartitioning(prod_id#567, 200), ENSURE_REQUIREMENTS, [plan_id=719]\n         +- Filter isnotnull(prod_id#567)\n            +- Scan ExistingRDD[prod_id#567,prod_cat#568,prod_brand#569,prod_value#570]\n\n\n\n\n\n\nCode\nnew_purchases = spark.createDataFrame([\n    (date(2017, 11, 1), 2, '1'),\n    (date(2017, 11, 2), 1, '3'),\n], ['date', 'quantity', 'prod_id_x'])\n\n# The default join type is the \"INNER\" join\njoin_rule = new_purchases.prod_id_x == products.prod_id\n\nprint(type(join_rule))\n\nnew_purchases.join(products, join_rule, 'left').show()\n\n\n&lt;class 'pyspark.sql.column.Column'&gt;\n+----------+--------+---------+-------+--------+----------+----------+\n|      date|quantity|prod_id_x|prod_id|prod_cat|prod_brand|prod_value|\n+----------+--------+---------+-------+--------+----------+----------+\n|2017-11-01|       2|        1|      1|   mouse| microsoft|     39.99|\n|2017-11-02|       1|        3|   NULL|    NULL|      NULL|      NULL|\n+----------+--------+---------+-------+--------+----------+----------+\n\n\n\n\n\nCode\njoin_rule.info\n\n\nColumn&lt;'(prod_id_x = prod_id)[info]'&gt;\n\n\n\n\nCode\nnew_purchases = spark.createDataFrame([\n    (date(2017, 11, 1), 2, '1'),\n    (date(2017, 11, 2), 1, '3'),\n], ['date', 'quantity', 'prod_id_x'])\n\n# The default join type is the \"INNER\" join\njoin_rule = new_purchases.prod_id_x == products.prod_id\n\nnew_purchases.join(products, join_rule, 'left').show()\n\n\n+----------+--------+---------+-------+--------+----------+----------+\n|      date|quantity|prod_id_x|prod_id|prod_cat|prod_brand|prod_value|\n+----------+--------+---------+-------+--------+----------+----------+\n|2017-11-01|       2|        1|      1|   mouse| microsoft|     39.99|\n|2017-11-02|       1|        3|   NULL|    NULL|      NULL|      NULL|\n+----------+--------+---------+-------+--------+----------+----------+"
  },
  {
    "objectID": "core/notebooks/notebook06_sparksql.html#various-types-of-joins",
    "href": "core/notebooks/notebook06_sparksql.html#various-types-of-joins",
    "title": "DataFrame",
    "section": "Various types of joins",
    "text": "Various types of joins\n\n\nCode\nleft = spark.createDataFrame([\n    (1, \"A1\"), (2, \"A2\"), (3, \"A3\"), (4, \"A4\")], \n    [\"id\", \"value\"])\n\nright = spark.createDataFrame([\n    (3, \"A3\"), (4, \"A4\"), (4, \"A4_1\"), (5, \"A5\"), (6, \"A6\")], \n    [\"id\", \"value\"])\n\njoin_types = [\n    \"inner\", \"outer\", \"left\", \"right\",\n    \"leftsemi\", \"leftanti\"\n]\n\n\n\n\nCode\nfor join_type in join_types:\n    print(join_type)\n    left.join(right, on=\"id\", how=join_type)\\\n        .orderBy(\"id\")\\\n        .show()\n\n\ninner\n+---+-----+-----+\n| id|value|value|\n+---+-----+-----+\n|  3|   A3|   A3|\n|  4|   A4|   A4|\n|  4|   A4| A4_1|\n+---+-----+-----+\n\nouter\n+---+-----+-----+\n| id|value|value|\n+---+-----+-----+\n|  1|   A1| NULL|\n|  2|   A2| NULL|\n|  3|   A3|   A3|\n|  4|   A4|   A4|\n|  4|   A4| A4_1|\n|  5| NULL|   A5|\n|  6| NULL|   A6|\n+---+-----+-----+\n\nleft\n+---+-----+-----+\n| id|value|value|\n+---+-----+-----+\n|  1|   A1| NULL|\n|  2|   A2| NULL|\n|  3|   A3|   A3|\n|  4|   A4|   A4|\n|  4|   A4| A4_1|\n+---+-----+-----+\n\nright\n+---+-----+-----+\n| id|value|value|\n+---+-----+-----+\n|  3|   A3|   A3|\n|  4|   A4|   A4|\n|  4|   A4| A4_1|\n|  5| NULL|   A5|\n|  6| NULL|   A6|\n+---+-----+-----+\n\nleftsemi\n+---+-----+\n| id|value|\n+---+-----+\n|  3|   A3|\n|  4|   A4|\n+---+-----+\n\nleftanti\n+---+-----+\n| id|value|\n+---+-----+\n|  1|   A1|\n|  2|   A2|\n+---+-----+"
  },
  {
    "objectID": "core/notebooks/notebook06_sparksql.html#examples-using-the-api",
    "href": "core/notebooks/notebook06_sparksql.html#examples-using-the-api",
    "title": "DataFrame",
    "section": "Examples using the API",
    "text": "Examples using the API\n\n\nCode\nfrom pyspark.sql import functions as fn\n\nproducts = spark.createDataFrame([\n    ('1', 'mouse', 'microsoft', 39.99),\n    ('2', 'mouse', 'microsoft', 59.99),\n    ('3', 'keyboard', 'microsoft', 59.99),\n    ('4', 'keyboard', 'logitech', 59.99),\n    ('5', 'mouse', 'logitech', 29.99),\n], ['prod_id', 'prod_cat', 'prod_brand', 'prod_value'])\n\n( \n    products\n        .groupBy('prod_cat')\n        .avg('prod_value')\n        .show()\n)\n\n\n+--------+-----------------+\n|prod_cat|  avg(prod_value)|\n+--------+-----------------+\n|   mouse|43.32333333333333|\n|keyboard|            59.99|\n+--------+-----------------+\n\n\n\n\n\nCode\n(\n    products\n        .groupBy('prod_cat')\n        .agg(fn.avg('prod_value'))\n        .show()\n)\n\n\n+--------+-----------------+\n|prod_cat|  avg(prod_value)|\n+--------+-----------------+\n|   mouse|43.32333333333333|\n|keyboard|            59.99|\n+--------+-----------------+\n\n\n\n\n\nCode\n(\n    products\n        .groupBy('prod_cat')\n        .agg(\n            fn.mean('prod_value'), \n            fn.stddev('prod_value')\n        )\n        .show()\n)\n\n\n+--------+-----------------+------------------+\n|prod_cat|  avg(prod_value)|stddev(prod_value)|\n+--------+-----------------+------------------+\n|   mouse|43.32333333333333|15.275252316519468|\n|keyboard|            59.99|               0.0|\n+--------+-----------------+------------------+\n\n\n\n\n\nCode\nfrom pyspark.sql import functions as fn\n\n(\n    products\n        .groupBy('prod_brand', 'prod_cat')\\\n        .agg(\n            fn.avg('prod_value')\n        )\n        .show()\n)\n\n\n+----------+--------+---------------+\n|prod_brand|prod_cat|avg(prod_value)|\n+----------+--------+---------------+\n| microsoft|   mouse|          49.99|\n| microsoft|keyboard|          59.99|\n|  logitech|keyboard|          59.99|\n|  logitech|   mouse|          29.99|\n+----------+--------+---------------+\n\n\n\n\n\nCode\nfrom pyspark.sql import functions as fn\n\n(\n    products\n        .groupBy('prod_brand')\n        .agg(\n            fn.round(\n                fn.avg('prod_value'), 1)\n                .alias('average'),\n            fn.ceil(\n                fn.sum('prod_value'))\n                .alias('sum'),\n            fn.min('prod_value')\n                .alias('min')\n        )\n        .show()\n)\n\n\n+----------+-------+---+-----+\n|prod_brand|average|sum|  min|\n+----------+-------+---+-----+\n| microsoft|   53.3|160|39.99|\n|  logitech|   45.0| 90|29.99|\n+----------+-------+---+-----+"
  },
  {
    "objectID": "core/notebooks/notebook06_sparksql.html#example-using-a-query",
    "href": "core/notebooks/notebook06_sparksql.html#example-using-a-query",
    "title": "DataFrame",
    "section": "Example using a query",
    "text": "Example using a query\n\n\nCode\nproducts.createOrReplaceTempView(\"products\")\n\n\n\n\nCode\nquery = \"\"\"\nSELECT\n    prod_brand,\n    round(avg(prod_value), 1) AS average,\n    min(prod_value) AS min\nFROM \n    products\nGROUP BY \n    prod_brand\n\"\"\"\n\nspark.sql(query).show()\n\n\n+----------+-------+-----+\n|prod_brand|average|  min|\n+----------+-------+-----+\n| microsoft|   53.3|39.99|\n|  logitech|   45.0|29.99|\n+----------+-------+-----+"
  },
  {
    "objectID": "core/notebooks/notebook06_sparksql.html#numerical-window-functions",
    "href": "core/notebooks/notebook06_sparksql.html#numerical-window-functions",
    "title": "DataFrame",
    "section": "Numerical window functions",
    "text": "Numerical window functions\n\n\nCode\nfrom pyspark.sql import Window\nfrom pyspark.sql import functions as fn\n\n# First, we create the Window definition\nwindow = Window.partitionBy('prod_brand')\n\nprint(type(window))\n\n\n&lt;class 'pyspark.sql.window.WindowSpec'&gt;\n\n\nThen, we can use over to aggregate on this window\n\n\nCode\navg = fn.avg('prod_value').over(window)\n\n# Finally, we can it as a classical column\n(\n    products\n        .withColumn('avg_brand_value', fn.round(avg, 2))\n        .show()\n)\n\n\n+-------+--------+----------+----------+---------------+\n|prod_id|prod_cat|prod_brand|prod_value|avg_brand_value|\n+-------+--------+----------+----------+---------------+\n|      4|keyboard|  logitech|     59.99|          44.99|\n|      5|   mouse|  logitech|     29.99|          44.99|\n|      1|   mouse| microsoft|     39.99|          53.32|\n|      2|   mouse| microsoft|     59.99|          53.32|\n|      3|keyboard| microsoft|     59.99|          53.32|\n+-------+--------+----------+----------+---------------+\n\n\n\nWith SQL queries, using multiple windows is not a problem\n\n\nCode\nquery = \"\"\"\n    SELECT \n        *, \n        ROUND(AVG(prod_value) OVER w1, 2)  AS avg_brand_value,\n        ROUND(AVG(prod_value) OVER w2, 1)  AS avg_prod_value\n    FROM \n        products\n    WINDOW \n        w1 AS (PARTITION BY prod_brand),\n        w2 AS (PARTITION BY prod_cat)\n\"\"\"\n\nspark.sql(query).show()\n\n\n+-------+--------+----------+----------+---------------+--------------+\n|prod_id|prod_cat|prod_brand|prod_value|avg_brand_value|avg_prod_value|\n+-------+--------+----------+----------+---------------+--------------+\n|      4|keyboard|  logitech|     59.99|          44.99|          60.0|\n|      3|keyboard| microsoft|     59.99|          53.32|          60.0|\n|      5|   mouse|  logitech|     29.99|          44.99|          43.3|\n|      1|   mouse| microsoft|     39.99|          53.32|          43.3|\n|      2|   mouse| microsoft|     59.99|          53.32|          43.3|\n+-------+--------+----------+----------+---------------+--------------+\n\n\n\n\n\nCode\nwindow2 = Window.partitionBy('prod_cat')\n\navg2 = fn.avg('prod_value').over(window2)\n\n# Finally, we can it as a classical column\n( \n    products\n        .withColumn('avg_brand_value', fn.round(avg, 2))\n        .withColumn('avg_prod_value', fn.round(avg2, 1))\n        .show()\n)\n\n\n+-------+--------+----------+----------+---------------+--------------+\n|prod_id|prod_cat|prod_brand|prod_value|avg_brand_value|avg_prod_value|\n+-------+--------+----------+----------+---------------+--------------+\n|      4|keyboard|  logitech|     59.99|          44.99|          60.0|\n|      3|keyboard| microsoft|     59.99|          53.32|          60.0|\n|      5|   mouse|  logitech|     29.99|          44.99|          43.3|\n|      1|   mouse| microsoft|     39.99|          53.32|          43.3|\n|      2|   mouse| microsoft|     59.99|          53.32|          43.3|\n+-------+--------+----------+----------+---------------+--------------+\n\n\n\nNow we can compare the physical plans associated with the two jobs.\n\n\nCode\n( \n    products\n        .withColumn('avg_brand_value', fn.round(avg, 2))\n        .withColumn('avg_prod_value', fn.round(avg2, 1))\n        .explain()\n)\n\n\n== Physical Plan ==\nAdaptiveSparkPlan isFinalPlan=false\n+- Project [prod_id#854, prod_cat#855, prod_brand#856, prod_value#857, avg_brand_value#1195, round(_we0#1203, 1) AS avg_prod_value#1202]\n   +- Window [avg(prod_value#857) windowspecdefinition(prod_cat#855, specifiedwindowframe(RowFrame, unboundedpreceding$(), unboundedfollowing$())) AS _we0#1203], [prod_cat#855]\n      +- Sort [prod_cat#855 ASC NULLS FIRST], false, 0\n         +- Exchange hashpartitioning(prod_cat#855, 200), ENSURE_REQUIREMENTS, [plan_id=2249]\n            +- Project [prod_id#854, prod_cat#855, prod_brand#856, prod_value#857, round(_we0#1196, 2) AS avg_brand_value#1195]\n               +- Window [avg(prod_value#857) windowspecdefinition(prod_brand#856, specifiedwindowframe(RowFrame, unboundedpreceding$(), unboundedfollowing$())) AS _we0#1196], [prod_brand#856]\n                  +- Sort [prod_brand#856 ASC NULLS FIRST], false, 0\n                     +- Exchange hashpartitioning(prod_brand#856, 200), ENSURE_REQUIREMENTS, [plan_id=2244]\n                        +- Scan ExistingRDD[prod_id#854,prod_cat#855,prod_brand#856,prod_value#857]\n\n\n\n\n\n\nCode\nspark.sql(query).explain()\n\n\n== Physical Plan ==\nAdaptiveSparkPlan isFinalPlan=false\n+- Project [prod_id#854, prod_cat#855, prod_brand#856, prod_value#857, round(_we0#1214, 2) AS avg_brand_value#1210, round(_we1#1215, 1) AS avg_prod_value#1211]\n   +- Window [avg(prod_value#857) windowspecdefinition(prod_cat#855, specifiedwindowframe(RowFrame, unboundedpreceding$(), unboundedfollowing$())) AS _we1#1215], [prod_cat#855]\n      +- Sort [prod_cat#855 ASC NULLS FIRST], false, 0\n         +- Exchange hashpartitioning(prod_cat#855, 200), ENSURE_REQUIREMENTS, [plan_id=2273]\n            +- Window [avg(prod_value#857) windowspecdefinition(prod_brand#856, specifiedwindowframe(RowFrame, unboundedpreceding$(), unboundedfollowing$())) AS _we0#1214], [prod_brand#856]\n               +- Sort [prod_brand#856 ASC NULLS FIRST], false, 0\n                  +- Exchange hashpartitioning(prod_brand#856, 200), ENSURE_REQUIREMENTS, [plan_id=2269]\n                     +- Scan ExistingRDD[prod_id#854,prod_cat#855,prod_brand#856,prod_value#857]"
  },
  {
    "objectID": "core/notebooks/notebook06_sparksql.html#lag-and-lead",
    "href": "core/notebooks/notebook06_sparksql.html#lag-and-lead",
    "title": "DataFrame",
    "section": "Lag and Lead",
    "text": "Lag and Lead\n\n\nCode\npurchases = spark.createDataFrame(\n    [\n        (date(2017, 11, 1), 'mouse'),\n        (date(2017, 11, 2), 'mouse'),\n        (date(2017, 11, 4), 'keyboard'),\n        (date(2017, 11, 6), 'keyboard'),\n        (date(2017, 11, 9), 'keyboard'),\n        (date(2017, 11, 12), 'mouse'),\n        (date(2017, 11, 18), 'keyboard')\n    ], \n    ['date', 'prod_cat']\n)\n\npurchases.show()\n\nwindow = Window.partitionBy('prod_cat').orderBy('date')\n\nprev_purch = fn.lag('date', 1).over(window)\nnext_purch = fn.lead('date', 1).over(window)\n\npurchases\\\n    .withColumn('prev', prev_purch)\\\n    .withColumn('next', next_purch)\\\n    .orderBy('prod_cat', 'date')\\\n    .show()\n\n\n+----------+--------+\n|      date|prod_cat|\n+----------+--------+\n|2017-11-01|   mouse|\n|2017-11-02|   mouse|\n|2017-11-04|keyboard|\n|2017-11-06|keyboard|\n|2017-11-09|keyboard|\n|2017-11-12|   mouse|\n|2017-11-18|keyboard|\n+----------+--------+\n\n+----------+--------+----------+----------+\n|      date|prod_cat|      prev|      next|\n+----------+--------+----------+----------+\n|2017-11-04|keyboard|      NULL|2017-11-06|\n|2017-11-06|keyboard|2017-11-04|2017-11-09|\n|2017-11-09|keyboard|2017-11-06|2017-11-18|\n|2017-11-18|keyboard|2017-11-09|      NULL|\n|2017-11-01|   mouse|      NULL|2017-11-02|\n|2017-11-02|   mouse|2017-11-01|2017-11-12|\n|2017-11-12|   mouse|2017-11-02|      NULL|\n+----------+--------+----------+----------+"
  },
  {
    "objectID": "core/notebooks/notebook06_sparksql.html#rank-denserank-and-rownumber",
    "href": "core/notebooks/notebook06_sparksql.html#rank-denserank-and-rownumber",
    "title": "DataFrame",
    "section": "Rank, DenseRank and RowNumber",
    "text": "Rank, DenseRank and RowNumber\n\n\nCode\ncontestants = spark.createDataFrame(\n    [   \n        ('veterans', 'John', 3000),\n        ('veterans', 'Bob', 3200),\n        ('veterans', 'Mary', 4000),\n        ('young', 'Jane', 4000),\n        ('young', 'April', 3100),\n        ('young', 'Alice', 3700),\n        ('young', 'Micheal', 4000),\n    ], \n    ['category', 'name', 'points']\n)\n\ncontestants.show()\n\n\n+--------+-------+------+\n|category|   name|points|\n+--------+-------+------+\n|veterans|   John|  3000|\n|veterans|    Bob|  3200|\n|veterans|   Mary|  4000|\n|   young|   Jane|  4000|\n|   young|  April|  3100|\n|   young|  Alice|  3700|\n|   young|Micheal|  4000|\n+--------+-------+------+\n\n\n\n\n\nCode\nwindow = (\n    Window\n        .partitionBy('category')\n        .orderBy(contestants.points.desc())\n)\n\nrank = fn.rank().over(window)\ndense_rank = fn.dense_rank().over(window)\nrow_number = fn.row_number().over(window)\n\ncontestants\\\n    .withColumn('rank', rank)\\\n    .withColumn('dense_rank', dense_rank)\\\n    .withColumn('row_number', row_number)\\\n    .orderBy('category', fn.col('points').desc())\\\n    .show()\n\n\n+--------+-------+------+----+----------+----------+\n|category|   name|points|rank|dense_rank|row_number|\n+--------+-------+------+----+----------+----------+\n|veterans|   Mary|  4000|   1|         1|         1|\n|veterans|    Bob|  3200|   2|         2|         2|\n|veterans|   John|  3000|   3|         3|         3|\n|   young|   Jane|  4000|   1|         1|         1|\n|   young|Micheal|  4000|   1|         1|         2|\n|   young|  Alice|  3700|   3|         2|         3|\n|   young|  April|  3100|   4|         3|         4|\n+--------+-------+------+----+----------+----------+"
  },
  {
    "objectID": "core/notebooks/notebook04_pandas_spark.html",
    "href": "core/notebooks/notebook04_pandas_spark.html",
    "title": "Data preprocessing and visualisation of a credit scoring dataset",
    "section": "",
    "text": "We’ll work on a dataset gro.csv for credit scoring that was proposed some years ago as a data challenge on some data challenge website. It is a realistic and somewhat messy dataset that contains a lot of missing values, several types of features (dates, categories, continuous features), so that serious data cleaning and formating is required. This dataset contains the following columns:"
  },
  {
    "objectID": "core/notebooks/notebook04_pandas_spark.html#lets-assess-what-we-did",
    "href": "core/notebooks/notebook04_pandas_spark.html#lets-assess-what-we-did",
    "title": "Data preprocessing and visualisation of a credit scoring dataset",
    "section": "Let’s assess what we did",
    "text": "Let’s assess what we did\nIt appears that we have to work a little bit more for a correct import of the data. Here is a list of the problems we face. - The last three columns are empty - Dates are actually str (python’s string type) - There is a lot of missing values - Categorial features are str - The Net_Annual_Income is imported as a string\nBy looking at the column names, the descriptions of the columns and using some basic, we infer the type of features that we have. There are dates features, continuous features, categorical features, and some features that could be either treated as categorical or continuous.\n\nThere are many missing values, that need to be handled.\nThe annual net income is imported as a string, we need to understand why.\nWe really need to treat dates as dates and not strings (because we want to compute the age of a client based on its birth year for instance).\n\nHere is a tentative structure of the features\nContinuous features\n\nYears_At_Residence\nNet_Annual_Income\nYears_At_Business\n\nFeatures to be decided\n\nNumber_Of_Dependant\nNb_Of_Products\n\nCategorical features\n\nCustomer_Type\nP_Client\nEducational_Level\nMarital_Status\nProd_Sub_Category\nSource\nType_Of_Residence\nProd_Category\n\nDate features\n\nBirthDate\nCustomer_Open_Date\nProd_Decision_Date\nProd_Closed_Date"
  },
  {
    "objectID": "core/notebooks/notebook04_pandas_spark.html#the-last-three-columns-are-weird-and-empty",
    "href": "core/notebooks/notebook04_pandas_spark.html#the-last-three-columns-are-weird-and-empty",
    "title": "Data preprocessing and visualisation of a credit scoring dataset",
    "section": "The last three columns are weird and empty",
    "text": "The last three columns are weird and empty\nIt seems to come from the fact that the data always ends with several ';' characters. We can remove them simply using the usecols option from read_csv."
  },
  {
    "objectID": "core/notebooks/notebook04_pandas_spark.html#dates-are-actually-str",
    "href": "core/notebooks/notebook04_pandas_spark.html#dates-are-actually-str",
    "title": "Data preprocessing and visualisation of a credit scoring dataset",
    "section": "Dates are actually str",
    "text": "Dates are actually str\nWe need to specify which columns must be encoded as dates using the parse_dates option from read_csv. Fortunately enough, pandas is clever enough to interpret the date format.\n\n\nCode\ntype(psdf.loc[0, 'BirthDate'])\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[15], line 1\n----&gt; 1 type(psdf.loc[0, 'BirthDate'])\n\nNameError: name 'psdf' is not defined"
  },
  {
    "objectID": "core/notebooks/notebook04_pandas_spark.html#there-is-a-lot-of-missing-values",
    "href": "core/notebooks/notebook04_pandas_spark.html#there-is-a-lot-of-missing-values",
    "title": "Data preprocessing and visualisation of a credit scoring dataset",
    "section": "There is a lot of missing values",
    "text": "There is a lot of missing values\nWe’ll see below that actually a single column mostly contain missing values.\n\n\nCode\npsdf.isnull().sum()\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[16], line 1\n----&gt; 1 psdf.isnull().sum()\n\nNameError: name 'psdf' is not defined\n\n\n\nThe column Prod_Closed_Date contains mostly missing values !\n\n\nCode\npsdf[['Prod_Closed_Date']].head(5)\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[17], line 1\n----&gt; 1 psdf[['Prod_Closed_Date']].head(5)\n\nNameError: name 'psdf' is not defined\n\n\n\nLet’s remove the useless columns and check the remaining missing values\nAgain there are variations. Keyword inplace is not legal in Pandas API on Spark\n\n\nCode\n# df.drop(['Prod_Closed_Date', 'Unnamed: 19', \n#          'Unnamed: 20', 'Unnamed: 21'], axis=\"columns\", inplace=True)\n\npsdf = psdf.drop(['Prod_Closed_Date', \n        'Unnamed: 19', \n        'Unnamed: 20', \n        'Unnamed: 21'], \n        axis=\"columns\")\n        \npsdf.head()         \n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[18], line 4\n      1 # df.drop(['Prod_Closed_Date', 'Unnamed: 19', \n      2 #          'Unnamed: 20', 'Unnamed: 21'], axis=\"columns\", inplace=True)\n----&gt; 4 psdf = psdf.drop(['Prod_Closed_Date', \n      5         'Unnamed: 19', \n      6         'Unnamed: 20', \n      7         'Unnamed: 21'], \n      8         axis=\"columns\")\n     10 psdf.head()         \n\nNameError: name 'psdf' is not defined\n\n\n\nLet’s display the rows with missing values and let’s highlight them\n\n\nCode\n# psdf[psdf.isnull().any(axis=\"columns\")].style.highlight_null()"
  },
  {
    "objectID": "core/notebooks/notebook04_pandas_spark.html#categorial-features-are-str",
    "href": "core/notebooks/notebook04_pandas_spark.html#categorial-features-are-str",
    "title": "Data preprocessing and visualisation of a credit scoring dataset",
    "section": "Categorial features are str",
    "text": "Categorial features are str\nWe need to say the dtype we want to use for some columns using the dtype option of read_csv.\n\n\nCode\ntype(psdf.loc[0, 'Prod_Sub_Category'])\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[20], line 1\n----&gt; 1 type(psdf.loc[0, 'Prod_Sub_Category'])\n\nNameError: name 'psdf' is not defined\n\n\n\n\n\nCode\npsdf['Prod_Sub_Category'].unique()\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[21], line 1\n----&gt; 1 psdf['Prod_Sub_Category'].unique()\n\nNameError: name 'psdf' is not defined"
  },
  {
    "objectID": "core/notebooks/notebook04_pandas_spark.html#the-annual-net-income-is-imported-as-a-string",
    "href": "core/notebooks/notebook04_pandas_spark.html#the-annual-net-income-is-imported-as-a-string",
    "title": "Data preprocessing and visualisation of a credit scoring dataset",
    "section": "The annual net income is imported as a string",
    "text": "The annual net income is imported as a string\nThis problem comes from the fact that the decimal separator is in European notation: it’s a ',' and not a '.', so we need to specify it using the decimal option to read_csv. (Data is French, pardon my French…)\n\n\nCode\ntype(psdf.loc[0, 'Net_Annual_Income'])\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[22], line 1\n----&gt; 1 type(psdf.loc[0, 'Net_Annual_Income'])\n\nNameError: name 'psdf' is not defined\n\n\n\n\n\nCode\npsdf['Net_Annual_Income'].head(n=10)\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[23], line 1\n----&gt; 1 psdf['Net_Annual_Income'].head(n=10)\n\nNameError: name 'psdf' is not defined"
  },
  {
    "objectID": "core/notebooks/notebook04_pandas_spark.html#comment-on-file-formats",
    "href": "core/notebooks/notebook04_pandas_spark.html#comment-on-file-formats",
    "title": "Data preprocessing and visualisation of a credit scoring dataset",
    "section": "Comment on file formats",
    "text": "Comment on file formats\nYou can use other methods starting with .to_XX to save in another format. Here are some main examples\n\nOK to use csv for “small” datasets (several MB)\nUse pickle for more compressed and faster format (limited to 4GB). It’s the standard binary serialization format of Python\nfeather is another fast and lightweight file format for storing data frames. A very popular exchange format.\nparquet is a format for big distributed data (works nicely with Spark)\n\namong several others…\n\n\nCode\n#df.to_pickle(\"gro_cleaned.pkl\")\npssdf.to_parquet(\"gro_cleaned.parquet\")\n# pssdf.reset_index().to_feather(\"gro_cleaned.feather\")\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[37], line 2\n      1 #df.to_pickle(\"gro_cleaned.pkl\")\n----&gt; 2 pssdf.to_parquet(\"gro_cleaned.parquet\")\n      3 # pssdf.reset_index().to_feather(\"gro_cleaned.feather\")\n\nNameError: name 'pssdf' is not defined\n\n\n\n\n\nCode\npssdf.index\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[38], line 1\n----&gt; 1 pssdf.index\n\nNameError: name 'pssdf' is not defined\n\n\n\nAnd you can read again using the corresponding read_XX function\n\n\nCode\npssdf = ps.read_parquet(\"gro_cleaned.parquet\")\npssdf.head()\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[39], line 1\n----&gt; 1 pssdf = ps.read_parquet(\"gro_cleaned.parquet\")\n      2 pssdf.head()\n\nNameError: name 'ps' is not defined\n\n\n\n\n\nCode\n!ls -alh gro_cleaned*\n\n\nls: cannot access 'gro_cleaned*': No such file or directory"
  },
  {
    "objectID": "core/notebooks/notebook04_pandas_spark.html#the-net-income-columns-is-very-weird",
    "href": "core/notebooks/notebook04_pandas_spark.html#the-net-income-columns-is-very-weird",
    "title": "Data preprocessing and visualisation of a credit scoring dataset",
    "section": "The net income columns is very weird",
    "text": "The net income columns is very weird\n\n\nCode\nincome = pssdf['Net_Annual_Income']\nincome.describe()\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[41], line 1\n----&gt; 1 income = pssdf['Net_Annual_Income']\n      2 income.describe()\n\nNameError: name 'pssdf' is not defined\n\n\n\n\n\nCode\n(income &lt;= 100).sum(), (income &gt; 100).sum()\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[42], line 1\n----&gt; 1 (income &lt;= 100).sum(), (income &gt; 100).sum()\n\nNameError: name 'income' is not defined\n\n\n\nMost values are smaller than 100, while some are much much larger…\n\n\nCode\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n%matplotlib inline\n\nsns.set_context(\"notebook\", font_scale=1.2)\n\n\n\n\nCode\nsns.displot(x='Net_Annual_Income', \n            data=pssdf, \n            bins=20,\n            height=4, \n            aspect=1.5)\n\n\n\n\nCode\npssdf[\"Net_Annual_Income\"].plot.hist(bins=40, \nhitsnorm='density', log_x=True)\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[43], line 1\n----&gt; 1 pssdf[\"Net_Annual_Income\"].plot.hist(bins=40, \n      2 hitsnorm='density', log_x=True)\n\nNameError: name 'pssdf' is not defined\n\n\n\n\n\nCode\npssdf[\"Net_Annual_Income\"].plot.kde(bw_method=10, log_x=True)\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[44], line 1\n----&gt; 1 pssdf[\"Net_Annual_Income\"].plot.kde(bw_method=10, log_x=True)\n\nNameError: name 'pssdf' is not defined\n\n\n\nThis is annoying, we don’t really see much…\n\n\nCode\nsns.displot(x='Net_Annual_Income', \n            data=pssdf, \n            bins=20, \n            height=4, \n            aspect=1.5, \n            log_scale=(False, True))\n\n\nDistribution for less than 100K revenue\n\n\nCode\nsns.displot(x='Net_Annual_Income', \n            data=pssdf[pssdf['Net_Annual_Income'] &lt; 100], \n            bins=15, \n            height=4, \n            aspect=1.5)\n\n\nDistribution for less than 400K revenue\n\n\nCode\nsns.displot(x='Net_Annual_Income', data=pssdf[pssdf['Net_Annual_Income'] &lt; 400], \n            bins=15, height=4, aspect=1.5)\n\n\n\n\nCode\n(pssdf['Net_Annual_Income'] == 36.0).sum()\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[45], line 1\n----&gt; 1 (pssdf['Net_Annual_Income'] == 36.0).sum()\n\nNameError: name 'pssdf' is not defined\n\n\n\n\n\nCode\nincome_counts = (\n    ps.DataFrame({\n        \"income_category\": pssdf['Net_Annual_Income'].astype(\"category\"),\n        \"income\": pssdf['Net_Annual_Income']\n    })\n    .groupby(\"income_category\")\n    .count()\n    .reset_index()\n    .rename(columns={\"income\": \"#customers\"})\n    .sort_values(by=\"#customers\", axis=\"index\", ascending=False)\n)\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[46], line 2\n      1 income_counts = (\n----&gt; 2     ps.DataFrame({\n      3         \"income_category\": pssdf['Net_Annual_Income'].astype(\"category\"),\n      4         \"income\": pssdf['Net_Annual_Income']\n      5     })\n      6     .groupby(\"income_category\")\n      7     .count()\n      8     .reset_index()\n      9     .rename(columns={\"income\": \"#customers\"})\n     10     .sort_values(by=\"#customers\", axis=\"index\", ascending=False)\n     11 )\n\nNameError: name 'ps' is not defined\n\n\n\n\n\nCode\nincome_counts[\"%cummulative clients\"] \\\n    = income_counts[\"#customers\"].cumsum() / income_counts[\"#customers\"].sum()\n\nincome_counts.iloc[:20].style.bar(subset=[\"%cummulative clients\"], vmin=0, vmax=1)\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[47], line 2\n      1 income_counts[\"%cummulative clients\"] \\\n----&gt; 2     = income_counts[\"#customers\"].cumsum() / income_counts[\"#customers\"].sum()\n      4 income_counts.iloc[:20].style.bar(subset=[\"%cummulative clients\"], vmin=0, vmax=1)\n\nNameError: name 'income_counts' is not defined\n\n\n\n\nWe have some overrepresented values (many possible explanations for this)\nTo clean the data, we can, for instance, keep only the revenues between [10, 200], or leave it as such\n\n\n\nCode\ndf = df[(df['Net_Annual_Income'] &gt;= 10) & (df['Net_Annual_Income'] &lt;= 200)]\n\nsns.displot(x='Net_Annual_Income', data=df, bins=15, height=4, aspect=1.5)"
  },
  {
    "objectID": "core/notebooks/notebook04_pandas_spark.html#creation-of-the-features-matrix",
    "href": "core/notebooks/notebook04_pandas_spark.html#creation-of-the-features-matrix",
    "title": "Data preprocessing and visualisation of a credit scoring dataset",
    "section": "Creation of the features matrix",
    "text": "Creation of the features matrix\n\n\nCode\ndf[cnt_featnames].head()\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[54], line 1\n----&gt; 1 df[cnt_featnames].head()\n\nNameError: name 'df' is not defined\n\n\n\n\n\nCode\nbin_features = pd.get_dummies(df[cat_featnames],\n                              prefix_sep='#', drop_first=True)\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[55], line 1\n----&gt; 1 bin_features = pd.get_dummies(df[cat_featnames],\n      2                               prefix_sep='#', drop_first=True)\n\nNameError: name 'df' is not defined\n\n\n\n\n\nCode\nbin_features.head()\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[56], line 1\n----&gt; 1 bin_features.head()\n\nNameError: name 'bin_features' is not defined\n\n\n\n\n\nCode\ncnt_features = df[cnt_featnames]\ncnt_features.head()\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[57], line 1\n----&gt; 1 cnt_features = df[cnt_featnames]\n      2 cnt_features.head()\n\nNameError: name 'df' is not defined\n\n\n\n\n\nCode\nfrom pandas import Timestamp\n\ndef age(x):\n    today = Timestamp.today()\n    return (today - x).dt.days\n\ndate_features = df[date_featnames].apply(age, axis=\"index\")\ndate_features.head()\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[58], line 7\n      4     today = Timestamp.today()\n      5     return (today - x).dt.days\n----&gt; 7 date_features = df[date_featnames].apply(age, axis=\"index\")\n      8 date_features.head()\n\nNameError: name 'df' is not defined\n\n\n\n\n\nCode\ntoday = Timestamp.today()\ntoday\n\n\nTimestamp('2025-03-10 16:54:41.022760')\n\n\n\n\nCode\ntt = (today - df[\"BirthDate\"]).loc[0]\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[60], line 1\n----&gt; 1 tt = (today - df[\"BirthDate\"]).loc[0]\n\nNameError: name 'df' is not defined\n\n\n\n\n\nCode\n(today - df[\"BirthDate\"]).dt.days\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[61], line 1\n----&gt; 1 (today - df[\"BirthDate\"]).dt.days\n\nNameError: name 'df' is not defined\n\n\n\n\n\nCode\ntt\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[62], line 1\n----&gt; 1 tt\n\nNameError: name 'tt' is not defined"
  },
  {
    "objectID": "core/notebooks/notebook04_pandas_spark.html#final-features-matrix",
    "href": "core/notebooks/notebook04_pandas_spark.html#final-features-matrix",
    "title": "Data preprocessing and visualisation of a credit scoring dataset",
    "section": "Final features matrix",
    "text": "Final features matrix\n\n\nCode\nall_features = pd.concat([bin_features, cnt_features, date_features], axis=1)\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[63], line 1\n----&gt; 1 all_features = pd.concat([bin_features, cnt_features, date_features], axis=1)\n\nNameError: name 'bin_features' is not defined\n\n\n\n\n\nCode\nall_features.columns\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[64], line 1\n----&gt; 1 all_features.columns\n\nNameError: name 'all_features' is not defined\n\n\n\n\n\nCode\nall_features.head()\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[65], line 1\n----&gt; 1 all_features.head()\n\nNameError: name 'all_features' is not defined\n\n\n\n\n\nCode\ndf_debile = pd.DataFrame({\"nom etudiant\": [\"yiyang\", \"jaouad\", \"mokhtar\", \"massil\", \"simon\"], \n              \"portable\": [True, True, None, True, False]})\n\n\n\n\nCode\ndf_debile\n\n\n\n\n\n\n\n\n\nnom etudiant\nportable\n\n\n\n\n0\nyiyang\nTrue\n\n\n1\njaouad\nTrue\n\n\n2\nmokhtar\nNone\n\n\n3\nmassil\nTrue\n\n\n4\nsimon\nFalse\n\n\n\n\n\n\n\n\n\nCode\ndf_debile.index\n\n\nRangeIndex(start=0, stop=5, step=1)\n\n\n\n\nCode\ndf_debile.dropna().index\n\n\nIndex([0, 1, 3, 4], dtype='int64')\n\n\n\n\nCode\ndf_debile.info()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 5 entries, 0 to 4\nData columns (total 2 columns):\n #   Column        Non-Null Count  Dtype \n---  ------        --------------  ----- \n 0   nom etudiant  5 non-null      object\n 1   portable      4 non-null      object\ndtypes: object(2)\nmemory usage: 212.0+ bytes\n\n\nVERY IMPORTANT: we removed lines of data that contained missing values. The index of the dataframe is therefore not contiguous anymore\n\n\nCode\nall_features.index.max()\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[71], line 1\n----&gt; 1 all_features.index.max()\n\nNameError: name 'all_features' is not defined\n\n\n\nThis could be a problem for later. So let’s reset the index to get a contiguous one\n\n\nCode\nall_features.shape\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[72], line 1\n----&gt; 1 all_features.shape\n\nNameError: name 'all_features' is not defined\n\n\n\n\n\nCode\nall_features.reset_index(inplace=True, drop=True)\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[73], line 1\n----&gt; 1 all_features.reset_index(inplace=True, drop=True)\n\nNameError: name 'all_features' is not defined\n\n\n\n\n\nCode\nall_features.head()\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[74], line 1\n----&gt; 1 all_features.head()\n\nNameError: name 'all_features' is not defined"
  },
  {
    "objectID": "core/notebooks/notebook04_pandas_spark.html#lets-save-the-data-using-pickle",
    "href": "core/notebooks/notebook04_pandas_spark.html#lets-save-the-data-using-pickle",
    "title": "Data preprocessing and visualisation of a credit scoring dataset",
    "section": "Let’s save the data using pickle",
    "text": "Let’s save the data using pickle\n\n\nCode\nimport pickle as pkl\n\nX = all_features\ny = df['Y']\n\n# Let's put eveything in a dictionary\ndf_pkl = {}\n# The features and the labels\ndf_pkl['features'] = X\ndf_pkl['labels'] = y\n# And also the list of columns we built above\ndf_pkl['cnt_featnames'] = cnt_featnames\ndf_pkl['cat_featnames'] = cat_featnames\ndf_pkl['date_featnames'] = date_featnames\n\nwith open(\"gro_training.pkl\", 'wb') as f:\n    pkl.dump(df_pkl, f)\n\n\n\n\nCode\nls -al gro*\n\n\n-rw-rw-r-- 1 boucheron boucheron 9115 janv. 14 22:37 gro.csv.gz\n\n\nThe preprocessed data is saved in a pickle file called gro_training.pkdfl.\nDatabricks blog about Koalas, SPIP, Zen\n\npandas users will be able scale their workloads with one simple line change in the upcoming Spark 3.2 release:\n\n&lt;s&gt;from pandas import read_csv&lt;/s&gt;\nfrom pyspark.pandas import read_csv\npdf = read_csv(\"data.csv\")"
  },
  {
    "objectID": "core/notebooks/notebook02_numpy.html",
    "href": "core/notebooks/notebook02_numpy.html",
    "title": "Introduction to numpy",
    "section": "",
    "text": "NumPy is the fundamental package for scientific computing with Python. It contains among other things:\nBesides its obvious scientific uses, NumPy can also be used as an efficient multi-dimensional container for general data. Arbitrary data-types can be defined. This allows NumPy to seamlessly and speedily integrate with a wide variety of databases.\nLibrary documentation: http://numpy.org/"
  },
  {
    "objectID": "core/notebooks/notebook02_numpy.html#the-base-numpy.array-object",
    "href": "core/notebooks/notebook02_numpy.html#the-base-numpy.array-object",
    "title": "Introduction to numpy",
    "section": "The base numpy.array object",
    "text": "The base numpy.array object\n\n\nCode\nimport numpy as np\n\n# declare a vector using a list as the argument\nv = np.array([1, 2.0, 3, 4])\nv\n\n\narray([1., 2., 3., 4.])\n\n\n\n\nCode\nlist([1, 2.0, 3, 4])\n\n\n[1, 2.0, 3, 4]\n\n\n\n\nCode\ntype(v)\n\n\nnumpy.ndarray\n\n\n\n\nCode\nv.shape\n\n\n(4,)\n\n\n\n\nCode\nv.ndim\n\n\n1\n\n\n\n\nCode\nv.dtype is float\n\n\nFalse\n\n\n\n\nCode\nv.dtype \n\n\ndtype('float64')\n\n\n\n\nCode\nnp.uint8 is int\n\n\nFalse\n\n\n\n\n\n\n\n\nTip\n\n\n\n\n\nUse copilot explain to understand the chunks:\n\nThe np.uint8 is a data type in NumPy, representing an unsigned 8-bit integer, which can store values from 0 to 255. The int type is the built-in integer type in Python, which can represent any integer value without a fixed size limit.\n\n\n\n\n\n\nCode\nnp.array([2**120, 2**40], dtype=np.int64)\n\n\n\n---------------------------------------------------------------------------\nOverflowError                             Traceback (most recent call last)\nCell In[9], line 1\n----&gt; 1 np.array([2**120, 2**40], dtype=np.int64)\n\nOverflowError: Python int too large to convert to C long\n\n\n\n\n\nCode\nnp.uint16 is int \n\n\nFalse\n\n\n\n\nCode\nnp.uint32  is int\n\n\nFalse\n\n\n\n\nCode\nw = np.array([1.3, 2, 3, 4], dtype=np.int64)\nw\n\n\narray([1, 2, 3, 4])\n\n\n\n\nCode\nw.dtype\n\n\ndtype('int64')\n\n\n\n\nCode\na = np.arange(100)\n\n\n\n\nCode\ntype(a)\n\n\nnumpy.ndarray\n\n\n\n\nCode\nnp.array(range(100))\n\n\narray([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n       34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50,\n       51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67,\n       68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84,\n       85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99])\n\n\n\n\nCode\na\n\n\narray([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n       34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50,\n       51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67,\n       68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84,\n       85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99])\n\n\n\n\nCode\na.dtype\n\n\ndtype('int64')\n\n\n\n\nCode\n-3 * a ** 2\n\n\narray([     0,     -3,    -12,    -27,    -48,    -75,   -108,   -147,\n         -192,   -243,   -300,   -363,   -432,   -507,   -588,   -675,\n         -768,   -867,   -972,  -1083,  -1200,  -1323,  -1452,  -1587,\n        -1728,  -1875,  -2028,  -2187,  -2352,  -2523,  -2700,  -2883,\n        -3072,  -3267,  -3468,  -3675,  -3888,  -4107,  -4332,  -4563,\n        -4800,  -5043,  -5292,  -5547,  -5808,  -6075,  -6348,  -6627,\n        -6912,  -7203,  -7500,  -7803,  -8112,  -8427,  -8748,  -9075,\n        -9408,  -9747, -10092, -10443, -10800, -11163, -11532, -11907,\n       -12288, -12675, -13068, -13467, -13872, -14283, -14700, -15123,\n       -15552, -15987, -16428, -16875, -17328, -17787, -18252, -18723,\n       -19200, -19683, -20172, -20667, -21168, -21675, -22188, -22707,\n       -23232, -23763, -24300, -24843, -25392, -25947, -26508, -27075,\n       -27648, -28227, -28812, -29403])\n\n\n\n\nCode\na[42] = 13\n\n\n\n\nCode\na[42] = 1025\n\n\n\n\nCode\nnp.info(np.int16)\n\n\n int16()\n\nSigned integer type, compatible with C ``short``.\n\n:Character code: ``'h'``\n:Canonical name: `numpy.short`\n:Alias on this platform (Linux x86_64): `numpy.int16`: 16-bit signed integer (``-32_768`` to ``32_767``).\n\n\nMethods:\n\n  all  --  Scalar method identical to the corresponding array attribute.\n  any  --  Scalar method identical to the corresponding array attribute.\n  argmax  --  Scalar method identical to the corresponding array attribute.\n  argmin  --  Scalar method identical to the corresponding array attribute.\n  argsort  --  Scalar method identical to the corresponding array attribute.\n  astype  --  Scalar method identical to the corresponding array attribute.\n  bit_count  --  int16.bit_count() -&gt; int\n  byteswap  --  Scalar method identical to the corresponding array attribute.\n  choose  --  Scalar method identical to the corresponding array attribute.\n  clip  --  Scalar method identical to the corresponding array attribute.\n  compress  --  Scalar method identical to the corresponding array attribute.\n  conj  --  None\n  conjugate  --  Scalar method identical to the corresponding array attribute.\n  copy  --  Scalar method identical to the corresponding array attribute.\n  cumprod  --  Scalar method identical to the corresponding array attribute.\n  cumsum  --  Scalar method identical to the corresponding array attribute.\n  diagonal  --  Scalar method identical to the corresponding array attribute.\n  dump  --  Scalar method identical to the corresponding array attribute.\n  dumps  --  Scalar method identical to the corresponding array attribute.\n  fill  --  Scalar method identical to the corresponding array attribute.\n  flatten  --  Scalar method identical to the corresponding array attribute.\n  getfield  --  Scalar method identical to the corresponding array attribute.\n  is_integer  --  integer.is_integer() -&gt; bool\n  item  --  Scalar method identical to the corresponding array attribute.\n  max  --  Scalar method identical to the corresponding array attribute.\n  mean  --  Scalar method identical to the corresponding array attribute.\n  min  --  Scalar method identical to the corresponding array attribute.\n  nonzero  --  Scalar method identical to the corresponding array attribute.\n  prod  --  Scalar method identical to the corresponding array attribute.\n  put  --  Scalar method identical to the corresponding array attribute.\n  ravel  --  Scalar method identical to the corresponding array attribute.\n  repeat  --  Scalar method identical to the corresponding array attribute.\n  reshape  --  Scalar method identical to the corresponding array attribute.\n  resize  --  Scalar method identical to the corresponding array attribute.\n  round  --  Scalar method identical to the corresponding array attribute.\n  searchsorted  --  Scalar method identical to the corresponding array attribute.\n  setfield  --  Scalar method identical to the corresponding array attribute.\n  setflags  --  Scalar method identical to the corresponding array attribute.\n  sort  --  Scalar method identical to the corresponding array attribute.\n  squeeze  --  Scalar method identical to the corresponding array attribute.\n  std  --  Scalar method identical to the corresponding array attribute.\n  sum  --  Scalar method identical to the corresponding array attribute.\n  swapaxes  --  Scalar method identical to the corresponding array attribute.\n  take  --  Scalar method identical to the corresponding array attribute.\n  to_device  --  None\n  tobytes  --  None\n  tofile  --  Scalar method identical to the corresponding array attribute.\n  tolist  --  Scalar method identical to the corresponding array attribute.\n  tostring  --  Scalar method identical to the corresponding array attribute.\n  trace  --  Scalar method identical to the corresponding array attribute.\n  transpose  --  Scalar method identical to the corresponding array attribute.\n  var  --  Scalar method identical to the corresponding array attribute.\n  view  --  Scalar method identical to the corresponding array attribute.\n\n\n\n\nCode\nnp.int16\n\n\nnumpy.int16\n\n\n\n\nCode\ndict(enumerate(a))\n\n\n{0: np.int64(0),\n 1: np.int64(1),\n 2: np.int64(2),\n 3: np.int64(3),\n 4: np.int64(4),\n 5: np.int64(5),\n 6: np.int64(6),\n 7: np.int64(7),\n 8: np.int64(8),\n 9: np.int64(9),\n 10: np.int64(10),\n 11: np.int64(11),\n 12: np.int64(12),\n 13: np.int64(13),\n 14: np.int64(14),\n 15: np.int64(15),\n 16: np.int64(16),\n 17: np.int64(17),\n 18: np.int64(18),\n 19: np.int64(19),\n 20: np.int64(20),\n 21: np.int64(21),\n 22: np.int64(22),\n 23: np.int64(23),\n 24: np.int64(24),\n 25: np.int64(25),\n 26: np.int64(26),\n 27: np.int64(27),\n 28: np.int64(28),\n 29: np.int64(29),\n 30: np.int64(30),\n 31: np.int64(31),\n 32: np.int64(32),\n 33: np.int64(33),\n 34: np.int64(34),\n 35: np.int64(35),\n 36: np.int64(36),\n 37: np.int64(37),\n 38: np.int64(38),\n 39: np.int64(39),\n 40: np.int64(40),\n 41: np.int64(41),\n 42: np.int64(1025),\n 43: np.int64(43),\n 44: np.int64(44),\n 45: np.int64(45),\n 46: np.int64(46),\n 47: np.int64(47),\n 48: np.int64(48),\n 49: np.int64(49),\n 50: np.int64(50),\n 51: np.int64(51),\n 52: np.int64(52),\n 53: np.int64(53),\n 54: np.int64(54),\n 55: np.int64(55),\n 56: np.int64(56),\n 57: np.int64(57),\n 58: np.int64(58),\n 59: np.int64(59),\n 60: np.int64(60),\n 61: np.int64(61),\n 62: np.int64(62),\n 63: np.int64(63),\n 64: np.int64(64),\n 65: np.int64(65),\n 66: np.int64(66),\n 67: np.int64(67),\n 68: np.int64(68),\n 69: np.int64(69),\n 70: np.int64(70),\n 71: np.int64(71),\n 72: np.int64(72),\n 73: np.int64(73),\n 74: np.int64(74),\n 75: np.int64(75),\n 76: np.int64(76),\n 77: np.int64(77),\n 78: np.int64(78),\n 79: np.int64(79),\n 80: np.int64(80),\n 81: np.int64(81),\n 82: np.int64(82),\n 83: np.int64(83),\n 84: np.int64(84),\n 85: np.int64(85),\n 86: np.int64(86),\n 87: np.int64(87),\n 88: np.int64(88),\n 89: np.int64(89),\n 90: np.int64(90),\n 91: np.int64(91),\n 92: np.int64(92),\n 93: np.int64(93),\n 94: np.int64(94),\n 95: np.int64(95),\n 96: np.int64(96),\n 97: np.int64(97),\n 98: np.int64(98),\n 99: np.int64(99)}\n\n\n\n\nCode\na + 1\n\n\narray([   1,    2,    3,    4,    5,    6,    7,    8,    9,   10,   11,\n         12,   13,   14,   15,   16,   17,   18,   19,   20,   21,   22,\n         23,   24,   25,   26,   27,   28,   29,   30,   31,   32,   33,\n         34,   35,   36,   37,   38,   39,   40,   41,   42, 1026,   44,\n         45,   46,   47,   48,   49,   50,   51,   52,   53,   54,   55,\n         56,   57,   58,   59,   60,   61,   62,   63,   64,   65,   66,\n         67,   68,   69,   70,   71,   72,   73,   74,   75,   76,   77,\n         78,   79,   80,   81,   82,   83,   84,   85,   86,   87,   88,\n         89,   90,   91,   92,   93,   94,   95,   96,   97,   98,   99,\n        100])\n\n\n\n\nCode\nb = a + 1\nb\n\n\narray([   1,    2,    3,    4,    5,    6,    7,    8,    9,   10,   11,\n         12,   13,   14,   15,   16,   17,   18,   19,   20,   21,   22,\n         23,   24,   25,   26,   27,   28,   29,   30,   31,   32,   33,\n         34,   35,   36,   37,   38,   39,   40,   41,   42, 1026,   44,\n         45,   46,   47,   48,   49,   50,   51,   52,   53,   54,   55,\n         56,   57,   58,   59,   60,   61,   62,   63,   64,   65,   66,\n         67,   68,   69,   70,   71,   72,   73,   74,   75,   76,   77,\n         78,   79,   80,   81,   82,   83,   84,   85,   86,   87,   88,\n         89,   90,   91,   92,   93,   94,   95,   96,   97,   98,   99,\n        100])\n\n\n\n\nCode\na is b\n\n\nFalse\n\n\n\n\nCode\nf = id(a)\na += 1\nf, id(a)\n\n\n(137393579285968, 137393579285968)\n\n\n\n\nCode\na\n\n\narray([   1,    2,    3,    4,    5,    6,    7,    8,    9,   10,   11,\n         12,   13,   14,   15,   16,   17,   18,   19,   20,   21,   22,\n         23,   24,   25,   26,   27,   28,   29,   30,   31,   32,   33,\n         34,   35,   36,   37,   38,   39,   40,   41,   42, 1026,   44,\n         45,   46,   47,   48,   49,   50,   51,   52,   53,   54,   55,\n         56,   57,   58,   59,   60,   61,   62,   63,   64,   65,   66,\n         67,   68,   69,   70,   71,   72,   73,   74,   75,   76,   77,\n         78,   79,   80,   81,   82,   83,   84,   85,   86,   87,   88,\n         89,   90,   91,   92,   93,   94,   95,   96,   97,   98,   99,\n        100])\n\n\n\n\nCode\nb\n\n\narray([   1,    2,    3,    4,    5,    6,    7,    8,    9,   10,   11,\n         12,   13,   14,   15,   16,   17,   18,   19,   20,   21,   22,\n         23,   24,   25,   26,   27,   28,   29,   30,   31,   32,   33,\n         34,   35,   36,   37,   38,   39,   40,   41,   42, 1026,   44,\n         45,   46,   47,   48,   49,   50,   51,   52,   53,   54,   55,\n         56,   57,   58,   59,   60,   61,   62,   63,   64,   65,   66,\n         67,   68,   69,   70,   71,   72,   73,   74,   75,   76,   77,\n         78,   79,   80,   81,   82,   83,   84,   85,   86,   87,   88,\n         89,   90,   91,   92,   93,   94,   95,   96,   97,   98,   99,\n        100])\n\n\n\n\n\n\n\n\nWarning\n\n\n\nBeware of the dimensions: a 1D array is not the same as a 2D array with 1 column\n\n\n\n\nCode\na1 = np.array([1, 2, 3])\nprint(a1, a1.shape, a1.ndim)\n\n\n[1 2 3] (3,) 1\n\n\n\n\nCode\na2 = np.array([1, 2, 3])\nprint(a2, a2.shape, a2.ndim)\n\n\n[1 2 3] (3,) 1\n\n\nMore on NumPy quickstart\n\n\n\n\n\n\nNote\n\n\n\nList the attributes and methods of class numpy.ndarray. You may use function dir() and filter the result using methods for objects of class string."
  },
  {
    "objectID": "core/notebooks/notebook02_numpy.html#matrix-multiplication",
    "href": "core/notebooks/notebook02_numpy.html#matrix-multiplication",
    "title": "Introduction to numpy",
    "section": "Matrix multiplication",
    "text": "Matrix multiplication\n\n\nCode\na2.dot(a1) # inner product \n\n\nnp.int64(14)\n\n\n\n\nCode\n( \n    np.array([a2])\n        .transpose() # column vector\n        .dot(np.array([a1]))\n) # column vector multiplied by row vector\n\n\narray([[1, 2, 3],\n       [2, 4, 6],\n       [3, 6, 9]])\n\n\n\n\nCode\n(\n    np.array([a2])\n    .transpose()#.shape\n)\n\n\narray([[1],\n       [2],\n       [3]])\n\n\n\n\nCode\n(\n    a2.reshape(3,1)  # all explicit\n      .dot(a1.reshape(1, 3))\n)\n\n\narray([[1, 2, 3],\n       [2, 4, 6],\n       [3, 6, 9]])\n\n\n\n\nCode\n# Declare a 2D array using a nested list as the constructor argument\nM = np.array([[1,2], \n              [3,4], \n              [3.14, -9.17]])\nM\n\n\narray([[ 1.  ,  2.  ],\n       [ 3.  ,  4.  ],\n       [ 3.14, -9.17]])\n\n\n\n\nCode\nM.shape, M.size\n\n\n((3, 2), 6)\n\n\n\n\nCode\nM.ravel(), M.ndim, M.ravel().shape\n\n\n(array([ 1.  ,  2.  ,  3.  ,  4.  ,  3.14, -9.17]), 2, (6,))\n\n\n\n\nCode\n# arguments: start, stop, step\nx = (\n     np.arange(12)\n       .reshape(4, 3)\n)\nx\n\n\narray([[ 0,  1,  2],\n       [ 3,  4,  5],\n       [ 6,  7,  8],\n       [ 9, 10, 11]])\n\n\n\n\nCode\ny = np.arange(3).reshape(3,1)\n\ny\n\n\narray([[0],\n       [1],\n       [2]])\n\n\n\n\nCode\nx @ y, x.dot(y)\n\n\n(array([[ 5],\n        [14],\n        [23],\n        [32]]),\n array([[ 5],\n        [14],\n        [23],\n        [32]]))\n\n\n\n\n\n\n\n\nNote"
  },
  {
    "objectID": "core/notebooks/notebook02_numpy.html#generating-arrays",
    "href": "core/notebooks/notebook02_numpy.html#generating-arrays",
    "title": "Introduction to numpy",
    "section": "Generating arrays",
    "text": "Generating arrays\n\n\nCode\nnp.linspace(0, 10, 51)  # meaning of the 3 positional parameters ? \n\n\narray([ 0. ,  0.2,  0.4,  0.6,  0.8,  1. ,  1.2,  1.4,  1.6,  1.8,  2. ,\n        2.2,  2.4,  2.6,  2.8,  3. ,  3.2,  3.4,  3.6,  3.8,  4. ,  4.2,\n        4.4,  4.6,  4.8,  5. ,  5.2,  5.4,  5.6,  5.8,  6. ,  6.2,  6.4,\n        6.6,  6.8,  7. ,  7.2,  7.4,  7.6,  7.8,  8. ,  8.2,  8.4,  8.6,\n        8.8,  9. ,  9.2,  9.4,  9.6,  9.8, 10. ])\n\n\n\n\nCode\nnp.logspace(0, 10, 11, base=np.e), np.e**(np.arange(11))\n\n\n(array([1.00000000e+00, 2.71828183e+00, 7.38905610e+00, 2.00855369e+01,\n        5.45981500e+01, 1.48413159e+02, 4.03428793e+02, 1.09663316e+03,\n        2.98095799e+03, 8.10308393e+03, 2.20264658e+04]),\n array([1.00000000e+00, 2.71828183e+00, 7.38905610e+00, 2.00855369e+01,\n        5.45981500e+01, 1.48413159e+02, 4.03428793e+02, 1.09663316e+03,\n        2.98095799e+03, 8.10308393e+03, 2.20264658e+04]))\n\n\n\n\nCode\nimport matplotlib.pyplot as plt\n\n# Random standard Gaussian numbers\nfig = plt.figure(figsize=(8, 4))\nwn = np.random.randn(1000)\nbm = wn.cumsum()\n\nplt.plot(bm, lw=3)\n\n\n\n\n\n\n\n\n\n\n\nCode\nnp.diag(np.arange(10))\n\n\narray([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 2, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 3, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 4, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 5, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 6, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 7, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 8, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 0, 9]])\n\n\n\n\nCode\nzozo = np.zeros((10, 10), dtype=np.float32)\nzozo\n\n\narray([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]], dtype=float32)\n\n\n\n\nCode\nzozo.shape\n\n\n(10, 10)\n\n\n\n\nCode\nprint(M)\n\n\n[[ 1.    2.  ]\n [ 3.    4.  ]\n [ 3.14 -9.17]]\n\n\n\n\nCode\nM[1, 1]\n\n\nnp.float64(4.0)\n\n\n\n\nCode\n# assign new value\nM[0, 0] = 7\nM[:, 0] = 42\nM\n\n\narray([[42.  ,  2.  ],\n       [42.  ,  4.  ],\n       [42.  , -9.17]])\n\n\n\n\nCode\nM\n\n\narray([[42.  ,  2.  ],\n       [42.  ,  4.  ],\n       [42.  , -9.17]])\n\n\n\n\nCode\n# Warning: the next m is a **view** on M. \n# One again, no copies unless you ask for one!\nm = M[0, :]\nm\n\n\narray([42.,  2.])\n\n\n\n\nCode\nm[:] = 3.14\nM\n\n\narray([[ 3.14,  3.14],\n       [42.  ,  4.  ],\n       [42.  , -9.17]])\n\n\n\n\nCode\nm[:] = 7\nM\n\n\narray([[ 7.  ,  7.  ],\n       [42.  ,  4.  ],\n       [42.  , -9.17]])"
  },
  {
    "objectID": "core/notebooks/notebook02_numpy.html#copies",
    "href": "core/notebooks/notebook02_numpy.html#copies",
    "title": "Introduction to numpy",
    "section": "Copies",
    "text": "Copies\nDon’t forget that python does not make copies unless told to do so (same as with any mutable type)\nIf you are not careful enough, this typically leads to a lot of errors and to being fired !!\n\n\nCode\ny = x = np.arange(6)\nx[2] = 123\ny\n\n\narray([  0,   1, 123,   3,   4,   5])\n\n\n\n\nCode\nx is y\n\n\nTrue\n\n\n\n\nCode\n# A real copy\ny = x.copy()\nx is y \n\n\nFalse\n\n\n\n\nCode\n# Or equivalently (but the one above is better...)\ny = np.copy(x)\n\n\n\n\nCode\nx[0] = -12\nprint(x, y, x is y)\n\n\n[-12   1 123   3   4   5] [  0   1 123   3   4   5] False\n\n\nTo put values of x in y (copy values into an existing array) use\n\n\nCode\nx = np.random.randn(10)\nx, id(x)\n\n\n(array([ 0.34735265,  0.38783658, -0.01660985, -0.14114233, -0.27206558,\n        -0.97844654,  0.69630894,  0.18247432,  0.91764587, -1.26549418]),\n 137393486622672)\n\n\n\n\nCode\nx.fill(2.78)   # in place. \nx, id(x)\n\n\n(array([2.78, 2.78, 2.78, 2.78, 2.78, 2.78, 2.78, 2.78, 2.78, 2.78]),\n 137393486622672)\n\n\n\n\nCode\nx[:] = 3.14  # x.fill(3.14)  can. be chained ...\nx, id(x)\n\n\n(array([3.14, 3.14, 3.14, 3.14, 3.14, 3.14, 3.14, 3.14, 3.14, 3.14]),\n 137393486622672)\n\n\n\n\nCode\nx[:] = np.random.randn(x.shape[0])\nx, id(x)\n\n\n(array([ 0.26189023, -1.95759172, -2.0419464 ,  0.32512999,  0.73671901,\n        -2.24966756,  0.9421588 ,  0.52023431, -0.10153539, -0.29100377]),\n 137393486622672)\n\n\n\n\nCode\ny = np.empty(x.shape)  # how does empty() work ?\ny, id(y)\n\n\n(array([0.26189023, 1.95759172, 2.0419464 , 0.32512999, 0.73671901,\n        2.24966756, 0.9421588 , 0.52023431, 0.10153539, 0.29100377]),\n 137393486622384)\n\n\n\n\nCode\ny = x\ny, id(y), id(x), y is x\n\n\n(array([ 0.26189023, -1.95759172, -2.0419464 ,  0.32512999,  0.73671901,\n        -2.24966756,  0.9421588 ,  0.52023431, -0.10153539, -0.29100377]),\n 137393486622672,\n 137393486622672,\n True)\n\n\n\n\n\n\n\n\nFinal warning\n\n\n\n\n\n\nIn the next line you copy the values of x into an existing array y (of same size…)\n\n\nCode\ny = np.zeros(x.shape)\ny[:] = x\ny, y is x, np.all(y==x)\n\n\n(array([ 0.26189023, -1.95759172, -2.0419464 ,  0.32512999,  0.73671901,\n        -2.24966756,  0.9421588 ,  0.52023431, -0.10153539, -0.29100377]),\n False,\n np.True_)\n\n\nWhile in the next line, you are aliasing, you are giving a new name y to the object named x (you should never, ever write something like this)\n\n\nCode\ny = x\ny is x\n\n\nTrue"
  },
  {
    "objectID": "core/notebooks/notebook02_numpy.html#miscellanea",
    "href": "core/notebooks/notebook02_numpy.html#miscellanea",
    "title": "Introduction to numpy",
    "section": "Miscellanea",
    "text": "Miscellanea\n\nNon-numerical values\nA numpy array can contain other things than numeric types\n\n\nCode\narr = np.array(['Labore', 'neque', 'ipsum', 'ut', 'non', 'quiquia', 'dolore.'])\narr, arr.shape, arr.dtype\n\n\n(array(['Labore', 'neque', 'ipsum', 'ut', 'non', 'quiquia', 'dolore.'],\n       dtype='&lt;U7'),\n (7,),\n dtype('&lt;U7'))\n\n\n\n\nCode\n# arr.sum()\n\n\n\n\nCode\n\"_\".join(arr)\n\n\n'Labore_neque_ipsum_ut_non_quiquia_dolore.'\n\n\n\n\nCode\narr.dtype\n\n\ndtype('&lt;U7')"
  },
  {
    "objectID": "core/notebooks/notebook02_numpy.html#a-matrix-is-no-2d-array-in-numpy",
    "href": "core/notebooks/notebook02_numpy.html#a-matrix-is-no-2d-array-in-numpy",
    "title": "Introduction to numpy",
    "section": "A matrix is no 2D array in numpy",
    "text": "A matrix is no 2D array in numpy\nSo far, we have only used array or ndarray objects\nThe is another type: the matrix type\nIn words: don’t use it (IMhO) and stick with arrays\n\n\nCode\n# Matrix VS array objects in numpy\nm1 = np.matrix(np.arange(3))\nm2 = np.matrix(np.arange(3))\nm1, m2\n\n\n(matrix([[0, 1, 2]]), matrix([[0, 1, 2]]))\n\n\n\n\nCode\nm1.transpose() @ m2, m1.shape, m1.transpose() * m2\n\n\n(matrix([[0, 0, 0],\n         [0, 1, 2],\n         [0, 2, 4]]),\n (1, 3),\n matrix([[0, 0, 0],\n         [0, 1, 2],\n         [0, 2, 4]]))\n\n\n\n\nCode\na1 = np.arange(3)\na2 = np.arange(3)\na1, a2\n\n\n(array([0, 1, 2]), array([0, 1, 2]))\n\n\n\n\nCode\nm1 * m2.T, m1.dot(m2.T)\n\n\n(matrix([[5]]), matrix([[5]]))\n\n\n\n\nCode\na1 * a2\n\n\narray([0, 1, 4])\n\n\n\n\nCode\na1.dot(a2)\n\n\nnp.int64(5)\n\n\n\n\nCode\nnp.outer(a1, a2)\n\n\narray([[0, 0, 0],\n       [0, 1, 2],\n       [0, 2, 4]])\n\n\n\n\n\n\n\n\nNote\n\n\n\nVisit https://numpy.org/doc/stable/reference/arrays.ndarray.html#arrays-ndarray"
  },
  {
    "objectID": "core/notebooks/notebook02_numpy.html#sparse-matrices",
    "href": "core/notebooks/notebook02_numpy.html#sparse-matrices",
    "title": "Introduction to numpy",
    "section": "Sparse matrices",
    "text": "Sparse matrices\n\n\nCode\nfrom scipy.sparse import csc_matrix, csr_matrix, coo_matrix\n\n\n\n\nCode\nprobs = np.full(fill_value=1/4, shape=(4,))\nprobs\n\n\narray([0.25, 0.25, 0.25, 0.25])\n\n\n\n\nCode\nX = np.random.multinomial(n=2, pvals=probs, size=4)   # check you understand what is going on \nX\n\n\narray([[0, 2, 0, 0],\n       [0, 1, 1, 0],\n       [1, 0, 0, 1],\n       [1, 1, 0, 0]])\n\n\n\n\nCode\nprobs\n\n\narray([0.25, 0.25, 0.25, 0.25])\n\n\n\n\nCode\nX_coo = coo_matrix(X)  ## coordinate format\n\n\n\n\nCode\nprint(X_coo)\nX_coo\n\n\n&lt;COOrdinate sparse matrix of dtype 'int64'\n    with 7 stored elements and shape (4, 4)&gt;\n  Coords    Values\n  (0, 1)    2\n  (1, 1)    1\n  (1, 2)    1\n  (2, 0)    1\n  (2, 3)    1\n  (3, 0)    1\n  (3, 1)    1\n\n\n&lt;COOrdinate sparse matrix of dtype 'int64'\n    with 7 stored elements and shape (4, 4)&gt;\n\n\n\n\nCode\nX_coo.nnz    # number pf non-zero coordinates \n\n\n7\n\n\n\n\nCode\nprint(X, end='\\n----\\n')\nprint(X_coo.data, end='\\n----\\n')\nprint(X_coo.row, end='\\n----\\n')\nprint(X_coo.col, end='\\n----\\n')\n\n\n[[0 2 0 0]\n [0 1 1 0]\n [1 0 0 1]\n [1 1 0 0]]\n----\n[2 1 1 1 1 1 1]\n----\n[0 1 1 2 2 3 3]\n----\n[1 1 2 0 3 0 1]\n----\n\n\nThere is also\n\ncsr_matrix: sparse rows format\ncsc_matrix: sparse columns format\n\nSparse rows is often used for machine learning: sparse features vectors\nBut sparse column format useful as well (e.g. coordinate gradient descent)"
  },
  {
    "objectID": "core/notebooks/notebook02_numpy.html#bored-with-decimals",
    "href": "core/notebooks/notebook02_numpy.html#bored-with-decimals",
    "title": "Introduction to numpy",
    "section": "Bored with decimals?",
    "text": "Bored with decimals?\n\n\nCode\nX = np.random.randn(5, 5)\nX\n\n\narray([[ 0.50279636, -0.89142068,  0.14705019, -1.97484462,  0.14565327],\n       [ 0.81568164,  0.13153904,  0.96874511,  0.16661565, -0.81720499],\n       [-2.0552225 ,  0.22125786,  0.270791  , -0.54197397, -1.32477109],\n       [-0.85336519,  0.28360894,  0.13626591, -0.21327729, -0.24759177],\n       [-1.13969827,  0.72759541, -0.0920562 ,  1.03212989,  0.38869068]])\n\n\n\n\nCode\n# All number displayed by numpy (in the current kernel) are with 3 decimals max\nnp.set_printoptions(precision=3)\nprint(X)\nnp.set_printoptions(precision=8)\n\n\n[[ 0.503 -0.891  0.147 -1.975  0.146]\n [ 0.816  0.132  0.969  0.167 -0.817]\n [-2.055  0.221  0.271 -0.542 -1.325]\n [-0.853  0.284  0.136 -0.213 -0.248]\n [-1.14   0.728 -0.092  1.032  0.389]]"
  },
  {
    "objectID": "core/notebooks/notebook02_numpy.html#not-limited-to-2d",
    "href": "core/notebooks/notebook02_numpy.html#not-limited-to-2d",
    "title": "Introduction to numpy",
    "section": "Not limited to 2D!",
    "text": "Not limited to 2D!\nnumpy arrays can have any number of dimension (hence the name ndarray)\n\n\nCode\nX = np.arange(18).reshape(3, 2, 3)\nX\n\n\narray([[[ 0,  1,  2],\n        [ 3,  4,  5]],\n\n       [[ 6,  7,  8],\n        [ 9, 10, 11]],\n\n       [[12, 13, 14],\n        [15, 16, 17]]])\n\n\n\n\nCode\nX.shape\n\n\n(3, 2, 3)\n\n\n\n\nCode\nX.ndim\n\n\n3\n\n\nVisit https://numpy.org/doc/stable/reference/arrays.ndarray.html#arrays-ndarray"
  },
  {
    "objectID": "core/notebooks/notebook02_numpy.html#inner-products",
    "href": "core/notebooks/notebook02_numpy.html#inner-products",
    "title": "Introduction to numpy",
    "section": "Inner products",
    "text": "Inner products\n\n\nCode\n# Inner product between vectors\nprint(v1.dot(v2))\n\n# You can use also (but first solution is better)\nprint(np.dot(v1, v2))\n\n\n80\n80\n\n\n\n\nCode\nA, v1\n\n\n(array([[ 0,  1,  2,  3,  4],\n        [ 5,  6,  7,  8,  9],\n        [10, 11, 12, 13, 14],\n        [15, 16, 17, 18, 19],\n        [20, 21, 22, 23, 24],\n        [25, 26, 27, 28, 29]]),\n array([0, 1, 2, 3, 4]))\n\n\n\n\nCode\nA.shape, v1.shape\n\n\n((6, 5), (5,))\n\n\n\n\nCode\n# Matrix-vector inner product\nA.dot(v1)\n\n\narray([ 30,  80, 130, 180, 230, 280])\n\n\n\n\nCode\n# Transpose\nA.T\n\n\narray([[ 0,  5, 10, 15, 20, 25],\n       [ 1,  6, 11, 16, 21, 26],\n       [ 2,  7, 12, 17, 22, 27],\n       [ 3,  8, 13, 18, 23, 28],\n       [ 4,  9, 14, 19, 24, 29]])\n\n\n\n\nCode\nprint(v1)\n# Inline operations (same for *=, /=, -=)\nv1 += 2\n\n\n[0 1 2 3 4]"
  },
  {
    "objectID": "core/notebooks/notebook02_numpy.html#linear-systems",
    "href": "core/notebooks/notebook02_numpy.html#linear-systems",
    "title": "Introduction to numpy",
    "section": "Linear systems",
    "text": "Linear systems\n\n\nCode\nA = np.array([[42,2,3], [4,5,6], [7,8,9]])\nb = np.array([1,2,3])\nprint(A, b, sep=2 * '\\n')\n\n\n[[42  2  3]\n [ 4  5  6]\n [ 7  8  9]]\n\n[1 2 3]\n\n\n\n\nCode\n# solve a system of linear equations\nx = np.linalg.solve(A, b)\nx\n\n\narray([2.18366847e-18, 2.31698718e-16, 3.33333333e-01])\n\n\n\n\nCode\nA.dot(x)\n\n\narray([1., 2., 3.])"
  },
  {
    "objectID": "core/notebooks/notebook02_numpy.html#eigenvalues-and-eigenvectors",
    "href": "core/notebooks/notebook02_numpy.html#eigenvalues-and-eigenvectors",
    "title": "Introduction to numpy",
    "section": "Eigenvalues and eigenvectors",
    "text": "Eigenvalues and eigenvectors\n\n\nCode\nA = np.random.rand(3,3)\nB = np.random.rand(3,3)\n\nevals, evecs = np.linalg.eig(A)\nevals\n\n\narray([ 1.70949739, -0.59712886, -0.23704844])\n\n\n\n\nCode\nevecs\n\n\narray([[-0.67211104, -0.47074745,  0.26588469],\n       [-0.50132133, -0.15775954, -0.82213875],\n       [-0.54492539,  0.86804882,  0.50338178]])"
  },
  {
    "objectID": "core/notebooks/notebook02_numpy.html#singular-value-decomposition-svd",
    "href": "core/notebooks/notebook02_numpy.html#singular-value-decomposition-svd",
    "title": "Introduction to numpy",
    "section": "Singular value decomposition (SVD)",
    "text": "Singular value decomposition (SVD)\nDecomposes any matrix \\(A \\in \\mathbb R^{m \\times n}\\) as follows: \\[\nA = U \\times S \\times V^\\top\n\\] where - \\(U\\) and \\(V\\) are orthonormal matrices (meaning that \\(U^\\top \\times U = I\\) and \\(V^\\top \\times V = I\\)) - \\(S\\) is a diagonal matrix that contains the singular values in non-increasing order\n\n\nCode\nprint(A)\nU, S, V = np.linalg.svd(A)\n\n\n[[0.55784428 0.7207817  0.75734318]\n [0.64741652 0.28551913 0.51151019]\n [0.99678968 0.48707519 0.03195668]]\n\n\n\n\nCode\nU.dot(np.diag(S)).dot(V)\n\n\narray([[0.55784428, 0.7207817 , 0.75734318],\n       [0.64741652, 0.28551913, 0.51151019],\n       [0.99678968, 0.48707519, 0.03195668]])\n\n\n\n\nCode\nA - U @ np.diag(S) @ V\n\n\narray([[ 0.00000000e+00, -3.33066907e-16, -2.22044605e-16],\n       [-1.11022302e-16,  0.00000000e+00,  1.11022302e-16],\n       [-3.33066907e-16, -1.66533454e-16,  1.38777878e-17]])\n\n\n\n\nCode\n# U and V are indeed orthonormal\nnp.set_printoptions(precision=2)\nprint(U.T.dot(U), V.T.dot(V), sep=2 * '\\n')\nnp.set_printoptions(precision=8)\n\n\n[[ 1.00e+00 -3.87e-17 -1.43e-16]\n [-3.87e-17  1.00e+00  2.77e-16]\n [-1.43e-16  2.77e-16  1.00e+00]]\n\n[[ 1.00e+00 -1.01e-17 -6.45e-17]\n [-1.01e-17  1.00e+00  1.14e-16]\n [-6.45e-17  1.14e-16  1.00e+00]]"
  },
  {
    "objectID": "core/notebooks/notebook02_numpy.html#exercice-the-racoon-svd",
    "href": "core/notebooks/notebook02_numpy.html#exercice-the-racoon-svd",
    "title": "Introduction to numpy",
    "section": "Exercice: the racoon SVD",
    "text": "Exercice: the racoon SVD\n\nLoad the racoon face picture using scipy.misc.face()\nVisualize the picture\nWrite a function which reshapes the picture into a 2D array, and computes the best rank-r approximation of it (the prototype of the function is compute_approx(X, r)\nDisplay the different approximations for r between 5 and 100\n\n\n\nCode\n!pip3 install pooch\n\n\nRequirement already satisfied: pooch in /home/boucheron/Documents/IFEBY310/.venv/lib/python3.12/site-packages (1.8.2)\nRequirement already satisfied: platformdirs&gt;=2.5.0 in /home/boucheron/Documents/IFEBY310/.venv/lib/python3.12/site-packages (from pooch) (4.3.6)\nRequirement already satisfied: packaging&gt;=20.0 in /home/boucheron/Documents/IFEBY310/.venv/lib/python3.12/site-packages (from pooch) (24.2)\nRequirement already satisfied: requests&gt;=2.19.0 in /home/boucheron/Documents/IFEBY310/.venv/lib/python3.12/site-packages (from pooch) (2.32.3)\nRequirement already satisfied: charset-normalizer&lt;4,&gt;=2 in /home/boucheron/Documents/IFEBY310/.venv/lib/python3.12/site-packages (from requests&gt;=2.19.0-&gt;pooch) (3.4.1)\nRequirement already satisfied: idna&lt;4,&gt;=2.5 in /home/boucheron/Documents/IFEBY310/.venv/lib/python3.12/site-packages (from requests&gt;=2.19.0-&gt;pooch) (3.10)\nRequirement already satisfied: urllib3&lt;3,&gt;=1.21.1 in /home/boucheron/Documents/IFEBY310/.venv/lib/python3.12/site-packages (from requests&gt;=2.19.0-&gt;pooch) (2.3.0)\nRequirement already satisfied: certifi&gt;=2017.4.17 in /home/boucheron/Documents/IFEBY310/.venv/lib/python3.12/site-packages (from requests&gt;=2.19.0-&gt;pooch) (2024.12.14)\n\n[notice] A new release of pip is available: 25.0 -&gt; 25.0.1\n[notice] To update, run: pip install --upgrade pip\n\n\n\n\nCode\nimport numpy as np\nfrom scipy.datasets import face\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nX = face()\n\n\n\n\nCode\ntype(X)\n\n\nnumpy.ndarray\n\n\n\n\nCode\nplt.imshow(X)\n_ = plt.axis('off')\n\n\n\n\n\n\n\n\n\n\n\nCode\nn_rows, n_cols, n_channels = X.shape\nX_reshaped = X.reshape(n_rows, n_cols * n_channels)\nU, S, V = np.linalg.svd(X_reshaped, full_matrices=False)\n\n\n\n\nCode\nX_reshaped.shape\n\n\n(768, 3072)\n\n\n\n\nCode\nX.shape\n\n\n(768, 1024, 3)\n\n\n\n\nCode\nplt.plot(S**2)  ## a kind of screeplot\nplt.yscale(\"log\")\n\n\n\n\n\n\n\n\n\n\n\nCode\ndef compute_approx(X: np.ndarray, r: int):\n    \"\"\"Computes the best rank-r approximation of X using SVD.\n    We expect X to the 3D array corresponding to a color image, that we \n    reduce to a 2D one to apply SVD (no broadcasting).\n    \n    Parameters\n    ----------\n    X : `np.ndarray`, shape=(n_rows, n_cols, 3)\n        The input 3D ndarray\n    \n    r : `int`\n        The desired rank\n        \n    Return\n    ------\n    output : `np.ndarray`, shape=(n_rows, n_cols, 3)\n        The best rank-r approximation of X\n    \"\"\"\n    n_rows, n_cols, n_channels = X.shape\n    # Reshape X to a 2D array\n    X_reshape = X.reshape(n_rows, n_cols * n_channels)\n    # Compute SVD\n    U, S, V = np.linalg.svd(X_reshape, full_matrices=False)\n    # Keep only the top r first singular values\n    S[r:] = 0\n    # Compute the approximation\n    X_reshape_r = U.dot(np.diag(S)).dot(V)\n    # Put it between 0 and 255 again and cast to integer type\n    return X_reshape_r.clip(min=0, max=255).astype('int')\\\n        .reshape(n_rows, n_cols, n_channels)\n\n\n\n\nCode\nranks = [100, 70, 50, 30, 10, 5]\nn_ranks = len(ranks)\nfor i, r in enumerate(ranks):\n    X_r = compute_approx(X, r)\n    # plt.subplot(n_ranks, 1, i + 1)\n    plt.figure(figsize=(5, 5))\n    plt.imshow(X_r)\n    _ = plt.axis('off')\n    # plt.title(f'Rank {r} approximation of the racoon' % r, fontsize=16)\n    plt.tight_layout()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVariations\n\n\n\nIn the code above, we recompute the SVD of X for every element in list rank.\nIn the next chunk, we compute the SVD once, and define a generator to generate the low rank approximations of matrix X. We take advantage of the fact that the SVD defines an orthonormal basis for the space of matrices. In this adapted orthonormal basis the optimal low rank approximations of \\(X\\) have a sparse expansion.\n\n\n\n\nCode\ndef gen_rank_k_approx(X):\n    \"\"\"Generator for low rank \n    approximation of a matrix X using truncated SVD.\n\n    Args:\n        X (numpy.ndarray): a numerical matrix\n\n    Yields:\n        (int,numpy.ndarray): rank k and best rank-k approximation of X using truncated SVD(according to Eckart-Young theorem).\n    \"\"\"  \n    U, S, V = np.linalg.svd(X, full_matrices=False)\n    r = 0\n    Y = np.zeros_like(X, dtype='float64')\n    while (r&lt;len(S)):\n      Y = Y + S[r] * (U[:,r,np.newaxis] @ V[r,:, np.newaxis].T)\n      r += 1\n      yield r, Y\n\n\n\n\nCode\ng = gen_rank_k_approx(X_reshaped) \n\n\n\n\nCode\nfor i in range(100):\n    _, Xr = next(g)\n    if i % 10 ==0:  \n      plt.figure(figsize=(5, 5))\n      plt.imshow(\n          Xr\n          .clip(min=0, max=255)\n          .astype('int')\n          .reshape(n_rows, n_cols, n_channels)\n      )\n      _ = plt.axis('off')\n      plt.tight_layout()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVisit https://numpy.org/numpy-tutorials/content/tutorial-svd.html"
  },
  {
    "objectID": "core/notebooks/notebook-0.html",
    "href": "core/notebooks/notebook-0.html",
    "title": "Notebook 0",
    "section": "",
    "text": "Iterators\n\n\nGenerators\n\n\nCoroutines"
  },
  {
    "objectID": "computing-virtualenv.html",
    "href": "computing-virtualenv.html",
    "title": "Virtual environments",
    "section": "",
    "text": "In order to work in a comfortable way, it is at least useful to use virtual environments for Python and R. Why? During the academic year, you shall work on several software projects. Each project is likely to require installing packages and dependencies. Packages can be downloaded from pypi.org and installed using pip (for example). You may try to work with a large set of installed packages that fits all your needs and requirements. But what if packages required in different projetcs have incompatible dependencies? Installing packages at system level will take you to a dead end. It is much safer and easier to hava a virtual environment for each project.\nhttps://virtualenv.pypa.io/en/latest/",
    "crumbs": [
      "Support",
      "Virtual environments"
    ]
  },
  {
    "objectID": "computing-virtualenv.html#references",
    "href": "computing-virtualenv.html#references",
    "title": "Virtual environments",
    "section": "References",
    "text": "References\n\nvenv\nvirtualenv\nrenv for R",
    "crumbs": [
      "Support",
      "Virtual environments"
    ]
  },
  {
    "objectID": "computing-pipelines.html",
    "href": "computing-pipelines.html",
    "title": "Pipelines",
    "section": "",
    "text": "Interroger une base de données avec R (via ODBC)\n\n\nInterroger une base de données avec Python (via ODBC)"
  },
  {
    "objectID": "computing-git.html",
    "href": "computing-git.html",
    "title": "Git",
    "section": "",
    "text": "On a terminal, specify the email address with which you will make your commits:\n$ git config --global user.email \"bb.wolf@forest.fr\"\nOf course Adapt email address to your case!.\nYou may also (optional) configure another option (yet mysterious)\n$ git config --global pull.rebase false"
  },
  {
    "objectID": "computing-git.html#setting-your-git-environment",
    "href": "computing-git.html#setting-your-git-environment",
    "title": "Git",
    "section": "",
    "text": "On a terminal, specify the email address with which you will make your commits:\n$ git config --global user.email \"bb.wolf@forest.fr\"\nOf course Adapt email address to your case!.\nYou may also (optional) configure another option (yet mysterious)\n$ git config --global pull.rebase false"
  },
  {
    "objectID": "computing-git.html#create-an-ssh-key",
    "href": "computing-git.html#create-an-ssh-key",
    "title": "Git",
    "section": "Create an SSH key",
    "text": "Create an SSH key\n\nUnix system\nThe SSH is needed to get a smooth authentication to the remote repository. In a terminal:\n$ ssh-keygen -t rsa -b 4096 -C bb.wolf@forest.fr\nAccept the default option (keys saved in ~/.ssh and no passphrase)\nssh-add\nReference: Github docs on connecting with SSH.\n\n\nWindows\nReference: The Server Side: How to SSH into GitHub on Windows example, by Cameron McKenzie"
  },
  {
    "objectID": "computing-git.html#create-a-remote-repository",
    "href": "computing-git.html#create-a-remote-repository",
    "title": "Git",
    "section": "Create a remote repository",
    "text": "Create a remote repository\nLet us create a remote repository hosted on your GitHub account.\nOn GitHub, click on the + symbol at the top right of the page, then New repository. Give the name FirstRepo to your new project and a short description.\nCreate a public repository, meaning that everyone can access your code (read-only). Finish by clicking on Create repository.\nFollow the instructions provided by GitHub to create your local copy of the repository:\n\nCreate a new folder called FirstRepo in your home directory and cd to it\nThen execute the following command changing the XXXXXXXXXXX with the relevent URL.\necho \"# FirstRepo\" &gt;&gt; README.md\ngit init\ngit add README.md\ngit commit -m \"first commit\"\ngit branch -M main\ngit remote add origin git@github.com:XXXXXXXXXXXXXXXXXX/FirstRepo.git\ngit push --set-upstream  origin main\n\n\n\n\n\n\n\nEXERCISE: gitignore\n\n\n\n\nCreate a text file called .gitignore with the following content:\n\n*.pdf\n*~\n\nCreate a commit and push it to your repository. What is the purpose of this file? See https://github.com/github/gitignore"
  },
  {
    "objectID": "computing-git.html#using-an-existing-repository",
    "href": "computing-git.html#using-an-existing-repository",
    "title": "Git",
    "section": "Using an existing repository",
    "text": "Using an existing repository\nBrowse the course repository at https://github.com/s-v-b/IFEBY310. What is this module able to do?\n\n\n\n\n\n\nEXERCISE: Forking a Git repo\n\n\n\nFork the repository https://github.com/s-v-b/IFEBY310 by following these steps:\n\nOn GitHub, click on the fork icon.\nA copy is added to your GitHub space. Clone it (this copy!) to get a local repository.\nIn a terminal, inspect the output of the command git remote get-url origin"
  },
  {
    "objectID": "computing-git.html#debugging",
    "href": "computing-git.html#debugging",
    "title": "Git",
    "section": "Debugging",
    "text": "Debugging\nA bug has appeared in the Python module after a commit. An issue has been opened in the bug tracking system at https://github.com/s-v-b/IFEBY310/issues/. Your goal is to find the problem… and then fix it on your forked repository. Finally, you will be able to submit a Pull Request to the original repository to share your fix.\n\nIdentification of the bad commit\nYour goal is to identify the commit(s) that caused the bug. Use git log, git diff, git checkout to identify the commit responsible for the problem.\nReference: Git Bisect\n\n\nCreate a new branch to fix the problem\nTo fix a complex bug or add a new feature, it is often necessary to modify several parts of the code. We create a branch, where we make all the commits dedicated to solving the bug. The idea is to maintain a stable version, in the branch main, separated from the developing version, which may contain bugs.\n\n\n\n\n\n\nEXERCISE: branches\n\n\n\n\nCreate a local branch Fix_EOL_Error\nPush this local branch to your remote repo.\nSwitch to the Fix_EOL_Error branch, and fix the bugs. The branch main will not be affected.\nMerge the fix into the branch main\nDelete the local branch Fix_EOL_Error and the remote origin/Fix_EOL_Error branch\n\n\n\n\n\nPull request\nYour work about bug fixing may interest the original author of the project. On GitHub, open a pull request (PR). PRs are a set of commits that can be integrated directly by the author of the project in its repository and are thus a powerful tool for working with others."
  },
  {
    "objectID": "computing-git.html#branch-merging-and-solving-conflicts",
    "href": "computing-git.html#branch-merging-and-solving-conflicts",
    "title": "Git",
    "section": "Branch Merging and Solving conflicts",
    "text": "Branch Merging and Solving conflicts\n\n\n\n\n\n\nEXERCISE: Conflicts identification\n\n\n\n\nSwitch to the branch NonGaussian. Try to figure out what has changed compared to the main branch.\nTry to merge the branch NonGaussian to the branch main.\nWhere are located the conflicts? They are shown with the following decorator.\n\n&lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD\nSome code on the current branch\n=======\n\nSome code on the branch to be merged\n&gt;&gt;&gt;&gt;&gt;&gt;&gt; NonGaussian\n\nResolve them by plotting the two histograms on the same plot. Namely, produce a figure like this:"
  },
  {
    "objectID": "computing-git.html#rebase",
    "href": "computing-git.html#rebase",
    "title": "Git",
    "section": "Rebase",
    "text": "Rebase"
  },
  {
    "objectID": "computing-git.html#references",
    "href": "computing-git.html#references",
    "title": "Git",
    "section": "References",
    "text": "References\n\nPlease visit https://learngitbranching.js.org/"
  },
  {
    "objectID": "computing-access.html",
    "href": "computing-access.html",
    "title": "Moyens de calcul",
    "section": "",
    "text": "Serveur\nLe cours et les TP\n\nServeur PostGreSQL\nMachine dédiée : etu-pgsql.math.univ-paris-diderot.fr\n\n\n\nClients\nEn salle TP, vous pourrez choisir entre trois clients\n\npsql\npgcli\ndbeaver\n\npsql et pgcli sont très proches. Ce sont des applications qui fonctionnent en mode ligne de commande. pgcli est un peu plus conviviale que psql avec un système de complétion plus performant. L’ensemble des commandes spéciales proposées par pgcli est un peu moins vaste que celui proposé par psql\ndbeaver est un client graphique qui ne tombe pas dans le cliquodrome. dbeaver permet d’attaquer une grande famille de SGBDs.\nTous ces clients doivent utiliser des connexions sécurisées ssh.\n\n\nConnexions ssh (Linux/MacOS)\nPour accéder au serveur Postgres (SGBD), il faut d’abord se connecter à la machine qui héberge ce serveur etu-pgsql.math.univ-paris-diderot.fr.\nPour se connecter à etu-pgsql.math.univ-paris-diderot.fr, on utilise le protocole ssh, avec son identifiant et son mot de passe ENT.\n$ ssh username@etu-pgsql.math.univ-paris-diderot.fr\n(username@etu-pgsql.math.univ-paris-diderot.fr) Password for username@etu-pgsql.math.univ-paris-diderot.fr:\nLast login: ..................\n....\n....\nThis fortune brought to you by:\n$FreeBSD$\n[username@etu-pgsql ~]$ ...\nAttention : pas d’écho lorsque vous saisissez votre mot de passe.\nOn peut maintenant se connecter au serveur Postgres, toujours avec son identifiant et son mot de passe ENT\nIci, nous proposons d’utiliser le client pgcli exécuté sur etu-pgsql.math.univ-paris-diderot.fr :\n[username@etu-pgsql ~]$ pgcli -d bd_2023-24 \nPassword for username: \nServer: PostgreSQL 13.8\nVersion: 3.5.0\nHome: http://pgcli.com\nuserame@/tmp:bd_2023-24&gt; \n\n\n\n\n\n\nPourquoi préciser -d bd_2023-24 ?\n\n\n\nbd_2023-24 est un des catalogues hébergés par le serveur PostGres. bd_2023-24 contient les schemas sur lesquels nous travaillerons.\n\n\nNous sommes maintenant dans une session du serveur\nusername@/tmp:bd_2023-24&gt; \\conninfo\nYou are connected to database \"bd_2023-24\" as user \"username\" on socket \"/tmp\" at port \"None\".\nTime: 0.000s\nPour obtenir de l’aide, utiliser la commande \\d? et laissez vous guider.\nNous nous intéresserons en général à un ensemble de tables formant un schéma. Ici, nous choisissons comme schéma par défaut world et nous affichons les tables du schéma.\nusername@/tmp:bd_2023-24&gt; set search_path to world ;\nSET\nTime: 0.001s\nusername@/tmp:bd_2023-24&gt; \\dt\n+--------+-----------------+-------+-----------+\n| Schema | Name            | Type  | Owner     |\n|--------+-----------------+-------+-----------|\n| world  | city            | table | boucheron |\n| world  | country         | table | boucheron |\n| world  | countrylanguage | table | boucheron |\n+--------+-----------------+-------+-----------+\nSELECT 3\nTime: 0.011s\nusername@/tmp:bd_2023-24&gt; ...\n\n\n\n\n\n\nPlus pratique\n\n\n\nPour pgcli et psql, il est plus pratique d’exécuter psql et/ou pgcli sur votre machine et de communiquer avec le serveur Postgres via un tunnel ssh. Voir détails pour pgcli et détails pour psql.\n\n\n\n\nConnexions ssh sous windows\nPour accéder au serveur Postgres (SGBD), il faut d’abord se connecter à la machine qui héberge ce serveur etu-pgsql.math.univ-paris-diderot.fr.\nPour se connecter à etu-pgsql.math.univ-paris-diderot.fr, on utilise le protocole ssh, avec son identifiant et son mot de passe ENT.\nOn lance d’abord une fenêtre Powershell.\n$ ssh username@etu-pgsql.math.univ-paris-diderot.fr\n(username@etu-pgsql.math.univ-paris-diderot.fr) Password for username@etu-pgsql.math.univ-paris-diderot.fr:\nLast login: ..................\n....\n....\nThis fortune brought to you by:\n$FreeBSD$\n[username@etu-pgsql ~]$ ...\n Attention : pas d’écho lorsque vous saisissez votre mot de passe.\nOn peut maintenant se connecter au serveur Postgres, toujours avec son identifiant et son mot de passe ENT\nIci, nous proposons d’utiliser le client pgcli :\n[username@etu-pgsql ~]$ pgcli -d bd_2023-24 \nPassword for username: \nServer: PostgreSQL 13.8\nVersion: 3.5.0\nHome: http://pgcli.com\nuserame@/tmp:bd_2023-24&gt; \n\n\n\n\n\n\nPourquoi préciser -d bd_2023-24 ?\n\n\n\nbd_2023-24 est un des catalogues hébergés par le serveur PostGres. bd_2023-24 contient les schemas sur lesquels nous travaillerons.\n\n\nNous sommes maintenant dans une session du serveur\nusername@/tmp:bd_2023-24&gt; \\conninfo\nYou are connected to database \"bd_2023-24\" as user \"username\" on socket \"/tmp\" at port \"None\".\nTime: 0.000s\nPour obtenir de l’aide, utiliser la commande \\d? et laissez vous guider.\nNous nous intéresserons en général à un ensemble de tables formant un schéma. Ici, nous choisissons comme schéma par défaut world et nous affichons les tables du schéma.\nusername@/tmp:bd_2023-24&gt; set search_path to world ;\nSET\nTime: 0.001s\nusername@/tmp:bd_2023-24&gt; \\dt\n+--------+-----------------+-------+-----------+\n| Schema | Name            | Type  | Owner     |\n|--------+-----------------+-------+-----------|\n| world  | city            | table | boucheron |\n| world  | country         | table | boucheron |\n| world  | countrylanguage | table | boucheron |\n+--------+-----------------+-------+-----------+\nSELECT 3\nTime: 0.011s\nusername@/tmp:bd_2023-24&gt; ...\n\n\nConnexions ssh avec tunnel\nVous avez pu constater que les connexions ssh sous MacOS, Linux et Windows sont presque identiques.\nMais utiliser une connexion ssh et un client base de données qui s’exécute sur etu-pgsql.math.univ-paris-diderot.fr n’est pas la manière la plus confortable de travailler.\nIl est plus agréable d’utiliser un client base de données qui s’exécute sur sa propre machine (en local) et qui interagit avec le serveur PostGres au travers d’un tunnel ssh.\nLa commande suivante établit un tunnel en tâche de fond (background job) grâce à l’option -f\n$ ssh -f username@etu-pgsql.math.univ-paris-diderot.fr -L 5436:localhost:5432 -N\n(username@etu-pgsql.math.univ-paris-diderot.fr) Password for username@etu-pgsql.math.univ-paris-diderot.fr:\n$ \nUne fois le tunnel établi, on peut continuer d’utiliser la fenêtre terminal, par exemple pour lancer pgcli ou psql.\nLa commande suivante établit aussi un tunnel mais en tâche de premier plan.\n$ ssh username@etu-pgsql.math.univ-paris-diderot.fr -L 5436:localhost:5432 -N\n(username@etu-pgsql.math.univ-paris-diderot.fr) Password for username@etu-pgsql.math.univ-paris-diderot.fr:\n$ \nUne fois le tunnel établi, la fenêtre terminal est bloquée. Si on veut lancer pgcli ou psql, il faut disposer d’une autre fenêtre.\n\n\n\n\n\n\nTunnel en tâche de fond ou pas ?\n\n\n\nLe lancement du tunnel en tâche de premier plan peut paraître moins pratique que le lancement du tunnel en tâche de fond.\nIl présente un avantage : lorsque le tunnel cesse de fonctionner (en général parce qu’on ne s’en est pas servi depuis quelques minutes), il faut termniner (tuer) le processus qui contrôle le tunnel, pour pouvoir récupérer l’usage du port local ; si le tunnel est contrôlé par une tache de premier plan, c’est trivial (^C sous Unix), si le tunnel est contrôlé par une tâche de fond, il faut déterminer le processus contrôleur, puis le terminer explicitement ($ kill -9 pid).\n\n\n\n\n\n\n\n\nRenvoi de port -L 5436:localhost:5432\n\n\n\nUn serveur PostGres écoute (attend) d’éventuels clients sur le port officiel 5432. Le serveur que nous utiliserons attend effectivement ses clients sur le port 5432 de la machine qui l’héberge. Notre client local ne va pas s’adresser directement au port 5432 de etu-pgsql.math.univ-paris-diderot.fr (c’est interdit). Notre client local s’adressera au port 5436 de la machine qui héberge le client et qui est lui-même renvoyé via le tunnel ssh vers le port 5432 de la machine qui héberge le serveur.\n\n\nOn peut maintenant lancer un client sur sa propre machine (localhost) en précisant qu’on s’adresse au port local 5436 (ou le port que vous choisissez), la requête de conexion au serveur PostGres distant sera transmise par le tunnel : elle sera envoyée sur le port officiel 5432 de la machine distante. Une fois la session établie, tout se passsera comme précédemment (ou presque).\n$ pgcli -d bd_2023-24 -h localhost -p 5436 -u username -W\nServer: PostgreSQL 13.8\nVersion: 3.5.0\nHome: http://pgcli.com\nbd_2023-24&gt; \\dn\n+----------------+--------------+\n| Name           | Owner        |\n|----------------+--------------|\n...\n...\n\n\nClient dbeaver\nLe mécanisme du tunnel ssh peut être utilisé pour connecter un client plus ambitieux au serveur. Le client dbeaver est particulièrement facile à utiliser.\n\n\nClient VS Code + extensions SQLTools\nSi vous êtes déjà habitué à l’éditeur Visual Studio Code (VS Code), vous pouvez utiliser l’extension SQLToos et son pilote ‘PostgreSQL/Cockroach’.\nVotre configuration de connexion devrait ressembler à :\n{\n  \"label\": \"etu-pgsql\",\n  \"host\": \"localhost\",\n  \"user\": \"&lt;identifiant ENT&gt;\",\n  \"port\": 5436,\n  \"ssl\": false,\n  \"database\": \"bd_2023-24\",\n  \"schema\": \"world\",\n  \"password\": \"Ask on connect\"\n}\nIl faut par ailleurs ouvrir un tunnel SSH dans un terminal\n$ ssh  username@etu-pgsql.math.univ-paris-diderot.fr -L 5436:localhost:5432 -N\n(username@etu-pgsql.math.univ-paris-diderot.fr) Password for username@etu-pgsql.math.univ-paris-diderot.fr:\n$ \nen remplaçant username par votre identifiant ENT.",
    "crumbs": [
      "Support",
      "Computing resources"
    ]
  },
  {
    "objectID": "bck_course-syllabus.html",
    "href": "bck_course-syllabus.html",
    "title": "IFEBY310 Syllabus",
    "section": "",
    "text": "Schedule\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDay\nTime\nLocation\nStart\n\n\n\n\nLab session\nFriday\n9:00-10:30\nSophie Germain 2012\n2024-01-17\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOrganization \n\n\n\nWe will have one weeky lecture. Each session is organized around Slides and Notebooks. We will switch from blackboard to laptop and back. You are invited to bring your laptop to the lectures.\n We will not attempt to complete the notebooks during the sessions. You are expected to complete the noteboks on your own time. Solutions (at least partial solutions) are available on the course website.\nYou can fork the course repository and post issues, comments, and corrections.\n\n\n\n\n\n\n\n\nObjectives\n\n\n\nDuring this course, you shall learn to:\n\nHandle middlesize data using Python Data Stack: Numpy/Scipy/Pandas\nScale up and down with Dask\nHandle Big Data with Spark (PySpark)\nManage and store data using dedicated columnar formats (Parquet, ORC, Avro, Arrow)\n\n\n\n\n\n\n\n\n\nCommunication \n\n\n\nMaterial is available from s-v-b.github.io/IFEBY310\nMoodle\nSubscribe to Moodle portal\n\n\n\n\n\n\n\n\nSoftware \n\n\n\n\nR\nPosit\nrstudio\nquarto\nvs code\ngit\ndocker\npostgresql\n\n\n\n\n\n\n\n\n\nReferences \n\n\n\n\nBin Yu and Rebecca Barter, Veridical Data Science\nHadley Wickham, ggplot2: Elegant Graphics for Data Analysis\nHadley Wickham, Mine Çetinkaya-Rundel, and Garrett Grolemund, R for Data Science\nHadley Wickham, Advanced R\nHadley Wickham and Jennifer Bryan., R packages\nHadley Wickham, Mastering Shiny\nkaggle\n\n\n\n\n\n\n\n\n\nCourse material \n\n\n\nSlides\nLabs are available (html and pdf)\nLabs with corrections are available\n\n\n\n\n\n\n\n\n\n\n\n\n\n3 projects:\n\n\\(\\textsf{P}_1\\) Visualization\n\\(\\textsf{P}_2\\) Regression\n\\(\\textsf{P}_3\\) Packaging\n\nGrading\n\n\\[.2 \\textsf{P}_1 + .3 \\textsf{P}_2 + .5 \\textsf{P}_3\\]\n\n\n\n\n\n\n\n\nCode of conduct\n\n\n\nTL;DR: No cheating!\n\n\n\n\n\n\n\n\nSave the dates ! \n\n\n\nClick here for U Paris Cite Calendar.\nClick here for M1 MIDS Calendar\n\n\n\n\n\n\n\n\nUniversité Paris Cité\n\n\n\nUseful links:\n\nCharte Université Paris Cité\nDémarches et accessibilité\nCentre de contact\nRelais handicap"
  },
  {
    "objectID": "computing-docker.html",
    "href": "computing-docker.html",
    "title": ": Docker",
    "section": "",
    "text": "Download, install and launch docker on your laptop\n\nFollow instructions at https://www.docker.com/get-started\n\n\n\n\n\n\nNote\n\n\n\nIt is enough to install the CLI tool.\n\n\n\nCreate an account on docker hub (if you don’t have one already) at https://hub.docker.com\n\n\nDocker Hub is a service provided by Docker for finding and sharing container images with your team. Learn more and find images at https://hub.docker.com\n\n\nOpen a terminal (powershell on windows) and type\n\ndocker login\nand provide the username and password you use at https://hub.docker.com",
    "crumbs": [
      "Support",
      "Docker"
    ]
  },
  {
    "objectID": "computing-docker.html#get-a-docker-account-and-connect",
    "href": "computing-docker.html#get-a-docker-account-and-connect",
    "title": ": Docker",
    "section": "",
    "text": "Download, install and launch docker on your laptop\n\nFollow instructions at https://www.docker.com/get-started\n\n\n\n\n\n\nNote\n\n\n\nIt is enough to install the CLI tool.\n\n\n\nCreate an account on docker hub (if you don’t have one already) at https://hub.docker.com\n\n\nDocker Hub is a service provided by Docker for finding and sharing container images with your team. Learn more and find images at https://hub.docker.com\n\n\nOpen a terminal (powershell on windows) and type\n\ndocker login\nand provide the username and password you use at https://hub.docker.com",
    "crumbs": [
      "Support",
      "Docker"
    ]
  },
  {
    "objectID": "computing-docker.html#docker-desktop-optional",
    "href": "computing-docker.html#docker-desktop-optional",
    "title": ": Docker",
    "section": "Docker desktop (optional)",
    "text": "Docker desktop (optional)",
    "crumbs": [
      "Support",
      "Docker"
    ]
  },
  {
    "objectID": "computing-docker.html#pull-docker-iamges",
    "href": "computing-docker.html#pull-docker-iamges",
    "title": ": Docker",
    "section": "Pull docker iamges",
    "text": "Pull docker iamges\ndocker pull svbo/ifeby310\ndocker image ls",
    "crumbs": [
      "Support",
      "Docker"
    ]
  },
  {
    "objectID": "computing-docker.html#run-containers",
    "href": "computing-docker.html#run-containers",
    "title": ": Docker",
    "section": "Run containers",
    "text": "Run containers\n\nA container is a runtime instance of a docker image. A container will always run the same, regardless of the infrastructure. Containers isolate software from its environment and ensure that it works uniformly despite differences for instance between development and staging.\n\n\nConfigure docker-compose.yml\ndocker-compose.yml\nversion: \"3.7\"\nservices:\n  big_data_course:\n    container_name: ifeby310  \n    image: svbo/ifeby310\n    ports:\n      - \"8192:8192\"\n      - \"8888:8888\"\n      - \"4040:4040\"\n    restart: always\n    volumes:\n      - \"PATH_GROSSES_DATA:/opt/polynote/notebooks/\"\n    restart: always\n    environment:\n      - PYSPARK_ALLOW_INSECURE_GATEWAY=1\n\n\n\n\n\n\nImportant\n\n\n\nPATH_GROSSES_DATA denotes the path on your hard drive where you will work during this course. It denotes a local volume that is mapped on container path /opt/polynote/notebooks\n\n\n\n\nCompose the container\ndocker-compose up\ndocker container ls",
    "crumbs": [
      "Support",
      "Docker"
    ]
  },
  {
    "objectID": "computing-docker.html#what-happens",
    "href": "computing-docker.html#what-happens",
    "title": ": Docker",
    "section": "What happens?",
    "text": "What happens?",
    "crumbs": [
      "Support",
      "Docker"
    ]
  },
  {
    "objectID": "computing-docker.html#use-jupyter-notebooks",
    "href": "computing-docker.html#use-jupyter-notebooks",
    "title": ": Docker",
    "section": "Use jupyter notebooks",
    "text": "Use jupyter notebooks",
    "crumbs": [
      "Support",
      "Docker"
    ]
  },
  {
    "objectID": "computing-docker.html#use-pyspark-and-spark-submit",
    "href": "computing-docker.html#use-pyspark-and-spark-submit",
    "title": ": Docker",
    "section": "Use pyspark and spark-submit",
    "text": "Use pyspark and spark-submit",
    "crumbs": [
      "Support",
      "Docker"
    ]
  },
  {
    "objectID": "computing-docker.html#use-polynote",
    "href": "computing-docker.html#use-polynote",
    "title": ": Docker",
    "section": "Use polynote",
    "text": "Use polynote",
    "crumbs": [
      "Support",
      "Docker"
    ]
  },
  {
    "objectID": "computing-docker.html#docker-cheatsheet",
    "href": "computing-docker.html#docker-cheatsheet",
    "title": ": Docker",
    "section": "Docker cheatsheet",
    "text": "Docker cheatsheet\nFrom docker.com",
    "crumbs": [
      "Support",
      "Docker"
    ]
  },
  {
    "objectID": "computing-jupyter.html",
    "href": "computing-jupyter.html",
    "title": "Jupyter Lab",
    "section": "",
    "text": "Jupyter lab\n\nhttps://jupyter.org\nhttps://jupyterlab.readthedocs.io/en/stable/\n\n\n\nBasic usage\n$ jupyter lab \n\n\nConversion between Quarto notebooks and Jupyter notebooks (and vice versa)\n\n\nUsing rise to build Revealjs presentations\n\n\nJupyter lab and Spark\n\n\nJupyter lab and Dask\n\n\nVersions\n$ jupyter --version\nSelected Jupyter core packages...\nIPython          : 8.31.0\nipykernel        : 6.29.5\nipywidgets       : 8.1.5\njupyter_client   : 8.6.3\njupyter_core     : 5.7.2\njupyter_server   : 2.15.0\njupyterlab       : 4.3.4\nnbclient         : 0.10.2\nnbconvert        : 7.16.5\nnbformat         : 5.10.4\nnotebook         : 7.3.2\nqtconsole        : not installed\ntraitlets        : 5.14.3",
    "crumbs": [
      "Support",
      "Jupyter"
    ]
  },
  {
    "objectID": "computing-psql.html",
    "href": "computing-psql.html",
    "title": "Client psql",
    "section": "",
    "text": "Note\n\n\n\nQuelques possibilités si vous disposez d’une machine sur laquelle on peut installer psql et sur laquelle on peut établir des tunnels ssh\n\n\n\nInstaller\n\nGénéra\nWindows\nMacOS\nUbuntu\n\nDocumentation\n\n\nUtiliser\n\n\n\n\n\n\nÉtablissement d’un tunel SSH sur votre machine (ici sous Linux)\n\n\n\nRemplacer id_ent par votre identifiant ENT dans la suite.\nSaisissez votre mot de passe (attention : pas d’écho)\n$ ssh id_ent@etu-pgsql.math.univ-paris-diderot.fr -L 5436:localhost:5432 -N\n(id_ent@etu-pgsql.math.univ-paris-diderot.fr) Password for id_ent@etu-pgsql.math.univ-paris-diderot.fr:\n$\n\n\n\n\n\n\n\n\nConnexion au serveur PostGres, demander la liste des commandes disponibles\n\n\n\nUtilisez votre tunnel SSH pour accéder au serveur PostGres. Dans une autre fenêtre terminal, lancer psql, saisissez à nouveau votre mot de passe.\n$ psql -p 5436 -U id_ent -W -h localhost -d bd_2023-24\nPassword for id_ent: \n\nbd_2023-24=# \\?  \nVous êtes maintenant dans une session sur le serveur PostGres. Vous êtes connecté au catalogue bd_2023-24\nVous pouvez utiliser une grande partie des commandes magiques de psql\n\n\n\n\n\n\n\n\nChoisir un schéma par défaut (ici world)\n\n\n\nbd_2023-24=# SET search_path TO world ;\nSET\n\n\n\n\n\n\n\n\nLister les tables du schéma par défaut\n\n\n\nbd_2023-24=# \\d\n+--------+-----------------+-------+-----------+\n| Schema | Name            | Type  | Owner     |\n|--------+-----------------+-------+-----------|\n| world  | city            | table | boucheron |\n| world  | country         | table | boucheron |\n| world  | countrylanguage | table | boucheron |\n+--------+-----------------+-------+-----------+\n(3 rows)\n\n\n\n\n\n\n\n\nSchéma d’une table\n\n\n\nbd_2023-24=# \\d city\n \n+-------------+--------------+-----------+\n| Column      | Type         | Modifiers |\n|-------------+--------------+-----------|\n| id          | integer      |  not null |\n| name        | text         |  not null |\n| countrycode | character(3) |  not null |\n| district    | text         |  not null |\n| population  | integer      |  not null |\n+-------------+--------------+-----------+\nIndexes:\n    \"city_pkey\" PRIMARY KEY, btree (id)\nForeign-key constraints:\n    \"city_country_fk\" FOREIGN KEY (countrycode) REFERENCES country(countrycode) ON UPDATE CASCADE ON DELETE SET NULL DEFE&gt;\nReferenced by:\n    TABLE \"country\" CONSTRAINT \"country_capital_fkey\" FOREIGN KEY (capital) REFERENCES city(id)\n\n\n\n\n\n\n\n\nInformations de connexion\n\n\n\nbd_2023-24=# \\conninfo\nYou are connected to database \"bd_2023-24\" as user \"id_end\" on host \"localhost\"  (address \"127.0.0.1\") at port \"5436\".\n\n\n\n\n\n\n\n\nEditer, sauvegarder et exécuter des requêtes\n\n\n\nbd_2023-24=# \\e \n\nSelect an editor.  To change later, run 'select-editor'.\n  1. /bin/nano        &lt;---- easiest\n  2. /usr/bin/vim.basic\n  3. /usr/bin/nvim\n  4. /usr/bin/vim.tiny\n  5. /usr/bin/emacs\n  6. /usr/bin/code\n  7. /bin/ed\nChoose 1-7 [1]: 6\nSous mon éditeur préféré (vs code ici), j’edite une requête\nSELECT ci.name, co.name_country\nFROM \n  world.city ci JOIN \n  world.country co ON (\n    ci.countrycode=co.countrycode AND \n    ci.id = co.capital\n  ) \nORDER BY co.name_country;\nsauvegardée dans un fichier de chemin d’accès /tmp/psql.edit.23866.sql (construit automatiquement)\nDans ma session sur bd_2023-24, je peux maintenant inclure et exécuter cette requête.\nbd_2023-24=# \\i /tmp/psql.edit.23866.sql\n               name                |             name_country              \n-----------------------------------+---------------------------------------\n Kabul                             | Afghanistan\n Tirana                            | Albania\n Alger                             | Algeria\n Fagatogo                          | American Samoa\n Andorra la Vella                  | Andorra\n Luanda                            | Angola\n:\n...\nEntrez q pour sortir du pager\n\n\n\n\n\n\n\n\nUn fichier par TP ?\n\n\n\nIl est commode d’archiver le travail d’une séance de TP dans un fichier *.sql. On peut créer les fichiers avant la session ou en cours de session (ici dans un dialecte d’Unix)\nbd_2023-24=# \\! touch tp-x.sql\nbd_2023-24=# -- editer tp-x.sql\nbd_2023-24=# \\e tp-x.sql \nbd_2023-24=# -- charger/exécuter tp-x.sql\nbd_2023-24=# \\i tp-x.sql\n\n\n\n\nRenseignements utiles\nDocumentation psql)"
  },
  {
    "objectID": "core/notebooks/checking_parquet_citibike.html",
    "href": "core/notebooks/checking_parquet_citibike.html",
    "title": "Check consistency of parquet files",
    "section": "",
    "text": "Code\nimport glob\n\nimport os\nimport sys\nimport re \nimport shutils\n\nimport pandas as pd\nimport numpy as np\n\nimport datetime\n\nimport itertools\n\nimport zipfile\nfrom zipfile import ZipFile\nfrom tqdm import tqdm\n\nimport pyarrow as pa\nimport comet    as co\nimport pyarrow.parquet as pq\nimport pyarrow.dataset as ds\n\nos.environ['PYSPARK_PYTHON'] = sys.executable\nos.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable\n\n\n\n---------------------------------------------------------------------------\nModuleNotFoundError                       Traceback (most recent call last)\nCell In[1], line 6\n      4 import sys\n      5 import re \n----&gt; 6 import shutils\n      8 import pandas as pd\n      9 import numpy as np\n\nModuleNotFoundError: No module named 'shutils'"
  },
  {
    "objectID": "core/notebooks/checking_parquet_citibike.html#paths",
    "href": "core/notebooks/checking_parquet_citibike.html#paths",
    "title": "Check consistency of parquet files",
    "section": "Paths",
    "text": "Paths\n\n\nCode\ndata_dir = \"../data\"\nos.path.exists(data_dir)\n\nextract_dir = os.path.join(data_dir, \"xcitibike\")\nif not os.path.exists(extract_dir):\n    os.mkdir(extract_dir)\n\nparquet_dir = os.path.join(data_dir, \"pq_citibike\")\nif not os.path.exists(parquet_dir):\n    os.mkdir(parquet_dir)\n\n\n\n---------------------------------------------------------------------------\nFileNotFoundError                         Traceback (most recent call last)\nCell In[2], line 6\n      4 extract_dir = os.path.join(data_dir, \"xcitibike\")\n      5 if not os.path.exists(extract_dir):\n----&gt; 6     os.mkdir(extract_dir)\n      8 parquet_dir = os.path.join(data_dir, \"pq_citibike\")\n      9 if not os.path.exists(parquet_dir):\n\nFileNotFoundError: [Errno 2] No such file or directory: '../data/xcitibike'"
  },
  {
    "objectID": "core/notebooks/checking_parquet_citibike.html#spark-session",
    "href": "core/notebooks/checking_parquet_citibike.html#spark-session",
    "title": "Check consistency of parquet files",
    "section": "Spark session",
    "text": "Spark session\n\n\nCode\nfrom pyspark import SparkConf, SparkContext\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql import functions as fn\nfrom pyspark.sql.types import *\n\n\n\n\nCode\nspark = (SparkSession\n    .builder\n    .appName(\"Spark checking citibike parquet file\")\n    .getOrCreate()\n)\n\n\n25/01/15 06:07:57 WARN Utils: Your hostname, boucheron-Precision-5480 resolves to a loopback address: 127.0.1.1; using 192.168.10.120 instead (on interface wlp0s20f3)\n25/01/15 06:07:57 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\nSetting default log level to \"WARN\".\nTo adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n25/01/15 06:07:58 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable"
  },
  {
    "objectID": "core/notebooks/checking_parquet_citibike.html#try-to-load-parquet-file",
    "href": "core/notebooks/checking_parquet_citibike.html#try-to-load-parquet-file",
    "title": "Check consistency of parquet files",
    "section": "Try to load parquet file",
    "text": "Try to load parquet file\n\n\nCode\nsch_1 = StructType([\n    StructField('trip_duration', StringType(), True), \n    StructField('started_at', TimestampType(), True), \n    StructField('ended_at', TimestampType(), True), \n    StructField('start_station_id', StringType(), True), \n    StructField('start_station_name', StringType(), True), \n    StructField('start_lat', StringType(), True), \n    StructField('start_lng', StringType(), True), \n    StructField('end_station_id', StringType(), True), \n    StructField('end_station_name', StringType(), True), \n    StructField('end_lat', StringType(), True), \n    StructField('end_lng', StringType(), True), \n    StructField('bike_id', StringType(), True), \n    StructField('user_type', StringType(), True), \n    StructField('birth_year', StringType(), True), \n    StructField('gender', StringType(), True), \n    StructField('start_year', IntegerType(), True), \n    StructField('start_month', IntegerType(), True)\n    ]\n)\n\n\n\n\nCode\ninput_file = os.path.join(parquet_dir, 'start_year=2022')\n\ndf = ( \n    spark.read\n        .option(\"mergeSchema\", \"true\")\n        .parquet(parquet_dir)\n)\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[6], line 1\n----&gt; 1 input_file = os.path.join(parquet_dir, 'start_year=2022')\n      3 df = ( \n      4     spark.read\n      5         .option(\"mergeSchema\", \"true\")\n      6         .parquet(parquet_dir)\n      7 )\n\nNameError: name 'parquet_dir' is not defined\n\n\n\n\n\nCode\ndf.printSchema()\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[7], line 1\n----&gt; 1 df.printSchema()\n\nNameError: name 'df' is not defined\n\n\n\nroot\n |-- trip_duration: string (nullable = true)\n |-- started_at: timestamp (nullable = true)\n |-- ended_at: timestamp (nullable = true)\n |-- start_station_id: string (nullable = true)\n |-- start_station_name: string (nullable = true)\n |-- start_lat: string (nullable = true)\n |-- start_lng: string (nullable = true)\n |-- end_station_id: string (nullable = true)\n |-- end_station_name: string (nullable = true)\n |-- end_lat: string (nullable = true)\n |-- end_lng: string (nullable = true)\n |-- bike_id: string (nullable = true)\n |-- user_type: string (nullable = true)\n |-- birth_year: string (nullable = true)\n |-- gender: string (nullable = true)\n |-- ride_id: string (nullable = true)\n |-- rideable_type: string (nullable = true)\n |-- member_casual: string (nullable = true)\n |-- start_year: integer (nullable = true)\n |-- start_month: integer (nullable = true)\n\n\nCode\ndf.select([\"started_at\", \"ended_at\"]).show(5)\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[8], line 1\n----&gt; 1 df.select([\"started_at\", \"ended_at\"]).show(5)\n\nNameError: name 'df' is not defined\n\n\n\n\n\nCode\nprint(df.rdd.toDebugString().decode(\"utf-8\"))\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[9], line 1\n----&gt; 1 print(df.rdd.toDebugString().decode(\"utf-8\"))\n\nNameError: name 'df' is not defined\n\n\n\n\n\nCode\ndf.rdd.getNumPartitions()\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[10], line 1\n----&gt; 1 df.rdd.getNumPartitions()\n\nNameError: name 'df' is not defined\n\n\n\n\n\nCode\ndf_pd = df.groupBy(['start_year', 'start_month']).count().toPandas()\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[11], line 1\n----&gt; 1 df_pd = df.groupBy(['start_year', 'start_month']).count().toPandas()\n\nNameError: name 'df' is not defined\n\n\n\n\n\nCode\ndf_pd.sort_values(by=['start_year', 'start_month'])\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[12], line 1\n----&gt; 1 df_pd.sort_values(by=['start_year', 'start_month'])\n\nNameError: name 'df_pd' is not defined\n\n\n\n\n\nCode\n(\n    df\n        .groupBy('rideable_type')\n        .agg(fn.count('started_at'))\n        .show()\n)\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[13], line 2\n      1 (\n----&gt; 2     df\n      3         .groupBy('rideable_type')\n      4         .agg(fn.count('started_at'))\n      5         .show()\n      6 )\n\nNameError: name 'df' is not defined\n\n\n\n\n\nCode\nspark.stop()\n\n\n\n\nCode\nfrom pyspark.sql.types import *\n\n\n\n\nCode\nprint(sch_1)\n\n\nStructType([StructField('trip_duration', StringType(), True), StructField('started_at', TimestampType(), True), StructField('ended_at', TimestampType(), True), StructField('start_station_id', StringType(), True), StructField('start_station_name', StringType(), True), StructField('start_lat', StringType(), True), StructField('start_lng', StringType(), True), StructField('end_station_id', StringType(), True), StructField('end_station_name', StringType(), True), StructField('end_lat', StringType(), True), StructField('end_lng', StringType(), True), StructField('bike_id', StringType(), True), StructField('user_type', StringType(), True), StructField('birth_year', StringType(), True), StructField('gender', StringType(), True), StructField('start_year', IntegerType(), True), StructField('start_month', IntegerType(), True)])"
  },
  {
    "objectID": "core/notebooks/notebook01_python.html",
    "href": "core/notebooks/notebook01_python.html",
    "title": "Introduction to Python",
    "section": "",
    "text": "We introduce here the python language. Only the bare minimum necessary for getting started with the data-science stack (a bunch of libraries for data science). Python is a programming language, as are C++, java, fortran, javascript, etc."
  },
  {
    "objectID": "core/notebooks/notebook01_python.html#specific-features-of-python",
    "href": "core/notebooks/notebook01_python.html#specific-features-of-python",
    "title": "Introduction to Python",
    "section": "Specific features of Python",
    "text": "Specific features of Python\n\nan interpreted (as opposed to compiled) language. Contrary to e.g. C++ or fortran, one does not compile Python code before executing it.\nUsed as a scripting language, by python python script.py in a terminal\nBut can be used also interactively: the jupyter notebook, iPython, etc.\nA free software released under an open-source license: Python can be used and distributed free of charge, even for building commercial software.\nmulti-platform: Python is available for all major operating systems, Windows, Linux/Unix, MacOS X, most likely your mobile phone OS, etc.\nA very readable language with clear non-verbose syntax\nA language for which a large amount of high-quality packages are available for various applications, including web-frameworks and scientific computing\nIt has been one of the top languages for data science and machine learning for several years, because it is expressive and and easy to deploy\nAn object-oriented language\n\nSee https://www.python.org/about/ for more information about distinguishing features of Python.\n\n\n\n\n\n\nPython 2 or Python 3?\n\n\n\n\nSimple answer: don’t use Python 2, use Python 3\nPython 2 is mostly deprecated and has not been maintained for years\nYou’ll end up hanged if you use Python 2\nIf Python 2 is mandatory at your workplace, find another work\n\n\n\n\n\n\n\n\n\nJupyter or Quarto notebooks?\n\n\n\n\nquarto is more git friendly than jupyter\nEnjoy authentic editors\nGo for quarto"
  },
  {
    "objectID": "core/notebooks/notebook01_python.html#integers",
    "href": "core/notebooks/notebook01_python.html#integers",
    "title": "Introduction to Python",
    "section": "Integers",
    "text": "Integers\n\n\nCode\n1 + 42\n\n\n43\n\n\n\n\nCode\ntype(1+1)\n\n\nint\n\n\nWe can assign values to variables with =\n\n\nCode\na = (3 + 5 ** 2) % 4\na\n\n\n0"
  },
  {
    "objectID": "core/notebooks/notebook01_python.html#remark",
    "href": "core/notebooks/notebook01_python.html#remark",
    "title": "Introduction to Python",
    "section": "Remark",
    "text": "Remark\nWe don’t declare the type of a variable before assigning its value. In C, conversely, one should write\nint a = 4;"
  },
  {
    "objectID": "core/notebooks/notebook01_python.html#something-cool",
    "href": "core/notebooks/notebook01_python.html#something-cool",
    "title": "Introduction to Python",
    "section": "Something cool",
    "text": "Something cool\n\nArbitrary large integer arithmetics\n\n\n\nCode\n17 ** 542\n\n\n8004153099680695240677662228684856314409365427758266999205063931175132640587226837141154215226851187899067565063096026317140186260836873939218139105634817684999348008544433671366043519135008200013865245747791955240844192282274023825424476387832943666754140847806277355805648624376507618604963106833797989037967001806494232055319953368448928268857747779203073913941756270620192860844700087001827697624308861431399538404552468712313829522630577767817531374612262253499813723569981496051353450351968993644643291035336065584116155321928452618573467361004489993801594806505273806498684433633838323916674207622468268867047187858269410016150838175127772100983052010703525089"
  },
  {
    "objectID": "core/notebooks/notebook01_python.html#floats",
    "href": "core/notebooks/notebook01_python.html#floats",
    "title": "Introduction to Python",
    "section": "Floats",
    "text": "Floats\nThere exists a floating point type that is created when the variable has decimal values\n\n\nCode\nc = 2.\n\n\n\n\nCode\ntype(c)\n\n\nfloat\n\n\n\n\nCode\nc = 2\ntype(c)\n\n\nint\n\n\n\n\nCode\ntruc = 1 / 2\ntruc\n\n\n0.5\n\n\n\n\nCode\n1 // 2 + 1 % 2\n\n\n1\n\n\n\n\nCode\ntype(truc)\n\n\nfloat"
  },
  {
    "objectID": "core/notebooks/notebook01_python.html#boolean",
    "href": "core/notebooks/notebook01_python.html#boolean",
    "title": "Introduction to Python",
    "section": "Boolean",
    "text": "Boolean\nSimilarly, boolean types are created from a comparison\n\n\nCode\ntest = 3 &gt; 4\ntest\n\n\nFalse\n\n\n\n\nCode\ntype(test)\n\n\nbool\n\n\n\n\nCode\nFalse == (not True)\n\n\nTrue\n\n\n\n\nCode\n1.41 &lt; 2.71 and 2.71 &lt; 3.14\n\n\nTrue\n\n\n\n\nCode\n# It's equivalent to\n1.41 &lt; 2.71 &lt; 3.14\n\n\nTrue"
  },
  {
    "objectID": "core/notebooks/notebook01_python.html#type-conversion-casting",
    "href": "core/notebooks/notebook01_python.html#type-conversion-casting",
    "title": "Introduction to Python",
    "section": "Type conversion (casting)",
    "text": "Type conversion (casting)\n\n\nCode\na = 1\ntype(a)\n\n\nint\n\n\n\n\nCode\nb = float(a)\ntype(b)\n\n\nfloat\n\n\n\n\nCode\nstr(b)\n\n\n'1.0'\n\n\n\n\nCode\nbool(b)\n# All non-zero, non empty objects are casted to boolean as True (more later)\n\n\nTrue\n\n\n\n\nCode\nbool(1-1)\n\n\nFalse"
  },
  {
    "objectID": "core/notebooks/notebook01_python.html#tuples",
    "href": "core/notebooks/notebook01_python.html#tuples",
    "title": "Introduction to Python",
    "section": "Tuples",
    "text": "Tuples\n\n\nCode\ntt = 'truc', 3.14, \"truc\"\ntt\n\n\n('truc', 3.14, 'truc')\n\n\n\n\nCode\ntt[0]\n\n\n'truc'\n\n\nYou can’t change a tuple, we say that it’s immutable\n\n\nCode\ntry:\n    tt[0] = 1\nexcept TypeError:\n    print(f\"TypeError: 'tuple' object does not support item assignment\")\n\n\nTypeError: 'tuple' object does not support item assignment\n\n\nThree ways of doing the same thing\n\n\nCode\n# Method 1\ntuple([1, 2, 3])\n\n\n(1, 2, 3)\n\n\n\n\nCode\n# Method 2\n1, 2, 3\n\n\n(1, 2, 3)\n\n\n\n\nCode\n# Method 3\n(1, 2, 3)\n\n\n(1, 2, 3)\n\n\nSimpler is better in Python, so usually you want to use Method 2.\n\n\nCode\ntoto = 1, 2, 3\ntoto\n\n\n(1, 2, 3)\n\n\n\nThis is serious !"
  },
  {
    "objectID": "core/notebooks/notebook01_python.html#the-zen-of-python-easters-egg",
    "href": "core/notebooks/notebook01_python.html#the-zen-of-python-easters-egg",
    "title": "Introduction to Python",
    "section": "The Zen of Python easter’s egg",
    "text": "The Zen of Python easter’s egg\n\n\nCode\nimport this\n\n\nThe Zen of Python, by Tim Peters\n\nBeautiful is better than ugly.\nExplicit is better than implicit.\nSimple is better than complex.\nComplex is better than complicated.\nFlat is better than nested.\nSparse is better than dense.\nReadability counts.\nSpecial cases aren't special enough to break the rules.\nAlthough practicality beats purity.\nErrors should never pass silently.\nUnless explicitly silenced.\nIn the face of ambiguity, refuse the temptation to guess.\nThere should be one-- and preferably only one --obvious way to do it.\nAlthough that way may not be obvious at first unless you're Dutch.\nNow is better than never.\nAlthough never is often better than *right* now.\nIf the implementation is hard to explain, it's a bad idea.\nIf the implementation is easy to explain, it may be a good idea.\nNamespaces are one honking great idea -- let's do more of those!"
  },
  {
    "objectID": "core/notebooks/notebook01_python.html#lists",
    "href": "core/notebooks/notebook01_python.html#lists",
    "title": "Introduction to Python",
    "section": "Lists",
    "text": "Lists\nA list is an ordered collection of objects. These objects may have different types. For example:\n\n\nCode\ncolors = ['red', 'blue', 'green', 'black', 'white']\n\n\n\n\nCode\ncolors[0]\n\n\n'red'\n\n\n\n\nCode\ntype(colors)\n\n\nlist\n\n\nIndexing: accessing individual objects contained in the list by their position\n\n\nCode\ncolors[2]\n\n\n'green'\n\n\n\n\nCode\ncolors[2] = 3.14\ncolors\n\n\n['red', 'blue', 3.14, 'black', 'white']\n\n\n\n\n\n\n\n\nWarning\n\n\n\nFor any iterable object in Python, indexing starts at 0 (as in C), not at 1 (as in Fortran, R, or Matlab).\n\n\nCounting from the end with negative indices:\n\n\nCode\ncolors[-1]\n\n\n'white'\n\n\nIndex must remain in the range of the list\n\n\nCode\ntry:\n    colors[10]\nexcept IndexError:\n    print(f\"IndexError: 10 &gt;= {len(colors)} ==len(colors), index out of range \")\n\n\n\n\nCode\ncolors\n\n\n['red', 'blue', 3.14, 'black', 'white']\n\n\n\n\nCode\ntt\n\n\n('truc', 3.14, 'truc')\n\n\n\n\nCode\ncolors.append(tt)\ncolors\n\n\n['red', 'blue', 3.14, 'black', 'white', ('truc', 3.14, 'truc')]\n\n\n\n\nCode\nlen(colors)\n\n\n6\n\n\n\n\nCode\nlen(tt)\n\n\n3"
  },
  {
    "objectID": "core/notebooks/notebook01_python.html#slicing-obtaining-sublists-of-regularly-spaced-elements",
    "href": "core/notebooks/notebook01_python.html#slicing-obtaining-sublists-of-regularly-spaced-elements",
    "title": "Introduction to Python",
    "section": "Slicing: obtaining sublists of regularly-spaced elements",
    "text": "Slicing: obtaining sublists of regularly-spaced elements\nThis work with anything iterable whenever it makes sense (list, str, tuple, etc.)\n\n\nCode\ncolors\n\n\n['red', 'blue', 3.14, 'black', 'white', ('truc', 3.14, 'truc')]\n\n\n\n\nCode\nlist(reversed(colors))\n\n\n[('truc', 3.14, 'truc'), 'white', 'black', 3.14, 'blue', 'red']\n\n\n\n\nCode\ncolors[::-1]\n\n\n[('truc', 3.14, 'truc'), 'white', 'black', 3.14, 'blue', 'red']\n\n\n\n\n\n\n\n\nSlicing syntax:\n\n\n\ncolors[start:stop:stride]\nstart, stop, stride are optional, with default values 0, len(sequence), 1\n\n\nl\n\n\nCode\nprint(slice(4))\nprint(slice(1,5))\nprint(slice(None,13,3))\n\n\nslice(None, 4, None)\nslice(1, 5, None)\nslice(None, 13, 3)\n\n\n\n\nCode\nsl = slice(1,5,2)\ncolors[sl]\n\n\n['blue', 'black']\n\n\n\n\nCode\ncolors\n\n\n['red', 'blue', 3.14, 'black', 'white', ('truc', 3.14, 'truc')]\n\n\n\n\nCode\ncolors[3:]\n\n\n['black', 'white', ('truc', 3.14, 'truc')]\n\n\n\n\nCode\ncolors[:3]\n\n\n['red', 'blue', 3.14]\n\n\n\n\nCode\ncolors[1::2]\n\n\n['blue', 'black', ('truc', 3.14, 'truc')]\n\n\n\n\nCode\ncolors[::-1]\n\n\n[('truc', 3.14, 'truc'), 'white', 'black', 3.14, 'blue', 'red']"
  },
  {
    "objectID": "core/notebooks/notebook01_python.html#strings",
    "href": "core/notebooks/notebook01_python.html#strings",
    "title": "Introduction to Python",
    "section": "Strings",
    "text": "Strings\nDifferent string syntaxes (simple, double or triple quotes):\n\n\nCode\ns = 'tintin'\ntype(s)\n\n\nstr\n\n\n\n\nCode\ns\n\n\n'tintin'\n\n\n\n\nCode\ns = \"\"\"         Bonjour,\nJe m'appelle Stephane.\nJe vous souhaite une bonne journée.\nSalut.       \n\"\"\"\ns\n\n\n\"         Bonjour,\\nJe m'appelle Stephane.\\nJe vous souhaite une bonne journée.\\nSalut.       \\n\"\n\n\n\n\nCode\ns.strip()\n\n\n\"Bonjour,\\nJe m'appelle Stephane.\\nJe vous souhaite une bonne journée.\\nSalut.\"\n\n\n\n\nCode\nprint(s.strip())\n\n\nBonjour,\nJe m'appelle Stephane.\nJe vous souhaite une bonne journée.\nSalut.\n\n\n\n\nCode\nlen(s)\n\n\n91\n\n\n\n\nCode\n# Casting to a list\nlist(s.strip()[:15])\n\n\n['B', 'o', 'n', 'j', 'o', 'u', 'r', ',', '\\n', 'J', 'e', ' ', 'm', \"'\", 'a']\n\n\n\n\nCode\n# Arithmetics\nprint('Bonjour' * 2)\nprint('Hello' + ' all')\n\n\nBonjourBonjour\nHello all\n\n\n\n\nCode\nsss = 'A'\nsss += 'bc'\nsss += 'dE'\nsss.lower()\n\n\n'abcde'\n\n\n\n\nCode\nss = s.strip()\nprint(ss[:10] + ss[24:28])\n\n\nBonjour,\nJepha\n\n\n\n\nCode\ns.strip()\n\n\n\"Bonjour,\\nJe m'appelle Stephane.\\nJe vous souhaite une bonne journée.\\nSalut.\"\n\n\n\n\nCode\ns.strip().split('\\n')\n\n\n['Bonjour,',\n \"Je m'appelle Stephane.\",\n 'Je vous souhaite une bonne journée.',\n 'Salut.']\n\n\n\n\nCode\ns[::3]\n\n\n'   BjrJmpl ea.eo ui eoeon.at  \\n'\n\n\n\n\nCode\ns[3:10]\n\n\n'      B'\n\n\n\n\nCode\n\" \".join(['Il', 'fait', 'super', 'beau', \"aujourd'hui\"])\n\n\n\"Il fait super beau aujourd'hui\"\n\n\nChaining method calls is the basic of pipeline building.\n\n\nCode\n( \n    \" \".join(['Il', 'fait', 'super', 'beau', \"aujourd'hui\"])\n       .title()\n       .replace(' ', '')\n       .replace(\"'\",\"\")\n)\n\n\n'IlFaitSuperBeauAujourdHui'\n\n\n\nImportant\nA string is immutable !!\n\n\nCode\ns = 'I am an immutable guy'\n\n\n\n\nCode\ntry:  \n    s[2] = 's'\nexcept TypeError:\n    print(f\"Strings are immutable! s is still '{s}'\")\n\n\nStrings are immutable! s is still 'I am an immutable guy'\n\n\n\n\nCode\nid(s)\n\n\n127235522000496\n\n\n\n\nCode\nprint(s + ', for sure')\nid(s), id(s + ' for sure')\n\n\nI am an immutable guy, for sure\n\n\n(127235522000496, 127235521820464)\n\n\n\n\nExtra stuff with strings\n\n\nCode\n'square of 2 is ' + str(2 ** 2)\n\n\n'square of 2 is 4'\n\n\n\n\nCode\n'square of 2 is %d' % 2 ** 2\n\n\n'square of 2 is 4'\n\n\n\n\nCode\n'square of 2 is {}'.format(2 ** 2)\n\n\n'square of 2 is 4'\n\n\n\n\nCode\n'square of 2 is {square}'.format(square=2 ** 2)\n\n\n'square of 2 is 4'\n\n\n\n\nCode\n# And since Python 3.6 you can use an `f-string`\nnumber = 2\nsquare = number ** 2\n\nf'square of {number} is {square}'\n\n\n'square of 2 is 4'\n\n\n\n\nThe in keyword\nYou can use the in keyword with any container, whenever it makes sense\n\n\nCode\nprint(s)\nprint('Salut' in s)\n\n\nI am an immutable guy\nFalse\n\n\n\n\nCode\nprint(tt)\nprint('truc' in tt)\n\n\n('truc', 3.14, 'truc')\nTrue\n\n\n\n\nCode\nprint(colors)\nprint('truc' in colors)\n\n\n['red', 'blue', 3.14, 'black', 'white', ('truc', 3.14, 'truc')]\nFalse\n\n\n\n\nCode\n('truc', 3.14, 'truc') in colors\n\n\nTrue\n\n\n\n\n\n\n\n\nWarning\n\n\n\nStrings are not bytes. Have a look at chapter 4 Unicode Text versus Bytes in Fluent Python\n\n\n\n\nBrain-teasing\nExplain this weird behaviour:\n\n\nCode\n5 in [1, 2, 3, 4] == False\n\n\nFalse\n\n\n\n\nCode\n[1, 2, 3, 4] == False\n\n\nFalse\n\n\n\n\nCode\n5 not in [1, 2, 3, 4]\n\n\nTrue\n\n\n\n\nCode\n(5 in [1, 2, 3, 4]) == False\n\n\nTrue\n\n\n\n\nCode\n# ANSWER.\n# This is a chained comparison. We have seen that \n1 &lt; 2 &lt; 3\n# is equivalent to\n(1 &lt; 2) and (2 &lt; 3)\n# so that\n5 in [1, 2, 3, 4] == False\n# is equivalent to\n(5 in [1, 2, 3, 4]) and ([1, 2, 3, 4] == False)\n\n\nFalse\n\n\n\n\nCode\n(5 in [1, 2, 3, 4])\n\n\nFalse\n\n\n\n\nCode\n([1, 2, 3, 4] == False)\n\n\nFalse"
  },
  {
    "objectID": "core/notebooks/notebook01_python.html#dictionaries",
    "href": "core/notebooks/notebook01_python.html#dictionaries",
    "title": "Introduction to Python",
    "section": "Dictionaries",
    "text": "Dictionaries\n\nA dictionary is basically an efficient table that maps keys to values.\nThe MOST important container in Python.\nMany things are actually a dict under the hood in Python\n\n\n\nCode\ntel = {'emmanuelle': 5752, 'sebastian': 5578}\nprint(tel)\nprint(type(tel))\n\n\n{'emmanuelle': 5752, 'sebastian': 5578}\n&lt;class 'dict'&gt;\n\n\n\n\nCode\ntel['emmanuelle'], tel['sebastian']\n\n\n(5752, 5578)\n\n\n\n\nCode\ntel['francis'] = '5919'\ntel\n\n\n{'emmanuelle': 5752, 'sebastian': 5578, 'francis': '5919'}\n\n\n\n\nCode\nlen(tel)\n\n\n3\n\n\n\nImportant remarks\n\nKeys can be of different types\nA key must be of immutable type\n\n\n\nCode\ntel[7162453] = [1, 3, 2]\ntel[3.14] = 'bidule'\ntel[('jaouad', 2)] = 1234\ntel\n\n\n{'emmanuelle': 5752,\n 'sebastian': 5578,\n 'francis': '5919',\n 7162453: [1, 3, 2],\n 3.14: 'bidule',\n ('jaouad', 2): 1234}\n\n\n\n\nCode\ntry:\n    sorted(tel)\nexcept TypeError:\n    print(\"TypeError: '&lt;' not supported between instances of 'int' and 'str'\")    \n\n\nTypeError: '&lt;' not supported between instances of 'int' and 'str'\n\n\n\n\nCode\n# A list is mutable and not hashable\ntry:\n    tel[['jaouad']] = '5678'\nexcept TypeError:\n    print(\"TypeError: unhashable type: 'list'\")\n\n\nTypeError: unhashable type: 'list'\n\n\n\n\nCode\ntry:\n    tel[2]\nexcept KeyError:\n    print(\"KeyError: 2\")\n\n\nKeyError: 2\n\n\n\n\nCode\ntel = {'emmanuelle': 5752, 'sebastian' : 5578, 'jaouad' : 1234}\nprint(tel.keys())\nprint(tel.values())\nprint(tel.items())\n\n\ndict_keys(['emmanuelle', 'sebastian', 'jaouad'])\ndict_values([5752, 5578, 1234])\ndict_items([('emmanuelle', 5752), ('sebastian', 5578), ('jaouad', 1234)])\n\n\n\n\nCode\nlist(tel.keys())[2]\n\n\n'jaouad'\n\n\n\n\nCode\ntel.values().mapping\n\n\nmappingproxy({'emmanuelle': 5752, 'sebastian': 5578, 'jaouad': 1234})\n\n\n\n\nCode\ntype(tel.keys())\n\n\ndict_keys\n\n\n\n\nCode\n'rémi' in tel\n\n\nFalse\n\n\n\n\nCode\nlist(tel)\n\n\n['emmanuelle', 'sebastian', 'jaouad']\n\n\n\n\nCode\n'rémi' in tel.keys()\n\n\nFalse\n\n\nYou can swap values like this\n\n\nCode\nprint(tel)\ntel['emmanuelle'], tel['sebastian'] = tel['sebastian'], tel['emmanuelle']\nprint(tel)\n\n\n{'emmanuelle': 5752, 'sebastian': 5578, 'jaouad': 1234}\n{'emmanuelle': 5578, 'sebastian': 5752, 'jaouad': 1234}\n\n\n\n\nCode\n# It works, since\na, b = 2.71, 3.14\na, b = b, a\na, b\n\n\n(3.14, 2.71)\n\n\n\n\nExercise 1\nGet keys of tel sorted by decreasing order\n\n\nCode\ntel = {'emmanuelle': 5752, 'sebastian' : 5578, 'jaouad' : 1234}\n\n\n\n\nExercise 2\nGet keys of tel sorted by increasing values\n\n\nCode\ntel = {'emmanuelle': 5752, 'sebastian' : 5578, 'jaouad' : 1234}\n\n\n\n\nExercise 3\nObtain a sorted-by-key version of tel\n\n\nCode\ntel = {'emmanuelle': 5752, 'sebastian' : 5578, 'jaouad' : 1234}"
  },
  {
    "objectID": "core/notebooks/notebook01_python.html#sets",
    "href": "core/notebooks/notebook01_python.html#sets",
    "title": "Introduction to Python",
    "section": "Sets",
    "text": "Sets\nA set is an unordered container, containing unique elements\n\n\nCode\nss = {1, 2, 2, 2, 3, 3, 'tintin', 'tintin', 'toto'}\nss\n\n\n{1, 2, 3, 'tintin', 'toto'}\n\n\n\n\nCode\ns = 'truc truc bidule truc'\nset(s)\n\n\n{' ', 'b', 'c', 'd', 'e', 'i', 'l', 'r', 't', 'u'}\n\n\n\n\nCode\nset(list(s))\n\n\n{' ', 'b', 'c', 'd', 'e', 'i', 'l', 'r', 't', 'u'}\n\n\n\n\nCode\n{1, 5, 2, 1, 1}.union({1, 2, 3})\n\n\n{1, 2, 3, 5}\n\n\n\n\nCode\nset((1, 5, 3, 2))\n\n\n{1, 2, 3, 5}\n\n\n\n\nCode\nset([1, 5, 2, 1, 1]).intersection(set([1, 2, 3]))\n\n\n{1, 2}\n\n\n\n\nCode\nss.add('tintin')\nss\n\n\n{1, 2, 3, 'tintin', 'toto'}\n\n\n\n\nCode\nss.difference(range(6))\n\n\n{'tintin', 'toto'}\n\n\nYou can combine all containers together\n\n\nCode\ndd = {\n    'truc': [1, 2, 3], \n    5: (1, 4, 2),\n    (1, 3): {'hello', 'world'}\n}\ndd\n\n\n{'truc': [1, 2, 3], 5: (1, 4, 2), (1, 3): {'hello', 'world'}}"
  },
  {
    "objectID": "core/notebooks/notebook01_python.html#everything-is-either-mutable-or-immutable",
    "href": "core/notebooks/notebook01_python.html#everything-is-either-mutable-or-immutable",
    "title": "Introduction to Python",
    "section": "Everything is either mutable or immutable",
    "text": "Everything is either mutable or immutable\n\n\nCode\nss = {1, 2, 3}\nsss = ss\nsss, ss\n\n\n({1, 2, 3}, {1, 2, 3})\n\n\n\n\nCode\nid(ss), id(sss)\n\n\n(127233984671840, 127233984671840)\n\n\n\n\nCode\nsss.add(\"Truc\")\n\n\nQuestion. What is in ss ?\n\n\nCode\nss, sss\n\n\n({1, 2, 3, 'Truc'}, {1, 2, 3, 'Truc'})\n\n\nss and sss are names for the same object\n\n\nCode\nid(ss), id(sss)\n\n\n(127233984671840, 127233984671840)\n\n\n\n\nCode\nss is sss\n\n\nTrue\n\n\n\n\nCode\nhelp('is')\n\n\nComparisons\n***********\n\nUnlike C, all comparison operations in Python have the same priority,\nwhich is lower than that of any arithmetic, shifting or bitwise\noperation.  Also unlike C, expressions like \"a &lt; b &lt; c\" have the\ninterpretation that is conventional in mathematics:\n\n   comparison    ::= or_expr (comp_operator or_expr)*\n   comp_operator ::= \"&lt;\" | \"&gt;\" | \"==\" | \"&gt;=\" | \"&lt;=\" | \"!=\"\n                     | \"is\" [\"not\"] | [\"not\"] \"in\"\n\nComparisons yield boolean values: \"True\" or \"False\". Custom *rich\ncomparison methods* may return non-boolean values. In this case Python\nwill call \"bool()\" on such value in boolean contexts.\n\nComparisons can be chained arbitrarily, e.g., \"x &lt; y &lt;= z\" is\nequivalent to \"x &lt; y and y &lt;= z\", except that \"y\" is evaluated only\nonce (but in both cases \"z\" is not evaluated at all when \"x &lt; y\" is\nfound to be false).\n\nFormally, if *a*, *b*, *c*, …, *y*, *z* are expressions and *op1*,\n*op2*, …, *opN* are comparison operators, then \"a op1 b op2 c ... y\nopN z\" is equivalent to \"a op1 b and b op2 c and ... y opN z\", except\nthat each expression is evaluated at most once.\n\nNote that \"a op1 b op2 c\" doesn’t imply any kind of comparison between\n*a* and *c*, so that, e.g., \"x &lt; y &gt; z\" is perfectly legal (though\nperhaps not pretty).\n\n\nValue comparisons\n=================\n\nThe operators \"&lt;\", \"&gt;\", \"==\", \"&gt;=\", \"&lt;=\", and \"!=\" compare the values\nof two objects.  The objects do not need to have the same type.\n\nChapter Objects, values and types states that objects have a value (in\naddition to type and identity).  The value of an object is a rather\nabstract notion in Python: For example, there is no canonical access\nmethod for an object’s value.  Also, there is no requirement that the\nvalue of an object should be constructed in a particular way, e.g.\ncomprised of all its data attributes. Comparison operators implement a\nparticular notion of what the value of an object is.  One can think of\nthem as defining the value of an object indirectly, by means of their\ncomparison implementation.\n\nBecause all types are (direct or indirect) subtypes of \"object\", they\ninherit the default comparison behavior from \"object\".  Types can\ncustomize their comparison behavior by implementing *rich comparison\nmethods* like \"__lt__()\", described in Basic customization.\n\nThe default behavior for equality comparison (\"==\" and \"!=\") is based\non the identity of the objects.  Hence, equality comparison of\ninstances with the same identity results in equality, and equality\ncomparison of instances with different identities results in\ninequality.  A motivation for this default behavior is the desire that\nall objects should be reflexive (i.e. \"x is y\" implies \"x == y\").\n\nA default order comparison (\"&lt;\", \"&gt;\", \"&lt;=\", and \"&gt;=\") is not provided;\nan attempt raises \"TypeError\".  A motivation for this default behavior\nis the lack of a similar invariant as for equality.\n\nThe behavior of the default equality comparison, that instances with\ndifferent identities are always unequal, may be in contrast to what\ntypes will need that have a sensible definition of object value and\nvalue-based equality.  Such types will need to customize their\ncomparison behavior, and in fact, a number of built-in types have done\nthat.\n\nThe following list describes the comparison behavior of the most\nimportant built-in types.\n\n* Numbers of built-in numeric types (Numeric Types — int, float,\n  complex) and of the standard library types \"fractions.Fraction\" and\n  \"decimal.Decimal\" can be compared within and across their types,\n  with the restriction that complex numbers do not support order\n  comparison.  Within the limits of the types involved, they compare\n  mathematically (algorithmically) correct without loss of precision.\n\n  The not-a-number values \"float('NaN')\" and \"decimal.Decimal('NaN')\"\n  are special.  Any ordered comparison of a number to a not-a-number\n  value is false. A counter-intuitive implication is that not-a-number\n  values are not equal to themselves.  For example, if \"x =\n  float('NaN')\", \"3 &lt; x\", \"x &lt; 3\" and \"x == x\" are all false, while \"x\n  != x\" is true.  This behavior is compliant with IEEE 754.\n\n* \"None\" and \"NotImplemented\" are singletons.  **PEP 8** advises that\n  comparisons for singletons should always be done with \"is\" or \"is\n  not\", never the equality operators.\n\n* Binary sequences (instances of \"bytes\" or \"bytearray\") can be\n  compared within and across their types.  They compare\n  lexicographically using the numeric values of their elements.\n\n* Strings (instances of \"str\") compare lexicographically using the\n  numerical Unicode code points (the result of the built-in function\n  \"ord()\") of their characters. [3]\n\n  Strings and binary sequences cannot be directly compared.\n\n* Sequences (instances of \"tuple\", \"list\", or \"range\") can be compared\n  only within each of their types, with the restriction that ranges do\n  not support order comparison.  Equality comparison across these\n  types results in inequality, and ordering comparison across these\n  types raises \"TypeError\".\n\n  Sequences compare lexicographically using comparison of\n  corresponding elements.  The built-in containers typically assume\n  identical objects are equal to themselves.  That lets them bypass\n  equality tests for identical objects to improve performance and to\n  maintain their internal invariants.\n\n  Lexicographical comparison between built-in collections works as\n  follows:\n\n  * For two collections to compare equal, they must be of the same\n    type, have the same length, and each pair of corresponding\n    elements must compare equal (for example, \"[1,2] == (1,2)\" is\n    false because the type is not the same).\n\n  * Collections that support order comparison are ordered the same as\n    their first unequal elements (for example, \"[1,2,x] &lt;= [1,2,y]\"\n    has the same value as \"x &lt;= y\").  If a corresponding element does\n    not exist, the shorter collection is ordered first (for example,\n    \"[1,2] &lt; [1,2,3]\" is true).\n\n* Mappings (instances of \"dict\") compare equal if and only if they\n  have equal \"(key, value)\" pairs. Equality comparison of the keys and\n  values enforces reflexivity.\n\n  Order comparisons (\"&lt;\", \"&gt;\", \"&lt;=\", and \"&gt;=\") raise \"TypeError\".\n\n* Sets (instances of \"set\" or \"frozenset\") can be compared within and\n  across their types.\n\n  They define order comparison operators to mean subset and superset\n  tests.  Those relations do not define total orderings (for example,\n  the two sets \"{1,2}\" and \"{2,3}\" are not equal, nor subsets of one\n  another, nor supersets of one another).  Accordingly, sets are not\n  appropriate arguments for functions which depend on total ordering\n  (for example, \"min()\", \"max()\", and \"sorted()\" produce undefined\n  results given a list of sets as inputs).\n\n  Comparison of sets enforces reflexivity of its elements.\n\n* Most other built-in types have no comparison methods implemented, so\n  they inherit the default comparison behavior.\n\nUser-defined classes that customize their comparison behavior should\nfollow some consistency rules, if possible:\n\n* Equality comparison should be reflexive. In other words, identical\n  objects should compare equal:\n\n     \"x is y\" implies \"x == y\"\n\n* Comparison should be symmetric. In other words, the following\n  expressions should have the same result:\n\n     \"x == y\" and \"y == x\"\n\n     \"x != y\" and \"y != x\"\n\n     \"x &lt; y\" and \"y &gt; x\"\n\n     \"x &lt;= y\" and \"y &gt;= x\"\n\n* Comparison should be transitive. The following (non-exhaustive)\n  examples illustrate that:\n\n     \"x &gt; y and y &gt; z\" implies \"x &gt; z\"\n\n     \"x &lt; y and y &lt;= z\" implies \"x &lt; z\"\n\n* Inverse comparison should result in the boolean negation. In other\n  words, the following expressions should have the same result:\n\n     \"x == y\" and \"not x != y\"\n\n     \"x &lt; y\" and \"not x &gt;= y\" (for total ordering)\n\n     \"x &gt; y\" and \"not x &lt;= y\" (for total ordering)\n\n  The last two expressions apply to totally ordered collections (e.g.\n  to sequences, but not to sets or mappings). See also the\n  \"total_ordering()\" decorator.\n\n* The \"hash()\" result should be consistent with equality. Objects that\n  are equal should either have the same hash value, or be marked as\n  unhashable.\n\nPython does not enforce these consistency rules. In fact, the\nnot-a-number values are an example for not following these rules.\n\n\nMembership test operations\n==========================\n\nThe operators \"in\" and \"not in\" test for membership.  \"x in s\"\nevaluates to \"True\" if *x* is a member of *s*, and \"False\" otherwise.\n\"x not in s\" returns the negation of \"x in s\".  All built-in sequences\nand set types support this as well as dictionary, for which \"in\" tests\nwhether the dictionary has a given key. For container types such as\nlist, tuple, set, frozenset, dict, or collections.deque, the\nexpression \"x in y\" is equivalent to \"any(x is e or x == e for e in\ny)\".\n\nFor the string and bytes types, \"x in y\" is \"True\" if and only if *x*\nis a substring of *y*.  An equivalent test is \"y.find(x) != -1\".\nEmpty strings are always considered to be a substring of any other\nstring, so \"\"\" in \"abc\"\" will return \"True\".\n\nFor user-defined classes which define the \"__contains__()\" method, \"x\nin y\" returns \"True\" if \"y.__contains__(x)\" returns a true value, and\n\"False\" otherwise.\n\nFor user-defined classes which do not define \"__contains__()\" but do\ndefine \"__iter__()\", \"x in y\" is \"True\" if some value \"z\", for which\nthe expression \"x is z or x == z\" is true, is produced while iterating\nover \"y\". If an exception is raised during the iteration, it is as if\n\"in\" raised that exception.\n\nLastly, the old-style iteration protocol is tried: if a class defines\n\"__getitem__()\", \"x in y\" is \"True\" if and only if there is a non-\nnegative integer index *i* such that \"x is y[i] or x == y[i]\", and no\nlower integer index raises the \"IndexError\" exception.  (If any other\nexception is raised, it is as if \"in\" raised that exception).\n\nThe operator \"not in\" is defined to have the inverse truth value of\n\"in\".\n\n\nIdentity comparisons\n====================\n\nThe operators \"is\" and \"is not\" test for an object’s identity: \"x is\ny\" is true if and only if *x* and *y* are the same object.  An\nObject’s identity is determined using the \"id()\" function.  \"x is not\ny\" yields the inverse truth value. [4]\n\nRelated help topics: EXPRESSIONS, BASICMETHODS"
  },
  {
    "objectID": "core/notebooks/notebook01_python.html#about-assigments",
    "href": "core/notebooks/notebook01_python.html#about-assigments",
    "title": "Introduction to Python",
    "section": "About assigments",
    "text": "About assigments\n\nPython never copies an object\nUnless you ask him to\n\nWhen you code\nx = [1, 2, 3]\ny = x\nyou just - bind the variable name x to a list [1, 2, 3] - give another name y to the same object\nImportant remarks\n\nEverything is an object in Python\nEither immutable or mutable\n\n\n\nCode\nid(1), id(1+1), id(2)\n\n\n(11753896, 11753928, 11753928)\n\n\nA list is mutable\n\n\nCode\nx = [1, 2, 3]\nprint(id(x), x)\nx[0] += 42; x.append(3.14)\nprint(id(x), x)\n\n\n127235521652992 [1, 2, 3]\n127235521652992 [43, 2, 3, 3.14]\n\n\nA str is immutable\nIn order to “change” an immutable object, Python creates a new one\n\n\nCode\ns = 'to'\nprint(id(s), s)\ns += 'to'\nprint(id(s), s)\n\n\n127235606307792 to\n127235521897760 toto\n\n\nOnce again, a list is mutable\n\n\nCode\nsuper_list = [3.14, (1, 2, 3), 'tintin']\nother_list = super_list\nid(other_list), id(super_list)\n\n\n(127235521949312, 127235521949312)\n\n\n\nother_list and super_list are the same list\nIf you change one, you change the other.\nid returns the identity of an object. Two objects with the same idendity are the same (not only the same type, but the same instance)\n\n\n\nCode\nother_list[1] = 'youps'\nother_list, super_list\n\n\n([3.14, 'youps', 'tintin'], [3.14, 'youps', 'tintin'])\n\n\n\n\nCode\nid(super_list), id(other_list)\n\n\n(127235521949312, 127235521949312)"
  },
  {
    "objectID": "core/notebooks/notebook01_python.html#if-you-want-a-copy-to-need-to-ask-for-one",
    "href": "core/notebooks/notebook01_python.html#if-you-want-a-copy-to-need-to-ask-for-one",
    "title": "Introduction to Python",
    "section": "If you want a copy, to need to ask for one",
    "text": "If you want a copy, to need to ask for one\n\n\nCode\nother_list = super_list.copy()\nid(other_list), id(super_list)\n\n\n(127235521988352, 127235521949312)\n\n\n\n\nCode\nother_list[1] = 'copy'\nother_list, super_list\n\n\n([3.14, 'copy', 'tintin'], [3.14, 'youps', 'tintin'])\n\n\nOnly other_list is modified.\nBut… what if you have a list of list ? (or a mutable object containing mutable objects)\n\n\nCode\nl1, l2 = [1, 2, 3], [4, 5, 6]\nlist_list = [l1, l2]\nlist_list\n\n\n[[1, 2, 3], [4, 5, 6]]\n\n\n\n\nCode\nid(list_list), id(list_list[0]), id(l1), list_list[0] is l1\n\n\n(127235522125696, 127235522164864, 127235522164864, True)\n\n\nLet’s make a copy of list_list\n\n\nCode\ncopy_list = list_list.copy()\ncopy_list.append('super')\nlist_list, copy_list\n\n\n([[1, 2, 3], [4, 5, 6]], [[1, 2, 3], [4, 5, 6], 'super'])\n\n\n\n\nCode\nid(list_list[0]), id(copy_list[0])\n\n\n(127235522164864, 127235522164864)\n\n\nOK, only copy_list is modified, as expected\nBut now…\n\n\nCode\ncopy_list[0][1] = 'oups'\ncopy_list, list_list\n\n\n([[1, 'oups', 3], [4, 5, 6], 'super'], [[1, 'oups', 3], [4, 5, 6]])\n\n\nQuestion. What happened ?!?\n\nThe list_list object is copied\nBut NOT what it’s containing !\nBy default copy does a shallow copy, not a deep copy\nIt does not build copies of what is contained\nIf you want to copy an object and all that is contained in it, you need to use deepcopy.\n\n\n\nCode\nfrom copy import deepcopy\n\ncopy_list = deepcopy(list_list)\ncopy_list[0][1] = 'incredible !'\nlist_list, copy_list\n\n\n([[1, 'oups', 3], [4, 5, 6]], [[1, 'incredible !', 3], [4, 5, 6]])"
  },
  {
    "objectID": "core/notebooks/notebook01_python.html#final-remarks",
    "href": "core/notebooks/notebook01_python.html#final-remarks",
    "title": "Introduction to Python",
    "section": "Final remarks",
    "text": "Final remarks\n\n\nCode\ntt = ([1, 2, 3], [4, 5, 6])\nprint(id(tt), tt)\nprint(list(map(id, tt)))\n\n\n127235522228480 ([1, 2, 3], [4, 5, 6])\n[127235521944320, 127235521658816]\n\n\n\n\nCode\ntt[0][1] = '42'\nprint(id(tt), tt)\nprint(list(map(id, tt)))\n\n\n127235522228480 ([1, '42', 3], [4, 5, 6])\n[127235521944320, 127235521658816]\n\n\n\n\nCode\ns = [1, 2, 3]\n\n\n\n\nCode\ns2 = s\n\n\n\n\nCode\ns2 is s\n\n\nTrue\n\n\n\n\nCode\nid(s2), id(s)\n\n\n(127235521938880, 127235521938880)"
  },
  {
    "objectID": "core/notebooks/notebook01_python.html#blocks-are-delimited-by-indentation",
    "href": "core/notebooks/notebook01_python.html#blocks-are-delimited-by-indentation",
    "title": "Introduction to Python",
    "section": "Blocks are delimited by indentation!",
    "text": "Blocks are delimited by indentation!\n\n\nCode\na = 3\nif a &gt; 0:\n    if a == 1:\n        print(1)\n    elif a == 2:\n        print(2)\nelif a == 2:\n    print(2)\nelif a == 3:\n    print(3)\nelse:\n    print(a)"
  },
  {
    "objectID": "core/notebooks/notebook01_python.html#anything-can-be-understood-as-a-boolean",
    "href": "core/notebooks/notebook01_python.html#anything-can-be-understood-as-a-boolean",
    "title": "Introduction to Python",
    "section": "Anything can be understood as a boolean",
    "text": "Anything can be understood as a boolean\nFor example, don’t do this to test if a list is empty\n\n\nCode\nl2 = ['hello', 'everybody']\n\nif len(l2) &gt; 0:\n    print(l2[0])\n\n\nhello\n\n\nbut this\n\n\nCode\nif l2:\n    print(l2[0])\n\n\nhello\n\n\nSome poetry\n\nAn empty dict is False\nAn empty string is False\nAn empty list is False\nAn empty tuple is False\nAn empty set is False\n0 is False\n.0 is False\netc…\neverything else is True"
  },
  {
    "objectID": "core/notebooks/notebook01_python.html#while-loops",
    "href": "core/notebooks/notebook01_python.html#while-loops",
    "title": "Introduction to Python",
    "section": "While loops",
    "text": "While loops\n\n\nCode\na = 10\nb = 1\nwhile b &lt; a:\n    b = b + 1\n    print(b)\n\n\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\nCompute the decimals of Pi using the Wallis formula\n\\[\n\\pi = 2 \\prod_{i=1}^{100} \\frac{4i^2}{4i^2 - 1}\n\\]\n\n\nCode\npi = 2\neps = 1e-10\ndif = 2 * eps\ni = 1\nwhile dif &gt; eps:\n    pi, i, old_pi = pi * 4 * i ** 2 / (4 * i ** 2 - 1), i + 1, pi\n    dif = pi - old_pi\n\n\n\n\nCode\npi\n\n\n3.1415837914138556\n\n\n\n\nCode\nfrom math import pi\n\npi\n\n\n3.141592653589793"
  },
  {
    "objectID": "core/notebooks/notebook01_python.html#for-loop-with-range",
    "href": "core/notebooks/notebook01_python.html#for-loop-with-range",
    "title": "Introduction to Python",
    "section": "for loop with range",
    "text": "for loop with range\n\nIteration with an index, with a list, with many things !\nrange has the same parameters as with slicing start:end:stride, all parameters being optional\n\n\n\nCode\nfor i in range(10):\n    print(i)\n\n\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n\n\nCode\nfor i in range(4):\n    print(i + 1)\nprint('-')\n\nfor i in range(1, 5):\n    print(i)\nprint('-')\n\nfor i in range(1, 10, 3):\n    print(i)\n\n\n1\n2\n3\n4\n-\n1\n2\n3\n4\n-\n1\n4\n7\n\n\nSomething for nerds. You can use else in a for loop\n\n\nCode\nnames = ['stephane', 'mokhtar', 'jaouad', 'simon', 'yiyang']\n\nfor name in names:\n    if name.startswith('u'):\n        print(name)\n        break\nelse:\n    print('Not found.')\n\n\nNot found.\n\n\n\n\nCode\nnames = ['stephane', 'mokhtar', 'jaouad', 'ulysse', 'simon', 'yiyang']\n\nfor name in names:\n    if name.startswith('u'):\n        print(name)\n        break\nelse:\n    print('Not found.')\n\n\nulysse"
  },
  {
    "objectID": "core/notebooks/notebook01_python.html#for-loops-over-iterable-objects",
    "href": "core/notebooks/notebook01_python.html#for-loops-over-iterable-objects",
    "title": "Introduction to Python",
    "section": "For loops over iterable objects",
    "text": "For loops over iterable objects\nYou can iterate using for over any container: list, tuple, dict, str, set among others…\n\n\nCode\ncolors = ['red', 'blue', 'black', 'white']\npeoples = ['stephane', 'jaouad', 'mokhtar', 'yiyang', 'rémi']\n\n\n\n\nCode\n# This is stupid\nfor i in range(len(colors)):\n    print(colors[i])\n    \n# This is better\nfor color in colors:\n    print(color)\n\n\nred\nblue\nblack\nwhite\nred\nblue\nblack\nwhite\n\n\nTo iterate over several sequences at the same time, use zip\n\n\nCode\nfor color, people in zip(colors, peoples):\n    print(color, people)\n\n\nred stephane\nblue jaouad\nblack mokhtar\nwhite yiyang\n\n\n\n\nCode\nl = [\"Bonjour\", {'francis': 5214, 'stephane': 5123}, ('truc', 3)]\nfor e in l:\n    print(e, len(e))\n\n\nBonjour 7\n{'francis': 5214, 'stephane': 5123} 2\n('truc', 3) 2\n\n\nLoop over a str\n\n\nCode\ns = 'Bonjour'\nfor c in s:\n    print(c)\n\n\nB\no\nn\nj\no\nu\nr\n\n\nLoop over a dict\n\n\nCode\ndd = {(1, 3): {'hello', 'world'}, 'truc': [1, 2, 3], 5: (1, 4, 2)}\n\n# Default is to loop over keys\nfor key in dd:\n    print(key)\n\n\n(1, 3)\ntruc\n5\n\n\n\n\nCode\n# Loop over values\nfor e in dd.values():\n    print(e)\n\n\n{'hello', 'world'}\n[1, 2, 3]\n(1, 4, 2)\n\n\n\n\nCode\n# Loop over items (key, value) pairs\nfor key, val in dd.items():\n    print(key, val)\n\n\n(1, 3) {'hello', 'world'}\ntruc [1, 2, 3]\n5 (1, 4, 2)\n\n\n\n\nCode\nfor t in dd.items():\n    print(t)\n\n\n((1, 3), {'hello', 'world'})\n('truc', [1, 2, 3])\n(5, (1, 4, 2))"
  },
  {
    "objectID": "core/notebooks/notebook01_python.html#comprehensions",
    "href": "core/notebooks/notebook01_python.html#comprehensions",
    "title": "Introduction to Python",
    "section": "Comprehensions",
    "text": "Comprehensions\nYou can construct a list, dict, set and others using the comprehension syntax\nlist comprehension\n\n\nCode\nprint(colors)\nprint(peoples)\n\n\n['red', 'blue', 'black', 'white']\n['stephane', 'jaouad', 'mokhtar', 'yiyang', 'rémi']\n\n\n\n\nCode\nl = []\nfor p, c in zip(peoples, colors):\n    if len(c)&lt;=4 :\n        l.append(p)\nprint(l)\n\n\n['stephane', 'jaouad']\n\n\n\n\nCode\n# The list of people with favorite color that has no more than 4 characters\n\n[people for color, people in zip(colors, peoples) if len(color) &lt;= 4]\n\n\n['stephane', 'jaouad']\n\n\ndict comprehension\n\n\nCode\n{people: color for color, people in zip(colors, peoples) if len(color) &lt;= 4}\n\n\n{'stephane': 'red', 'jaouad': 'blue'}\n\n\n\n\nCode\n# Allows to build a dict from two lists (for keys and values)\n{key: value for (key, value) in zip(peoples, colors)}\n\n\n{'stephane': 'red', 'jaouad': 'blue', 'mokhtar': 'black', 'yiyang': 'white'}\n\n\n\n\nCode\n# But it's simpler (so better) to use\ndict(zip(peoples, colors))\n\n\n{'stephane': 'red', 'jaouad': 'blue', 'mokhtar': 'black', 'yiyang': 'white'}\n\n\nSomething very convenient is enumerate\n\n\nCode\nfor i, color in enumerate(colors):\n    print(i, color)\n\n\n0 red\n1 blue\n2 black\n3 white\n\n\n\n\nCode\nlist(enumerate(colors))\n\n\n[(0, 'red'), (1, 'blue'), (2, 'black'), (3, 'white')]\n\n\n\n\nCode\ndict(enumerate(s))\n\n\n{0: 'B', 1: 'o', 2: 'n', 3: 'j', 4: 'o', 5: 'u', 6: 'r'}\n\n\n\n\nCode\nprint(dict(enumerate(s)))\n\n\n{0: 'B', 1: 'o', 2: 'n', 3: 'j', 4: 'o', 5: 'u', 6: 'r'}\n\n\n\n\nCode\ns = 'Hey everyone'\n{c: i for i, c in enumerate(s)}\n\n\n{'H': 0, 'e': 11, 'y': 8, ' ': 3, 'v': 5, 'r': 7, 'o': 9, 'n': 10}"
  },
  {
    "objectID": "core/notebooks/notebook01_python.html#about-functional-programming",
    "href": "core/notebooks/notebook01_python.html#about-functional-programming",
    "title": "Introduction to Python",
    "section": "About functional programming",
    "text": "About functional programming\nWe can use lambda to define anonymous functions, and use them in the map and reduce functions\n\n\nCode\nsquare = lambda x: x ** 2\nsquare(2)\n\n\n4\n\n\n\n\nCode\ntype(square)\n\n\nfunction\n\n\n\n\nCode\ndir(square)\n\n\n['__annotations__',\n '__builtins__',\n '__call__',\n '__class__',\n '__closure__',\n '__code__',\n '__defaults__',\n '__delattr__',\n '__dict__',\n '__dir__',\n '__doc__',\n '__eq__',\n '__format__',\n '__ge__',\n '__get__',\n '__getattribute__',\n '__getstate__',\n '__globals__',\n '__gt__',\n '__hash__',\n '__init__',\n '__init_subclass__',\n '__kwdefaults__',\n '__le__',\n '__lt__',\n '__module__',\n '__name__',\n '__ne__',\n '__new__',\n '__qualname__',\n '__reduce__',\n '__reduce_ex__',\n '__repr__',\n '__setattr__',\n '__sizeof__',\n '__str__',\n '__subclasshook__',\n '__type_params__']\n\n\n\n\nCode\ns = \"a\"\n\n\n\n\nCode\ntry:\n    square(\"a\")\nexcept TypeError:\n    print(\"TypeError: unsupported operand type(s) for ** or pow(): 'str' and 'int'\")\n\n\nTypeError: unsupported operand type(s) for ** or pow(): 'str' and 'int'\n\n\n\n\nCode\nsum2 = lambda a, b: a + b\nprint(sum2('Hello', ' world'))\nprint(sum2(1, 2))\n\n\nHello world\n3\n\n\nIntended for short and one-line function.\nMore complex functions use def (see below)"
  },
  {
    "objectID": "core/notebooks/notebook01_python.html#exercise",
    "href": "core/notebooks/notebook01_python.html#exercise",
    "title": "Introduction to Python",
    "section": "Exercise",
    "text": "Exercise\nPrint the squares of even numbers between 0 et 15\n\nUsing a list comprehension as before\nUsing map"
  },
  {
    "objectID": "core/notebooks/notebook01_python.html#brain-teasing-1",
    "href": "core/notebooks/notebook01_python.html#brain-teasing-1",
    "title": "Introduction to Python",
    "section": "Brain-teasing",
    "text": "Brain-teasing\nWhat is the output of\n\n\nCode\nreduce(lambda a, b: a + b[0] * b[1], enumerate('abcde'), 'A')"
  },
  {
    "objectID": "core/notebooks/notebook01_python.html#why-generators",
    "href": "core/notebooks/notebook01_python.html#why-generators",
    "title": "Introduction to Python",
    "section": "Why generators ?",
    "text": "Why generators ?\nThe memory used by range(i) does not scale linearly with i\nWhat is happening ?\n\nrange(n) does not allocate a list of n elements !\nIt generates on the fly the list of required integers\nWe say that such an object behaves like a generator in Python\nMany things in the Python standard library behaves like this\n\nWarning. Getting the real memory footprint of a Python object is difficult. Note that sizeof calls the __sizeof__ method of r, which does not give in general the actual memory used by an object. But nevermind here.\nThe following computation has no memory footprint:\n\n\nCode\nsum(range(10**8))\n\n\n4999999950000000\n\n\n\n\nCode\nmap(lambda x: x**2, range(10**7))\n\n\n&lt;map at 0x73b7f810f250&gt;\n\n\nmap does not return a list for the same reason\n\n\nCode\nsum(map(lambda x: x**2, range(10**6)))\n\n\n333332833333500000"
  },
  {
    "objectID": "core/notebooks/notebook01_python.html#generator-expression",
    "href": "core/notebooks/notebook01_python.html#generator-expression",
    "title": "Introduction to Python",
    "section": "Generator expression",
    "text": "Generator expression\nNamely generators defined through comprehensions. Just replace [] by () in the comprehension.\nA generator can be iterated on only once\n\n\nCode\nrange(10)\n\n\nrange(0, 10)\n\n\n\n\nCode\ncarres = (i**2 for i in range(10))\n\n\n\n\nCode\ncarres\n\n\n&lt;generator object &lt;genexpr&gt; at 0x73b85412a330&gt;\n\n\n\n\nCode\nfor c in carres:\n    print(c)\n\n\n0\n1\n4\n9\n16\n25\n36\n49\n64\n81\n\n\n\n\nCode\nfor i in range(4):\n    for j in range(3):\n        print(i, j)\n\n\n0 0\n0 1\n0 2\n1 0\n1 1\n1 2\n2 0\n2 1\n2 2\n3 0\n3 1\n3 2\n\n\n\n\nCode\nfrom itertools import product\n\nfor t in product(range(4), range(3)):\n    print(t)\n\n\n(0, 0)\n(0, 1)\n(0, 2)\n(1, 0)\n(1, 1)\n(1, 2)\n(2, 0)\n(2, 1)\n(2, 2)\n(3, 0)\n(3, 1)\n(3, 2)\n\n\n\n\nCode\nfrom itertools import product\n\ngene = (i + j for i, j in product(range(3), range(3)))\ngene\n\n\n&lt;generator object &lt;genexpr&gt; at 0x73b7fcd2adc0&gt;\n\n\n\n\nCode\nprint(list(gene))\nprint(list(gene))\n\n\n[0, 1, 2, 1, 2, 3, 2, 3, 4]\n[]"
  },
  {
    "objectID": "core/notebooks/notebook01_python.html#yield",
    "href": "core/notebooks/notebook01_python.html#yield",
    "title": "Introduction to Python",
    "section": "yield",
    "text": "yield\nSomething very powerful\n\n\nCode\ndef startswith(words, letter):\n    for word in words:\n        if word.startswith(letter):\n            yield word\n\n\n\n\nCode\nwords = [\n    'Python', \"is\", 'awesome', 'in', 'particular', 'generators', \n    'are', 'really', 'cool'\n]\n\n\n\n\nCode\nlist(word for word in words if word.startswith(\"a\"))\n\n\n['awesome', 'are']\n\n\n\n\nCode\na = 2\n\n\n\n\nCode\nfloat(a)\n\n\n2.0\n\n\nBut also with a for loop\n\n\nCode\nfor word in startswith(words, letter='a'):\n    print(word)\n\n\nawesome\nare\n\n\n\n\nCode\nit = startswith(words, letter='a')\n\n\n\n\nCode\ntype(it)\n\n\ngenerator\n\n\n\n\nCode\nnext(it)\n\n\n'awesome'\n\n\n\n\nCode\nnext(it)\n\n\n'are'\n\n\n\n\nCode\ntry:\n    next(it)\nexcept StopIteration:\n    print(\"StopIteration exception!\")\n\n\nStopIteration exception!"
  },
  {
    "objectID": "core/notebooks/notebook01_python.html#exercise-4",
    "href": "core/notebooks/notebook01_python.html#exercise-4",
    "title": "Introduction to Python",
    "section": "Exercise",
    "text": "Exercise\nCount the number of occurences of all the words in words.\nOutput must be a dictionary containg word: count\n\n\nCode\nprint(words)\n\n\n['Bonjour', 'Python', 'c', 'est', 'super', 'Python', 'ca', 'a', 'l', 'air', 'quand', 'même', 'un', 'peu', 'compliqué', 'Mais', 'bon', 'ca', 'a', 'l', 'air', 'pratique', 'Peut-être', 'que', 'je', 'pourrais', 'm', 'en', 'servir', 'pour', 'faire', 'des', 'trucs', 'super']"
  },
  {
    "objectID": "core/notebooks/notebook01_python.html#exercise-5",
    "href": "core/notebooks/notebook01_python.html#exercise-5",
    "title": "Introduction to Python",
    "section": "Exercise",
    "text": "Exercise\nCompute the number of occurences AND the length of each word in words.\nOutput must be a dictionary containing word: (count, length)"
  },
  {
    "objectID": "core/notebooks/notebook01_python.html#exercise-6",
    "href": "core/notebooks/notebook01_python.html#exercise-6",
    "title": "Introduction to Python",
    "section": "Exercise",
    "text": "Exercise\nCount the number of occurences of each word in the text file miserables.txt. We use a open context and the Counter from before."
  },
  {
    "objectID": "core/notebooks/notebook01_python.html#contexts",
    "href": "core/notebooks/notebook01_python.html#contexts",
    "title": "Introduction to Python",
    "section": "Contexts",
    "text": "Contexts\n\nA context in Python is something that we use with the with keyword.\nIt allows to deal automatically with the opening and the closing of the file.\n\nNote the for loop:\nfor line in f:\n    ...\nYou loop directly over the lines of the open file from within the open context"
  },
  {
    "objectID": "core/notebooks/notebook01_python.html#about-pickle",
    "href": "core/notebooks/notebook01_python.html#about-pickle",
    "title": "Introduction to Python",
    "section": "About pickle",
    "text": "About pickle\nYou can save your computation with pickle.\n\npickle is a way of saving almost anything with Python.\nIt serializes the object in a binary format, and is usually the simplest and fastest way to go.\n\n\n\nCode\nimport pickle as pkl\n\n# Let's save it\nwith open('miserable_word_counts.pkl', 'wb') as f:\n    pkl.dump(counter, f)\n\n# And read it again\nwith open('miserable_word_counts.pkl', 'rb') as f:\n    counter = pkl.load(f)\n\n\n\n\nCode\ncounter.most_common(10)\n\n\n[('{', 15),\n ('}', 15),\n ('0', 8),\n ('img', 6),\n ('margin:', 6),\n ('font', 6),\n ('logo', 6),\n ('only', 6),\n ('screen', 6),\n ('and', 6)]"
  },
  {
    "objectID": "core/notebooks/notebook01_python.html#function-definition",
    "href": "core/notebooks/notebook01_python.html#function-definition",
    "title": "Introduction to Python",
    "section": "Function definition",
    "text": "Function definition\nFunction blocks must be indented as other control-flow blocks.\n\n\nCode\ndef test():\n    return 'in test function'\n\ntest()\n\n\n'in test function'"
  },
  {
    "objectID": "core/notebooks/notebook01_python.html#return-statement",
    "href": "core/notebooks/notebook01_python.html#return-statement",
    "title": "Introduction to Python",
    "section": "Return statement",
    "text": "Return statement\nFunctions can optionally return values. By default, functions return None.\nThe syntax to define a function:\n\nthe def keyword;\nis followed by the function’s name, then\nthe arguments of the function are given between parentheses followed by a colon\nthe function body;\nand return object for optionally returning values.\n\n\n\nCode\nNone is None\n\n\nTrue\n\n\n\n\nCode\ndef f(x):\n    return x + 10\nf(20)\n\n\n30\n\n\nA function that returns several elements returns a tuple\n\n\nCode\ndef f(x):\n    return x + 1, x + 4\n\nf(5)\n\n\n(6, 9)\n\n\n\n\nCode\ntype(f)\n\n\nfunction\n\n\n\n\nCode\nf.truc = \"bonjour\"\n\n\n\n\nCode\ntype(f(5))\n\n\ntuple"
  },
  {
    "objectID": "core/notebooks/notebook01_python.html#parameters",
    "href": "core/notebooks/notebook01_python.html#parameters",
    "title": "Introduction to Python",
    "section": "Parameters",
    "text": "Parameters\nMandatory parameters (positional arguments)\n\n\nCode\ndef double_it(x):\n    return x * 2\n\ndouble_it(2)\n\n\n4\n\n\n\n\nCode\ntry:\n    double_it()\nexcept TypeError:\n    print(\"TypeError: double_it() missing 1 required positional argument: 'x'\")\n\n\nTypeError: double_it() missing 1 required positional argument: 'x'\n\n\nOptimal parameters\n\n\nCode\ndef double_it(x=2):\n    return x * 2\n\ndouble_it()\n\n\n4\n\n\n\n\nCode\ndouble_it(3)\n\n\n6\n\n\n\n\nCode\ndef f(x, y=2, z=10):\n    print(x, '+', y, '+', z, '=', x + y + z)\n\n\n\n\nCode\nf(5)\n\n\n5 + 2 + 10 = 17\n\n\n\n\nCode\nf(5, -2)\n\n\n5 + -2 + 10 = 13\n\n\n\n\nCode\nf(5, -2, 8)\n\n\n5 + -2 + 8 = 11\n\n\n\n\nCode\nf(z=5, x=-2, y=8)\n\n\n-2 + 8 + 5 = 11"
  },
  {
    "objectID": "core/notebooks/notebook01_python.html#argument-unpacking-and-keyword-argument-unpacking",
    "href": "core/notebooks/notebook01_python.html#argument-unpacking-and-keyword-argument-unpacking",
    "title": "Introduction to Python",
    "section": "Argument unpacking and keyword argument unpacking",
    "text": "Argument unpacking and keyword argument unpacking\nYou can do stuff like this, using unpacking * notation\n\n\nCode\na, *b, c = 1, 2, 3, 4, 5\na, b, c\n\n\n(1, [2, 3, 4], 5)\n\n\nBack to function f you can unpack a tuple as positional arguments\n\n\nCode\ntt = (1, 2, 3)\nf(*tt)\n\n\n1 + 2 + 3 = 6\n\n\n\n\nCode\ndd = {'y': 10, 'z': -5}\n\n\n\n\nCode\nf(3, **dd)\n\n\n3 + 10 + -5 = 8\n\n\n\n\nCode\ndef g(x, z, y, t=1, u=2):\n    print(x, '+', y, '+', z, '+', t, '+', \n          u, '=', x + y + z + t + u)\n\n\n\n\nCode\ntt = (1, -4, 2)\ndd = {'t': 10, 'u': -5}\ng(*tt, **dd)\n\n\n1 + 2 + -4 + 10 + -5 = 4"
  },
  {
    "objectID": "core/notebooks/notebook01_python.html#the-prototype-of-all-functions-in-python",
    "href": "core/notebooks/notebook01_python.html#the-prototype-of-all-functions-in-python",
    "title": "Introduction to Python",
    "section": "The prototype of all functions in Python",
    "text": "The prototype of all functions in Python\n\n\nCode\ndef f(*args, **kwargs):\n    print('args=', args)\n    print('kwargs=', kwargs)\n\nf(1, 2, 'truc', lastname='gaiffas', firstname='stephane')\n\n\nargs= (1, 2, 'truc')\nkwargs= {'lastname': 'gaiffas', 'firstname': 'stephane'}\n\n\n\nUses * for argument unpacking and ** for keyword argument unpacking\nThe names args and kwargs are a convention, not mandatory\n(but you are fired if you name these arguments otherwise)\n\n\n\nCode\n# How to get fired\ndef f(*aaa, **bbb):\n    print('args=', aaa)\n    print('kwargs=', bbb)\nf(1, 2, 'truc', lastname='gaiffas', firstname='stephane')    \n\n\nargs= (1, 2, 'truc')\nkwargs= {'lastname': 'gaiffas', 'firstname': 'stephane'}\n\n\nRemark. A function is a regular an object… you can add attributes on it !\n\n\nCode\nf.truc = 4\n\n\n\n\nCode\nf(1, 3)\n\n\nargs= (1, 3)\nkwargs= {}\n\n\n\n\nCode\nf(3, -2, y='truc')\n\n\nargs= (3, -2)\nkwargs= {'y': 'truc'}"
  },
  {
    "objectID": "core/notebooks/notebook01_python.html#exercise-7",
    "href": "core/notebooks/notebook01_python.html#exercise-7",
    "title": "Introduction to Python",
    "section": "Exercise",
    "text": "Exercise\nAdd a age method to the Student class that computes the age of the student. - You can (and should) use the datetime module. - Since we only know about the birth year, let’s assume that the day of the birth is January, 1st."
  },
  {
    "objectID": "core/notebooks/notebook01_python.html#properties",
    "href": "core/notebooks/notebook01_python.html#properties",
    "title": "Introduction to Python",
    "section": "Properties",
    "text": "Properties\nWe can make methods look like attributes using properties, as shown below\n\n\nCode\nclass Student(object):\n\n    def __init__(self, name, birthyear, major='computer science'):\n        self.name = name\n        self.birthyear = birthyear\n        self.major = major\n\n    def __repr__(self):\n        return \"Student(name='{name}', birthyear={birthyear}, major='{major}')\"\\\n                .format(name=self.name, birthyear=self.birthyear, major=self.major)\n\n    @property\n    def age(self):\n        return datetime.now().year - self.birthyear\n        \nanna = Student('anna', 1987)\nanna.age\n\n\n38"
  },
  {
    "objectID": "core/notebooks/notebook01_python.html#inheritance",
    "href": "core/notebooks/notebook01_python.html#inheritance",
    "title": "Introduction to Python",
    "section": "Inheritance",
    "text": "Inheritance\nA MasterStudent is a Student with a new extra mandatory internship attribute\n\n\nCode\n\"%d\" % 2\n\n\n'2'\n\n\n\n\nCode\nx = 2\n\nf\"truc {x}\"\n\n\n'truc 2'\n\n\n\n\nCode\nclass MasterStudent(Student):\n    \n    def __init__(self, name, age, internship, major='computer science'):\n        # Student.__init__(self, name, age, major)\n        Student.__init__(self, name, age, major)\n        self.internship = internship\n\n    def __repr__(self):\n        return f\"MasterStudent(name='{self.name}', internship={self.internship}, birthyear={self.birthyear}, major={self.major})\"\n    \nMasterStudent('djalil', 22, 'pwc')\n\n\nMasterStudent(name='djalil', internship=pwc, birthyear=22, major=computer science)\n\n\n\n\nCode\nclass MasterStudent(Student):\n    \n    def __init__(self, name, age, internship, major='computer science'):\n        # Student.__init__(self, name, age, major)\n        Student.__init__(self, name, age, major)\n        self.internship = internship\n\n    def __repr__(self):\n        return \"MasterStudent(name='{name}', internship='{internship}'\" \\\n               \", birthyear={birthyear}, major='{major}')\"\\\n                .format(name=self.name, internship=self.internship,\n                        birthyear=self.birthyear, major=self.major)\n    \ndjalil = MasterStudent('djalil', 1996, 'pwc')\n\n\n\n\nCode\ndjalil.__dict__\n\n\n{'name': 'djalil',\n 'birthyear': 1996,\n 'major': 'computer science',\n 'internship': 'pwc'}\n\n\n\n\nCode\ndjalil.birthyear\n\n\n1996\n\n\n\n\nCode\ndjalil.__dict__[\"birthyear\"]\n\n\n1996"
  },
  {
    "objectID": "core/notebooks/notebook01_python.html#monkey-patching",
    "href": "core/notebooks/notebook01_python.html#monkey-patching",
    "title": "Introduction to Python",
    "section": "Monkey patching",
    "text": "Monkey patching\n\nClasses in Python are objects and actually dicts under the hood…\nTherefore classes are objects that can be changed on the fly\n\n\n\nCode\nclass Monkey(object):\n    \n    def __init__(self, name):\n        self.name = name\n\n    def describe(self):\n        print(\"Old monkey %s\" % self.name)\n\ndef patch(self):\n    print(\"New monkey %s\" % self.name)\n\nmonkey = Monkey(\"Baloo\")\nmonkey.describe()\n\nMonkey.describe = patch\nmonkey.describe()\n\n\nOld monkey Baloo\nNew monkey Baloo\n\n\n\n\nCode\nmonkeys = [Monkey(\"Baloo\"), Monkey(\"Super singe\")]\n\n\nmonkey_name = monkey.name\n\nfor i in range(1000):    \n    monkey_name"
  },
  {
    "objectID": "core/notebooks/notebook01_python.html#data-classes",
    "href": "core/notebooks/notebook01_python.html#data-classes",
    "title": "Introduction to Python",
    "section": "Data classes",
    "text": "Data classes\nSince Python 3.7 you can use a dataclass for this\nDoes a lot of work for you (produces the __repr__ among many other things for you)\n\n\nCode\nfrom dataclasses import dataclass\nfrom datetime import datetime \n\n@dataclass\nclass Student(object):\n    name: str\n    birthyear: int\n    major: str = 'computer science'\n\n    @property\n    def age(self):\n        return datetime.now().year - self.birthyear\n        \nanna = Student(name=\"anna\", birthyear=1987)\nanna\n\n\nStudent(name='anna', birthyear=1987, major='computer science')\n\n\n\n\nCode\nprint(anna.age)\n\n\n38"
  },
  {
    "objectID": "core/notebooks/notebook01_python.html#using-a-mutable-value-as-a-default-value",
    "href": "core/notebooks/notebook01_python.html#using-a-mutable-value-as-a-default-value",
    "title": "Introduction to Python",
    "section": "Using a mutable value as a default value",
    "text": "Using a mutable value as a default value\n\n\nCode\ndef foo(bar=[]):\n    bar.append('oops')\n    return bar\n\nprint(foo())\nprint(foo())\nprint(foo())\n\nprint('-' * 8)\nprint(foo(['Ah ah']))\nprint(foo([]))\n\n\n['oops']\n['oops', 'oops']\n['oops', 'oops', 'oops']\n--------\n['Ah ah', 'oops']\n['oops']\n\n\n\n\nCode\nprint(foo.__defaults__)\nfoo()\nprint(foo.__defaults__)\n\n\n(['oops', 'oops', 'oops'],)\n(['oops', 'oops', 'oops', 'oops'],)\n\n\n\nThe default value for a function argument is evaluated once, when the function is defined\nthe bar argument is initialized to its default (i.e., an empty list) only when foo() is first defined\nsuccessive calls to foo() (with no a bar argument specified) use the same list!\n\nOne should use instead\n\n\nCode\ndef foo(bar=None):\n    if bar is None:\n        bar = []\n    bar.append('oops')\n    return bar\n\nprint(foo())\nprint(foo())\nprint(foo())\nprint(foo(['OK']))\n\n\n['oops']\n['oops']\n['oops']\n['OK', 'oops']\n\n\n\n\nCode\nprint(foo.__defaults__)\nfoo()\nprint(foo.__defaults__)\n\n\n(None,)\n(None,)\n\n\nNo problem with immutable types\n\n\nCode\ndef foo(bar=()):\n    bar += ('oops',)\n    return bar\n\nprint(foo())\nprint(foo())\nprint(foo())\n\n\n('oops',)\n('oops',)\n('oops',)\n\n\n\n\nCode\nprint(foo.__defaults__)\n\n\n((),)"
  },
  {
    "objectID": "core/notebooks/notebook01_python.html#class-attributes-vs-object-attributes",
    "href": "core/notebooks/notebook01_python.html#class-attributes-vs-object-attributes",
    "title": "Introduction to Python",
    "section": "Class attributes VS object attributes",
    "text": "Class attributes VS object attributes\n\n\nCode\nclass A(object):\n    x = 1\n\n    def __init__(self):\n        self.y = 2\n\nclass B(A):\n    def __init__(self):\n        super().__init__()\n\nclass C(A):\n    def __init__(self):\n        super().__init__()\n\na, b, c = A(), B(), C()\n\n\n\n\nCode\nprint(a.x, b.x, c.x)\nprint(a.y, b.y, c.y)\n\n\n1 1 1\n2 2 2\n\n\n\n\nCode\na.y = 3\nprint(a.y, b.y, c.y)\n\n\n3 2 2\n\n\n\n\nCode\na.x = 3  # Adds a new attribute named x in object a\nprint(a.x, b.x, c.x)\n\n\n3 1 1\n\n\n\n\nCode\nA.x = 4 # Changes the class attribute x of class A\nprint(a.x, b.x, c.x)\n\n\n3 4 4\n\n\n\nAttribute x is not an attribute of b nor c\nIt is also not a class attribute of classes B and C\nSo, it is is looked up in the base class A, which contains a class attribute x\n\nClasses and objects contain a hidden dict to store their attributes, and are accessed following a method resolution order (MRO)\n\n\nCode\na.__dict__, b.__dict__, c.__dict__\n\n\n({'y': 3, 'x': 3}, {'y': 2}, {'y': 2})\n\n\n\n\nCode\nA.__dict__, B.__dict__, C.__dict__\n\n\n(mappingproxy({'__module__': '__main__',\n               'x': 4,\n               '__init__': &lt;function __main__.A.__init__(self)&gt;,\n               '__dict__': &lt;attribute '__dict__' of 'A' objects&gt;,\n               '__weakref__': &lt;attribute '__weakref__' of 'A' objects&gt;,\n               '__doc__': None}),\n mappingproxy({'__module__': '__main__',\n               '__init__': &lt;function __main__.B.__init__(self)&gt;,\n               '__doc__': None}),\n mappingproxy({'__module__': '__main__',\n               '__init__': &lt;function __main__.C.__init__(self)&gt;,\n               '__doc__': None}))\n\n\nThis can lead to nasty errors when using class attributes: learn more about this"
  },
  {
    "objectID": "core/notebooks/notebook01_python.html#python-scope-rules",
    "href": "core/notebooks/notebook01_python.html#python-scope-rules",
    "title": "Introduction to Python",
    "section": "Python scope rules",
    "text": "Python scope rules\n\n\nCode\ntry:\n    ints += [4]\nexcept NameError:\n    print(\"NameError: name 'ints' is not defined\")\n\n\nNameError: name 'ints' is not defined\n\n\n\n\nCode\nints = [1]\n\ndef foo1():\n    ints.append(2)\n    return ints\n\ndef foo2():\n    ints += [2]\n    return ints\n\n\n\n\nCode\nfoo1()\n\n\n[1, 2]\n\n\n\n\nCode\ntry:    \n    foo2()\nexcept UnboundLocalError as inst:\n    print(inst)\n\n\ncannot access local variable 'ints' where it is not associated with a value\n\n\n\nWhat the hell ?\n\nAn assignment to a variable in a scope assumes that the variable is local to that scope\nand shadows any similarly named variable in any outer scope\n\nints += [2]\nmeans\nints = ints + [2]\nwhich is an assigment: ints must be defined in the local scope, but it is not, while\nints.append(2)\nis not an assignemnt"
  },
  {
    "objectID": "core/notebooks/notebook01_python.html#modify-a-list-while-iterating-over-it",
    "href": "core/notebooks/notebook01_python.html#modify-a-list-while-iterating-over-it",
    "title": "Introduction to Python",
    "section": "Modify a list while iterating over it",
    "text": "Modify a list while iterating over it\n\n\nCode\nodd = lambda x: bool(x % 2)\nnumbers = list(range(10))\n\ntry:\n  for i in range(len(numbers)):\n      if odd(numbers[i]):\n          del numbers[i]\nexcept IndexError as inst:\n    print(inst)\n\n\nlist index out of range\n\n\nTypically an example where one should use a list comprehension\n\n\nCode\n[number for number in numbers if not odd(number)]\n\n\n[0, 2, 4, 6, 8]"
  },
  {
    "objectID": "core/notebooks/notebook01_python.html#no-docstrings",
    "href": "core/notebooks/notebook01_python.html#no-docstrings",
    "title": "Introduction to Python",
    "section": "No docstrings",
    "text": "No docstrings\nAccept to spend time to write clean docstrings (look at numpydoc style)\n\n\nCode\ndef create_student(name, age, address, major='computer science'):\n    \"\"\"Add a student in the database\n    \n    Parameters\n    ----------\n    name: `str`\n        Name of the student\n    \n    age: `int`\n        Age of the student\n    \n    address: `str`\n        Address of the student\n    \n    major: `str`, default='computer science'\n        The major chosen by the student\n    \n    Returns\n    -------\n    output: `Student`\n        A fresh student\n    \"\"\"\n    pass\n\n\n\n\nCode\ncreate_student('Duduche', 28, 'Chalons')"
  },
  {
    "objectID": "core/notebooks/notebook01_python.html#not-using-available-methods-andor-the-simplest-solution",
    "href": "core/notebooks/notebook01_python.html#not-using-available-methods-andor-the-simplest-solution",
    "title": "Introduction to Python",
    "section": "Not using available methods and/or the simplest solution",
    "text": "Not using available methods and/or the simplest solution\n\n\nCode\ndd = {'stephane': 1234, 'gael': 4567, 'gontran': 891011}\n\n# Bad\nfor key in dd.keys():\n    print(key, dd[key])\n\nprint('-' * 8)\n\n# Good\nfor key, value in dd.items():\n    print(key, value)\n\n\nstephane 1234\ngael 4567\ngontran 891011\n--------\nstephane 1234\ngael 4567\ngontran 891011\n\n\n\n\nCode\ncolors = ['black', 'yellow', 'brown', 'red', 'pink']\n\n# Bad\nfor i in range(len(colors)):\n    print(i, colors[i])\n\nprint('-' * 8)\n\n# Good\nfor i, color in enumerate(colors):\n    print(i, color)\n\n\n0 black\n1 yellow\n2 brown\n3 red\n4 pink\n--------\n0 black\n1 yellow\n2 brown\n3 red\n4 pink"
  },
  {
    "objectID": "core/notebooks/notebook01_python.html#not-using-the-standard-library",
    "href": "core/notebooks/notebook01_python.html#not-using-the-standard-library",
    "title": "Introduction to Python",
    "section": "Not using the standard library",
    "text": "Not using the standard library\nWhile it’s always better than a hand-made solution\n\n\nCode\nlist1 = [1, 2]\nlist2 = [3, 4]\nlist3 = [5, 6, 7]\n\nfor a in list1:\n    for b in list2:\n        for c in list3:\n            print(a, b, c)\n\n\n1 3 5\n1 3 6\n1 3 7\n1 4 5\n1 4 6\n1 4 7\n2 3 5\n2 3 6\n2 3 7\n2 4 5\n2 4 6\n2 4 7\n\n\n\n\nCode\nfrom itertools import product\n\nfor a, b, c in product(list1, list2, list3):\n    print(a, b, c)\n\n\n1 3 5\n1 3 6\n1 3 7\n1 4 5\n1 4 6\n1 4 7\n2 3 5\n2 3 6\n2 3 7\n2 4 5\n2 4 6\n2 4 7"
  },
  {
    "objectID": "core/notebooks/notebook03_pandas.html",
    "href": "core/notebooks/notebook03_pandas.html",
    "title": "Introduction to pandas",
    "section": "",
    "text": "The pandas library (https://pandas.pydata.org) is one of the most used tool at the disposal of people working with data in python today."
  },
  {
    "objectID": "core/notebooks/notebook03_pandas.html#why",
    "href": "core/notebooks/notebook03_pandas.html#why",
    "title": "Introduction to pandas",
    "section": "Why ?",
    "text": "Why ?\nThrough pandas, you get acquainted with your data by analyzing it\n\nWhat’s the average, median, max, or min of each column?\nDoes column A correlate with column B?\nWhat does the distribution of data in column C look like?"
  },
  {
    "objectID": "core/notebooks/notebook03_pandas.html#why-cont",
    "href": "core/notebooks/notebook03_pandas.html#why-cont",
    "title": "Introduction to pandas",
    "section": "Why (con’t) ?",
    "text": "Why (con’t) ?\nyou get acquainted with your data by cleaning and transforming it\n\nRemoving missing values, filter rows or columns using some criteria\nStore the cleaned, transformed data back into virtually any format or database\nData visualization (when combined matplotlib or seaborn or others)"
  },
  {
    "objectID": "core/notebooks/notebook03_pandas.html#where",
    "href": "core/notebooks/notebook03_pandas.html#where",
    "title": "Introduction to pandas",
    "section": "Where ?",
    "text": "Where ?\npandas is a central component of the python “stack” for data science\n\npandas is built on top of numpy\noften used in conjunction with other libraries\na DataFrame is often fed to plotting functions or machine learning algorithms (such as scikit-learn)\nWell-interfaced with jupyter, leading to a nice interactive environment for data exploration and modeling"
  },
  {
    "objectID": "core/notebooks/notebook03_pandas.html#core-components-of-pandas",
    "href": "core/notebooks/notebook03_pandas.html#core-components-of-pandas",
    "title": "Introduction to pandas",
    "section": "Core components of pandas",
    "text": "Core components of pandas\nThe two primary components of pandas are the Series and DataFrame.\n\nA Series is essentially a column\nA DataFrame is a multi-dimensional table made up of a collection of Series with equal length"
  },
  {
    "objectID": "core/notebooks/notebook03_pandas.html#creating-a-dataframe-from-scratch",
    "href": "core/notebooks/notebook03_pandas.html#creating-a-dataframe-from-scratch",
    "title": "Introduction to pandas",
    "section": "Creating a DataFrame from scratch",
    "text": "Creating a DataFrame from scratch\n\n\nCode\nimport pandas as pd\n\nfruits = {\n    \"apples\": [3, 2, 0, 1],\n    \"oranges\": [0, 3, 7, 2]\n}\n\ndf_fruits = pd.DataFrame(fruits)\ndf_fruits\n\n\n\n\n\n\n\n\n\napples\noranges\n\n\n\n\n0\n3\n0\n\n\n1\n2\n3\n\n\n2\n0\n7\n\n\n3\n1\n2\n\n\n\n\n\n\n\n\n\nCode\ntype(df_fruits)\n\n\npandas.core.frame.DataFrame\n\n\n\n\nCode\ndf_fruits[\"apples\"]\n\n\n0    3\n1    2\n2    0\n3    1\nName: apples, dtype: int64\n\n\n\n\nCode\ntype(df_fruits[\"apples\"])\n\n\npandas.core.series.Series"
  },
  {
    "objectID": "core/notebooks/notebook03_pandas.html#indexing",
    "href": "core/notebooks/notebook03_pandas.html#indexing",
    "title": "Introduction to pandas",
    "section": "Indexing",
    "text": "Indexing\n\nBy default, a DataFrame uses a contiguous index\nBut what if we want to say who buys the fruits ?\n\n\n\nCode\ndf_fruits = pd.DataFrame(fruits, index=[\"Daniel\", \"Sean\", \"Pierce\", \"Roger\"])\ndf_fruits\n\n\n\n\n\n\n\n\n\napples\noranges\n\n\n\n\nDaniel\n3\n0\n\n\nSean\n2\n3\n\n\nPierce\n0\n7\n\n\nRoger\n1\n2"
  },
  {
    "objectID": "core/notebooks/notebook03_pandas.html#loc-versus-.iloc",
    "href": "core/notebooks/notebook03_pandas.html#loc-versus-.iloc",
    "title": "Introduction to pandas",
    "section": ".loc versus .iloc",
    "text": ".loc versus .iloc\n\n.loc locates by name\n.iloc locates by numerical index\n\n\n\nCode\ndf_fruits\n\n\n\n\n\n\n\n\n\napples\noranges\n\n\n\n\nDaniel\n3\n0\n\n\nSean\n2\n3\n\n\nPierce\n0\n7\n\n\nRoger\n1\n2\n\n\n\n\n\n\n\n\n\nCode\n# What's in Sean's basket ?\ndf_fruits.loc['Sean']\n\n\napples     2\noranges    3\nName: Sean, dtype: int64\n\n\n\n\nCode\n# Who has oranges ?\ndf_fruits.loc[:, 'oranges']\n\n\nDaniel    0\nSean      3\nPierce    7\nRoger     2\nName: oranges, dtype: int64\n\n\n\n\nCode\n# How many apples in Pierce's basket ?\ndf_fruits.loc['Pierce', 'apples']\n\n\nnp.int64(0)\n\n\n\n\nCode\ndf_fruits\n\n\n\n\n\n\n\n\n\napples\noranges\n\n\n\n\nDaniel\n3\n0\n\n\nSean\n2\n3\n\n\nPierce\n0\n7\n\n\nRoger\n1\n2\n\n\n\n\n\n\n\n\n\nCode\ndf_fruits.iloc[2, 1]\n\n\nnp.int64(7)"
  },
  {
    "objectID": "core/notebooks/notebook03_pandas.html#main-attributes-and-methods-of-a-dataframe",
    "href": "core/notebooks/notebook03_pandas.html#main-attributes-and-methods-of-a-dataframe",
    "title": "Introduction to pandas",
    "section": "Main attributes and methods of a DataFrame",
    "text": "Main attributes and methods of a DataFrame\nA DataFrame has many attributes\n\n\nCode\ndf_fruits.columns\n\n\nIndex(['apples', 'oranges'], dtype='object')\n\n\n\n\nCode\ndf_fruits.index\n\n\nIndex(['Daniel', 'Sean', 'Pierce', 'Roger'], dtype='object')\n\n\n\n\nCode\ndf_fruits.dtypes\n\n\napples     int64\noranges    int64\ndtype: object\n\n\nA DataFrame has many methods\n\n\nCode\ndf_fruits.info()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 4 entries, Daniel to Roger\nData columns (total 2 columns):\n #   Column   Non-Null Count  Dtype\n---  ------   --------------  -----\n 0   apples   4 non-null      int64\n 1   oranges  4 non-null      int64\ndtypes: int64(2)\nmemory usage: 268.0+ bytes\n\n\n\n\nCode\ndf_fruits.describe()\n\n\n\n\n\n\n\n\n\napples\noranges\n\n\n\n\ncount\n4.000000\n4.00000\n\n\nmean\n1.500000\n3.00000\n\n\nstd\n1.290994\n2.94392\n\n\nmin\n0.000000\n0.00000\n\n\n25%\n0.750000\n1.50000\n\n\n50%\n1.500000\n2.50000\n\n\n75%\n2.250000\n4.00000\n\n\nmax\n3.000000\n7.00000"
  },
  {
    "objectID": "core/notebooks/notebook03_pandas.html#missing-values",
    "href": "core/notebooks/notebook03_pandas.html#missing-values",
    "title": "Introduction to pandas",
    "section": "Missing values",
    "text": "Missing values\nWhat if we don’t know how many apples are in Sean’s basket ?\n\n\nCode\ndf_fruits.loc['Sean', 'apples'] = None\ndf_fruits\n\n\n\n\n\n\n\n\n\napples\noranges\n\n\n\n\nDaniel\n3.0\n0\n\n\nSean\nNaN\n3\n\n\nPierce\n0.0\n7\n\n\nRoger\n1.0\n2\n\n\n\n\n\n\n\n\n\nCode\ndf_fruits.describe()\n\n\n\n\n\n\n\n\n\napples\noranges\n\n\n\n\ncount\n3.000000\n4.00000\n\n\nmean\n1.333333\n3.00000\n\n\nstd\n1.527525\n2.94392\n\n\nmin\n0.000000\n0.00000\n\n\n25%\n0.500000\n1.50000\n\n\n50%\n1.000000\n2.50000\n\n\n75%\n2.000000\n4.00000\n\n\nmax\n3.000000\n7.00000\n\n\n\n\n\n\n\nNote that count is 3 for apples now, since we have 1 missing value among the 4\n\n\n\n\n\n\nNote\n\n\n\nTo review the members of objects of class pandas.DataFrame, dir() and module inspect are convenient.\n\n\n\n\nCode\n[x for x in dir(df_fruits) if not x.startswith('_') and not callable(x)]\n\n\n\n\nCode\nimport inspect\n\n# Get a list of methods\nmembres = inspect.getmembers(df_fruits)\n\nmethod_names = [m[0] for m in membres \n    if callable(m[1]) and not m[0].startswith('_')]\n\nprint(method_names)\n\n\n['abs', 'add', 'add_prefix', 'add_suffix', 'agg', 'aggregate', 'align', 'all', 'any', 'apply', 'applymap', 'asfreq', 'asof', 'assign', 'astype', 'at_time', 'backfill', 'between_time', 'bfill', 'bool', 'boxplot', 'clip', 'combine', 'combine_first', 'compare', 'convert_dtypes', 'copy', 'corr', 'corrwith', 'count', 'cov', 'cummax', 'cummin', 'cumprod', 'cumsum', 'describe', 'diff', 'div', 'divide', 'dot', 'drop', 'drop_duplicates', 'droplevel', 'dropna', 'duplicated', 'eq', 'equals', 'eval', 'ewm', 'expanding', 'explode', 'ffill', 'fillna', 'filter', 'first', 'first_valid_index', 'floordiv', 'from_dict', 'from_records', 'ge', 'get', 'groupby', 'gt', 'head', 'hist', 'idxmax', 'idxmin', 'iloc', 'infer_objects', 'info', 'insert', 'interpolate', 'isetitem', 'isin', 'isna', 'isnull', 'items', 'iterrows', 'itertuples', 'join', 'keys', 'kurt', 'kurtosis', 'last', 'last_valid_index', 'le', 'loc', 'lt', 'map', 'mask', 'max', 'mean', 'median', 'melt', 'memory_usage', 'merge', 'min', 'mod', 'mode', 'mul', 'multiply', 'ne', 'nlargest', 'notna', 'notnull', 'nsmallest', 'nunique', 'pad', 'pct_change', 'pipe', 'pivot', 'pivot_table', 'plot', 'pop', 'pow', 'prod', 'product', 'quantile', 'query', 'radd', 'rank', 'rdiv', 'reindex', 'reindex_like', 'rename', 'rename_axis', 'reorder_levels', 'replace', 'resample', 'reset_index', 'rfloordiv', 'rmod', 'rmul', 'rolling', 'round', 'rpow', 'rsub', 'rtruediv', 'sample', 'select_dtypes', 'sem', 'set_axis', 'set_flags', 'set_index', 'shift', 'skew', 'sort_index', 'sort_values', 'squeeze', 'stack', 'std', 'sub', 'subtract', 'sum', 'swapaxes', 'swaplevel', 'tail', 'take', 'to_clipboard', 'to_csv', 'to_dict', 'to_excel', 'to_feather', 'to_gbq', 'to_hdf', 'to_html', 'to_json', 'to_latex', 'to_markdown', 'to_numpy', 'to_orc', 'to_parquet', 'to_period', 'to_pickle', 'to_records', 'to_sql', 'to_stata', 'to_string', 'to_timestamp', 'to_xarray', 'to_xml', 'transform', 'transpose', 'truediv', 'truncate', 'tz_convert', 'tz_localize', 'unstack', 'update', 'value_counts', 'var', 'where', 'xs']\n\n\n\n\nCode\nothers = [x for x in membres\n    if not callable(x[1])]\n\n[x[0] for x in others if not x[0].startswith('_')]\n\n\n['T',\n 'apples',\n 'at',\n 'attrs',\n 'axes',\n 'columns',\n 'dtypes',\n 'empty',\n 'flags',\n 'iat',\n 'index',\n 'ndim',\n 'oranges',\n 'shape',\n 'size',\n 'style',\n 'values']"
  },
  {
    "objectID": "core/notebooks/notebook03_pandas.html#adding-a-column",
    "href": "core/notebooks/notebook03_pandas.html#adding-a-column",
    "title": "Introduction to pandas",
    "section": "Adding a column",
    "text": "Adding a column\nOoooops, we forgot about the bananas !\n\n\nCode\ndf_fruits[\"bananas\"] = [0, 2, 1, 6]\ndf_fruits\n\n\n\n\n\n\n\n\n\napples\noranges\nbananas\n\n\n\n\nDaniel\n3.0\n0\n0\n\n\nSean\nNaN\n3\n2\n\n\nPierce\n0.0\n7\n1\n\n\nRoger\n1.0\n2\n6"
  },
  {
    "objectID": "core/notebooks/notebook03_pandas.html#adding-a-column-with-the-date",
    "href": "core/notebooks/notebook03_pandas.html#adding-a-column-with-the-date",
    "title": "Introduction to pandas",
    "section": "Adding a column with the date",
    "text": "Adding a column with the date\nAnd we forgot the dates !\n\n\nCode\ndf_fruits['time'] = [\n    \"2020/10/08 12:13\", \"2020/10/07 11:37\", \n    \"2020/10/10 14:07\", \"2020/10/09 10:51\"\n]\ndf_fruits\n\n\n\n\n\n\n\n\n\napples\noranges\nbananas\ntime\n\n\n\n\nDaniel\n3.0\n0\n0\n2020/10/08 12:13\n\n\nSean\nNaN\n3\n2\n2020/10/07 11:37\n\n\nPierce\n0.0\n7\n1\n2020/10/10 14:07\n\n\nRoger\n1.0\n2\n6\n2020/10/09 10:51\n\n\n\n\n\n\n\n\n\nCode\ndf_fruits.dtypes\n\n\napples     float64\noranges      int64\nbananas      int64\ntime        object\ndtype: object\n\n\n\n\nCode\ntype(df_fruits.loc[\"Roger\", \"time\"])\n\n\nstr\n\n\nIt is not a date but a string (str) ! So we convert this column to something called datetime\n\n\nCode\ndf_fruits[\"time\"] = pd.to_datetime(df_fruits[\"time\"])\ndf_fruits\n\n\n\n\n\n\n\n\n\napples\noranges\nbananas\ntime\n\n\n\n\nDaniel\n3.0\n0\n0\n2020-10-08 12:13:00\n\n\nSean\nNaN\n3\n2\n2020-10-07 11:37:00\n\n\nPierce\n0.0\n7\n1\n2020-10-10 14:07:00\n\n\nRoger\n1.0\n2\n6\n2020-10-09 10:51:00\n\n\n\n\n\n\n\n\n\nCode\ndf_fruits.dtypes\n\n\napples            float64\noranges             int64\nbananas             int64\ntime       datetime64[ns]\ndtype: object\n\n\n\n\n\n\n\n\nNote\n\n\n\nEvery data science framework implements some datetime handling scheme. For Python see Python official documentation on datetime module\n\n\nWhat if we want to keep only the baskets after (including) October, 9th ?\n\n\nCode\ndf_fruits.loc[df_fruits[\"time\"] &gt;= pd.Timestamp(\"2020/10/09\")]\n\n\n\n\n\n\n\n\n\napples\noranges\nbananas\ntime\n\n\n\n\nPierce\n0.0\n7\n1\n2020-10-10 14:07:00\n\n\nRoger\n1.0\n2\n6\n2020-10-09 10:51:00"
  },
  {
    "objectID": "core/notebooks/notebook03_pandas.html#slices-and-subsets-of-rows-or-columns",
    "href": "core/notebooks/notebook03_pandas.html#slices-and-subsets-of-rows-or-columns",
    "title": "Introduction to pandas",
    "section": "Slices and subsets of rows or columns",
    "text": "Slices and subsets of rows or columns\n\n\nCode\ndf_fruits\n\n\n\n\n\n\n\n\n\napples\noranges\nbananas\ntime\n\n\n\n\nDaniel\n3.0\n0\n0\n2020-10-08 12:13:00\n\n\nSean\nNaN\n3\n2\n2020-10-07 11:37:00\n\n\nPierce\n0.0\n7\n1\n2020-10-10 14:07:00\n\n\nRoger\n1.0\n2\n6\n2020-10-09 10:51:00\n\n\n\n\n\n\n\n\n\nCode\ndf_fruits.loc[:, \"oranges\":\"time\"]\n\n\n\n\n\n\n\n\n\noranges\nbananas\ntime\n\n\n\n\nDaniel\n0\n0\n2020-10-08 12:13:00\n\n\nSean\n3\n2\n2020-10-07 11:37:00\n\n\nPierce\n7\n1\n2020-10-10 14:07:00\n\n\nRoger\n2\n6\n2020-10-09 10:51:00\n\n\n\n\n\n\n\n\n\nCode\ndf_fruits.loc[\"Daniel\":\"Sean\", \"apples\":\"bananas\"]\n\n\n\n\n\n\n\n\n\napples\noranges\nbananas\n\n\n\n\nDaniel\n3.0\n0\n0\n\n\nSean\nNaN\n3\n2\n\n\n\n\n\n\n\n\n\nCode\ndf_fruits[[\"apples\", \"time\"]]\n\n\n\n\n\n\n\n\n\napples\ntime\n\n\n\n\nDaniel\n3.0\n2020-10-08 12:13:00\n\n\nSean\nNaN\n2020-10-07 11:37:00\n\n\nPierce\n0.0\n2020-10-10 14:07:00\n\n\nRoger\n1.0\n2020-10-09 10:51:00"
  },
  {
    "objectID": "core/notebooks/notebook03_pandas.html#write-our-data-to-a-csv-file",
    "href": "core/notebooks/notebook03_pandas.html#write-our-data-to-a-csv-file",
    "title": "Introduction to pandas",
    "section": "Write our data to a CSV file",
    "text": "Write our data to a CSV file\nWhat if we want to write the file ?\n\n\nCode\ndf_fruits\n\n\n\n\n\n\n\n\n\napples\noranges\nbananas\ntime\n\n\n\n\nDaniel\n3.0\n0\n0\n2020-10-08 12:13:00\n\n\nSean\nNaN\n3\n2\n2020-10-07 11:37:00\n\n\nPierce\n0.0\n7\n1\n2020-10-10 14:07:00\n\n\nRoger\n1.0\n2\n6\n2020-10-09 10:51:00\n\n\n\n\n\n\n\n\n\nCode\ndf_fruits.to_csv(\"fruits.csv\")\n\n\n\n\nCode\n# Use !dir on windows\n!ls -alh | grep fru\n\n\n-rw-rw-r--  1 boucheron boucheron  163 mars  10 16:54 fruits.csv\n\n\n\n\nCode\n!head -n 5 fruits.csv\n\n\n,apples,oranges,bananas,time\nDaniel,3.0,0,0,2020-10-08 12:13:00\nSean,,3,2,2020-10-07 11:37:00\nPierce,0.0,7,1,2020-10-10 14:07:00\nRoger,1.0,2,6,2020-10-09 10:51:00"
  },
  {
    "objectID": "core/notebooks/notebook03_pandas.html#reading-data-and-working-with-it",
    "href": "core/notebooks/notebook03_pandas.html#reading-data-and-working-with-it",
    "title": "Introduction to pandas",
    "section": "Reading data and working with it",
    "text": "Reading data and working with it\n\n\n\n\n\n\nNote\n\n\n\nThe tips dataset comes through Kaggle\n\nThis dataset is a treasure trove of information from a collection of case studies for business statistics. Special thanks to Bryant and Smith for their diligent work:\n\n\nBryant, P. G. and Smith, M (1995) Practical Data Analysis: Case Studies in Business Statistics. Homewood, IL: Richard D. Irwin Publishing.\n\n\nYou can also access this dataset now through the Python package Seaborn.\n\n\n\nIt contains data about a restaurant: the bill, tip and some informations about the customers.\n\n\n\n\n\n\nA toy extraction pattern\n\n\n\nA data pipeline usually starts with Extraction, that is gathering data from some source, possibly in a galaxy far, far awy. Here follows a toy extraction pattern\n\nobtain the data from some URL using package requests\nsave the data on the hard drive\nload the data using Pandas\n\n\n\nCode\nimport requests\nimport os\n\n# The path containing your notebook\npath_data = './'\n# The name of the file\nfilename = 'tips.csv'\n\nif os.path.exists(os.path.join(path_data, filename)):\n    print('The file %s already exists.' % os.path.join(path_data, filename))\nelse:\n    url = 'https://raw.githubusercontent.com/mwaskom/seaborn-data/refs/heads/master/tips.csv'\n    r = requests.get(url)\n    with open(os.path.join(path_data, filename), 'wb') as f:\n        f.write(r.content)\n    print('Downloaded file %s.' % os.path.join(path_data, filename))\n\n\n\n\nCode\ndf = pd.read_csv(\n    \"tips.csv\", \n    delimiter=\",\"\n)\n\n\n\n\nThe data can be obtained from package seaborn.\n\n\nCode\nimport seaborn as sns\n\nsns_ds = sns.get_dataset_names()\n\n'tips' in sns_ds\n\ndf = sns.load_dataset('tips')\n\n\n\n\nCode\n# `.head()` shows the first rows of the dataframe\ndf.head(n=10)\n\n\n\n\n\n\n\n\n\ntotal_bill\ntip\nsex\nsmoker\nday\ntime\nsize\n\n\n\n\n0\n16.99\n1.01\nFemale\nNo\nSun\nDinner\n2\n\n\n1\n10.34\n1.66\nMale\nNo\nSun\nDinner\n3\n\n\n2\n21.01\n3.50\nMale\nNo\nSun\nDinner\n3\n\n\n3\n23.68\n3.31\nMale\nNo\nSun\nDinner\n2\n\n\n4\n24.59\n3.61\nFemale\nNo\nSun\nDinner\n4\n\n\n5\n25.29\n4.71\nMale\nNo\nSun\nDinner\n4\n\n\n6\n8.77\n2.00\nMale\nNo\nSun\nDinner\n2\n\n\n7\n26.88\n3.12\nMale\nNo\nSun\nDinner\n4\n\n\n8\n15.04\n1.96\nMale\nNo\nSun\nDinner\n2\n\n\n9\n14.78\n3.23\nMale\nNo\nSun\nDinner\n2\n\n\n\n\n\n\n\n\n\nCode\ndf.info()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 244 entries, 0 to 243\nData columns (total 7 columns):\n #   Column      Non-Null Count  Dtype   \n---  ------      --------------  -----   \n 0   total_bill  244 non-null    float64 \n 1   tip         244 non-null    float64 \n 2   sex         244 non-null    category\n 3   smoker      244 non-null    category\n 4   day         244 non-null    category\n 5   time        244 non-null    category\n 6   size        244 non-null    int64   \ndtypes: category(4), float64(2), int64(1)\nmemory usage: 7.4 KB\n\n\n\n\nCode\ndf.loc[42, \"day\"]\n\n\n'Sun'\n\n\n\n\nCode\ntype(df.loc[42, \"day\"])\n\n\nstr\n\n\nBy default, columns that are non-numerical contain strings (str type)"
  },
  {
    "objectID": "core/notebooks/notebook03_pandas.html#the-category-type",
    "href": "core/notebooks/notebook03_pandas.html#the-category-type",
    "title": "Introduction to pandas",
    "section": "The category type",
    "text": "The category type\nAn important type in pandas is category for variables that are non-numerical\nPro tip. It’s always a good idea to tell pandas which columns should be imported as categorical\nSo, let’s read again the file specifying some dtypes to the read_csv function\n\n\nCode\ndtypes = {\n    \"sex\": \"category\",\n    \"smoker\": \"category\",\n    \"day\": \"category\",\n    \"time\": \"category\"\n} \n\ndf = pd.read_csv(\"tips.csv\", dtype=dtypes)\n\n\n\n\nCode\ndf.dtypes\n\n\ntotal_bill     float64\ntip            float64\nsex           category\nsmoker        category\nday           category\ntime          category\nsize             int64\ndtype: object"
  },
  {
    "objectID": "core/notebooks/notebook03_pandas.html#computing-statistics",
    "href": "core/notebooks/notebook03_pandas.html#computing-statistics",
    "title": "Introduction to pandas",
    "section": "Computing statistics",
    "text": "Computing statistics\n\n\nCode\n# The describe method only shows statistics for the numerical columns by default\ndf.describe()\n\n\n\n\n\n\n\n\n\ntotal_bill\ntip\nsize\n\n\n\n\ncount\n244.000000\n244.000000\n244.000000\n\n\nmean\n19.785943\n2.998279\n2.569672\n\n\nstd\n8.902412\n1.383638\n0.951100\n\n\nmin\n3.070000\n1.000000\n1.000000\n\n\n25%\n13.347500\n2.000000\n2.000000\n\n\n50%\n17.795000\n2.900000\n2.000000\n\n\n75%\n24.127500\n3.562500\n3.000000\n\n\nmax\n50.810000\n10.000000\n6.000000\n\n\n\n\n\n\n\n\n\nCode\n# We use the include=\"all\" option to see everything\ndf.describe(include=\"all\")\n\n\n\n\n\n\n\n\n\ntotal_bill\ntip\nsex\nsmoker\nday\ntime\nsize\n\n\n\n\ncount\n244.000000\n244.000000\n244\n244\n244\n244\n244.000000\n\n\nunique\nNaN\nNaN\n2\n2\n4\n2\nNaN\n\n\ntop\nNaN\nNaN\nMale\nNo\nSat\nDinner\nNaN\n\n\nfreq\nNaN\nNaN\n157\n151\n87\n176\nNaN\n\n\nmean\n19.785943\n2.998279\nNaN\nNaN\nNaN\nNaN\n2.569672\n\n\nstd\n8.902412\n1.383638\nNaN\nNaN\nNaN\nNaN\n0.951100\n\n\nmin\n3.070000\n1.000000\nNaN\nNaN\nNaN\nNaN\n1.000000\n\n\n25%\n13.347500\n2.000000\nNaN\nNaN\nNaN\nNaN\n2.000000\n\n\n50%\n17.795000\n2.900000\nNaN\nNaN\nNaN\nNaN\n2.000000\n\n\n75%\n24.127500\n3.562500\nNaN\nNaN\nNaN\nNaN\n3.000000\n\n\nmax\n50.810000\n10.000000\nNaN\nNaN\nNaN\nNaN\n6.000000\n\n\n\n\n\n\n\n\n\nCode\n# Correlation between the numerical columns\ndf.corr(numeric_only = True)\n\n\n\n\n\n\n\n\n\ntotal_bill\ntip\nsize\n\n\n\n\ntotal_bill\n1.000000\n0.675734\n0.598315\n\n\ntip\n0.675734\n1.000000\n0.489299\n\n\nsize\n0.598315\n0.489299\n1.000000\n\n\n\n\n\n\n\n\n\nCode\n?df.corr"
  },
  {
    "objectID": "core/notebooks/notebook03_pandas.html#how-do-the-tip-depends-on-the-total-bill",
    "href": "core/notebooks/notebook03_pandas.html#how-do-the-tip-depends-on-the-total-bill",
    "title": "Introduction to pandas",
    "section": "How do the tip depends on the total bill ?",
    "text": "How do the tip depends on the total bill ?\n\n\nCode\nsns.jointplot(x=\"total_bill\", y=\"tip\", data=df)"
  },
  {
    "objectID": "core/notebooks/notebook03_pandas.html#when-do-customers-go-to-this-restaurant",
    "href": "core/notebooks/notebook03_pandas.html#when-do-customers-go-to-this-restaurant",
    "title": "Introduction to pandas",
    "section": "When do customers go to this restaurant ?",
    "text": "When do customers go to this restaurant ?\n\n\nCode\nsns.countplot(x='day', hue=\"time\", data=df)"
  },
  {
    "objectID": "core/notebooks/notebook03_pandas.html#when-do-customers-spend-the-most",
    "href": "core/notebooks/notebook03_pandas.html#when-do-customers-spend-the-most",
    "title": "Introduction to pandas",
    "section": "When do customers spend the most ?",
    "text": "When do customers spend the most ?\n\n\nCode\nplt.figure(figsize=(7, 5))\nsns.boxplot(x='day', y='total_bill', hue='time', data=df)\nplt.legend(loc=\"upper left\")\n\n\n\n\n\n\n\n\n\n\n\nCode\nplt.figure(figsize=(7, 5))\nsns.violinplot(x='day', y='total_bill', hue='time', split=True, data=df)\nplt.legend(loc=\"upper left\")"
  },
  {
    "objectID": "core/notebooks/notebook03_pandas.html#who-spends-the-most",
    "href": "core/notebooks/notebook03_pandas.html#who-spends-the-most",
    "title": "Introduction to pandas",
    "section": "Who spends the most ?",
    "text": "Who spends the most ?\n\n\nCode\nsns.boxplot(x='sex', y='total_bill', hue='smoker', data=df)"
  },
  {
    "objectID": "core/notebooks/notebook03_pandas.html#when-should-waiters-want-to-work",
    "href": "core/notebooks/notebook03_pandas.html#when-should-waiters-want-to-work",
    "title": "Introduction to pandas",
    "section": "When should waiters want to work ?",
    "text": "When should waiters want to work ?\n\n\nCode\nsns.boxplot(x='day', y='tip', hue='time', data=df)\n\n\n\n\n\n\n\n\n\n\n\nCode\nsns.violinplot(x='day', y='tip', hue='time', data=df)"
  },
  {
    "objectID": "core/notebooks/notebook03_pandas.html#computations-using-pandas-broadcasting",
    "href": "core/notebooks/notebook03_pandas.html#computations-using-pandas-broadcasting",
    "title": "Introduction to pandas",
    "section": "Computations using pandas : broadcasting",
    "text": "Computations using pandas : broadcasting\nLet’s add a column that contains the tip percentage\n\n\nCode\ndf[\"tip_percentage\"] = df[\"tip\"] / df[\"total_bill\"]\ndf.head()\n\n\n\n\n\n\n\n\n\ntotal_bill\ntip\nsex\nsmoker\nday\ntime\nsize\ntip_percentage\n\n\n\n\n0\n16.99\n1.01\nFemale\nNo\nSun\nDinner\n2\n0.059447\n\n\n1\n10.34\n1.66\nMale\nNo\nSun\nDinner\n3\n0.160542\n\n\n2\n21.01\n3.50\nMale\nNo\nSun\nDinner\n3\n0.166587\n\n\n3\n23.68\n3.31\nMale\nNo\nSun\nDinner\n2\n0.139780\n\n\n4\n24.59\n3.61\nFemale\nNo\nSun\nDinner\n4\n0.146808\n\n\n\n\n\n\n\nThe computation\n```{python}\ndf[\"tip\"] / df[\"total_bill\"]\n```\nuses a broadcast rule.\n\nWe can multiply, add, subtract, etc. together numpy arrays, Series or pandas dataframes when the computation makes sense in view of their respective shape\n\nThis principle is called broadcast or broadcasting.\n\n\n\n\n\n\nNote\n\n\n\nBroadcasting is a key feature of numpy ndarray, see\n\nNumpy User’s guide\nPandas book\n\n\n\n\n\nCode\ndf[\"tip\"].shape, df[\"total_bill\"].shape\n\n\n((244,), (244,))\n\n\nThe tip and total_billcolumns have the same shape, so broadcasting performs pairwise division.\nThis corresponds to the following “hand-crafted” approach with a for loop:\n\n\nCode\nfor i in range(df.shape[0]):\n    df.loc[i, \"tip_percentage\"] = df.loc[i, \"tip\"] / df.loc[i, \"total_bill\"]\n\n\nBut using such a loop is:\n\nlonger to write\nless readable\nprone to mistakes\nand slower :(\n\nNEVER use Python for-loops unless you need to !\n\n\nCode\n%%timeit -n 10\nfor i in range(df.shape[0]):\n    df.loc[i, \"tip_percentage\"] = df.loc[i, \"tip\"] / df.loc[i, \"total_bill\"]\n\n\n23.2 ms ± 58.7 μs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n\n\n\n\nCode\n%%timeit -n 10\ndf[\"tip_percentage\"] = df[\"tip\"] / df[\"total_bill\"]\n\n\n73.8 μs ± 14.1 μs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n\n\nThe for loop is \\(\\approx\\) 100 times slower ! (even worse on larger data)\n\nPitfall. Changing values in a DataFrame\nWhen you want to change a value in a DataFrame, never use\ndf[\"tip_percentage\"].loc[i] = 42\nbut use\ndf.loc[i, \"tip_percentage\"] = 42\n\n\n\n\n\n\nCaution\n\n\n\nUse a single loc or iloc statement. The first version might not work: it might modify a copy of the column and not the dataframe itself !\n\n\nAnother example of broadcasting is:\n\n\nCode\n(100 * df[[\"tip_percentage\"]]).head()\n\n\n\n\n\n\n\n\n\ntip_percentage\n\n\n\n\n0\n5.944673\n\n\n1\n16.054159\n\n\n2\n16.658734\n\n\n3\n13.978041\n\n\n4\n14.680765\n\n\n\n\n\n\n\nwhere we multiplied each entry of the tip_percentage column by 100.\n\n\n\n\n\n\nRemark\n\n\n\nNote the difference between\ndf[['tip_percentage']]\nwhich returns a DataFrame containing only the tip_percentage column and\ndf['tip_percentage']\nwhich returns a Series containing the data of the tip_percentage column"
  },
  {
    "objectID": "core/notebooks/notebook03_pandas.html#some-more-plots",
    "href": "core/notebooks/notebook03_pandas.html#some-more-plots",
    "title": "Introduction to pandas",
    "section": "Some more plots",
    "text": "Some more plots\n\nHow do the tip percentages relates to the total bill ?\n\n\nCode\nsns.jointplot(\n    x=\"total_bill\", \n    y=\"tip_percentage\", \n    data=df\n)\n\n\n\n\n\n\n\n\n\n\n\nWho tips best ?\n\n\nCode\nsns.boxplot(\n    x='sex', \n    y='tip_percentage', \n    hue='smoker', \n    data=df\n)\n\n\n\n\n\n\n\n\n\n\n\nWho tips best without the tip_percentage outliers ?\n\n\nCode\nsns.boxplot(\n    x='sex', \n    y='tip_percentage', \n    hue='smoker', \n    data=df.loc[df[\"tip_percentage\"] &lt;= 0.3]\n)\n\n\n\n\n\n\n\n\n\nObject identity\n\n\nCode\nid(df)\n\n\n131038972871552"
  },
  {
    "objectID": "core/notebooks/notebook03_pandas.html#the-all-mighty-groupby-and-aggregate",
    "href": "core/notebooks/notebook03_pandas.html#the-all-mighty-groupby-and-aggregate",
    "title": "Introduction to pandas",
    "section": "The all-mighty groupby and aggregate",
    "text": "The all-mighty groupby and aggregate\nMany computations can be formulated as a groupby followed by and aggregation.\n\nWhat is the mean tip and tip percentage each day ?\n\n\nCode\ndf.head()\n\n\n\n\n\n\n\n\n\ntotal_bill\ntip\nsex\nsmoker\nday\ntime\nsize\ntip_percentage\n\n\n\n\n0\n16.99\n1.01\nFemale\nNo\nSun\nDinner\n2\n0.059447\n\n\n1\n10.34\n1.66\nMale\nNo\nSun\nDinner\n3\n0.160542\n\n\n2\n21.01\n3.50\nMale\nNo\nSun\nDinner\n3\n0.166587\n\n\n3\n23.68\n3.31\nMale\nNo\nSun\nDinner\n2\n0.139780\n\n\n4\n24.59\n3.61\nFemale\nNo\nSun\nDinner\n4\n0.146808\n\n\n\n\n\n\n\n\n\nCode\ntry:\n\n    df.groupby(\"day\", observed=True).mean()\nexcept TypeError:\n    print('TypeError: category dtype does not support aggregation \"mean\"')\n\n\nTypeError: category dtype does not support aggregation \"mean\"\n\n\nBut we do not care about the size column here, so we can use instead\n\n\nCode\n(\n    df[[\"total_bill\", \"tip\", \"tip_percentage\", \"day\"]]\n        .groupby(\"day\")\n        .mean()\n)\n\n\n/tmp/ipykernel_276531/1740663163.py:3: FutureWarning:\n\nThe default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n\n\n\n\n\n\n\n\n\n\ntotal_bill\ntip\ntip_percentage\n\n\nday\n\n\n\n\n\n\n\nFri\n17.151579\n2.734737\n0.169913\n\n\nSat\n20.441379\n2.993103\n0.153152\n\n\nSun\n21.410000\n3.255132\n0.166897\n\n\nThur\n17.682742\n2.771452\n0.161276\n\n\n\n\n\n\n\nIf we want to be more precise, we can groupby using several columns\n\n\nCode\n(\n    df[[\"total_bill\", \"tip\", \"tip_percentage\", \"day\", \"time\"]]   # selection\n        .groupby([\"day\",\"time\"])                                # partition\n        .mean()                                                  # aggregation\n)\n\n\n/tmp/ipykernel_276531/391063870.py:3: FutureWarning:\n\nThe default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n\n\n\n\n\n\n\n\n\n\n\ntotal_bill\ntip\ntip_percentage\n\n\nday\ntime\n\n\n\n\n\n\n\nFri\nDinner\n19.663333\n2.940000\n0.158916\n\n\nLunch\n12.845714\n2.382857\n0.188765\n\n\nSat\nDinner\n20.441379\n2.993103\n0.153152\n\n\nLunch\nNaN\nNaN\nNaN\n\n\nSun\nDinner\n21.410000\n3.255132\n0.166897\n\n\nLunch\nNaN\nNaN\nNaN\n\n\nThur\nDinner\n18.780000\n3.000000\n0.159744\n\n\nLunch\n17.664754\n2.767705\n0.161301\n\n\n\n\n\n\n\n\n\n\n\n\n\nRemarks\n\n\n\n\nWe obtain a DataFrame with a two-level indexing: on the day and the time\nGroups must be homogeneous: we have NaN values for empty groups (e.g. Sat, Lunch)\n\n\n\n\n\nPro tip\nSometimes, it is more convenient to get the groups as columns instead of a multi-level index.\nFor this, use reset_index:\n\n\nCode\n(\n    df[[\"total_bill\", \"tip\", \"tip_percentage\", \"day\", \"time\"]]   # selection\n        .groupby([\"day\", \"time\"])                                # partition\n        .mean() # aggregation\n        .reset_index()   # ako ungroup\n)\n\n\n/tmp/ipykernel_276531/835267922.py:3: FutureWarning:\n\nThe default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n\n\n\n\n\n\n\n\n\n\nday\ntime\ntotal_bill\ntip\ntip_percentage\n\n\n\n\n0\nFri\nDinner\n19.663333\n2.940000\n0.158916\n\n\n1\nFri\nLunch\n12.845714\n2.382857\n0.188765\n\n\n2\nSat\nDinner\n20.441379\n2.993103\n0.153152\n\n\n3\nSat\nLunch\nNaN\nNaN\nNaN\n\n\n4\nSun\nDinner\n21.410000\n3.255132\n0.166897\n\n\n5\nSun\nLunch\nNaN\nNaN\nNaN\n\n\n6\nThur\nDinner\n18.780000\n3.000000\n0.159744\n\n\n7\nThur\nLunch\n17.664754\n2.767705\n0.161301\n\n\n\n\n\n\n\n\n\nAnother pro tip: care about code readers\nComputations with pandas can include many operations that are pipelined until the final computation.\nPipelining many operations is good practice and perfectly normal, but in order to make the code readable you can put it between parenthesis (python expression) as follows:\n\n\nCode\n(\n    df[[\"total_bill\", \"tip\", \"tip_percentage\", \"day\", \"time\"]]\n    .groupby([\"day\", \"time\"])\n    .mean()\n    .reset_index()\n    # and on top of all this we sort the dataframe with respect \n    # to the tip_percentage\n    .sort_values(\"tip_percentage\")\n)\n\n\n/tmp/ipykernel_276531/45053252.py:3: FutureWarning:\n\nThe default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n\n\n\n\n\n\n\n\n\n\nday\ntime\ntotal_bill\ntip\ntip_percentage\n\n\n\n\n2\nSat\nDinner\n20.441379\n2.993103\n0.153152\n\n\n0\nFri\nDinner\n19.663333\n2.940000\n0.158916\n\n\n6\nThur\nDinner\n18.780000\n3.000000\n0.159744\n\n\n7\nThur\nLunch\n17.664754\n2.767705\n0.161301\n\n\n4\nSun\nDinner\n21.410000\n3.255132\n0.166897\n\n\n1\nFri\nLunch\n12.845714\n2.382857\n0.188765\n\n\n3\nSat\nLunch\nNaN\nNaN\nNaN\n\n\n5\nSun\nLunch\nNaN\nNaN\nNaN"
  },
  {
    "objectID": "core/notebooks/notebook03_pandas.html#displaying-a-dataframe-with-style",
    "href": "core/notebooks/notebook03_pandas.html#displaying-a-dataframe-with-style",
    "title": "Introduction to pandas",
    "section": "Displaying a DataFrame with style",
    "text": "Displaying a DataFrame with style\nNow, we can answer, with style, to the question: what are the average tip percentages along the week ?\n\n\nCode\n(\n    df[[\"tip_percentage\", \"day\", \"time\"]]\n    .groupby([\"day\", \"time\"])\n    .mean()\n    # At the end of the pipeline you can use .style\n    .style\n    # Print numerical values as percentages \n    .format(\"{:.2%}\")\n    .background_gradient()\n)\n\n\n/tmp/ipykernel_276531/838795167.py:3: FutureWarning:\n\nThe default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n\n\n\n\n\n\n\n\n \n \ntip_percentage\n\n\nday\ntime\n \n\n\n\n\nFri\nDinner\n15.89%\n\n\nLunch\n18.88%\n\n\nSat\nDinner\n15.32%\n\n\nLunch\nnan%\n\n\nSun\nDinner\n16.69%\n\n\nLunch\nnan%\n\n\nThur\nDinner\n15.97%\n\n\nLunch\n16.13%"
  },
  {
    "objectID": "core/notebooks/notebook03_pandas.html#removing-the-nan-values",
    "href": "core/notebooks/notebook03_pandas.html#removing-the-nan-values",
    "title": "Introduction to pandas",
    "section": "Removing the NaN values",
    "text": "Removing the NaN values\nBut the NaN values are somewhat annoying. Let’s remove them\n\n\nCode\n(\n    df[[\"tip_percentage\", \"day\", \"time\"]]\n    .groupby([\"day\", \"time\"])\n    .mean()\n    # We just add this from the previous pipeline\n    .dropna()\n    .style\n    .format(\"{:.2%}\")\n    .background_gradient()\n)\n\n\n/tmp/ipykernel_276531/2662169510.py:3: FutureWarning:\n\nThe default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n\n\n\n\n\n\n\n\n \n \ntip_percentage\n\n\nday\ntime\n \n\n\n\n\nFri\nDinner\n15.89%\n\n\nLunch\n18.88%\n\n\nSat\nDinner\n15.32%\n\n\nSun\nDinner\n16.69%\n\n\nThur\nDinner\n15.97%\n\n\nLunch\n16.13%\n\n\n\n\n\nNow, we see when tip_percentage is maximal. But what about the standard deviation?\n\nWe used only .mean() for now, but we can use several aggregating function using .agg()\n\n\n\nCode\n(\n    df[[\"tip_percentage\", \"day\", \"time\"]]\n    .groupby([\"day\", \"time\"])\n    .agg([\"mean\", \"std\"])   # we feed `agg`  with a list of names of callables \n    .dropna()\n    .style\n    .format(\"{:.2%}\")\n    .background_gradient()\n)\n\n\n/tmp/ipykernel_276531/3957220442.py:3: FutureWarning:\n\nThe default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n\n\n\n\n\n\n\n\n \n \ntip_percentage\n\n\n \n \nmean\nstd\n\n\nday\ntime\n \n \n\n\n\n\nFri\nDinner\n15.89%\n4.70%\n\n\nLunch\n18.88%\n4.59%\n\n\nSat\nDinner\n15.32%\n5.13%\n\n\nSun\nDinner\n16.69%\n8.47%\n\n\nThur\nLunch\n16.13%\n3.90%\n\n\n\n\n\nAnd we can use also .describe() as aggregation function. Moreover we - use the subset option to specify which column we want to style - we use (\"tip_percentage\", \"count\") to access multi-level index\n\n\nCode\n(\n    df[[\"tip_percentage\", \"day\", \"time\"]]\n    .groupby([\"day\", \"time\"])\n    .describe()    # all-purpose summarising function\n)\n\n\n/tmp/ipykernel_276531/3924876303.py:3: FutureWarning:\n\nThe default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n\n\n\n\n\n\n\n\n\n\n\ntip_percentage\n\n\n\n\ncount\nmean\nstd\nmin\n25%\n50%\n75%\nmax\n\n\nday\ntime\n\n\n\n\n\n\n\n\n\n\n\n\nFri\nDinner\n12.0\n0.158916\n0.047024\n0.103555\n0.123613\n0.144742\n0.179199\n0.263480\n\n\nLunch\n7.0\n0.188765\n0.045885\n0.117735\n0.167289\n0.187735\n0.210996\n0.259314\n\n\nSat\nDinner\n87.0\n0.153152\n0.051293\n0.035638\n0.123863\n0.151832\n0.188271\n0.325733\n\n\nSun\nDinner\n76.0\n0.166897\n0.084739\n0.059447\n0.119982\n0.161103\n0.187889\n0.710345\n\n\nThur\nDinner\n1.0\n0.159744\nNaN\n0.159744\n0.159744\n0.159744\n0.159744\n0.159744\n\n\nLunch\n61.0\n0.161301\n0.038972\n0.072961\n0.137741\n0.153846\n0.193424\n0.266312\n\n\n\n\n\n\n\n\n\nCode\n(\n    df[[\"tip_percentage\", \"day\", \"time\"]]\n    .groupby([\"day\", \"time\"])\n    .describe()\n    .dropna()\n    .style\n    .bar(subset=[(\"tip_percentage\", \"count\")])\n    .background_gradient(subset=[(\"tip_percentage\", \"50%\")])\n)\n\n\n/tmp/ipykernel_276531/673231177.py:3: FutureWarning:\n\nThe default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n\n\n\n\n\n\n\n\n \n \ntip_percentage\n\n\n \n \ncount\nmean\nstd\nmin\n25%\n50%\n75%\nmax\n\n\nday\ntime\n \n \n \n \n \n \n \n \n\n\n\n\nFri\nDinner\n12.000000\n0.158916\n0.047024\n0.103555\n0.123613\n0.144742\n0.179199\n0.263480\n\n\nLunch\n7.000000\n0.188765\n0.045885\n0.117735\n0.167289\n0.187735\n0.210996\n0.259314\n\n\nSat\nDinner\n87.000000\n0.153152\n0.051293\n0.035638\n0.123863\n0.151832\n0.188271\n0.325733\n\n\nSun\nDinner\n76.000000\n0.166897\n0.084739\n0.059447\n0.119982\n0.161103\n0.187889\n0.710345\n\n\nThur\nLunch\n61.000000\n0.161301\n0.038972\n0.072961\n0.137741\n0.153846\n0.193424\n0.266312"
  },
  {
    "objectID": "core/notebooks/notebook03_pandas.html#supervised-learning-of-tip-based-on-the-total_bill",
    "href": "core/notebooks/notebook03_pandas.html#supervised-learning-of-tip-based-on-the-total_bill",
    "title": "Introduction to pandas",
    "section": "Supervised learning of tip based on the total_bill",
    "text": "Supervised learning of tip based on the total_bill\nAs an example of very simple machine-learning problem, let us try to understand how we can predict tip based on total_bill.\n\n\nCode\nimport numpy as np\n\nplt.scatter(df[\"total_bill\"], df[\"tip\"])\nplt.xlabel(\"total_bill\", fontsize=12)\nplt.ylabel(\"tip\", fontsize=12)\n\n\nText(0, 0.5, 'tip')\n\n\n\n\n\n\n\n\n\nThere’s a rough linear dependence between the two. Let us try to find it by hand! Namely, we look for numbers \\(b\\) and \\(w\\) such that\ntip ≈ b + w × total_bill\nfor all the examples of pairs of (tip, total_bill) we observe in the data.\nIn machine learning, we say that this is a very simple example of a supervised learning problem (here it is a regression problem), where tip is the label and where total_bill is the (only) feature, for which we intend to use a linear predictor.\n\n\nCode\nplt.scatter(df[\"total_bill\"], df[\"tip\"])\nplt.xlabel(\"total_bill\", fontsize=12)\nplt.ylabel(\"tip\", fontsize=12)\n\nslope = 1.0\nintercept = 0.0\n\nx = np.linspace(0, 50, 1000)\nplt.plot(x, intercept + slope * x, color=\"red\")\n\n\n\n\n\n\n\n\n\n\nA more interactive way\nThis might require\n\n\nCode\n# !pip install ipympl\n\n\n\n\nCode\nimport ipywidgets as widgets\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n%matplotlib widget\n%matplotlib inline\n\nx = np.linspace(0, 50, 1000)\n\n@widgets.interact(intercept=(-5, 5, 1.), slope=(0, 1, .05))\ndef update(intercept=0.0, slope=0.5):\n    plt.scatter(df[\"total_bill\"], df[\"tip\"])\n    plt.plot(x, intercept + slope * x, color=\"red\")\n    plt.xlim((0, 50))\n    plt.ylim((0, 10))\n    plt.xlabel(\"total_bill\", fontsize=12)\n    plt.ylabel(\"tip\", fontsize=12)\n\n\n\n\n\n\n\n\n\n\n\n\nThis is kind of tedious to do this by hand… it would be nice to come up with an automated way of doing this. Moreover:\n\nWe are using a linear function, while something more complicated (such as a polynomial) might be better\nMore importantly, we use only the total_bill column to predict the tip, while we know about many other things\n\n\n\nCode\ndf.head()\n\n\n\n\n\n\n\n\n\ntotal_bill\ntip\nsex\nsmoker\nday\ntime\nsize\ntip_percentage\n\n\n\n\n0\n16.99\n1.01\nFemale\nNo\nSun\nDinner\n2\n0.059447\n\n\n1\n10.34\n1.66\nMale\nNo\nSun\nDinner\n3\n0.160542\n\n\n2\n21.01\n3.50\nMale\nNo\nSun\nDinner\n3\n0.166587\n\n\n3\n23.68\n3.31\nMale\nNo\nSun\nDinner\n2\n0.139780\n\n\n4\n24.59\n3.61\nFemale\nNo\nSun\nDinner\n4\n0.146808"
  },
  {
    "objectID": "core/notebooks/notebook03_pandas.html#one-hot-encoding-of-categorical-variables",
    "href": "core/notebooks/notebook03_pandas.html#one-hot-encoding-of-categorical-variables",
    "title": "Introduction to pandas",
    "section": "One-hot encoding of categorical variables",
    "text": "One-hot encoding of categorical variables\nWe can’t perform computations (products and sums) with columns containing categorical variables. So, we can’t use them like this to predict the tip. We need to convert them to numbers somehow.\nThe most classical approach for this is one-hot encoding (or “create dummies” or “binarize”) of the categorical variables, which can be easily achieved with pandas.get_dummies\nWhy one-hot ? See wikipedia for a plausible explanation\n\n\nCode\ndf_one_hot = pd.get_dummies(df, prefix_sep='#')\ndf_one_hot.head(5)\n\n\n\n\n\n\n\n\n\ntotal_bill\ntip\nsize\ntip_percentage\nsex#Female\nsex#Male\nsmoker#No\nsmoker#Yes\nday#Fri\nday#Sat\nday#Sun\nday#Thur\ntime#Dinner\ntime#Lunch\n\n\n\n\n0\n16.99\n1.01\n2\n0.059447\nTrue\nFalse\nTrue\nFalse\nFalse\nFalse\nTrue\nFalse\nTrue\nFalse\n\n\n1\n10.34\n1.66\n3\n0.160542\nFalse\nTrue\nTrue\nFalse\nFalse\nFalse\nTrue\nFalse\nTrue\nFalse\n\n\n2\n21.01\n3.50\n3\n0.166587\nFalse\nTrue\nTrue\nFalse\nFalse\nFalse\nTrue\nFalse\nTrue\nFalse\n\n\n3\n23.68\n3.31\n2\n0.139780\nFalse\nTrue\nTrue\nFalse\nFalse\nFalse\nTrue\nFalse\nTrue\nFalse\n\n\n4\n24.59\n3.61\n4\n0.146808\nTrue\nFalse\nTrue\nFalse\nFalse\nFalse\nTrue\nFalse\nTrue\nFalse\n\n\n\n\n\n\n\nOnly the categorical columns have been one-hot encoded. For instance, the \"day\" column is replaced by 4 columns named \"day#Thur\", \"day#Fri\", \"day#Sat\", \"day#Sun\", since \"day\" has 4 modalities (see next line).\n\n\nCode\ndf['day'].unique()\n\n\n['Sun', 'Sat', 'Thur', 'Fri']\nCategories (4, object): ['Fri', 'Sat', 'Sun', 'Thur']\n\n\n\n\nCode\ndf_one_hot.dtypes\n\n\ntotal_bill        float64\ntip               float64\nsize                int64\ntip_percentage    float64\nsex#Female           bool\nsex#Male             bool\nsmoker#No            bool\nsmoker#Yes           bool\nday#Fri              bool\nday#Sat              bool\nday#Sun              bool\nday#Thur             bool\ntime#Dinner          bool\ntime#Lunch           bool\ndtype: object"
  },
  {
    "objectID": "core/notebooks/notebook03_pandas.html#pitfall.-colinearities-with-one-hot-encoding",
    "href": "core/notebooks/notebook03_pandas.html#pitfall.-colinearities-with-one-hot-encoding",
    "title": "Introduction to pandas",
    "section": "Pitfall. Colinearities with one-hot encoding",
    "text": "Pitfall. Colinearities with one-hot encoding\nSums over dummies for sex, smoker, day, time and size are all equal to one (by constrution of the one-hot encoded vectors).\n\nLeads to colinearities in the matrix of features\nIt is much harder to train a linear regressor when the columns of the features matrix has colinearities\n\n\n\nCode\nday_cols = [col for col in df_one_hot.columns if col.startswith(\"day\")]\ndf_one_hot[day_cols].head()\ndf_one_hot[day_cols].sum(axis=1)\n\n\n0      1\n1      1\n2      1\n3      1\n4      1\n      ..\n239    1\n240    1\n241    1\n242    1\n243    1\nLength: 244, dtype: int64\n\n\n\n\nCode\nall(df_one_hot[day_cols].sum(axis=1) == 1)\n\n\nTrue\n\n\nThe most standard solution is to remove a modality (i.e. remove a one-hot encoding vector). Simply achieved by specifying drop_first=True in the get_dummies function.\n\n\nCode\ndf[\"day\"].unique()\n\n\n['Sun', 'Sat', 'Thur', 'Fri']\nCategories (4, object): ['Fri', 'Sat', 'Sun', 'Thur']\n\n\n\n\nCode\npd.get_dummies(df, prefix_sep='#', drop_first=True).head()\n\n\n\n\n\n\n\n\n\ntotal_bill\ntip\nsize\ntip_percentage\nsex#Male\nsmoker#Yes\nday#Sat\nday#Sun\nday#Thur\ntime#Lunch\n\n\n\n\n0\n16.99\n1.01\n2\n0.059447\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\n\n\n1\n10.34\n1.66\n3\n0.160542\nTrue\nFalse\nFalse\nTrue\nFalse\nFalse\n\n\n2\n21.01\n3.50\n3\n0.166587\nTrue\nFalse\nFalse\nTrue\nFalse\nFalse\n\n\n3\n23.68\n3.31\n2\n0.139780\nTrue\nFalse\nFalse\nTrue\nFalse\nFalse\n\n\n4\n24.59\n3.61\n4\n0.146808\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\n\n\n\n\n\n\n\nNow, if a categorical feature has \\(K\\) modalities, we use only \\(K-1\\) dummies. For instance, there is no more sex#Female binary column.\nQuestion. So, a linear regression won’t fit a weight for sex#Female. But, where do the model weights of the dropped binary columns go ?\nAnswer. They just “go” to the intercept: interpretation of the population bias depends on the “dropped” one-hot encodings.\nSo, we actually fit: \\[\\begin{array}{rl} \\texttt{tip} \\approx b & + w_1 \\times \\texttt{total_bill} + w_2 \\times \\texttt{size} \\\\ & + w_3 \\times \\texttt{sex#Male} + w_4 \\times \\texttt{smoker#Yes} \\\\ & + w_5 \\times \\texttt{day#Sat} + w_6 \\times \\texttt{day#Sun} + w_7 \\times \\texttt{day#Thur} \\\\ & + w_8 \\times \\texttt{time#Lunch} \\end{array}\\]"
  },
  {
    "objectID": "core/notebooks/notebook05_sparkrdd.html",
    "href": "core/notebooks/notebook05_sparkrdd.html",
    "title": "Introduction to Spark RDD",
    "section": "",
    "text": "Code\nimport numpy as np\nCode\nimport os\nimport sys\nimport inspect\n\nos.environ['PYSPARK_PYTHON'] = sys.executable\nos.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable\nCode\nfrom pyspark import SparkConf, SparkContext\n\nconf = SparkConf().setAppName(\"Spark RDD Course\")\nsc = SparkContext(conf=conf)\n\n\n25/03/10 16:54:46 WARN Utils: Your hostname, boucheron-Precision-5480 resolves to a loopback address: 127.0.1.1; using 172.23.32.10 instead (on interface enxac91a1bd3e89)\n25/03/10 16:54:46 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\nSetting default log level to \"WARN\".\nTo adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n25/03/10 16:54:46 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n25/03/10 16:54:46 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\nCode\nrdd = sc.parallelize(range(64))\nNote that parallelize takes an optional argument to choose the number of partitions\nCode\nrdd.getNumPartitions()\n\n\n20\nCode\nrdd = sc.parallelize(range(1000), 10)\nrdd.getNumPartitions()\n\n\n10"
  },
  {
    "objectID": "core/notebooks/notebook05_sparkrdd.html#transformations",
    "href": "core/notebooks/notebook05_sparkrdd.html#transformations",
    "title": "Introduction to Spark RDD",
    "section": "Transformations",
    "text": "Transformations\n\nmap\n\n\nCode\nrdd = sc.parallelize([2, 3, 4])\nrdd = rdd.map(lambda x: list(range(1, x)))\n\n\n\n\nCode\nrdd\n\n\nPythonRDD[3] at RDD at PythonRDD.scala:53\n\n\n\n\nCode\n(\n    sc.parallelize([2, 3, 4])\n      .map(lambda x: list(range(1, x)))\n)\n\n\nPythonRDD[5] at RDD at PythonRDD.scala:53\n\n\nmap is a transformation. It is lazily evaluated. Hence execution is delayed until an action is met in the DAG).\n\n\nCode\nrdd.collect()  # collect is an action \n\n\n[Stage 0:&gt;                                                        (0 + 20) / 20]                                                                                \n\n\n[[1], [1, 2], [1, 2, 3]]\n\n\n\n\nCode\n(\n    sc.parallelize([2, 3, 4])\n      .map(lambda x: list(range(1, x)))\n      .collect()\n)\n\n\n[[1], [1, 2], [1, 2, 3]]\n\n\n\n\nExercice: map with a method\nWarning. This example is a bad practice !!! Don’t do this at home\n\n\nCode\ndbtel = {'arthur': 1234, 'riad': 4567, 'anatole': 3615}\n\n\n\n\nCode\nclass TelephoneDB(object):\n    \n    def __init__(self):\n        self.tel = {'arthur': 1234, 'riad': 4567, 'anatole': 3615}\n   \n    def add_tel(self, name):\n        return name, self.tel.get(name)\n\n\n\n\nCode\ntel_db = TelephoneDB()\nnames = ['arthur', 'riad']\n\n\n\n\nCode\nrdd = (\n    sc\n        .parallelize(names)\n        .map(tel_db.add_tel)\n        .collect()\n)\n\nrdd\n\n\n[('arthur', 1234), ('riad', 4567)]\n\n\n\nReplace the tel dictionary by a defaultdict with default number 999\nUse it on a rdd containing names as above including an unknown one, and try it\n\n\n\nCode\nfrom collections import defaultdict\n\nclass TelephoneDefaultDB(object):\n    \n    def __init__(self):\n        self.tel = defaultdict(lambda: 999, {'arthur': 1234, 'riad': 4567, 'anatole': 3615})\n    \n    def add_tel(self, name):\n        return name, self.tel[name]\n    \n    def add_tel_rdd(self, rdd):  \n        return rdd.map(self.add_tel)\n\n\n\n\nCode\ntel_db = TelephoneDefaultDB()\nnames = ['riad', 'anatole', 'yiyang']\nrdd = (\n    sc\n        .parallelize(names)\n        .map(tel_db.add_tel)\n        .collect()\n)\nrdd\n\n\n[('riad', 4567), ('anatole', 3615), ('yiyang', 999)]\n\n\n\n\n\n\n\n\nCaution\n\n\n\nIt is a bad idea to pass methods to spark’s map. Since add_tel needs self, the whole object is serialized so that spark can use it.\nThis breaks if the tel is large, or if it is not serializable.\n\n\n\n\nflatMap\n\n\nCode\nrdd = sc.parallelize([2, 3, 4, 5])\n( \n    rdd\n        .flatMap(lambda x: range(1, x))\n        .collect()\n)\n\n\n[1, 1, 2, 1, 2, 3, 1, 2, 3, 4]\n\n\n\n\nfilter\n\n\nCode\nrdd = sc.parallelize(range(10))\n\nrdd\\\n    .filter(lambda x: x % 2 == 0)\\\n    .collect()\n\n\n[0, 2, 4, 6, 8]\n\n\n\n\ndistinct\n\n\nCode\nrdd = sc.parallelize([1, 1, 4, 2, 1, 3, 3])\nrdd.distinct().collect()\n\n\n[1, 2, 3, 4]\n\n\n\n\n“Pseudo-set” operations\n\n\nCode\nrdd1 = sc.parallelize(range(5))\nrdd2 = sc.parallelize(range(3, 9))\nrdd3 = rdd1.union(rdd2)\nrdd3.collect()\n\n\n[0, 1, 2, 3, 4, 3, 4, 5, 6, 7, 8]\n\n\n\n\nCode\nrdd3.distinct().collect()\n\n\n[0, 1, 2, 3, 4, 5, 6, 7, 8]\n\n\n\n\nCode\nrdd1 = sc.parallelize([1, 2])\nrdd2 = sc.parallelize([\"a\", \"b\"])\nrdd1.cartesian(rdd2).collect()\n\n\n[(1, 'a'), (1, 'b'), (2, 'a'), (2, 'b')]"
  },
  {
    "objectID": "core/notebooks/notebook05_sparkrdd.html#actions",
    "href": "core/notebooks/notebook05_sparkrdd.html#actions",
    "title": "Introduction to Spark RDD",
    "section": "Actions",
    "text": "Actions\ncollect is obviously an action…\n\ncount, countByValue\n\n\nCode\nrdd = sc.parallelize([1, 3, 1, 2, 2, 2])\nrdd.count()\n\n\n6\n\n\n\n\nCode\nrdd.countByValue()\n\n\ndefaultdict(int, {1: 2, 3: 1, 2: 3})\n\n\nWhy does countByValue() returns a dictionary?\nAre count() and countByValue() actions or transformations?\n\n\nCode\nu = np.int32((np.random.sample(100000) * 100000))  # 100000 random integers uniformly distributed on 0, ..., 100000\n\np = (\n    sc.parallelize(u)\n    .countByValue()\n)\n\nq = sorted(\n    p.items(), \n    key = lambda x : x[1], \n    reverse=True\n)\n\nq[0:10]\n\nq[0], 1 + np.log(len(u))/ np.log(np.log(len(u))), len(q)\n\n\n((np.int32(84147), 7), np.float64(5.711710714547694), 63106)\n\n\n\nHow many distinct values do you expect in u ?\nHow large is the largest value in \\(q\\) ?\n\n\n\nCode\nfrom scipy.stats import poisson \n\n( \n    len(q), \n    (1-np.exp(-1)) * len(u),\n    poisson.ppf(1.-1./len(u), 1)\n)\n\n\n(63106, np.float64(63212.05588285577), np.float64(8.0))\n\n\n\n\ntake, takeOrdered\n\n\nCode\nrdd = sc.parallelize([(3, 'a'), (1, 'b'), (2, 'd')])\n\n\n\n\nCode\n(1, 'b') &lt;=  (2, 'd') &lt;= (3, 'a')\n\n\nTrue\n\n\n\n\nCode\nrdd.takeOrdered(2)\n\n\n[(1, 'b'), (2, 'd')]\n\n\n\n\nCode\nrdd.takeOrdered(2, key=lambda x: x[1])\n\n\n[(3, 'a'), (1, 'b')]\n\n\n\n\nreduce, fold\n\n\nCode\nrdd = sc.range(1, 4)\nrdd.reduce(lambda a, b: a + b)\n\n\n6\n\n\n\n\nCode\nrdd = sc.range(1, 4, numSlices=7)\nrdd.reduce(lambda a, b: a + b)\n\n\n6\n\n\n\n\nCode\nrdd = sc.parallelize(range(1,4), 3)\nrdd.reduce(lambda a, b: a + b)\n\n\n6\n\n\n\n\nCode\n( \n    sc.parallelize(range(1, 4), 2)\n      .fold(0, lambda a, b: a + b)\n)\n\n\n6\n\n\n\n\nCode\n( \n    sc.parallelize(range(1, 4), 1)\n      .fold(3, lambda a, b: a + b)\n),( \n    sc.parallelize(range(1, 4), 2)\n      .fold(2, lambda a, b: a + b)\n)\n\n\n(12, 12)\n\n\n\n\nCode\nrdd =  sc.parallelize(range(1, 4),3)\n( \n    rdd.fold(1, lambda a, b: a + b), \n    rdd.getNumPartitions()\n)\n\n\n(10, 3)\n\n\n\n\nCode\nrdd =  sc.parallelize(range(1, 4),4)\n\n(\n    rdd.fold(1, lambda a, b: a + b), \n    rdd.getNumPartitions()\n)\n\n\n(11, 4)\n\n\n\n\nCode\nrdd = sc.parallelize([1, 2, 4], 2)\nrdd.fold(2, lambda a, b: a + b)\n\n\n13\n\n\n\n\nCode\nrdd = sc.parallelize([1, 2, 4], 3)\nrdd.fold(2, lambda a, b: a + b)\n\n\n15\n\n\n\n\nCode\nrdd.getNumPartitions()\n\n\n3\n\n\n\n\naggregate\n\n\nCode\nseqOp = lambda x, y: (x[0] + y, x[1] + 1)\ncombOp = lambda x, y: (x[0] + y[0], x[1] + y[1])\n\nrdd = sc.parallelize([1, 2, 3, 4], 8)\n(\n    rdd.aggregate((0, 0), seqOp, combOp), rdd.getNumPartitions()\n)\n\n\n((10, 4), 8)\n\n\n\n\nCode\nop = lambda x, y: x+y\nrdd = sc.parallelize([1, 2, 3, 4], 4)\n(\n    rdd.aggregate(0, op, op),\n    rdd.getNumPartitions()\n)\n\n\n(10, 4)\n\n\n\n\nExercice: sum of powers with aggregate\n\nUsing aggregate, compute the sum, the sum of squares \\(x^2\\) and the sum of cubes \\(x^3\\) for \\(x \\in \\{1, \\ldots, 10 \\}\\).\nCheck your computations using numpy\n\n\n\nCode\nseqOp = lambda x, y: (x[0] + y, x[1] + y ** 2, x[2] + y ** 3)\n\n\n\n\nCode\ncombOp = lambda x, y: (x[0] + y[0], x[1] + y[1], x[2] + y[2])\n\n\n\n\nCode\nsc.range(5)\n\n\nPythonRDD[68] at RDD at PythonRDD.scala:53\n\n\n\n\nCode\n( \n    sc\n        .range(1, 11)\n        .aggregate((0, 0, 0), seqOp, combOp)\n)\n\n\n(55, 385, 3025)\n\n\n\n\nCode\nimport numpy as np\n\nx = np.arange(1, 11)\nx\n\n\narray([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10])\n\n\n\n\nCode\nx.sum(), (x**2).sum(), (x**3).sum(), x.cumsum()\n\n\n(np.int64(55),\n np.int64(385),\n np.int64(3025),\n array([ 1,  3,  6, 10, 15, 21, 28, 36, 45, 55]))\n\n\n\n\nComputing an empirical variance with aggregate\nAssume a sample is stored as a RDD. Using aggregate, compute the sample variance \\(\\frac{1}{n}\\sum_{i=1}^n (x_i - \\overline{X}_n)^2\\) where \\(\\overline{X}_n = \\frac{1}{n} \\sum_{i=1}^n x_i\\)"
  },
  {
    "objectID": "core/notebooks/notebook05_sparkrdd.html#transformations-1",
    "href": "core/notebooks/notebook05_sparkrdd.html#transformations-1",
    "title": "Introduction to Spark RDD",
    "section": "Transformations",
    "text": "Transformations\n\nkeys, values\n\n\nCode\nrdd.keys().collect()\n\n\n[1, 2, 2]\n\n\n\n\nCode\nrdd.values().collect()\n\n\n[['a', 7], ['b', 13], ['c', 17]]\n\n\n\n\n\n\n\n\nWarning\n\n\n\nAll elements must be tuples with two elements (key and value)\n\n\n\n\nCode\nrdd = sc.parallelize([[1, \"a\", 7], [2, \"b\", 13], [2, \"c\", 17]])\nrdd.keys().collect()\n\n\n[1, 2, 2]\n\n\n\n\nCode\nrdd.values().collect()\n\n\n['a', 'b', 'c']\n\n\nThe values are not what we expected wrong… so we must do\n\n\nCode\nrdd = ( sc.parallelize([[1, \"a\", 7], [2, \"b\", 13], [2, \"c\", 17]])\n          .map(lambda x: (x[0], x[1:]))\n      )\nrdd.keys().collect()\n\n\n[1, 2, 2]\n\n\n\n\nCode\nrdd.values().collect()\n\n\n[['a', 7], ['b', 13], ['c', 17]]\n\n\nNow, the values are correct.\n\n\nmapValues, flatMapValues\n\n\nCode\nrdd = sc.parallelize([(\"a\", \"x y z\"), (\"b\", \"p r\")])\n\nrdd.mapValues(lambda v: v.split(' ')).collect(), rdd.collect()\n\n\n([('a', ['x', 'y', 'z']), ('b', ['p', 'r'])], [('a', 'x y z'), ('b', 'p r')])\n\n\n\n\nCode\nrdd.flatMapValues(lambda v: v.split(' ')).collect()\n\n\n[('a', 'x'), ('a', 'y'), ('a', 'z'), ('b', 'p'), ('b', 'r')]\n\n\n\n\ngroupByKey\n\n\nCode\nrdd = sc.parallelize([(\"a\", 1), (\"b\", 1), (\"a\", 1), (\"b\", 3), (\"c\", 42)])\n( \n    rdd.groupByKey()\n       .mapValues(list)\n       .collect()\n)\n\n\n[('b', [1, 3]), ('c', [42]), ('a', [1, 1])]\n\n\n\n\nCode\nrdd.groupByKey().collect()\n\n\n[('b', &lt;pyspark.resultiterable.ResultIterable at 0x706c3200c440&gt;),\n ('c', &lt;pyspark.resultiterable.ResultIterable at 0x706c325dd910&gt;),\n ('a', &lt;pyspark.resultiterable.ResultIterable at 0x706c325df440&gt;)]\n\n\n\n\nreduceByKey\n\n\nCode\nrdd = sc.parallelize([(\"a\", 1), (\"b\", 1), (\"a\", 1)])\nrdd.reduceByKey(lambda a, b: a + b).collect()\n\n\n[('b', 1), ('a', 2)]\n\n\n\n\ncombineByKey\n\n\nCode\nrdd = sc.parallelize([('a', 1), ('b', 2), ('a', 13)])\n\ndef add(a, b): \n    return a + str(b)\n\nrdd.combineByKey(str, add, add).collect()\n\n\n[('b', '2'), ('a', '113')]\n\n\n\n\njoin, rightOuterJoin, leftOuterJoin\n\n\nCode\nemployees = sc.parallelize([\n    (31, \"Rafferty\"),\n    (33, \"Jones\"),\n    (33, \"Heisenberg\"),\n    (34, \"Robinson\"),\n    (34, \"Smith\"),\n    (None, \"Williams\")\n])\n\n\n\n\nCode\ndepartments = sc.parallelize([\n    (31, \"Sales\"),\n    (33, \"Engineering\"),\n    (34, \"Clerical\"),\n    (35, \"Marketing\")\n])\n\n\n\n\nCode\n( \n    employees\n        .join(departments)\n        .sortByKey()\n        .collect()\n)\n\n\n[(31, ('Rafferty', 'Sales')),\n (33, ('Jones', 'Engineering')),\n (33, ('Heisenberg', 'Engineering')),\n (34, ('Robinson', 'Clerical')),\n (34, ('Smith', 'Clerical'))]\n\n\n\n\nCode\n( \n    employees\n        .rightOuterJoin(departments)\n        .sortByKey()\n        .collect()\n)\n\n\n[(31, ('Rafferty', 'Sales')),\n (33, ('Jones', 'Engineering')),\n (33, ('Heisenberg', 'Engineering')),\n (34, ('Robinson', 'Clerical')),\n (34, ('Smith', 'Clerical')),\n (35, (None, 'Marketing'))]\n\n\n\n\nCode\n(\n    employees\n        .leftOuterJoin(departments)\n        .collect()\n)\n\n\n[(None, ('Williams', None)),\n (31, ('Rafferty', 'Sales')),\n (33, ('Jones', 'Engineering')),\n (33, ('Heisenberg', 'Engineering')),\n (34, ('Robinson', 'Clerical')),\n (34, ('Smith', 'Clerical'))]"
  },
  {
    "objectID": "core/notebooks/notebook05_sparkrdd.html#actions-1",
    "href": "core/notebooks/notebook05_sparkrdd.html#actions-1",
    "title": "Introduction to Spark RDD",
    "section": "Actions",
    "text": "Actions\n\n\nCode\nemployees.countByKey()\n\n\ndefaultdict(int, {31: 1, 33: 2, 34: 2, None: 1})\n\n\n\n\nCode\nemployees.lookup(33)\n\n\n['Jones', 'Heisenberg']\n\n\n\n\nCode\nemployees.lookup(None)\n\n\n['Williams']\n\n\n\n\nCode\nemployees.collectAsMap()\n\n\n{31: 'Rafferty', 33: 'Heisenberg', 34: 'Smith', None: 'Williams'}"
  },
  {
    "objectID": "core/notebooks/notebook05_sparkrdd.html#references",
    "href": "core/notebooks/notebook05_sparkrdd.html#references",
    "title": "Introduction to Spark RDD",
    "section": "References",
    "text": "References\nSpark Core reference"
  },
  {
    "objectID": "core/notebooks/notebook07_json-format.html",
    "href": "core/notebooks/notebook07_json-format.html",
    "title": "Using JSON data with Python",
    "section": "",
    "text": "This notebook is concerned with JSON a format that serves many purposes. Just as csv files, json files are important sources and sinks for Spark. As a exchange format, JSON is also a serialization tool for Python and many other languages. JSON provides a way to accomodate semi-structured data in otherwise tabular environments (dataframes and databases tables).\nThe notebook is organized in the following way:"
  },
  {
    "objectID": "core/notebooks/notebook07_json-format.html#serialization-and-deserialization-of-built-in-types",
    "href": "core/notebooks/notebook07_json-format.html#serialization-and-deserialization-of-built-in-types",
    "title": "Using JSON data with Python",
    "section": "Serialization and deserialization of built-in types",
    "text": "Serialization and deserialization of built-in types\n\n\nCode\nimport json\n\nobj = {\n    \"name\": \"Foo Bar\",\n    \"age\": 78,\n    \"friends\": [\"Jane\",\"John\"],\n    \"balance\": 345.80,\n    \"other_names\":(\"Doe\",\"Joe\"),\n    \"active\": True,\n    \"spouse\": None\n}\n\nprint(json.dumps(obj, sort_keys=True, indent=4))\n\n\n{\n    \"active\": true,\n    \"age\": 78,\n    \"balance\": 345.8,\n    \"friends\": [\n        \"Jane\",\n        \"John\"\n    ],\n    \"name\": \"Foo Bar\",\n    \"other_names\": [\n        \"Doe\",\n        \"Joe\"\n    ],\n    \"spouse\": null\n}\n\n\n\n\n\n\n\n\nNote\n\n\n\njson.dumps() outputs a JSON formatted string.\nNot every type of object can be fed to json.dumps().\n\n\n\n\nCode\nwith open('user.json','w') as file:\n    json.dump(obj, file, sort_keys=True, indent=4)\n\n\n\n\nCode\n!cat user.json\n\n\n{\n    \"active\": true,\n    \"age\": 78,\n    \"balance\": 345.8,\n    \"friends\": [\n        \"Jane\",\n        \"John\"\n    ],\n    \"name\": \"Foo Bar\",\n    \"other_names\": [\n        \"Doe\",\n        \"Joe\"\n    ],\n    \"spouse\": null\n}\n\n\n\n\nCode\njson.loads('{\"active\": true, \"age\": 78, \"balance\": 345.8, \"friends\": [\"Jane\",\"John\"], \"name\": \"Foo Bar\", \"other_names\": [\"Doe\",\"Joe\"],\"spouse\":null}')\n\n\n{'active': True,\n 'age': 78,\n 'balance': 345.8,\n 'friends': ['Jane', 'John'],\n 'name': 'Foo Bar',\n 'other_names': ['Doe', 'Joe'],\n 'spouse': None}\n\n\n\n\nCode\nwith open('user.json', 'r') as file:\n    user_data = json.load(file)\n\nprint(user_data)\n\n\n{'active': True, 'age': 78, 'balance': 345.8, 'friends': ['Jane', 'John'], 'name': 'Foo Bar', 'other_names': ['Doe', 'Joe'], 'spouse': None}\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nWhat happens if we feed json.dumps() with a numpy array?\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nWhat happens if we feed json.dumps() with a datatime object?"
  },
  {
    "objectID": "core/notebooks/notebook07_json-format.html#serialization-and-deserialization-of-custom-objects",
    "href": "core/notebooks/notebook07_json-format.html#serialization-and-deserialization-of-custom-objects",
    "title": "Using JSON data with Python",
    "section": "Serialization and deserialization of custom objects",
    "text": "Serialization and deserialization of custom objects\n\n\nCode\nclass User(object):\n    \"\"\"Custom User Class\n    \"\"\"\n    def __init__(self, name, age, active, balance, \n                 other_names, friends, spouse):\n        self.name = name\n        self.age = age\n        self.active = active\n        self.balance = balance\n        self.other_names = other_names\n        self.friends = friends\n        self.spouse = spouse\n            \n    def __repr__(self):\n        s = \"User(\"\n        s += \"name=\" + repr(self.name)\n        s += \", age=\" + repr(self.age)\n        s += \", active=\" + repr(self.active)\n        s += \", other_names=\" + repr(self.other_names)\n        s += \", friends=\" + repr(self.friends)\n        s += \", spouse=\" + repr(self.spouse) + \")\"\n        return s\n\n\n\n\n\n\n\n\nTip\n\n\n\nBrush up your dunder/magic methods, for example in Fluent Python by Ramalho (Chapter I: The Python data model, Section Overview of Special Methods)\n\n\n\n\nCode\nnew_user = User(\n    name = \"Foo Bar\",\n    age = 78,\n    friends = [\"Jane\", \"John\"],\n    balance = 345.80,\n    other_names = (\"Doe\", \"Joe\"),\n    active = True,\n    spouse = None\n)\n\nnew_user\n\n\nUser(name='Foo Bar', age=78, active=True, other_names=('Doe', 'Joe'), friends=['Jane', 'John'], spouse=None)\n\n\n\n\n\n\n\n\nNote\n\n\n\nUncomment to see what happens\n\n\n\n\nCode\n# This will raise a TypeError\n# json.dumps(new_user)\n\n\nAs expected, the custom object new_user is not JSON serializable. So let’s build a method that does that for us.\n\nThis comes as no surprise to us, since earlier on we observed that the json module only handles the built-in types, and User is not one.\nWe need to send our user data to a client over a network, so how do we get ourselves out of this error state?\nA simple solution would be to convert our custom type into a serializable type that is a built-in type. We can conveniently define a method convert_to_dict() that returns a dictionary representation of our object. json.dumps() takes in a optional argument, default, which specifies a function to be called if the object is not serializable. This function returns a JSON encodable version of the object.\n\nRecall that class obj has a dunder attribute __dict__ that provides a basis for obtaining a dictionary with the attributes of any object:\n\n\nCode\nnew_user.__dict__\n\n\n{'name': 'Foo Bar',\n 'age': 78,\n 'active': True,\n 'balance': 345.8,\n 'other_names': ('Doe', 'Joe'),\n 'friends': ['Jane', 'John'],\n 'spouse': None}\n\n\n\n\nCode\ndef obj_to_dict(obj):\n    \"\"\"Converts an object to a dictionary representation of the object including \n    meta-data information about the object's module and class name.\n\n    Parameters\n    ----------\n    obj : `object`\n        A python object to be converted into a dictionary representation\n\n    Returns\n    -------\n    output : `dict`\n        A dictionary representation of the object\n    \"\"\"\n    # Add object meta data \n    obj_dict = {\n        \"__class__\": obj.__class__.__name__,\n        \"__module__\": obj.__module__\n    }\n    # Add the object properties\n    return obj_dict | obj.__dict__\n\n\n\n\nCode\nobj_to_dict(new_user)\n\n\n{'__class__': 'User',\n '__module__': '__main__',\n 'name': 'Foo Bar',\n 'age': 78,\n 'active': True,\n 'balance': 345.8,\n 'other_names': ('Doe', 'Joe'),\n 'friends': ['Jane', 'John'],\n 'spouse': None}\n\n\nThe function convert_to_dict does the following:\n\ncreate a dictionary named obj_dict to act as the dict representation of our object.\ndunder attributes __class__.__name__ and __module__ provide crucial metadata on the object: the class name and the module name\nadd the instance attributes of the object using obj.__dict__ (Python stores instance attributes in a dictionary)\n\nThe resulting obj_dict is now serializable (provided all attributes of our object are).\nNow we can comfortably call json.dumps() on the object and pass default=convert_to_dict\n\n\n\n\n\n\nNote\n\n\n\nObviously this fails if one of the attributes is not JSON serializable\n\n\n\n\nCode\nprint(json.dumps(new_user, default=obj_to_dict, indent=4, sort_keys=True))\n\n\n{\n    \"__class__\": \"User\",\n    \"__module__\": \"__main__\",\n    \"active\": true,\n    \"age\": 78,\n    \"balance\": 345.8,\n    \"friends\": [\n        \"Jane\",\n        \"John\"\n    ],\n    \"name\": \"Foo Bar\",\n    \"other_names\": [\n        \"Doe\",\n        \"Joe\"\n    ],\n    \"spouse\": null\n}\n\n\nNow, if we want to decode (deserialiaze) a custom object, and create the correct object type, we need a function that does the inverse of obj_to_dict, since json.loads simply returns a dict:\n\n\nCode\nuser_data = json.loads(json.dumps(new_user, default=obj_to_dict))\nprint(user_data)\n\n\n{'__class__': 'User', '__module__': '__main__', 'name': 'Foo Bar', 'age': 78, 'active': True, 'balance': 345.8, 'other_names': ['Doe', 'Joe'], 'friends': ['Jane', 'John'], 'spouse': None}\n\n\n\n\n\n\n\n\nNote\n\n\n\nWe need json.loads() to reconstruct a User object from this dictionary: json.loads() takes an optional argument object_hook which specifies a function that returns the desired custom object, given the decoded output (which in this case, is a dict).\n\n\n\n\nCode\ndef dict_to_obj(input_dict):\n    \"\"\"Converts a dictionary representation of an object to an instance of the object.\n\n    Parameters\n    ----------\n    input_dict : `dict`\n        A dictionary representation of the object, containing \"__module__\" \n        and \"__class__\" metadata\n\n    Returns\n    -------    \n    obj : `object`\n        A python object constructed from the dictionary representation    \n    \"\"\"\n    assert \"__class__\" in input_dict and \"__module__\" in input_dict\n    class_name = input_dict.pop(\"__class__\")\n    module_name = input_dict.pop(\"__module__\")\n    module = __import__(module_name)\n    class_ = getattr(module, class_name)\n    obj = class_(**input_dict)\n    return obj\n\n\nThis function does the following:\n\nExtract the class name from the dictionary under the key __class__\nExtract the module name from the dictionary under the key __module__\nImports the module and get the class\nInstantiate the class by giving to the class constructor all the instance arguments through dictionary unpacking\n\n\n\nCode\nobj_data = json.dumps(new_user, default=obj_to_dict)\nnew_object = json.loads(obj_data, object_hook=dict_to_obj)\nnew_object\n\n\nUser(name='Foo Bar', age=78, active=True, other_names=['Doe', 'Joe'], friends=['Jane', 'John'], spouse=None)\n\n\n\n\nCode\ntype(new_object)\n\n\n__main__.User\n\n\n\n\nCode\nnew_object.age\n\n\n78\n\n\n\n\n\n\n\n\nNote\n\n\n\nFunctions obj_to_dict() and dict_to_obj() are showcases for special/magic/dunder methods.\nIn the definition of class User, two special methods were explicitly defined: __init__() and __repr__(). But many more are available, including __dir__().\nRemember that some dunder members of the object are not callable.\n\n\n\n\nCode\n[dude for dude in dir(new_object) if dude.startswith('__') and callable(getattr(new_object, dude))]\n\n\n['__class__',\n '__delattr__',\n '__dir__',\n '__eq__',\n '__format__',\n '__ge__',\n '__getattribute__',\n '__getstate__',\n '__gt__',\n '__hash__',\n '__init__',\n '__init_subclass__',\n '__le__',\n '__lt__',\n '__ne__',\n '__new__',\n '__reduce__',\n '__reduce_ex__',\n '__repr__',\n '__setattr__',\n '__sizeof__',\n '__str__',\n '__subclasshook__']\n\n\n\n\nCode\n[dude for dude in dir(new_object) if dude.startswith('__') and not callable(getattr(new_object, dude))]\n\n\n['__dict__', '__doc__', '__module__', '__weakref__']\n\n\n\n\nCode\nnew_object.__getattribute__('age')\n\ngetattr(new_object, 'age')\n\n\n78\n\n\n\n\n\n\n\n\nNote\n\n\n\nClass User could have been implemented as a dataclass\n\n\n\n\nCode\nfrom dataclasses import dataclass\n\n@dataclass\nclass UserBis(object):\n    \"\"\"Custom User Class\n    \"\"\"\n    name: str \n    age: int\n    active: bool\n    balance: float\n    other_names: list[str]\n    friends: list[str]\n    spouse: str\n\n\n\n\n\n\n\n\nNote\n\n\n\n@dataclass is a decorator. Have a look at the chapter on decorators in [Fluent Python] by Ramalho\n\n\n\n\nCode\nother_user = UserBis(**(new_user.__dict__))\n\n\n\n\nCode\nrepr(other_user)\n\n\n\"UserBis(name='Foo Bar', age=78, active=True, balance=345.8, other_names=('Doe', 'Joe'), friends=['Jane', 'John'], spouse=None)\"\n\n\n\n\nCode\n{dude for dude in dir(other_user) if dude.startswith('__')} -  {dude for dude in dir(new_user) if dude.startswith('__')}\n\n\n{'__annotations__',\n '__dataclass_fields__',\n '__dataclass_params__',\n '__match_args__'}\n\n\n\n\n\n\n\n\nNote\n\n\n\nHave a look at dataclasses documentation.\nSee also Chapter 5: Data class builders in Fluent Python"
  },
  {
    "objectID": "core/notebooks/notebook07_json-format.html#reading-a-json-dataset-with-spark",
    "href": "core/notebooks/notebook07_json-format.html#reading-a-json-dataset-with-spark",
    "title": "Using JSON data with Python",
    "section": "Reading a JSON dataset with Spark",
    "text": "Reading a JSON dataset with Spark\n\n\nCode\nfrom pyspark import SparkConf, SparkContext\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql import functions as fn\nfrom pyspark.sql.functions import col\n\nspark = (SparkSession\n    .builder\n    .appName(\"Spark JSON\")\n    .getOrCreate()\n)\n\nsc = spark._sc\n\n\n25/03/10 16:55:31 WARN Utils: Your hostname, boucheron-Precision-5480 resolves to a loopback address: 127.0.1.1; using 172.23.32.10 instead (on interface enxac91a1bd3e89)\n25/03/10 16:55:31 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\nSetting default log level to \"WARN\".\nTo adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n25/03/10 16:55:31 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n25/03/10 16:55:32 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n\n\n\n\nCode\nfilename = \"drug-enforcement.json\"\n\n\nFirst, lets look at the data. It’s a large set of JSON records about drugs enforcement.\n\n\nCode\n!head -n 100 drug-enforcement.json\n\n\n[\n    {\n      \"classification\": \"Class II\",\n      \"center_classification_date\": \"20121025\",\n      \"report_date\": \"20121031\",\n      \"postal_code\": \"08816-2108\",\n      \"termination_date\": \"20141007\",\n      \"recall_initiation_date\": \"20120904\",\n      \"recall_number\": \"D-026-2013\",\n      \"city\": \"East Brunswick\",\n      \"event_id\": \"63384\",\n      \"distribution_pattern\": \"Nationwide\",\n      \"openfda\": {},\n      \"recalling_firm\": \"Raritan Pharmaceuticals, Inc.\",\n      \"voluntary_mandated\": \"Voluntary: Firm Initiated\",\n      \"state\": \"NJ\",\n      \"reason_for_recall\": \"Microbial Contamination of Non-Sterile Products: Product is being recalled due to possible microbial contamination by C. difficile discovered in the raw material.\",\n      \"initial_firm_notification\": \"E-Mail\",\n      \"status\": \"Terminated\",\n      \"product_type\": \"Drugs\",\n      \"country\": \"United States\",\n      \"product_description\": \"Wal-Mucil 100% Natural Fiber, 100% Natural Psyllium Seed Husk, Fiber Laxative/Supplement, a)160 capsules per bottle (item #503663), and b) 320 capsules per bottle (Item #586143), Distributed by: Walgreen Co., 200 Wilmot Road, Deerfield, IL 60015-4616, www.walgreens.com, a) UPC 3-11917-08151-9, b) UPC 3-11917-07658-4\",\n      \"code_info\": \"Lots a) 15952, 16270,16425, Exp 06/15; b)16459, 16466, 16467, Exp 07/15\",\n      \"address_1\": \"8 Joanna Ct\",\n      \"address_2\": \"\",\n      \"product_quantity\": \"56,808 bottles\"\n    },\n    {\n      \"classification\": \"Class II\",\n      \"center_classification_date\": \"20121025\",\n      \"report_date\": \"20121031\",\n      \"postal_code\": \"08816-2108\",\n      \"termination_date\": \"20141007\",\n      \"recall_initiation_date\": \"20120904\",\n      \"recall_number\": \"D-031-2013\",\n      \"city\": \"East Brunswick\",\n      \"event_id\": \"63384\",\n      \"distribution_pattern\": \"Nationwide\",\n      \"openfda\": {},\n      \"recalling_firm\": \"Raritan Pharmaceuticals, Inc.\",\n      \"voluntary_mandated\": \"Voluntary: Firm Initiated\",\n      \"state\": \"NJ\",\n      \"reason_for_recall\": \"Microbial Contamination of Non-Sterile Products: Product is being recalled due to possible microbial contamination by C. difficile discovered in the raw material.\",\n      \"initial_firm_notification\": \"E-Mail\",\n      \"status\": \"Terminated\",\n      \"product_type\": \"Drugs\",\n      \"country\": \"United States\",\n      \"product_description\": \"Premier Value Fiber Plus Calcium Supplement Capsules, 120 capsules per bottle, Distributed by: Chain Drug Consortium, LLC, Boca Raton, FL, UPC 8-40986-01987-6\",\n      \"code_info\": \"Lot 15087, Exp 08/15\",\n      \"address_1\": \"8 Joanna Ct\",\n      \"address_2\": \"\",\n      \"product_quantity\": \"96 bottles\"\n    },\n    {\n      \"classification\": \"Class III\",\n      \"center_classification_date\": \"20121106\",\n      \"report_date\": \"20121114\",\n      \"postal_code\": \"08807\",\n      \"termination_date\": \"20130325\",\n      \"recall_initiation_date\": \"20121015\",\n      \"recall_number\": \"D-047-2013\",\n      \"city\": \"Bridgewater\",\n      \"event_id\": \"63488\",\n      \"distribution_pattern\": \"Nationwide\",\n      \"openfda\": {},\n      \"recalling_firm\": \"Valeant Pharmaceuticals\",\n      \"voluntary_mandated\": \"Voluntary: Firm Initiated\",\n      \"state\": \"NJ\",\n      \"reason_for_recall\": \"Subpotent (Single Ingredient) Drug: This product was found to be subpotent for the salicylic acid ingredient.  Additionally, this product is mislabeled because the label either omits or erroneously added inactive ingredients to the label.\",\n      \"initial_firm_notification\": \"Letter\",\n      \"status\": \"Terminated\",\n      \"product_type\": \"Drugs\",\n      \"country\": \"United States\",\n      \"product_description\": \"AcneFree 3-in-1 Acne Night Repair Foam (retinol + salicylic acid 1.5% w/v), 3 oz (85 g) canister, Dist. by: University Medical Pharmaceuticals Corp., Irvine, CA  92618, UPC 7 88521 13548 6.\",\n      \"code_info\": \"All lots with expiration dates between 10/10/12 through 10/10/14 of UPC 7 88521 13548 6\",\n      \"address_1\": \"700 Rte 206 North\",\n      \"address_2\": \"\",\n      \"product_quantity\": \"81,319 canisters\"\n    },\n    {\n      \"classification\": \"Class III\",\n      \"center_classification_date\": \"20121220\",\n      \"report_date\": \"20121226\",\n      \"postal_code\": \"08558-1311\",\n      \"termination_date\": \"20140429\",\n      \"recall_initiation_date\": \"20121204\",\n      \"recall_number\": \"D-098-2013\",\n      \"city\": \"Skillman\",\n      \"more_code_info\": null,\n      \"event_id\": \"63787\",\n      \"distribution_pattern\": \"Nationwide\",\n      \"openfda\": {},\n      \"recalling_firm\": \"Johnson & Johnson\",\n      \"voluntary_mandated\": \"Voluntary: Firm Initiated\",\n      \"state\": \"NJ\",\n      \"reason_for_recall\": \"Superpotent (Single Ingredient Drug): salicylic acid\",\n      \"initial_firm_notification\": \"Letter\",\n      \"status\": \"Terminated\",\n      \"product_type\": \"Drugs\",\n      \"country\": \"United States\",\n\n\n\n\n\n\n\n\nNote\n\n\n\nWe need to tell spark that rows span on several lines with the multLine option\n\n\n\n\nCode\ndf = spark.read.json(filename, multiLine=True)\n\n\n\n\nCode\ndf.printSchema()\n\n\nroot\n |-- address_1: string (nullable = true)\n |-- address_2: string (nullable = true)\n |-- center_classification_date: string (nullable = true)\n |-- city: string (nullable = true)\n |-- classification: string (nullable = true)\n |-- code_info: string (nullable = true)\n |-- country: string (nullable = true)\n |-- distribution_pattern: string (nullable = true)\n |-- event_id: string (nullable = true)\n |-- initial_firm_notification: string (nullable = true)\n |-- more_code_info: string (nullable = true)\n |-- openfda: struct (nullable = true)\n |    |-- application_number: array (nullable = true)\n |    |    |-- element: string (containsNull = true)\n |    |-- brand_name: array (nullable = true)\n |    |    |-- element: string (containsNull = true)\n |    |-- generic_name: array (nullable = true)\n |    |    |-- element: string (containsNull = true)\n |    |-- is_original_packager: array (nullable = true)\n |    |    |-- element: boolean (containsNull = true)\n |    |-- manufacturer_name: array (nullable = true)\n |    |    |-- element: string (containsNull = true)\n |    |-- nui: array (nullable = true)\n |    |    |-- element: string (containsNull = true)\n |    |-- original_packager_product_ndc: array (nullable = true)\n |    |    |-- element: string (containsNull = true)\n |    |-- package_ndc: array (nullable = true)\n |    |    |-- element: string (containsNull = true)\n |    |-- pharm_class_cs: array (nullable = true)\n |    |    |-- element: string (containsNull = true)\n |    |-- pharm_class_epc: array (nullable = true)\n |    |    |-- element: string (containsNull = true)\n |    |-- pharm_class_moa: array (nullable = true)\n |    |    |-- element: string (containsNull = true)\n |    |-- pharm_class_pe: array (nullable = true)\n |    |    |-- element: string (containsNull = true)\n |    |-- product_ndc: array (nullable = true)\n |    |    |-- element: string (containsNull = true)\n |    |-- product_type: array (nullable = true)\n |    |    |-- element: string (containsNull = true)\n |    |-- route: array (nullable = true)\n |    |    |-- element: string (containsNull = true)\n |    |-- rxcui: array (nullable = true)\n |    |    |-- element: string (containsNull = true)\n |    |-- spl_id: array (nullable = true)\n |    |    |-- element: string (containsNull = true)\n |    |-- spl_set_id: array (nullable = true)\n |    |    |-- element: string (containsNull = true)\n |    |-- substance_name: array (nullable = true)\n |    |    |-- element: string (containsNull = true)\n |    |-- unii: array (nullable = true)\n |    |    |-- element: string (containsNull = true)\n |    |-- upc: array (nullable = true)\n |    |    |-- element: string (containsNull = true)\n |-- postal_code: string (nullable = true)\n |-- product_description: string (nullable = true)\n |-- product_quantity: string (nullable = true)\n |-- product_type: string (nullable = true)\n |-- reason_for_recall: string (nullable = true)\n |-- recall_initiation_date: string (nullable = true)\n |-- recall_number: string (nullable = true)\n |-- recalling_firm: string (nullable = true)\n |-- report_date: string (nullable = true)\n |-- state: string (nullable = true)\n |-- status: string (nullable = true)\n |-- termination_date: string (nullable = true)\n |-- voluntary_mandated: string (nullable = true)\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nIn a less user-friendly format:\n\n\n\n\nCode\ndf.schema\n\n\nStructType([StructField('address_1', StringType(), True), StructField('address_2', StringType(), True), StructField('center_classification_date', StringType(), True), StructField('city', StringType(), True), StructField('classification', StringType(), True), StructField('code_info', StringType(), True), StructField('country', StringType(), True), StructField('distribution_pattern', StringType(), True), StructField('event_id', StringType(), True), StructField('initial_firm_notification', StringType(), True), StructField('more_code_info', StringType(), True), StructField('openfda', StructType([StructField('application_number', ArrayType(StringType(), True), True), StructField('brand_name', ArrayType(StringType(), True), True), StructField('generic_name', ArrayType(StringType(), True), True), StructField('is_original_packager', ArrayType(BooleanType(), True), True), StructField('manufacturer_name', ArrayType(StringType(), True), True), StructField('nui', ArrayType(StringType(), True), True), StructField('original_packager_product_ndc', ArrayType(StringType(), True), True), StructField('package_ndc', ArrayType(StringType(), True), True), StructField('pharm_class_cs', ArrayType(StringType(), True), True), StructField('pharm_class_epc', ArrayType(StringType(), True), True), StructField('pharm_class_moa', ArrayType(StringType(), True), True), StructField('pharm_class_pe', ArrayType(StringType(), True), True), StructField('product_ndc', ArrayType(StringType(), True), True), StructField('product_type', ArrayType(StringType(), True), True), StructField('route', ArrayType(StringType(), True), True), StructField('rxcui', ArrayType(StringType(), True), True), StructField('spl_id', ArrayType(StringType(), True), True), StructField('spl_set_id', ArrayType(StringType(), True), True), StructField('substance_name', ArrayType(StringType(), True), True), StructField('unii', ArrayType(StringType(), True), True), StructField('upc', ArrayType(StringType(), True), True)]), True), StructField('postal_code', StringType(), True), StructField('product_description', StringType(), True), StructField('product_quantity', StringType(), True), StructField('product_type', StringType(), True), StructField('reason_for_recall', StringType(), True), StructField('recall_initiation_date', StringType(), True), StructField('recall_number', StringType(), True), StructField('recalling_firm', StringType(), True), StructField('report_date', StringType(), True), StructField('state', StringType(), True), StructField('status', StringType(), True), StructField('termination_date', StringType(), True), StructField('voluntary_mandated', StringType(), True)])\n\n\n\n\n\n\n\n\nNote\n\n\n\nThis dataset is a little bit of a mess!\nThis should not be surprising. The data used to populate the Spark dataframe are not classically tabular but what people call semi-structured. Json is well-suited to store, represent, and exchange such data.\nIn the classical age of tabular data (according to Codd’s principles), a table cell could only hold a scalar value (numeric, logical, text, date, timestamp, …), nowadays Relational Database Management Systems handle Arrays, Composite Types, Range Types, …, and Json (see PostgreSQL).\nSpark, R, and Pandas, and modern relational databases also allow us to work with complex types.\nModern column oriented file format like parquet also work with nested structures.\n\n\n\nFirst, there is a nested opendfa dictionary. Each element of the dictionary is an array\nA first good idea is to “flatten” the schema of the DataFrame, so that there are no nested types any more."
  },
  {
    "objectID": "core/notebooks/notebook07_json-format.html#flattening-the-schema",
    "href": "core/notebooks/notebook07_json-format.html#flattening-the-schema",
    "title": "Using JSON data with Python",
    "section": "Flattening the schema",
    "text": "Flattening the schema\nAll the columns in the nested structure openfda are put up in the schema. These columns nested in the openfda are as follows:\n\n\nCode\ndf.select('openfda.*').columns\n\n\n['application_number',\n 'brand_name',\n 'generic_name',\n 'is_original_packager',\n 'manufacturer_name',\n 'nui',\n 'original_packager_product_ndc',\n 'package_ndc',\n 'pharm_class_cs',\n 'pharm_class_epc',\n 'pharm_class_moa',\n 'pharm_class_pe',\n 'product_ndc',\n 'product_type',\n 'route',\n 'rxcui',\n 'spl_id',\n 'spl_set_id',\n 'substance_name',\n 'unii',\n 'upc']\n\n\n\n\nCode\ndf.select(\"openfda.*\").head(2)\n\n\n[Row(application_number=None, brand_name=None, generic_name=None, is_original_packager=None, manufacturer_name=None, nui=None, original_packager_product_ndc=None, package_ndc=None, pharm_class_cs=None, pharm_class_epc=None, pharm_class_moa=None, pharm_class_pe=None, product_ndc=None, product_type=None, route=None, rxcui=None, spl_id=None, spl_set_id=None, substance_name=None, unii=None, upc=None),\n Row(application_number=None, brand_name=None, generic_name=None, is_original_packager=None, manufacturer_name=None, nui=None, original_packager_product_ndc=None, package_ndc=None, pharm_class_cs=None, pharm_class_epc=None, pharm_class_moa=None, pharm_class_pe=None, product_ndc=None, product_type=None, route=None, rxcui=None, spl_id=None, spl_set_id=None, substance_name=None, unii=None, upc=None)]\n\n\n\n\nCode\nfor c in df.select(\"openfda.*\").columns:\n    df = df.withColumn(\"openfda_\" + c, col(\"openfda.\" + c))\n\n\n\n\nCode\ndf = df.select([c for c in df.columns if c != \"openfda\"])\n\n\n\n\nCode\ndf.printSchema()\n\n\nroot\n |-- address_1: string (nullable = true)\n |-- address_2: string (nullable = true)\n |-- center_classification_date: string (nullable = true)\n |-- city: string (nullable = true)\n |-- classification: string (nullable = true)\n |-- code_info: string (nullable = true)\n |-- country: string (nullable = true)\n |-- distribution_pattern: string (nullable = true)\n |-- event_id: string (nullable = true)\n |-- initial_firm_notification: string (nullable = true)\n |-- more_code_info: string (nullable = true)\n |-- postal_code: string (nullable = true)\n |-- product_description: string (nullable = true)\n |-- product_quantity: string (nullable = true)\n |-- product_type: string (nullable = true)\n |-- reason_for_recall: string (nullable = true)\n |-- recall_initiation_date: string (nullable = true)\n |-- recall_number: string (nullable = true)\n |-- recalling_firm: string (nullable = true)\n |-- report_date: string (nullable = true)\n |-- state: string (nullable = true)\n |-- status: string (nullable = true)\n |-- termination_date: string (nullable = true)\n |-- voluntary_mandated: string (nullable = true)\n |-- openfda_application_number: array (nullable = true)\n |    |-- element: string (containsNull = true)\n |-- openfda_brand_name: array (nullable = true)\n |    |-- element: string (containsNull = true)\n |-- openfda_generic_name: array (nullable = true)\n |    |-- element: string (containsNull = true)\n |-- openfda_is_original_packager: array (nullable = true)\n |    |-- element: boolean (containsNull = true)\n |-- openfda_manufacturer_name: array (nullable = true)\n |    |-- element: string (containsNull = true)\n |-- openfda_nui: array (nullable = true)\n |    |-- element: string (containsNull = true)\n |-- openfda_original_packager_product_ndc: array (nullable = true)\n |    |-- element: string (containsNull = true)\n |-- openfda_package_ndc: array (nullable = true)\n |    |-- element: string (containsNull = true)\n |-- openfda_pharm_class_cs: array (nullable = true)\n |    |-- element: string (containsNull = true)\n |-- openfda_pharm_class_epc: array (nullable = true)\n |    |-- element: string (containsNull = true)\n |-- openfda_pharm_class_moa: array (nullable = true)\n |    |-- element: string (containsNull = true)\n |-- openfda_pharm_class_pe: array (nullable = true)\n |    |-- element: string (containsNull = true)\n |-- openfda_product_ndc: array (nullable = true)\n |    |-- element: string (containsNull = true)\n |-- openfda_product_type: array (nullable = true)\n |    |-- element: string (containsNull = true)\n |-- openfda_route: array (nullable = true)\n |    |-- element: string (containsNull = true)\n |-- openfda_rxcui: array (nullable = true)\n |    |-- element: string (containsNull = true)\n |-- openfda_spl_id: array (nullable = true)\n |    |-- element: string (containsNull = true)\n |-- openfda_spl_set_id: array (nullable = true)\n |    |-- element: string (containsNull = true)\n |-- openfda_substance_name: array (nullable = true)\n |    |-- element: string (containsNull = true)\n |-- openfda_unii: array (nullable = true)\n |    |-- element: string (containsNull = true)\n |-- openfda_upc: array (nullable = true)\n |    |-- element: string (containsNull = true)\n\n\n\n\n\nCode\ndf.head(2)\n\n\n25/03/10 16:55:35 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n\n\n[Row(address_1='8 Joanna Ct', address_2='', center_classification_date='20121025', city='East Brunswick', classification='Class II', code_info='Lots a) 15952, 16270,16425, Exp 06/15; b)16459, 16466, 16467, Exp 07/15', country='United States', distribution_pattern='Nationwide', event_id='63384', initial_firm_notification='E-Mail', more_code_info=None, postal_code='08816-2108', product_description='Wal-Mucil 100% Natural Fiber, 100% Natural Psyllium Seed Husk, Fiber Laxative/Supplement, a)160 capsules per bottle (item #503663), and b) 320 capsules per bottle (Item #586143), Distributed by: Walgreen Co., 200 Wilmot Road, Deerfield, IL 60015-4616, www.walgreens.com, a) UPC 3-11917-08151-9, b) UPC 3-11917-07658-4', product_quantity='56,808 bottles', product_type='Drugs', reason_for_recall='Microbial Contamination of Non-Sterile Products: Product is being recalled due to possible microbial contamination by C. difficile discovered in the raw material.', recall_initiation_date='20120904', recall_number='D-026-2013', recalling_firm='Raritan Pharmaceuticals, Inc.', report_date='20121031', state='NJ', status='Terminated', termination_date='20141007', voluntary_mandated='Voluntary: Firm Initiated', openfda_application_number=None, openfda_brand_name=None, openfda_generic_name=None, openfda_is_original_packager=None, openfda_manufacturer_name=None, openfda_nui=None, openfda_original_packager_product_ndc=None, openfda_package_ndc=None, openfda_pharm_class_cs=None, openfda_pharm_class_epc=None, openfda_pharm_class_moa=None, openfda_pharm_class_pe=None, openfda_product_ndc=None, openfda_product_type=None, openfda_route=None, openfda_rxcui=None, openfda_spl_id=None, openfda_spl_set_id=None, openfda_substance_name=None, openfda_unii=None, openfda_upc=None),\n Row(address_1='8 Joanna Ct', address_2='', center_classification_date='20121025', city='East Brunswick', classification='Class II', code_info='Lot 15087, Exp 08/15', country='United States', distribution_pattern='Nationwide', event_id='63384', initial_firm_notification='E-Mail', more_code_info=None, postal_code='08816-2108', product_description='Premier Value Fiber Plus Calcium Supplement Capsules, 120 capsules per bottle, Distributed by: Chain Drug Consortium, LLC, Boca Raton, FL, UPC 8-40986-01987-6', product_quantity='96 bottles', product_type='Drugs', reason_for_recall='Microbial Contamination of Non-Sterile Products: Product is being recalled due to possible microbial contamination by C. difficile discovered in the raw material.', recall_initiation_date='20120904', recall_number='D-031-2013', recalling_firm='Raritan Pharmaceuticals, Inc.', report_date='20121031', state='NJ', status='Terminated', termination_date='20141007', voluntary_mandated='Voluntary: Firm Initiated', openfda_application_number=None, openfda_brand_name=None, openfda_generic_name=None, openfda_is_original_packager=None, openfda_manufacturer_name=None, openfda_nui=None, openfda_original_packager_product_ndc=None, openfda_package_ndc=None, openfda_pharm_class_cs=None, openfda_pharm_class_epc=None, openfda_pharm_class_moa=None, openfda_pharm_class_pe=None, openfda_product_ndc=None, openfda_product_type=None, openfda_route=None, openfda_rxcui=None, openfda_spl_id=None, openfda_spl_set_id=None, openfda_substance_name=None, openfda_unii=None, openfda_upc=None)]\n\n\nNote that the display of the DataFrame is not as usual… it displays the dataframe like a list of Row, since the columns “openfda*” contain arrays of varying length\n\n\n\n\n\n\nNote\n\n\n\nA principled approach to schema flattening is embodied in the next chunk.\ndf.schema allows us to perform flattening in a programmatic way.\n\n\n\n\nCode\nfrom pyspark.sql.types import StructType\nfrom pyspark.sql.functions import col\n\ndef flatten_schema(df):\n    # Get fields and their data types\n    fields = df.schema.fields\n    \n    # Flatten array of column names\n    flat_cols = []\n    \n    for field in fields:\n        # Handle nested structures\n        if isinstance(field.dataType, StructType):\n            nested = df.select(field.name + \".*\").columns\n            flat_cols.extend([field.name + \".\" + x for x in nested])\n        else:\n            flat_cols.append(field.name)\n    \n    # Select all flattened columns\n    df_flattened = df.select([col(x).alias(x.replace(\".\",\"_\")) for x in flat_cols])\n    \n    return df_flattened\n\n\n\n\n\n\n\n\nNote\n\n\n\nThis function definition is from copilot under the following prompt:\nHow can I flatten the schema of a spark dataframe?\n\n\n\n\nCode\ndf = spark.read.json(filename, multiLine=True)\n\ndf_flat = flatten_schema(df)\n\ndf_flat.printSchema()\n\n\nroot\n |-- address_1: string (nullable = true)\n |-- address_2: string (nullable = true)\n |-- center_classification_date: string (nullable = true)\n |-- city: string (nullable = true)\n |-- classification: string (nullable = true)\n |-- code_info: string (nullable = true)\n |-- country: string (nullable = true)\n |-- distribution_pattern: string (nullable = true)\n |-- event_id: string (nullable = true)\n |-- initial_firm_notification: string (nullable = true)\n |-- more_code_info: string (nullable = true)\n |-- openfda_application_number: array (nullable = true)\n |    |-- element: string (containsNull = true)\n |-- openfda_brand_name: array (nullable = true)\n |    |-- element: string (containsNull = true)\n |-- openfda_generic_name: array (nullable = true)\n |    |-- element: string (containsNull = true)\n |-- openfda_is_original_packager: array (nullable = true)\n |    |-- element: boolean (containsNull = true)\n |-- openfda_manufacturer_name: array (nullable = true)\n |    |-- element: string (containsNull = true)\n |-- openfda_nui: array (nullable = true)\n |    |-- element: string (containsNull = true)\n |-- openfda_original_packager_product_ndc: array (nullable = true)\n |    |-- element: string (containsNull = true)\n |-- openfda_package_ndc: array (nullable = true)\n |    |-- element: string (containsNull = true)\n |-- openfda_pharm_class_cs: array (nullable = true)\n |    |-- element: string (containsNull = true)\n |-- openfda_pharm_class_epc: array (nullable = true)\n |    |-- element: string (containsNull = true)\n |-- openfda_pharm_class_moa: array (nullable = true)\n |    |-- element: string (containsNull = true)\n |-- openfda_pharm_class_pe: array (nullable = true)\n |    |-- element: string (containsNull = true)\n |-- openfda_product_ndc: array (nullable = true)\n |    |-- element: string (containsNull = true)\n |-- openfda_product_type: array (nullable = true)\n |    |-- element: string (containsNull = true)\n |-- openfda_route: array (nullable = true)\n |    |-- element: string (containsNull = true)\n |-- openfda_rxcui: array (nullable = true)\n |    |-- element: string (containsNull = true)\n |-- openfda_spl_id: array (nullable = true)\n |    |-- element: string (containsNull = true)\n |-- openfda_spl_set_id: array (nullable = true)\n |    |-- element: string (containsNull = true)\n |-- openfda_substance_name: array (nullable = true)\n |    |-- element: string (containsNull = true)\n |-- openfda_unii: array (nullable = true)\n |    |-- element: string (containsNull = true)\n |-- openfda_upc: array (nullable = true)\n |    |-- element: string (containsNull = true)\n |-- postal_code: string (nullable = true)\n |-- product_description: string (nullable = true)\n |-- product_quantity: string (nullable = true)\n |-- product_type: string (nullable = true)\n |-- reason_for_recall: string (nullable = true)\n |-- recall_initiation_date: string (nullable = true)\n |-- recall_number: string (nullable = true)\n |-- recalling_firm: string (nullable = true)\n |-- report_date: string (nullable = true)\n |-- state: string (nullable = true)\n |-- status: string (nullable = true)\n |-- termination_date: string (nullable = true)\n |-- voluntary_mandated: string (nullable = true)\n\n\n\n\n\nCode\nfrom pyspark.sql.types import StructType, StructField, StringType, IntegerType\n\nmessy_schema = StructType([\n    StructField(\"id\", IntegerType()),\n    StructField(\"info\", StructType([\n        StructField(\"name\", StringType()),\n        StructField(\"age\", IntegerType()),\n        StructField(\"zoo\", StructType([\n            StructField(\"cat\", StringType()),\n            StructField(\"dog\", StringType())\n        ]))\n    ]))\n])\n\n\n\n\n\n\n\n\nNote\n\n\n\nThis principled approach is not the end of the story. If the schema exhibits hierarchical nesting, flatten_schema() only removes one level of nesting.\n\n\n\n\nCode\ndata = [(1, (\"John\", 30, (\"Fritz\", \"Medor\"))), (2, (\"Jane\", 25, (\"Grominet\", \"Goofy\")))]\n\nvery_nested_df = spark.createDataFrame(data, messy_schema)\n\n\n\n\nCode\nflatten_schema(very_nested_df).show()\n\n\n+---+---------+--------+-----------------+\n| id|info_name|info_age|         info_zoo|\n+---+---------+--------+-----------------+\n|  1|     John|      30|   {Fritz, Medor}|\n|  2|     Jane|      25|{Grominet, Goofy}|\n+---+---------+--------+-----------------+\n\n\n\n\n\nCode\nflatten_schema(very_nested_df).printSchema()\n\n\nroot\n |-- id: integer (nullable = true)\n |-- info_name: string (nullable = true)\n |-- info_age: integer (nullable = true)\n |-- info_zoo: struct (nullable = true)\n |    |-- cat: string (nullable = true)\n |    |-- dog: string (nullable = true)\n\n\n\n\n\n\n\n\n\nWarning\n\n\n\ncopilot pretends that the flattening function above handles nested structure recursively. This is not the case.\nFix this"
  },
  {
    "objectID": "core/notebooks/notebook07_json-format.html#missing-data",
    "href": "core/notebooks/notebook07_json-format.html#missing-data",
    "title": "Using JSON data with Python",
    "section": "Missing data",
    "text": "Missing data\nA strategy can be to remove rows with missing data. dropna() has several options, explained below.\n\n\nCode\ndf.dropna().count()\n\n\n2\n\n\nIf we remove all lines with at least one missing value, we end up with an empty dataframe !\n\n\nCode\ndf.dropna(how='all').count()\n\n\n11292\n\n\ndropna() accepts the following arguments\n\nhow: can be 'any' or 'all'. If 'any', rows containing any null values will be dropped entirely (this is the default). If 'all', only rows which are entirely empty will be dropped.\nthresh: accepts an integer representing the “threshold” for how many empty cells a row must have before being dropped. tresh is a middle ground between how='any' and how='all'. As a result, the presence of thresh will override how\nsubset: accepts a list of column names. When a subset is present, N/A values will only be checked against the columns whose names are provided.\n\n\n\nCode\nn_columns = len(df.columns)\n\n\n\n\nCode\ndf.dropna(thresh=n_columns).count()\n\n\n2\n\n\n\n\nCode\ndf.dropna(thresh=n_columns-1).count()\n\n\n7550\n\n\n\n\nCode\ndf.dropna(thresh=n_columns-10).count()\n\n\n11292\n\n\n\n\nCode\ndf = df.dropna(subset=['postal_code', 'city', 'country', 'address_1'])\ndf.count()\n\n\n11292\n\n\nBut before this, let’s count the number of missing value for each column\n\n\nCode\n# For each column we create a new column containing 1 if the value is null and 0 otherwise.\n# We need to bast Boolean to Int so that we can use fn.sum after\nfor c in df.columns:\n    # Do not do this for _isnull columns (just in case you run this cell twice...)\n    if not c.endswith(\"_isnull\"):\n        df = df.withColumn(c + \"_isnull\", fn.isnull(col(c)).cast('int'))\n\n\n\n\nCode\ndf.head()\n\n\nRow(address_1='8 Joanna Ct', address_2='', center_classification_date='20121025', city='East Brunswick', classification='Class II', code_info='Lots a) 15952, 16270,16425, Exp 06/15; b)16459, 16466, 16467, Exp 07/15', country='United States', distribution_pattern='Nationwide', event_id='63384', initial_firm_notification='E-Mail', more_code_info=None, openfda=Row(application_number=None, brand_name=None, generic_name=None, is_original_packager=None, manufacturer_name=None, nui=None, original_packager_product_ndc=None, package_ndc=None, pharm_class_cs=None, pharm_class_epc=None, pharm_class_moa=None, pharm_class_pe=None, product_ndc=None, product_type=None, route=None, rxcui=None, spl_id=None, spl_set_id=None, substance_name=None, unii=None, upc=None), postal_code='08816-2108', product_description='Wal-Mucil 100% Natural Fiber, 100% Natural Psyllium Seed Husk, Fiber Laxative/Supplement, a)160 capsules per bottle (item #503663), and b) 320 capsules per bottle (Item #586143), Distributed by: Walgreen Co., 200 Wilmot Road, Deerfield, IL 60015-4616, www.walgreens.com, a) UPC 3-11917-08151-9, b) UPC 3-11917-07658-4', product_quantity='56,808 bottles', product_type='Drugs', reason_for_recall='Microbial Contamination of Non-Sterile Products: Product is being recalled due to possible microbial contamination by C. difficile discovered in the raw material.', recall_initiation_date='20120904', recall_number='D-026-2013', recalling_firm='Raritan Pharmaceuticals, Inc.', report_date='20121031', state='NJ', status='Terminated', termination_date='20141007', voluntary_mandated='Voluntary: Firm Initiated', address_1_isnull=0, address_2_isnull=0, center_classification_date_isnull=0, city_isnull=0, classification_isnull=0, code_info_isnull=0, country_isnull=0, distribution_pattern_isnull=0, event_id_isnull=0, initial_firm_notification_isnull=0, more_code_info_isnull=1, openfda_isnull=0, postal_code_isnull=0, product_description_isnull=0, product_quantity_isnull=0, product_type_isnull=0, reason_for_recall_isnull=0, recall_initiation_date_isnull=0, recall_number_isnull=0, recalling_firm_isnull=0, report_date_isnull=0, state_isnull=0, status_isnull=0, termination_date_isnull=0, voluntary_mandated_isnull=0)\n\n\n\n\nCode\n# Get the list of _isnull columns\nisnull_columns = [c for c in df.columns if c.endswith(\"_isnull\")]\n\n# On the _isnull columns :\n#  - we compute the sum to have the number of null values and rename the column\n#  - convert to pandas for better readability\n#  - transpose the pandas dataframe for better readability\nmissing_values = df.select(isnull_columns)\\\n    .agg(*[fn.sum(c).alias(c.replace(\"_isnull\", \"\")) for c in isnull_columns])\\\n    .toPandas()\n\nmissing_values.T\\\n    .rename({0: \"missing values\"}, axis=\"columns\")\n\n\n\n\n\n\n\n\n\nmissing values\n\n\n\n\naddress_1\n0\n\n\naddress_2\n0\n\n\ncenter_classification_date\n47\n\n\ncity\n0\n\n\nclassification\n0\n\n\ncode_info\n0\n\n\ncountry\n0\n\n\ndistribution_pattern\n0\n\n\nevent_id\n0\n\n\ninitial_firm_notification\n0\n\n\nmore_code_info\n11290\n\n\nopenfda\n0\n\n\npostal_code\n0\n\n\nproduct_description\n0\n\n\nproduct_quantity\n0\n\n\nproduct_type\n0\n\n\nreason_for_recall\n0\n\n\nrecall_initiation_date\n0\n\n\nrecall_number\n0\n\n\nrecalling_firm\n0\n\n\nreport_date\n0\n\n\nstate\n0\n\n\nstatus\n0\n\n\ntermination_date\n3741\n\n\nvoluntary_mandated\n0\n\n\n\n\n\n\n\nWe see that more_code_info is always null and that termination_date if often null. Most of the openfda* columns are also almost always empty.\nWe can keep only the columns with no missing values\n\n\nCode\n# This line can seem complicated, run pieces of each to understand\nkept_columns = list(\n    missing_values.columns[(missing_values.iloc[0] == 0).values]\n)\n\n\n\n\nCode\ndf_kept = df.select(kept_columns)\n\n\n\n\nCode\ndf_kept.head(2)\n\n\n[Row(address_1='8 Joanna Ct', address_2='', city='East Brunswick', classification='Class II', code_info='Lots a) 15952, 16270,16425, Exp 06/15; b)16459, 16466, 16467, Exp 07/15', country='United States', distribution_pattern='Nationwide', event_id='63384', initial_firm_notification='E-Mail', openfda=Row(application_number=None, brand_name=None, generic_name=None, is_original_packager=None, manufacturer_name=None, nui=None, original_packager_product_ndc=None, package_ndc=None, pharm_class_cs=None, pharm_class_epc=None, pharm_class_moa=None, pharm_class_pe=None, product_ndc=None, product_type=None, route=None, rxcui=None, spl_id=None, spl_set_id=None, substance_name=None, unii=None, upc=None), postal_code='08816-2108', product_description='Wal-Mucil 100% Natural Fiber, 100% Natural Psyllium Seed Husk, Fiber Laxative/Supplement, a)160 capsules per bottle (item #503663), and b) 320 capsules per bottle (Item #586143), Distributed by: Walgreen Co., 200 Wilmot Road, Deerfield, IL 60015-4616, www.walgreens.com, a) UPC 3-11917-08151-9, b) UPC 3-11917-07658-4', product_quantity='56,808 bottles', product_type='Drugs', reason_for_recall='Microbial Contamination of Non-Sterile Products: Product is being recalled due to possible microbial contamination by C. difficile discovered in the raw material.', recall_initiation_date='20120904', recall_number='D-026-2013', recalling_firm='Raritan Pharmaceuticals, Inc.', report_date='20121031', state='NJ', status='Terminated', voluntary_mandated='Voluntary: Firm Initiated'),\n Row(address_1='8 Joanna Ct', address_2='', city='East Brunswick', classification='Class II', code_info='Lot 15087, Exp 08/15', country='United States', distribution_pattern='Nationwide', event_id='63384', initial_firm_notification='E-Mail', openfda=Row(application_number=None, brand_name=None, generic_name=None, is_original_packager=None, manufacturer_name=None, nui=None, original_packager_product_ndc=None, package_ndc=None, pharm_class_cs=None, pharm_class_epc=None, pharm_class_moa=None, pharm_class_pe=None, product_ndc=None, product_type=None, route=None, rxcui=None, spl_id=None, spl_set_id=None, substance_name=None, unii=None, upc=None), postal_code='08816-2108', product_description='Premier Value Fiber Plus Calcium Supplement Capsules, 120 capsules per bottle, Distributed by: Chain Drug Consortium, LLC, Boca Raton, FL, UPC 8-40986-01987-6', product_quantity='96 bottles', product_type='Drugs', reason_for_recall='Microbial Contamination of Non-Sterile Products: Product is being recalled due to possible microbial contamination by C. difficile discovered in the raw material.', recall_initiation_date='20120904', recall_number='D-031-2013', recalling_firm='Raritan Pharmaceuticals, Inc.', report_date='20121031', state='NJ', status='Terminated', voluntary_mandated='Voluntary: Firm Initiated')]\n\n\n\n\nCode\ndf_kept.printSchema()\n\n\nroot\n |-- address_1: string (nullable = true)\n |-- address_2: string (nullable = true)\n |-- city: string (nullable = true)\n |-- classification: string (nullable = true)\n |-- code_info: string (nullable = true)\n |-- country: string (nullable = true)\n |-- distribution_pattern: string (nullable = true)\n |-- event_id: string (nullable = true)\n |-- initial_firm_notification: string (nullable = true)\n |-- openfda: struct (nullable = true)\n |    |-- application_number: array (nullable = true)\n |    |    |-- element: string (containsNull = true)\n |    |-- brand_name: array (nullable = true)\n |    |    |-- element: string (containsNull = true)\n |    |-- generic_name: array (nullable = true)\n |    |    |-- element: string (containsNull = true)\n |    |-- is_original_packager: array (nullable = true)\n |    |    |-- element: boolean (containsNull = true)\n |    |-- manufacturer_name: array (nullable = true)\n |    |    |-- element: string (containsNull = true)\n |    |-- nui: array (nullable = true)\n |    |    |-- element: string (containsNull = true)\n |    |-- original_packager_product_ndc: array (nullable = true)\n |    |    |-- element: string (containsNull = true)\n |    |-- package_ndc: array (nullable = true)\n |    |    |-- element: string (containsNull = true)\n |    |-- pharm_class_cs: array (nullable = true)\n |    |    |-- element: string (containsNull = true)\n |    |-- pharm_class_epc: array (nullable = true)\n |    |    |-- element: string (containsNull = true)\n |    |-- pharm_class_moa: array (nullable = true)\n |    |    |-- element: string (containsNull = true)\n |    |-- pharm_class_pe: array (nullable = true)\n |    |    |-- element: string (containsNull = true)\n |    |-- product_ndc: array (nullable = true)\n |    |    |-- element: string (containsNull = true)\n |    |-- product_type: array (nullable = true)\n |    |    |-- element: string (containsNull = true)\n |    |-- route: array (nullable = true)\n |    |    |-- element: string (containsNull = true)\n |    |-- rxcui: array (nullable = true)\n |    |    |-- element: string (containsNull = true)\n |    |-- spl_id: array (nullable = true)\n |    |    |-- element: string (containsNull = true)\n |    |-- spl_set_id: array (nullable = true)\n |    |    |-- element: string (containsNull = true)\n |    |-- substance_name: array (nullable = true)\n |    |    |-- element: string (containsNull = true)\n |    |-- unii: array (nullable = true)\n |    |    |-- element: string (containsNull = true)\n |    |-- upc: array (nullable = true)\n |    |    |-- element: string (containsNull = true)\n |-- postal_code: string (nullable = true)\n |-- product_description: string (nullable = true)\n |-- product_quantity: string (nullable = true)\n |-- product_type: string (nullable = true)\n |-- reason_for_recall: string (nullable = true)\n |-- recall_initiation_date: string (nullable = true)\n |-- recall_number: string (nullable = true)\n |-- recalling_firm: string (nullable = true)\n |-- report_date: string (nullable = true)\n |-- state: string (nullable = true)\n |-- status: string (nullable = true)\n |-- voluntary_mandated: string (nullable = true)\n\n\n\n\n\nCode\ndf_kept.count()\n\n\n11292"
  },
  {
    "objectID": "core/notebooks/notebook07_json-format.html#filtering-by-string-values",
    "href": "core/notebooks/notebook07_json-format.html#filtering-by-string-values",
    "title": "Using JSON data with Python",
    "section": "Filtering by string values",
    "text": "Filtering by string values\nCases from South San Francisco\n\n\nCode\ndf.filter(df.city == \"South San Francisco\")\\\n    .toPandas()\n\n\n\n\n\n\n\n\n\naddress_1\naddress_2\ncenter_classification_date\ncity\nclassification\ncode_info\ncountry\ndistribution_pattern\nevent_id\ninitial_firm_notification\n...\nproduct_type_isnull\nreason_for_recall_isnull\nrecall_initiation_date_isnull\nrecall_number_isnull\nrecalling_firm_isnull\nreport_date_isnull\nstate_isnull\nstatus_isnull\ntermination_date_isnull\nvoluntary_mandated_isnull\n\n\n\n\n0\n1 DNA Way\n\n20130122\nSouth San Francisco\nClass I\nLot #: 454138, Exp 07/14 (containing Trastuzum...\nUnited States\nNationwide\n63258\nE-Mail\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n1\n1 DNA Way\n\n20121204\nSouth San Francisco\nClass II\nLot# 936674 Exp. 09/30/13\nUnited States\nNationwide\n63243\nLetter\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n2\n1 DNA Way\n\n20121204\nSouth San Francisco\nClass II\nLot # 936670 Exp. 09/30/13\nUnited States\nNationwide\n63243\nLetter\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n3\n1 DNA Way\n\n20121220\nSouth San Francisco\nClass III\nLot #: a) M1365B01, Exp 04/15; b) M1365, Exp 0...\nUnited States\nNationwide\n63874\nLetter\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n4\n1 Dna Way\n\n20180213\nSouth San Francisco\nClass III\nLot # 3141989, EXP 08/31/2019\nUnited States\nDistributed throughout the United States\n79175\nLetter\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n5\n5000 Shoreline Ct. Ste 200\n\n20130201\nSouth San Francisco\nClass III\nLot #: MA00AD5, Exp: 11/30/2014\nUnited States\nCA & VA\n64151\nLetter\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n6\n1 Dna Way\n\n20171201\nSouth San Francisco\nClass I\nLot# 3128243, 3141239, EXP. 9/30/2018; 3166728...\nUnited States\nNationwide in the USA\n78088\nLetter\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n7\n1 Dna Way\n\n20170405\nSouth San Francisco\nClass II\nB1009MC, B1009M9, B1009MA; Exp. 02/18 B1009MT...\nUnited States\nNJ and IL\n76726\nLetter\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n\n\n8 rows × 50 columns\n\n\n\n\n\n\n\n\n\nCaution\n\n\n\nOnce again, we use .toPandas() to pretty format the results in the notebook.\nBut it’s a BAD idea to do this if the spark DataFrame is large, since it requires a collect()\n\n\nAside from filtering strings by a perfect match, there are plenty of other powerful ways to filter by strings in pyspark :\n\ndf.filter(df.city.contains('San Francisco')): returns rows where strings of a column contain a provided substring. In our example, filtering by rows which contain the substring “San Francisco” would be a good way to get all rows in San Francisco, instead of just “South San Francisco”.\ndf.filter(df.city.startswith('San')): Returns rows where a string starts with a provided substring.\ndf.filter(df.city.endswith('ice')): Returns rows where a string starts with a provided substring.\ndf.filter(df.city.isNull()): Returns rows where values in a provided column are null.\ndf.filter(df.city.isNotNull()): Opposite of the above.\ndf.filter(df.city.like('San%')): Performs a SQL-like query containing the LIKE clause.\ndf.filter(df.city.rlike('[A-Z]*ice$')): Performs a regexp filter.\ndf.filter(df.city.isin('San Francisco', 'Los Angeles')): Looks for rows where the string value of a column matches any of the provided strings exactly.\n\nYou can try some of these to understand\n\n\nCode\ndf.filter(df.city.contains('San Francisco'))\\\n    .toPandas()\n\n\n\n\n\n\n\n\n\naddress_1\naddress_2\ncenter_classification_date\ncity\nclassification\ncode_info\ncountry\ndistribution_pattern\nevent_id\ninitial_firm_notification\n...\nproduct_type_isnull\nreason_for_recall_isnull\nrecall_initiation_date_isnull\nrecall_number_isnull\nrecalling_firm_isnull\nreport_date_isnull\nstate_isnull\nstatus_isnull\ntermination_date_isnull\nvoluntary_mandated_isnull\n\n\n\n\n0\n1 DNA Way\n\n20130122\nSouth San Francisco\nClass I\nLot #: 454138, Exp 07/14 (containing Trastuzum...\nUnited States\nNationwide\n63258\nE-Mail\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n1\n1 DNA Way\n\n20121204\nSouth San Francisco\nClass II\nLot# 936674 Exp. 09/30/13\nUnited States\nNationwide\n63243\nLetter\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n2\n1 DNA Way\n\n20121204\nSouth San Francisco\nClass II\nLot # 936670 Exp. 09/30/13\nUnited States\nNationwide\n63243\nLetter\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n3\n1 DNA Way\n\n20121220\nSouth San Francisco\nClass III\nLot #: a) M1365B01, Exp 04/15; b) M1365, Exp 0...\nUnited States\nNationwide\n63874\nLetter\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n4\n801 Irving St\n\n20160706\nSan Francisco\nClass II\nAll lots compounded between 03/24/2015 and 03/...\nUnited States\nU.S. Including: CA, HI, NM\n73662\nPress Release\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n73\n801 Irving St\n\n20160706\nSan Francisco\nClass II\nAll lots compounded between 03/24/2015 and 03/...\nUnited States\nU.S. Including: CA, HI, NM\n73662\nPress Release\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n74\n801 Irving St\n\n20160706\nSan Francisco\nClass II\nAll lots compounded between 03/24/2015 and 03/...\nUnited States\nU.S. Including: CA, HI, NM\n73662\nPress Release\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n75\n5000 Shoreline Ct. Ste 200\n\n20130201\nSouth San Francisco\nClass III\nLot #: MA00AD5, Exp: 11/30/2014\nUnited States\nCA & VA\n64151\nLetter\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n76\n1 Dna Way\n\n20171201\nSouth San Francisco\nClass I\nLot# 3128243, 3141239, EXP. 9/30/2018; 3166728...\nUnited States\nNationwide in the USA\n78088\nLetter\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n77\n1 Dna Way\n\n20170405\nSouth San Francisco\nClass II\nB1009MC, B1009M9, B1009MA; Exp. 02/18 B1009MT...\nUnited States\nNJ and IL\n76726\nLetter\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n\n\n78 rows × 50 columns\n\n\n\n\n\nCode\n(\n    df.filter(df.city.isin('San Francisco', 'Los Angeles'))\n      .toPandas()\n)\n\n\n\n\n\n\n\n\n\naddress_1\naddress_2\ncenter_classification_date\ncity\nclassification\ncode_info\ncountry\ndistribution_pattern\nevent_id\ninitial_firm_notification\n...\nproduct_type_isnull\nreason_for_recall_isnull\nrecall_initiation_date_isnull\nrecall_number_isnull\nrecalling_firm_isnull\nreport_date_isnull\nstate_isnull\nstatus_isnull\ntermination_date_isnull\nvoluntary_mandated_isnull\n\n\n\n\n0\n1990 Westwood Blvd Ste 135\n\n20170731\nLos Angeles\nClass II\nLot # 03022017+44906; BUD 08/29/17\nUnited States\nThere were only one customer in California\n77757\nLetter\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n1\n450 N Van Ness Ave Apt 107\n\n20180411\nLos Angeles\nClass I\nUPC # 891656002209, exp date 12/31/2021\nUnited States\nProduct was distributed online via eBay.\n79476\nE-Mail\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n2\n801 Irving St\n\n20160706\nSan Francisco\nClass II\nAll lots compounded between 03/24/2015 and 03/...\nUnited States\nU.S. Including: CA, HI, NM\n73662\nPress Release\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n3\n801 Irving St\n\n20160706\nSan Francisco\nClass II\nAll lots compounded between 03/24/2015 and 03/...\nUnited States\nU.S. Including: CA, HI, NM\n73662\nPress Release\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n4\n801 Irving St\n\n20160706\nSan Francisco\nClass II\nAll lots compounded between 03/24/2015 and 03/...\nUnited States\nU.S. Including: CA, HI, NM\n73662\nPress Release\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n70\n801 Irving St\n\n20160706\nSan Francisco\nClass II\nAll lots compounded between 03/24/2015 and 03/...\nUnited States\nU.S. Including: CA, HI, NM\n73662\nPress Release\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n71\n801 Irving St\n\n20160706\nSan Francisco\nClass II\nAll lots compounded between 03/24/2015 and 03/...\nUnited States\nU.S. Including: CA, HI, NM\n73662\nPress Release\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n72\n154 W 131st St\n\n20170524\nLos Angeles\nClass I\nAll lots within expiry\nUnited States\nNationwide in the USA and Puerto Rico.\n77009\nPress Release\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n73\n154 W 131st St\n\n20170524\nLos Angeles\nClass I\nAll lots within expiry\nUnited States\nNationwide in the USA and Puerto Rico.\n77009\nPress Release\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n74\n304 E 11th St\n\n20180313\nLos Angeles\nClass I\nLot #: MFD:10.15.2017, Exp.10/14/2019.\nUnited States\nProduct was distributed in California to onlin...\n79327\nPress Release\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n\n\n75 rows × 50 columns"
  },
  {
    "objectID": "core/notebooks/notebook07_json-format.html#filtering-by-date-values",
    "href": "core/notebooks/notebook07_json-format.html#filtering-by-date-values",
    "title": "Using JSON data with Python",
    "section": "Filtering by Date Values",
    "text": "Filtering by Date Values\nIn addition to filtering by strings, we can also filter by columns where the values are stored as dates or datetimes (or strings that can be inferred as dates). Perhaps the most useful way to filter dates is by using the between() method, which allows us to find results within a certain date range. Here we find all the results which were reported in the years 2013 and 2014:\n\n\nCode\n( \n    df\n        .filter(df.city == \"South San Francisco\")\n        .filter(df.report_date.between('2013-01-01 00:00:00','2015-03-11 00:00:00'))\n        .toPandas()\n)\n\n\n\n\n\n\n\n\n\naddress_1\naddress_2\ncenter_classification_date\ncity\nclassification\ncode_info\ncountry\ndistribution_pattern\nevent_id\ninitial_firm_notification\n...\nproduct_type_isnull\nreason_for_recall_isnull\nrecall_initiation_date_isnull\nrecall_number_isnull\nrecalling_firm_isnull\nreport_date_isnull\nstate_isnull\nstatus_isnull\ntermination_date_isnull\nvoluntary_mandated_isnull\n\n\n\n\n0\n1 DNA Way\n\n20130122\nSouth San Francisco\nClass I\nLot #: 454138, Exp 07/14 (containing Trastuzum...\nUnited States\nNationwide\n63258\nE-Mail\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n1\n5000 Shoreline Ct. Ste 200\n\n20130201\nSouth San Francisco\nClass III\nLot #: MA00AD5, Exp: 11/30/2014\nUnited States\nCA & VA\n64151\nLetter\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n\n\n2 rows × 50 columns\n\n\n\n\n\n\n\n\n\nCaution\n\n\n\nIs Spark smart enough to understand that the string in column report_date contains a date?\n\n\n\n\nCode\ndf.filter(df.city == \"South San Francisco\")\\\n    .filter(df.center_classification_date.between('2013-01-01 00:00:00','2013-12-31 00:00:00'))\\\n    .toPandas()\n\n\n\n\n\n\n\n\n\naddress_1\naddress_2\ncenter_classification_date\ncity\nclassification\ncode_info\ncountry\ndistribution_pattern\nevent_id\ninitial_firm_notification\n...\nproduct_type_isnull\nreason_for_recall_isnull\nrecall_initiation_date_isnull\nrecall_number_isnull\nrecalling_firm_isnull\nreport_date_isnull\nstate_isnull\nstatus_isnull\ntermination_date_isnull\nvoluntary_mandated_isnull\n\n\n\n\n\n\n0 rows × 50 columns\n\n\n\n\n\nCode\ndf_dates = df.select([c for c in df.columns if c.endswith(\"date\")])\n\ndf_dates.printSchema()\n\n\nroot\n |-- center_classification_date: string (nullable = true)\n |-- recall_initiation_date: string (nullable = true)\n |-- report_date: string (nullable = true)\n |-- termination_date: string (nullable = true)\n\n\n\n\n\nCode\ndf_dates.show(5)\n\n\n+--------------------------+----------------------+-----------+----------------+\n|center_classification_date|recall_initiation_date|report_date|termination_date|\n+--------------------------+----------------------+-----------+----------------+\n|                  20121025|              20120904|   20121031|        20141007|\n|                  20121025|              20120904|   20121031|        20141007|\n|                  20121106|              20121015|   20121114|        20130325|\n|                  20121220|              20121204|   20121226|        20140429|\n|                  20121231|              20120926|   20130109|        20161007|\n+--------------------------+----------------------+-----------+----------------+\nonly showing top 5 rows\n\n\n\nColumns are not dates (DateType) but strings (StringType). When comparing report_date with '2013-01-01 00:00:00' and '2015-03-11 00:00:00', we are comparing strings and are lucky enough that in unicode '-' &lt; '0' &lt; '...' &lt; '9' so that 2013-.... is less that any string starting with 20130..., while any string starting with 2013... is less than any string starting with 2015...\n\n\n\n\n\n\nCaution\n\n\n\nIf some field in a Json string is meant to represent a date or a datetime object, spark should be given a hint.\nJson loaders (from Python) as well as the Spark Json reader have optional arguments that can be used to indicate the date parser to be used.\n\n\n\n\n\n\n\n\nNote\n\n\n\nWe have to tell the json loader about two things:\n\nwhich columns should be read as dates\nwhich format should be used for those columns\n\nThe first point can be settled using the schema argument of .json() method (see Documentation)\n\n\n\n\nCode\nze_schema = df.schema \n\nlist_fields = []\n\nfor f in ze_schema.fields:\n  if f.name.endswith('date'):\n    list_fields.append(StructField(f.name, DateType(), True))\n  else:\n    list_fields.append(f)\n\nze_schema = StructType(list_fields)\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[67], line 7\n      5 for f in ze_schema.fields:\n      6   if f.name.endswith('date'):\n----&gt; 7     list_fields.append(StructField(f.name, DateType(), True))\n      8   else:\n      9     list_fields.append(f)\n\nNameError: name 'DateType' is not defined\n\n\n\n\n\nCode\n# Alternative syntax using a dictionary of options\noptions = {\n    \"dateFormat\": \"yyyyMMdd\",\n    \"multiLine\": \"true\"\n}\n\ndf = (\n    spark.read\n        .options(**options)\n        .json(filename, ze_schema)\n)\n\n\n\n\nCode\ndf.select([c for c in df.columns if c.endswith(\"date\")]).printSchema()\n\n\nroot\n |-- center_classification_date: string (nullable = true)\n |-- recall_initiation_date: string (nullable = true)\n |-- report_date: string (nullable = true)\n |-- termination_date: string (nullable = true)\n\n\n\n\n\nCode\n(\ndf.filter(df.city == \"South San Francisco\")\n  .filter(df.center_classification_date.between('2013-01-01 00:00:00','2013-12-31 00:00:00'))\n  .toPandas()\n)\n\n\n\n\n\n\n\n\n\naddress_1\naddress_2\ncenter_classification_date\ncity\nclassification\ncode_info\ncountry\ndistribution_pattern\nevent_id\ninitial_firm_notification\n...\nproduct_type_isnull\nreason_for_recall_isnull\nrecall_initiation_date_isnull\nrecall_number_isnull\nrecalling_firm_isnull\nreport_date_isnull\nstate_isnull\nstatus_isnull\ntermination_date_isnull\nvoluntary_mandated_isnull\n\n\n\n\n\n\n0 rows × 50 columns"
  },
  {
    "objectID": "core/notebooks/notebook07_json-format.html#handling-complex-types",
    "href": "core/notebooks/notebook07_json-format.html#handling-complex-types",
    "title": "Using JSON data with Python",
    "section": "Handling complex types",
    "text": "Handling complex types\nBridging the gap between tabular and semi-structured data.\n\n\n\n\n\n\nNote\n\n\n\nSQL, R, Pandas …\n\n\nstruct, array, map\n\n\nCode\n# struct\n\n\nThe problems we faced after loading data from the json file pertained to the fact that column fda was of complex StrucType() type. We shall revisit this dataframe.\n\n\nCode\ndf = spark.read.json(filename, multiLine=True)\n\n\nThe dataframe schema df.schema which is of type StructType (defined in pyspark.sql.types) can be converted to a json string which in turn can be converted into a Python dictionary.\n\n\nCode\ndf = spark.read.json(filename, multiLine=True)\n\nsj = json.loads(df.schema.json())\n\n\nWe equip the dataframe with a primary key\n\n\nCode\nfrom pyspark.sql import Window\n\nw = Window.orderBy(col(\"center_classification_date\"))\n\ndf = (\n  df\n    .withColumn(\"row_id\", fn.row_number().over(w))\n)\n\n\n\n\nCode\n[(f['name'], f['type'])  \n    for f in sj['fields'] if not isinstance(f['type'], str)]\n\n\n[('openfda',\n  {'fields': [{'metadata': {},\n     'name': 'application_number',\n     'nullable': True,\n     'type': {'containsNull': True, 'elementType': 'string', 'type': 'array'}},\n    {'metadata': {},\n     'name': 'brand_name',\n     'nullable': True,\n     'type': {'containsNull': True, 'elementType': 'string', 'type': 'array'}},\n    {'metadata': {},\n     'name': 'generic_name',\n     'nullable': True,\n     'type': {'containsNull': True, 'elementType': 'string', 'type': 'array'}},\n    {'metadata': {},\n     'name': 'is_original_packager',\n     'nullable': True,\n     'type': {'containsNull': True,\n      'elementType': 'boolean',\n      'type': 'array'}},\n    {'metadata': {},\n     'name': 'manufacturer_name',\n     'nullable': True,\n     'type': {'containsNull': True, 'elementType': 'string', 'type': 'array'}},\n    {'metadata': {},\n     'name': 'nui',\n     'nullable': True,\n     'type': {'containsNull': True, 'elementType': 'string', 'type': 'array'}},\n    {'metadata': {},\n     'name': 'original_packager_product_ndc',\n     'nullable': True,\n     'type': {'containsNull': True, 'elementType': 'string', 'type': 'array'}},\n    {'metadata': {},\n     'name': 'package_ndc',\n     'nullable': True,\n     'type': {'containsNull': True, 'elementType': 'string', 'type': 'array'}},\n    {'metadata': {},\n     'name': 'pharm_class_cs',\n     'nullable': True,\n     'type': {'containsNull': True, 'elementType': 'string', 'type': 'array'}},\n    {'metadata': {},\n     'name': 'pharm_class_epc',\n     'nullable': True,\n     'type': {'containsNull': True, 'elementType': 'string', 'type': 'array'}},\n    {'metadata': {},\n     'name': 'pharm_class_moa',\n     'nullable': True,\n     'type': {'containsNull': True, 'elementType': 'string', 'type': 'array'}},\n    {'metadata': {},\n     'name': 'pharm_class_pe',\n     'nullable': True,\n     'type': {'containsNull': True, 'elementType': 'string', 'type': 'array'}},\n    {'metadata': {},\n     'name': 'product_ndc',\n     'nullable': True,\n     'type': {'containsNull': True, 'elementType': 'string', 'type': 'array'}},\n    {'metadata': {},\n     'name': 'product_type',\n     'nullable': True,\n     'type': {'containsNull': True, 'elementType': 'string', 'type': 'array'}},\n    {'metadata': {},\n     'name': 'route',\n     'nullable': True,\n     'type': {'containsNull': True, 'elementType': 'string', 'type': 'array'}},\n    {'metadata': {},\n     'name': 'rxcui',\n     'nullable': True,\n     'type': {'containsNull': True, 'elementType': 'string', 'type': 'array'}},\n    {'metadata': {},\n     'name': 'spl_id',\n     'nullable': True,\n     'type': {'containsNull': True, 'elementType': 'string', 'type': 'array'}},\n    {'metadata': {},\n     'name': 'spl_set_id',\n     'nullable': True,\n     'type': {'containsNull': True, 'elementType': 'string', 'type': 'array'}},\n    {'metadata': {},\n     'name': 'substance_name',\n     'nullable': True,\n     'type': {'containsNull': True, 'elementType': 'string', 'type': 'array'}},\n    {'metadata': {},\n     'name': 'unii',\n     'nullable': True,\n     'type': {'containsNull': True, 'elementType': 'string', 'type': 'array'}},\n    {'metadata': {},\n     'name': 'upc',\n     'nullable': True,\n     'type': {'containsNull': True,\n      'elementType': 'string',\n      'type': 'array'}}],\n   'type': 'struct'})]\n\n\nColumn openfda has type StrucType() with fields with composite type.\n\n\nCode\n{f.dataType  for f in df.schema.fields if not f.dataType==StringType()}\n\n\n{IntegerType(),\n StructType([StructField('application_number', ArrayType(StringType(), True), True), StructField('brand_name', ArrayType(StringType(), True), True), StructField('generic_name', ArrayType(StringType(), True), True), StructField('is_original_packager', ArrayType(BooleanType(), True), True), StructField('manufacturer_name', ArrayType(StringType(), True), True), StructField('nui', ArrayType(StringType(), True), True), StructField('original_packager_product_ndc', ArrayType(StringType(), True), True), StructField('package_ndc', ArrayType(StringType(), True), True), StructField('pharm_class_cs', ArrayType(StringType(), True), True), StructField('pharm_class_epc', ArrayType(StringType(), True), True), StructField('pharm_class_moa', ArrayType(StringType(), True), True), StructField('pharm_class_pe', ArrayType(StringType(), True), True), StructField('product_ndc', ArrayType(StringType(), True), True), StructField('product_type', ArrayType(StringType(), True), True), StructField('route', ArrayType(StringType(), True), True), StructField('rxcui', ArrayType(StringType(), True), True), StructField('spl_id', ArrayType(StringType(), True), True), StructField('spl_set_id', ArrayType(StringType(), True), True), StructField('substance_name', ArrayType(StringType(), True), True), StructField('unii', ArrayType(StringType(), True), True), StructField('upc', ArrayType(StringType(), True), True)])}\n\n\n\n\nCode\n{f['type']['type']\n    for f in sj['fields'] if not isinstance(f['type'], str)}\n\n\n{'struct'}\n\n\nProjecting on row_id and openfda.* leads to a (partially) flattened datafame, that, thanks to the row_id column can be joined with the original dataframe.\n\n\nCode\ndf_proj = df.select('row_id', 'openfda.*')\n\ndf_proj.printSchema()\n\n\nroot\n |-- row_id: integer (nullable = false)\n |-- application_number: array (nullable = true)\n |    |-- element: string (containsNull = true)\n |-- brand_name: array (nullable = true)\n |    |-- element: string (containsNull = true)\n |-- generic_name: array (nullable = true)\n |    |-- element: string (containsNull = true)\n |-- is_original_packager: array (nullable = true)\n |    |-- element: boolean (containsNull = true)\n |-- manufacturer_name: array (nullable = true)\n |    |-- element: string (containsNull = true)\n |-- nui: array (nullable = true)\n |    |-- element: string (containsNull = true)\n |-- original_packager_product_ndc: array (nullable = true)\n |    |-- element: string (containsNull = true)\n |-- package_ndc: array (nullable = true)\n |    |-- element: string (containsNull = true)\n |-- pharm_class_cs: array (nullable = true)\n |    |-- element: string (containsNull = true)\n |-- pharm_class_epc: array (nullable = true)\n |    |-- element: string (containsNull = true)\n |-- pharm_class_moa: array (nullable = true)\n |    |-- element: string (containsNull = true)\n |-- pharm_class_pe: array (nullable = true)\n |    |-- element: string (containsNull = true)\n |-- product_ndc: array (nullable = true)\n |    |-- element: string (containsNull = true)\n |-- product_type: array (nullable = true)\n |    |-- element: string (containsNull = true)\n |-- route: array (nullable = true)\n |    |-- element: string (containsNull = true)\n |-- rxcui: array (nullable = true)\n |    |-- element: string (containsNull = true)\n |-- spl_id: array (nullable = true)\n |    |-- element: string (containsNull = true)\n |-- spl_set_id: array (nullable = true)\n |    |-- element: string (containsNull = true)\n |-- substance_name: array (nullable = true)\n |    |-- element: string (containsNull = true)\n |-- unii: array (nullable = true)\n |    |-- element: string (containsNull = true)\n |-- upc: array (nullable = true)\n |    |-- element: string (containsNull = true)\n\n\n\nWe can inspect the length of the arrays.\n\n\nCode\n# array\ndf_proj.select(\n    fn.max(fn.size(col(\"application_number\"))).alias(\"Max\"), \n    fn.min(fn.size(col(\"application_number\"))).alias(\"min\"), \n    fn.avg(fn.size(col(\"application_number\"))).alias(\"Mean\")).show(1)\n\n\n+---+---+-------------------+\n|Max|min|               Mean|\n+---+---+-------------------+\n|  2| -1|-0.7380446333687567|\n+---+---+-------------------+\n\n\n\nIn some rows, the size of the array is -1 because the field is NULL.\n\n\nCode\n(\n  df_proj\n    .where(fn.size(col(\"application_number\"))&gt;1)\n    .select(\"row_id\")\n    .show(5)\n)\n\n\n25/03/10 16:55:41 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n25/03/10 16:55:41 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n25/03/10 16:55:41 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n25/03/10 16:55:41 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n25/03/10 16:55:41 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n\n\n+------+\n|row_id|\n+------+\n|  7159|\n|  7344|\n+------+\n\n\n\nAn array column can be exploded. This is like pivoting into long form. The result contains one row per item in the array.\n\n\nCode\n(\n  df_proj\n    .select('row_id', 'application_number')\n    .withColumn(\"exploded\", \n                fn.explode(col(\"application_number\")))\n    .select('row_id', 'exploded')\n    .groupBy('row_id')\n    .agg(fn.count('exploded').alias(\"n_lignes\"))\n    .where(\"n_lignes &gt; 1\")\n    .show(5)\n)\n\n\n25/03/10 16:55:41 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n25/03/10 16:55:41 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n25/03/10 16:55:41 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n25/03/10 16:55:41 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n\n\n+------+--------+\n|row_id|n_lignes|\n+------+--------+\n|  7159|       2|\n|  7344|       2|\n+------+--------+"
  },
  {
    "objectID": "core/notebooks/notebook08_webdata.html",
    "href": "core/notebooks/notebook08_webdata.html",
    "title": "Using with pyspark for data preprocessing",
    "section": "",
    "text": "We want to use pyspark to preprocess a potentially huge dataset used for web-marketing."
  },
  {
    "objectID": "core/notebooks/notebook08_webdata.html#data-description",
    "href": "core/notebooks/notebook08_webdata.html#data-description",
    "title": "Using with pyspark for data preprocessing",
    "section": "Data description",
    "text": "Data description\nThe data is a parquet file which contains a dataframe with 8 columns:\n\nxid: unique user id\naction: type of action. ‘C’ is a click, ‘O’ or ‘VSL’ is a web-display\ndate: date of the action\nwebsite_id: unique id of the website\nurl: url of the webpage\ncategory_id: id of the display\nzipcode: postal zipcode of the user\ndevice: type of device used by the user"
  },
  {
    "objectID": "core/notebooks/notebook08_webdata.html#q1.-some-statistics-computations",
    "href": "core/notebooks/notebook08_webdata.html#q1.-some-statistics-computations",
    "title": "Using with pyspark for data preprocessing",
    "section": "Q1. Some statistics / computations",
    "text": "Q1. Some statistics / computations\nUsing pyspark.sql we want to do the following things:\n\nCompute the total number of unique users\nConstruct a column containing the total number of actions per user\nConstruct a column containing the number of days since the last action of the user\nConstruct a column containing the number of actions of each user for each modality of device"
  },
  {
    "objectID": "core/notebooks/notebook08_webdata.html#q2.-binary-classification",
    "href": "core/notebooks/notebook08_webdata.html#q2.-binary-classification",
    "title": "Using with pyspark for data preprocessing",
    "section": "Q2. Binary classification",
    "text": "Q2. Binary classification\nThen, we want to construct a classifier to predict the click on the category 1204. Here is an agenda for this:\n\nConstruction of a features matrix for which each line corresponds to the information concerning a user.\nIn this matrix, we need to keep only the users that have been exposed to the display in category 1204\nUsing this training dataset, train a binary classifier, and evaluate your classifier using a precision / recall curve computed on test data."
  },
  {
    "objectID": "core/notebooks/notebook08_webdata.html#compute-the-total-number-of-unique-users",
    "href": "core/notebooks/notebook08_webdata.html#compute-the-total-number-of-unique-users",
    "title": "Using with pyspark for data preprocessing",
    "section": "Compute the total number of unique users",
    "text": "Compute the total number of unique users\n\n\nCode\n( \n    df.select('xid')\n      .distinct()\n      .count()\n)\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[15], line 2\n      1 ( \n----&gt; 2     df.select('xid')\n      3       .distinct()\n      4       .count()\n      5 )\n\nNameError: name 'df' is not defined\n\n\n\n\n\nCode\ndef foo(x):\n   c = len(set(x))\n   print(c)\n   return c\n\n\n\n\nCode\nfoo([1, 1, 2])\n\n\n2\n\n\n2\n\n\n\n\nCode\ndf.rdd.map(lambda x : x.xid).foreachPartition(foo)\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[18], line 1\n----&gt; 1 df.rdd.map(lambda x : x.xid).foreachPartition(foo)\n\nNameError: name 'df' is not defined\n\n\n\n\n\nCode\n78120 + 78636 + 79090 + 78865 + 79296 + 79754\n\n\n473761\n\n\nThis might pump up some computational resources\n\n\nCode\n( \n    df.select('xid')\n      .distinct() \n      .explain()\n)\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[20], line 2\n      1 ( \n----&gt; 2     df.select('xid')\n      3       .distinct() \n      4       .explain()\n      5 )\n\nNameError: name 'df' is not defined\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe distinct values of xid seem to be evenly spread among the six files making the parquet directory"
  },
  {
    "objectID": "core/notebooks/notebook08_webdata.html#construct-a-column-containing-the-total-number-of-actions-per-user",
    "href": "core/notebooks/notebook08_webdata.html#construct-a-column-containing-the-total-number-of-actions-per-user",
    "title": "Using with pyspark for data preprocessing",
    "section": "Construct a column containing the total number of actions per user",
    "text": "Construct a column containing the total number of actions per user\n\n\nCode\nxid_partition = Window.partitionBy('xid')\n\nn_events = func.count(col('action')).over(xid_partition)\n\ndf = df.withColumn('n_events', n_events)\n\ndf.show(n=2)\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[21], line 5\n      1 xid_partition = Window.partitionBy('xid')\n      3 n_events = func.count(col('action')).over(xid_partition)\n----&gt; 5 df = df.withColumn('n_events', n_events)\n      7 df.show(n=2)\n\nNameError: name 'df' is not defined\n\n\n\n\n\nCode\n( \n  df\n    .groupBy('xid')\n    .agg(func.count('action'))\n    .show(n=5)\n)\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[22], line 2\n      1 ( \n----&gt; 2   df\n      3     .groupBy('xid')\n      4     .agg(func.count('action'))\n      5     .show(n=5)\n      6 )\n\nNameError: name 'df' is not defined\n\n\n\nVisualize the distribution of the number of users per number of actions.\n\n\n\n\n\n\nQuestion\n\n\n\nConstruct a column containing the number of days since the last action of the user\n\n\n\n\nCode\n# xid_partition = Window.partitionBy('xid')\n\nmax_date = (\n  func\n    .max(col('date'))\n    .over(xid_partition)\n)\n\nn_days_since_last_event = func.datediff(func.current_date(), max_date)\n\ndf = df.withColumn('n_days_since_last_event',\n                   n_days_since_last_event)\n\ndf.show(n=2)\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[23], line 11\n      3 max_date = (\n      4   func\n      5     .max(col('date'))\n      6     .over(xid_partition)\n      7 )\n      9 n_days_since_last_event = func.datediff(func.current_date(), max_date)\n---&gt; 11 df = df.withColumn('n_days_since_last_event',\n     12                    n_days_since_last_event)\n     14 df.show(n=2)\n\nNameError: name 'df' is not defined\n\n\n\n\n\nCode\ndf.printSchema()\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[24], line 1\n----&gt; 1 df.printSchema()\n\nNameError: name 'df' is not defined"
  },
  {
    "objectID": "core/notebooks/notebook08_webdata.html#construct-a-column-containing-the-number-of-actions-of-each-user-for-each-modality-of-device",
    "href": "core/notebooks/notebook08_webdata.html#construct-a-column-containing-the-number-of-actions-of-each-user-for-each-modality-of-device",
    "title": "Using with pyspark for data preprocessing",
    "section": "Construct a column containing the number of actions of each user for each modality of device",
    "text": "Construct a column containing the number of actions of each user for each modality of device\n\n\nCode\nxid_device_partition = xid_partition.partitionBy('device')\n\nn_events_per_device = func.count(col('action')).over(xid_device_partition)\n\ndf = df.withColumn('n_events_per_device', n_events_per_device)\n\ndf.head(n=2)\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[25], line 5\n      1 xid_device_partition = xid_partition.partitionBy('device')\n      3 n_events_per_device = func.count(col('action')).over(xid_device_partition)\n----&gt; 5 df = df.withColumn('n_events_per_device', n_events_per_device)\n      7 df.head(n=2)\n\nNameError: name 'df' is not defined"
  },
  {
    "objectID": "core/notebooks/notebook08_webdata.html#number-of-devices-per-user",
    "href": "core/notebooks/notebook08_webdata.html#number-of-devices-per-user",
    "title": "Using with pyspark for data preprocessing",
    "section": "Number of devices per user ",
    "text": "Number of devices per user \n\n\nCode\nxid_partition = Window.partitionBy('xid')\n\nrank_device = (\n  func\n    .dense_rank()\n    .over(xid_partition.orderBy('device'))\n)\n\nn_unique_device = (\n    func\n      .last(rank_device)\n      .over(xid_partition)\n)\n\ndf = df.withColumn('n_device', n_unique_device)\n\ndf.head(n=2)\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[26], line 15\n      3 rank_device = (\n      4   func\n      5     .dense_rank()\n      6     .over(xid_partition.orderBy('device'))\n      7 )\n      9 n_unique_device = (\n     10     func\n     11       .last(rank_device)\n     12       .over(xid_partition)\n     13 )\n---&gt; 15 df = df.withColumn('n_device', n_unique_device)\n     17 df.head(n=2)\n\nNameError: name 'df' is not defined\n\n\n\n\n\nCode\ndf\\\n    .where(col('n_device') &gt; 1)\\\n    .select('xid', 'device', 'n_events',  'n_device', 'n_events_per_device')\\\n    .head(n=8)\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[27], line 1\n----&gt; 1 df\\\n      2     .where(col('n_device') &gt; 1)\\\n      3     .select('xid', 'device', 'n_events',  'n_device', 'n_events_per_device')\\\n      4     .head(n=8)\n\nNameError: name 'df' is not defined\n\n\n\n\n\nCode\ndf\\\n    .where(col('n_device') &gt; 1)\\\n    .select('xid', 'device', 'n_events',  'n_device', 'n_events_per_device')\\\n    .count()\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[28], line 1\n----&gt; 1 df\\\n      2     .where(col('n_device') &gt; 1)\\\n      3     .select('xid', 'device', 'n_events',  'n_device', 'n_events_per_device')\\\n      4     .count()\n\nNameError: name 'df' is not defined"
  },
  {
    "objectID": "core/notebooks/notebook08_webdata.html#extraction",
    "href": "core/notebooks/notebook08_webdata.html#extraction",
    "title": "Using with pyspark for data preprocessing",
    "section": "Extraction",
    "text": "Extraction\nHere extraction is just about reading the data\n\n\nCode\ndf = spark.read.parquet(file_path)\ndf.show(n=3)\n\n\n\n---------------------------------------------------------------------------\nAnalysisException                         Traceback (most recent call last)\nCell In[29], line 1\n----&gt; 1 df = spark.read.parquet(file_path)\n      2 df.show(n=3)\n\nFile ~/Documents/IFEBY310/.venv/lib/python3.12/site-packages/pyspark/sql/readwriter.py:544, in DataFrameReader.parquet(self, *paths, **options)\n    533 int96RebaseMode = options.get(\"int96RebaseMode\", None)\n    534 self._set_opts(\n    535     mergeSchema=mergeSchema,\n    536     pathGlobFilter=pathGlobFilter,\n   (...)\n    541     int96RebaseMode=int96RebaseMode,\n    542 )\n--&gt; 544 return self._df(self._jreader.parquet(_to_seq(self._spark._sc, paths)))\n\nFile ~/Documents/IFEBY310/.venv/lib/python3.12/site-packages/py4j/java_gateway.py:1322, in JavaMember.__call__(self, *args)\n   1316 command = proto.CALL_COMMAND_NAME +\\\n   1317     self.command_header +\\\n   1318     args_command +\\\n   1319     proto.END_COMMAND_PART\n   1321 answer = self.gateway_client.send_command(command)\n-&gt; 1322 return_value = get_return_value(\n   1323     answer, self.gateway_client, self.target_id, self.name)\n   1325 for temp_arg in temp_args:\n   1326     if hasattr(temp_arg, \"_detach\"):\n\nFile ~/Documents/IFEBY310/.venv/lib/python3.12/site-packages/pyspark/errors/exceptions/captured.py:185, in capture_sql_exception.&lt;locals&gt;.deco(*a, **kw)\n    181 converted = convert_exception(e.java_exception)\n    182 if not isinstance(converted, UnknownException):\n    183     # Hide where the exception came from that shows a non-Pythonic\n    184     # JVM exception message.\n--&gt; 185     raise converted from None\n    186 else:\n    187     raise\n\nAnalysisException: [PATH_NOT_FOUND] Path does not exist: file:/home/boucheron/Documents/IFEBY310/core/notebooks/data/webdata.parquet."
  },
  {
    "objectID": "core/notebooks/notebook08_webdata.html#transformation-of-the-data",
    "href": "core/notebooks/notebook08_webdata.html#transformation-of-the-data",
    "title": "Using with pyspark for data preprocessing",
    "section": "Transformation of the data",
    "text": "Transformation of the data\nAt this step we compute a lot of extra things from the data. The aim is to build features that describe users.\n\n\nCode\ndef n_events_transformer(df):\n    xid_partition = Window.partitionBy('xid')\n    n_events = func.count(col('action')).over(xid_partition)\n    \n    df = df.withColumn('n_events', n_events)\n\n    return df\n\n\n\n\nCode\ndef n_events_per_action_transformer(df):\n    xid_action_partition = Window.partitionBy('xid', 'action')\n    n_events_per_action = func.count(col('action')).over(xid_action_partition)\n\n    df = df.withColumn('n_events_per_action', n_events_per_action)\n    \n    return df\n\n\n\n\nCode\ndef hour_transformer(df):\n    hour = func.hour(col('date'))\n    df = df.withColumn('hour', hour)\n    return df\n\ndef weekday_transformer(df):\n    weekday = func.date_format(col('date'), 'EEEE')\n    df = df.withColumn('weekday', weekday)\n    return df\n\ndef n_events_per_hour_transformer(df):\n    xid_hour_partition = Window.partitionBy('xid', 'hour')\n    n_events_per_hour = func.count(col('action')).over(xid_hour_partition)\n    df = df.withColumn('n_events_per_hour', n_events_per_hour)\n    return df\n\ndef n_events_per_weekday_transformer(df):\n    xid_weekday_partition = Window.partitionBy('xid', 'weekday')\n    n_events_per_weekday = func.count(col('action')).over(xid_weekday_partition)\n    df = df.withColumn('n_events_per_weekday', n_events_per_weekday)\n    return df\n\ndef n_days_since_last_event_transformer(df):\n    xid_partition = Window.partitionBy('xid')\n    max_date = func.max(col('date')).over(xid_partition)\n    n_days_since_last_event = func.datediff(func.current_date(), max_date)\n    df = df.withColumn('n_days_since_last_event',\n                       n_days_since_last_event + lit(0.1))\n    return df\n\ndef n_days_since_last_action_transformer(df):\n    xid_partition_action = Window.partitionBy('xid', 'action')\n    max_date = func.max(col('date')).over(xid_partition_action)\n    n_days_since_last_action = func.datediff(func.current_date(),\n                                                        max_date)\n    df = df.withColumn('n_days_since_last_action',\n                       n_days_since_last_action + lit(0.1))\n    return df\n\ndef n_unique_day_transformer(df):\n    xid_partition = Window.partitionBy('xid')\n    dayofyear = func.dayofyear(col('date'))\n    rank_day = func.dense_rank().over(xid_partition.orderBy(dayofyear))\n    n_unique_day = func.last(rank_day).over(xid_partition)\n    df = df.withColumn('n_unique_day', n_unique_day)\n    return df\n\ndef n_unique_hour_transformer(df):\n    xid_partition = Window.partitionBy('xid')\n    rank_hour = func.dense_rank().over(xid_partition.orderBy('hour'))\n    n_unique_hour = func.last(rank_hour).over(xid_partition)\n    df = df.withColumn('n_unique_hour', n_unique_hour)\n    return df\n\ndef n_events_per_device_transformer(df):\n    xid_device_partition = Window.partitionBy('xid', 'device')\n    n_events_per_device = func.count(func.col('device')) \\\n        .over(xid_device_partition)\n    df = df.withColumn('n_events_per_device', n_events_per_device)\n    return df\n\ndef n_unique_device_transformer(df):\n    xid_partition = Window.partitionBy('xid')\n    rank_device = func.dense_rank().over(xid_partition.orderBy('device'))\n    n_unique_device = func.last(rank_device).over(xid_partition)\n    df = df.withColumn('n_device', n_unique_device)\n    return df\n\ndef n_actions_per_category_id_transformer(df):\n    xid_category_id_partition = Window.partitionBy('xid', 'category_id',\n                                                   'action')\n    n_actions_per_category_id = func.count(func.col('action')) \\\n        .over(xid_category_id_partition)\n    df = df.withColumn('n_actions_per_category_id', n_actions_per_category_id)\n    return df\n\ndef n_unique_category_id_transformer(df):\n    xid_partition = Window.partitionBy('xid')\n    rank_category_id = func.dense_rank().over(xid_partition\\\n                                              .orderBy('category_id'))\n    n_unique_category_id = func.last(rank_category_id).over(xid_partition)\n    df = df.withColumn('n_unique_category_id', n_unique_category_id)\n    return df\n\ndef n_events_per_category_id_transformer(df):\n    xid_category_id_partition = Window.partitionBy('xid', 'category_id')\n    n_events_per_category_id = func.count(func.col('action')) \\\n        .over(xid_category_id_partition)\n    df = df.withColumn('n_events_per_category_id', n_events_per_category_id)\n    return df\n\ndef n_events_per_website_id_transformer(df):\n    xid_website_id_partition = Window.partitionBy('xid', 'website_id')\n    n_events_per_website_id = func.count(col('action'))\\\n        .over(xid_website_id_partition)\n    df = df.withColumn('n_events_per_website_id', n_events_per_website_id)\n    return df\n\n\n\n\nCode\ntransformers = [\n    hour_transformer,\n    weekday_transformer,\n    n_events_per_hour_transformer,\n    n_events_per_weekday_transformer,\n    n_days_since_last_event_transformer,\n    n_days_since_last_action_transformer,\n    n_unique_day_transformer,\n    n_unique_hour_transformer,\n    n_events_per_device_transformer,\n    n_unique_device_transformer,\n    n_actions_per_category_id_transformer,\n    n_events_per_category_id_transformer,\n    n_events_per_website_id_transformer,\n]\n\n\n\n\nCode\nN = 10000\n\n\n\n\nCode\nsample_df = df.sample(withReplacement=False, fraction=.05)\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[35], line 1\n----&gt; 1 sample_df = df.sample(withReplacement=False, fraction=.05)\n\nNameError: name 'df' is not defined\n\n\n\n\n\nCode\nsample_df.count()\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[36], line 1\n----&gt; 1 sample_df.count()\n\nNameError: name 'sample_df' is not defined\n\n\n\n\n\nCode\nfor transformer in transformers:\n    df = transformer(df)\n\ndf.head(n=1)\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[37], line 2\n      1 for transformer in transformers:\n----&gt; 2     df = transformer(df)\n      4 df.head(n=1)\n\nNameError: name 'df' is not defined\n\n\n\n\n\nCode\nfor transformer in transformers:\n    sample_df = transformer(sample_df)\n\nsample_df.head(n=1)\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[38], line 2\n      1 for transformer in transformers:\n----&gt; 2     sample_df = transformer(sample_df)\n      4 sample_df.head(n=1)\n\nNameError: name 'sample_df' is not defined\n\n\n\n\n\nCode\ndf = sample_df\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[39], line 1\n----&gt; 1 df = sample_df\n\nNameError: name 'sample_df' is not defined\n\n\n\n\n\nCode\nsorted(df.columns)\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[40], line 1\n----&gt; 1 sorted(df.columns)\n\nNameError: name 'df' is not defined"
  },
  {
    "objectID": "core/notebooks/notebook08_webdata.html#load-step",
    "href": "core/notebooks/notebook08_webdata.html#load-step",
    "title": "Using with pyspark for data preprocessing",
    "section": "Load step",
    "text": "Load step\nHere, we use all the previous computations (saved in the columns of the dataframe) to compute aggregated informations about each user.\n\n\nCode\ndef n_events_per_hour_loader(df):\n    csr = df\\\n        .select('xid', 'hour', 'n_events_per_hour')\\\n        .withColumnRenamed('n_events_per_hour', 'value')\\\n        .distinct()     # action\n    feature_name = func.concat(lit('n_events_per_hour#'), col('hour'))\n    csr = csr\\\n        .withColumn('feature_name', feature_name)\\\n        .drop('hour')\n    return csr\n\ndef n_events_per_website_id_loader(df):\n    csr = df.select('xid', 'website_id', 'n_events_per_website_id')\\\n        .withColumnRenamed('n_events_per_hour', 'value')\\\n        .distinct()\n    feature_name = func.concat(lit('n_events_per_website_id#'),\n                               col('website_id'))\n    csr = csr\\\n        .withColumn('feature_name', feature_name)\\\n        .drop('website_id')\n    return csr\n\ndef n_events_per_hour_loader(df):\n    csr = df\\\n        .select('xid', 'hour', 'n_events_per_hour')\\\n        .withColumnRenamed('n_events_per_hour', 'value')\\\n        .distinct()\n    feature_name = func.concat(lit('n_events_per_hour#'), col('hour'))\n    csr = csr\\\n        .withColumn('feature_name', feature_name)\\\n        .drop('hour')\n    return csr\n\ndef n_events_per_weekday_loader(df):\n    csr = df\\\n        .select('xid', 'weekday', 'n_events_per_weekday')\\\n        .withColumnRenamed('n_events_per_weekday', 'value')\\\n        .distinct()\n    feature_name = func.concat(lit('n_events_per_weekday#'), col('weekday'))\n    csr = csr\\\n        .withColumn('feature_name', feature_name)\\\n        .drop('weekday')\n    return csr\n\ndef n_days_since_last_event_loader(df):\n    csr = df.select('xid',  'n_days_since_last_event')\\\n        .withColumnRenamed('n_days_since_last_event#', 'value')\\\n        .distinct()\n    feature_name = lit('n_days_since_last_event')\n    csr = csr\\\n        .withColumn('feature_name', feature_name)\n    return csr\n\ndef n_days_since_last_action_loader(df):\n    csr = df.select('xid', 'action', 'n_days_since_last_action')\\\n        .withColumnRenamed('n_days_since_last_action', 'value')\\\n        .distinct()\n    feature_name = func.concat(lit('n_days_since_last_action#'), col('action'))\n    csr = csr\\\n        .withColumn('feature_name', feature_name)\\\n        .drop('action')\n    return csr\n\ndef n_unique_day_loader(df):\n    csr = df.select('xid', 'n_unique_day')\\\n        .withColumnRenamed('n_unique_day', 'value')\\\n        .distinct()\n    feature_name = lit('n_unique_day')\n    csr = csr\\\n        .withColumn('feature_name', feature_name)\n    return csr\n\ndef n_unique_hour_loader(df):\n    csr = df.select('xid', 'n_unique_hour')\\\n        .withColumnRenamed('n_unique_hour', 'value')\\\n        .distinct()\n    feature_name = lit('n_unique_hour')\n    csr = csr\\\n        .withColumn('feature_name', feature_name)\n    return csr\n\ndef n_events_per_device_loader(df):\n    csr = df\\\n        .select('xid', 'device', 'n_events_per_device')\\\n        .withColumnRenamed('n_events_per_device', 'value')\\\n        .distinct()\n    feature_name = func.concat(lit('n_events_per_device#'), col('device'))\n    csr = csr\\\n        .withColumn('feature_name', feature_name)\\\n        .drop('device')\n    return csr\n\ndef n_unique_device_loader(df):\n    csr = df.select('xid', 'n_device')\\\n        .withColumnRenamed('n_device', 'value')\\\n        .distinct()\n    feature_name = lit('n_device')\n    csr = csr\\\n        .withColumn('feature_name', feature_name)\n    return csr\n\ndef n_events_per_category_id_loader(df):\n    csr = df.select('xid', 'category_id', 'n_events_per_category_id')\\\n        .withColumnRenamed('n_events_per_category_id', 'value')\\\n        .distinct()\n    feature_name = func.concat(lit('n_events_per_category_id#'),\n                               col('category_id'))\n    csr = csr\\\n        .withColumn('feature_name', feature_name)\\\n        .drop('category_id')\n    return csr\n\ndef n_actions_per_category_id_loader(df):\n    csr = df.select('xid', 'category_id', 'action', 'n_actions_per_category_id')\\\n        .withColumnRenamed('n_actions_per_category_id', 'value')\\\n        .distinct()\n    feature_name = func.concat(lit('n_actions_per_category_id#'),\n                               col('action'), lit('#'), \n                               col('category_id'))\n    csr = csr\\\n        .withColumn('feature_name', feature_name)\\\n        .drop('category_id')\\\n        .drop('action')\n    return csr\n\ndef n_events_per_website_id_loader(df):\n    csr = df.select('xid', 'website_id', 'n_events_per_website_id')\\\n        .withColumnRenamed('n_events_per_website_id', 'value')\\\n        .distinct()\n    feature_name = func.concat(lit('n_events_per_website_id#'),\n                               col('website_id'))\n    csr = csr\\\n        .withColumn('feature_name', feature_name)\\\n        .drop('website_id')\n    return csr\n\n\n\n\nCode\nfrom functools import reduce\n\n\n\n\nCode\nloaders = [\n    n_events_per_hour_loader,\n    n_events_per_website_id_loader,\n    n_events_per_hour_loader,\n    n_events_per_weekday_loader,\n    n_days_since_last_event_loader,\n    n_days_since_last_action_loader,\n    n_unique_day_loader,\n    n_unique_hour_loader,\n    n_events_per_device_loader,\n    n_unique_device_loader,\n    n_events_per_category_id_loader,\n    n_actions_per_category_id_loader,\n    n_events_per_website_id_loader,\n]\n\n\n\n\nCode\ndef union(df, other):\n    return df.union(other)\n\n\n\n\n\n\n\n\nAbout DataFrame.union()\n\n\n\nThis method performs a SQL-style set union of the rows from both DataFrame objects, with no automatic deduplication of elements.\nUse the distinct() method to perform deduplication of rows.\nThe method resolves columns by position (not by name), following the standard behavior in SQL.\n\n\n\n\nCode\nspam = [loader(df) for loader in loaders]\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[45], line 1\n----&gt; 1 spam = [loader(df) for loader in loaders]\n\nNameError: name 'df' is not defined\n\n\n\n\n\nCode\nspam[0].printSchema()\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[46], line 1\n----&gt; 1 spam[0].printSchema()\n\nNameError: name 'spam' is not defined\n\n\n\n\n\nCode\nlen(spam)\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[47], line 1\n----&gt; 1 len(spam)\n\nNameError: name 'spam' is not defined\n\n\n\n\n\nCode\ncsr = reduce(\n    lambda df1, df2: df1.union(df2),\n    spam\n)\n\ncsr.head(n=3)\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[48], line 3\n      1 csr = reduce(\n      2     lambda df1, df2: df1.union(df2),\n----&gt; 3     spam\n      4 )\n      6 csr.head(n=3)\n\nNameError: name 'spam' is not defined\n\n\n\n\n\nCode\ncsr.columns\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[49], line 1\n----&gt; 1 csr.columns\n\nNameError: name 'csr' is not defined\n\n\n\n\n\nCode\ncsr.show(5)\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[50], line 1\n----&gt; 1 csr.show(5)\n\nNameError: name 'csr' is not defined\n\n\n\n\n\nCode\ncsr.rdd.getNumPartitions()\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[51], line 1\n----&gt; 1 csr.rdd.getNumPartitions()\n\nNameError: name 'csr' is not defined\n\n\n\n\n\nCode\n# Replace features names and xid by a unique number\nfeature_name_partition = Window().orderBy('feature_name')\nxid_partition = Window().orderBy('xid')\n\ncol_idx = func.dense_rank().over(feature_name_partition)\nrow_idx = func.dense_rank().over(xid_partition)\n\n\n\n\nCode\ncsr = csr.withColumn('col', col_idx)\\\n    .withColumn('row', row_idx)\n\ncsr = csr.na.drop('any')\n\ncsr.head(n=5)\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[53], line 1\n----&gt; 1 csr = csr.withColumn('col', col_idx)\\\n      2     .withColumn('row', row_idx)\n      4 csr = csr.na.drop('any')\n      6 csr.head(n=5)\n\nNameError: name 'csr' is not defined\n\n\n\n\n\nCode\n# Let's save the result of our hard work into a new parquet file\noutput_path = Path('./data')\noutput_file = str(output_path / 'csr.parquet')\ncsr.write.parquet(output_file, mode='overwrite')\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[54], line 4\n      2 output_path = Path('./data')\n      3 output_file = str(output_path / 'csr.parquet')\n----&gt; 4 csr.write.parquet(output_file, mode='overwrite')\n\nNameError: name 'csr' is not defined"
  },
  {
    "objectID": "core/notebooks/notebook08_webdata.html#finally",
    "href": "core/notebooks/notebook08_webdata.html#finally",
    "title": "Using with pyspark for data preprocessing",
    "section": "Finally !!",
    "text": "Finally !!\nWow ! That was a lot of work. Now we have a features matrix \\(X\\) and a vector of labels \\(y\\).\n\n\nCode\nX.indices\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[78], line 1\n----&gt; 1 X.indices\n\nNameError: name 'X' is not defined\n\n\n\n\n\nCode\nX.indptr\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[79], line 1\n----&gt; 1 X.indptr\n\nNameError: name 'X' is not defined\n\n\n\n\n\nCode\nX.shape, X.nnz\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[80], line 1\n----&gt; 1 X.shape, X.nnz\n\nNameError: name 'X' is not defined\n\n\n\n\n\nCode\ny.shape, y.sum()\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[81], line 1\n----&gt; 1 y.shape, y.sum()\n\nNameError: name 'y' is not defined"
  },
  {
    "objectID": "core/notebooks/notebook11_dive.html",
    "href": "core/notebooks/notebook11_dive.html",
    "title": "Diving deeer",
    "section": "",
    "text": "Code\nimport os\nimport sys\n\nos.environ['PYSPARK_PYTHON'] = sys.executable\nos.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable\nCode\nfrom pyspark import SparkConf, SparkContext\nfrom pyspark.sql import SparkSession\n\nconf = SparkConf().setAppName(\"Spark SQL Course\")\nsc = SparkContext(conf=conf)  # no need for Spark 3...\n\nspark = (SparkSession\n    .builder\n    .appName(\"Spark SQL Course\")\n    .getOrCreate()\n)\n\n\n25/03/10 16:56:11 WARN Utils: Your hostname, boucheron-Precision-5480 resolves to a loopback address: 127.0.1.1; using 172.23.32.10 instead (on interface enxac91a1bd3e89)\n25/03/10 16:56:11 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\nSetting default log level to \"WARN\".\nTo adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n25/03/10 16:56:11 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n25/03/10 16:56:12 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\nCode\nsc = spark._sc\nCode\nrdd = sc.parallelize([(\"a\", 1), (\"b\", 1), (\"a\", 1)])\nCode\nrdd.reduceByKey(lambda a, b: a + b).collect()\n\n\n[Stage 0:&gt;                                                        (0 + 20) / 20]                                                                                \n\n\n[('b', 1), ('a', 2)]\nCode\nimport requests, zipfile, io\nfrom pathlib import Path\n\npath = Path('webdata.parquet')\nif not path.exists():\n    url = \"https://stephanegaiffas.github.io/big_data_course/data/webdata.parquet.zip\"\n    r = requests.get(url)\n    z = zipfile.ZipFile(io.BytesIO(r.content))\n    z.extractall(path='./')\n\n\n\n---------------------------------------------------------------------------\nBadZipFile                                Traceback (most recent call last)\nCell In[6], line 8\n      6 url = \"https://stephanegaiffas.github.io/big_data_course/data/webdata.parquet.zip\"\n      7 r = requests.get(url)\n----&gt; 8 z = zipfile.ZipFile(io.BytesIO(r.content))\n      9 z.extractall(path='./')\n\nFile /usr/lib/python3.12/zipfile/__init__.py:1349, in ZipFile.__init__(self, file, mode, compression, allowZip64, compresslevel, strict_timestamps, metadata_encoding)\n   1347 try:\n   1348     if mode == 'r':\n-&gt; 1349         self._RealGetContents()\n   1350     elif mode in ('w', 'x'):\n   1351         # set the modified flag so central directory gets written\n   1352         # even if no files are added to the archive\n   1353         self._didModify = True\n\nFile /usr/lib/python3.12/zipfile/__init__.py:1416, in ZipFile._RealGetContents(self)\n   1414     raise BadZipFile(\"File is not a zip file\")\n   1415 if not endrec:\n-&gt; 1416     raise BadZipFile(\"File is not a zip file\")\n   1417 if self.debug &gt; 1:\n   1418     print(endrec)\n\nBadZipFile: File is not a zip file\nCode\ninput_path = './'\n\ninput_file = os.path.join(input_path, 'webdata.parquet')\ndf = spark.read.parquet(input_file)\n\n\n\n---------------------------------------------------------------------------\nAnalysisException                         Traceback (most recent call last)\nCell In[7], line 4\n      1 input_path = './'\n      3 input_file = os.path.join(input_path, 'webdata.parquet')\n----&gt; 4 df = spark.read.parquet(input_file)\n\nFile ~/Documents/IFEBY310/.venv/lib/python3.12/site-packages/pyspark/sql/readwriter.py:544, in DataFrameReader.parquet(self, *paths, **options)\n    533 int96RebaseMode = options.get(\"int96RebaseMode\", None)\n    534 self._set_opts(\n    535     mergeSchema=mergeSchema,\n    536     pathGlobFilter=pathGlobFilter,\n   (...)\n    541     int96RebaseMode=int96RebaseMode,\n    542 )\n--&gt; 544 return self._df(self._jreader.parquet(_to_seq(self._spark._sc, paths)))\n\nFile ~/Documents/IFEBY310/.venv/lib/python3.12/site-packages/py4j/java_gateway.py:1322, in JavaMember.__call__(self, *args)\n   1316 command = proto.CALL_COMMAND_NAME +\\\n   1317     self.command_header +\\\n   1318     args_command +\\\n   1319     proto.END_COMMAND_PART\n   1321 answer = self.gateway_client.send_command(command)\n-&gt; 1322 return_value = get_return_value(\n   1323     answer, self.gateway_client, self.target_id, self.name)\n   1325 for temp_arg in temp_args:\n   1326     if hasattr(temp_arg, \"_detach\"):\n\nFile ~/Documents/IFEBY310/.venv/lib/python3.12/site-packages/pyspark/errors/exceptions/captured.py:185, in capture_sql_exception.&lt;locals&gt;.deco(*a, **kw)\n    181 converted = convert_exception(e.java_exception)\n    182 if not isinstance(converted, UnknownException):\n    183     # Hide where the exception came from that shows a non-Pythonic\n    184     # JVM exception message.\n--&gt; 185     raise converted from None\n    186 else:\n    187     raise\n\nAnalysisException: [PATH_NOT_FOUND] Path does not exist: file:/home/boucheron/Documents/IFEBY310/core/notebooks/webdata.parquet.\nCode\ndf.head(6)\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[8], line 1\n----&gt; 1 df.head(6)\n\nNameError: name 'df' is not defined\nCode\ndf.describe()\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[9], line 1\n----&gt; 1 df.describe()\n\nNameError: name 'df' is not defined\nCode\ndf.count()\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[10], line 1\n----&gt; 1 df.count()\n\nNameError: name 'df' is not defined"
  },
  {
    "objectID": "core/notebooks/notebook11_dive.html#compute-the-total-number-of-unique-users",
    "href": "core/notebooks/notebook11_dive.html#compute-the-total-number-of-unique-users",
    "title": "Diving deeer",
    "section": "Compute the total number of unique users",
    "text": "Compute the total number of unique users\n\n\nCode\ndf.select('xid').distinct().count()\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[12], line 1\n----&gt; 1 df.select('xid').distinct().count()\n\nNameError: name 'df' is not defined"
  },
  {
    "objectID": "core/notebooks/notebook11_dive.html#construct-a-column-containing-the-total-number-of-actions-per-user",
    "href": "core/notebooks/notebook11_dive.html#construct-a-column-containing-the-total-number-of-actions-per-user",
    "title": "Diving deeer",
    "section": "Construct a column containing the total number of actions per user",
    "text": "Construct a column containing the total number of actions per user\n\n\nCode\nxid_partition = Window.partitionBy('xid')\nn_events = func.count(col('action')).over(xid_partition)\ndf = df.withColumn('n_events', n_events)\ndf.head(n=2)\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[13], line 3\n      1 xid_partition = Window.partitionBy('xid')\n      2 n_events = func.count(col('action')).over(xid_partition)\n----&gt; 3 df = df.withColumn('n_events', n_events)\n      4 df.head(n=2)\n\nNameError: name 'df' is not defined\n\n\n\n\n\nCode\ndf.groupBy('xid').agg(func.count('action')).head(5)\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[14], line 1\n----&gt; 1 df.groupBy('xid').agg(func.count('action')).head(5)\n\nNameError: name 'df' is not defined"
  },
  {
    "objectID": "core/notebooks/notebook11_dive.html#construct-a-column-containing-the-number-of-days-since-the-last-action-of-the-user",
    "href": "core/notebooks/notebook11_dive.html#construct-a-column-containing-the-number-of-days-since-the-last-action-of-the-user",
    "title": "Diving deeer",
    "section": "Construct a column containing the number of days since the last action of the user",
    "text": "Construct a column containing the number of days since the last action of the user\n\n\nCode\nxid_partition = Window.partitionBy('xid')\nmax_date = func.max(col('date')).over(xid_partition)\nn_days_since_last_event = func.datediff(func.current_date(), max_date)\ndf = df.withColumn('n_days_since_last_event',\n                   n_days_since_last_event)\ndf.head(n=2)\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[15], line 4\n      2 max_date = func.max(col('date')).over(xid_partition)\n      3 n_days_since_last_event = func.datediff(func.current_date(), max_date)\n----&gt; 4 df = df.withColumn('n_days_since_last_event',\n      5                    n_days_since_last_event)\n      6 df.head(n=2)\n\nNameError: name 'df' is not defined"
  },
  {
    "objectID": "core/notebooks/notebook11_dive.html#construct-a-column-containing-the-number-of-actions-of-each-user-for-each-modality-of-device",
    "href": "core/notebooks/notebook11_dive.html#construct-a-column-containing-the-number-of-actions-of-each-user-for-each-modality-of-device",
    "title": "Diving deeer",
    "section": "Construct a column containing the number of actions of each user for each modality of device",
    "text": "Construct a column containing the number of actions of each user for each modality of device\n\n\nCode\nxid_device_partition = Window.partitionBy('xid', 'device')\nn_events_per_device = func.count(col('action')).over(xid_device_partition)\ndf = df.withColumn('n_events_per_device', n_events_per_device)\ndf.head(n=2)\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[16], line 3\n      1 xid_device_partition = Window.partitionBy('xid', 'device')\n      2 n_events_per_device = func.count(col('action')).over(xid_device_partition)\n----&gt; 3 df = df.withColumn('n_events_per_device', n_events_per_device)\n      4 df.head(n=2)\n\nNameError: name 'df' is not defined"
  },
  {
    "objectID": "core/notebooks/notebook11_dive.html#number-of-device-per-user-some-mental-gymnastics",
    "href": "core/notebooks/notebook11_dive.html#number-of-device-per-user-some-mental-gymnastics",
    "title": "Diving deeer",
    "section": "Number of device per user: some mental gymnastics",
    "text": "Number of device per user: some mental gymnastics\n\n\nCode\nxid_partition = Window.partitionBy('xid')\nrank_device = func.dense_rank().over(xid_partition.orderBy('device'))\nn_unique_device = func.last(rank_device).over(xid_partition)\ndf = df.withColumn('n_device', n_unique_device)\ndf.head(n=2)\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[17], line 4\n      2 rank_device = func.dense_rank().over(xid_partition.orderBy('device'))\n      3 n_unique_device = func.last(rank_device).over(xid_partition)\n----&gt; 4 df = df.withColumn('n_device', n_unique_device)\n      5 df.head(n=2)\n\nNameError: name 'df' is not defined\n\n\n\n\n\nCode\ndf\\\n    .where(col('n_device') &gt; 1)\\\n    .select('xid', 'device', 'n_events',  'n_device', 'n_events_per_device')\\\n    .head(n=8)\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[18], line 1\n----&gt; 1 df\\\n      2     .where(col('n_device') &gt; 1)\\\n      3     .select('xid', 'device', 'n_events',  'n_device', 'n_events_per_device')\\\n      4     .head(n=8)\n\nNameError: name 'df' is not defined"
  },
  {
    "objectID": "core/notebooks/notebook11_dive.html#extraction",
    "href": "core/notebooks/notebook11_dive.html#extraction",
    "title": "Diving deeer",
    "section": "Extraction",
    "text": "Extraction\nExtraction is easy here, it’s just about reading the data\n\n\nCode\ndf = spark.read.parquet(input_file)\ndf.head(n=3)\n\n\n\n---------------------------------------------------------------------------\nAnalysisException                         Traceback (most recent call last)\nCell In[19], line 1\n----&gt; 1 df = spark.read.parquet(input_file)\n      2 df.head(n=3)\n\nFile ~/Documents/IFEBY310/.venv/lib/python3.12/site-packages/pyspark/sql/readwriter.py:544, in DataFrameReader.parquet(self, *paths, **options)\n    533 int96RebaseMode = options.get(\"int96RebaseMode\", None)\n    534 self._set_opts(\n    535     mergeSchema=mergeSchema,\n    536     pathGlobFilter=pathGlobFilter,\n   (...)\n    541     int96RebaseMode=int96RebaseMode,\n    542 )\n--&gt; 544 return self._df(self._jreader.parquet(_to_seq(self._spark._sc, paths)))\n\nFile ~/Documents/IFEBY310/.venv/lib/python3.12/site-packages/py4j/java_gateway.py:1322, in JavaMember.__call__(self, *args)\n   1316 command = proto.CALL_COMMAND_NAME +\\\n   1317     self.command_header +\\\n   1318     args_command +\\\n   1319     proto.END_COMMAND_PART\n   1321 answer = self.gateway_client.send_command(command)\n-&gt; 1322 return_value = get_return_value(\n   1323     answer, self.gateway_client, self.target_id, self.name)\n   1325 for temp_arg in temp_args:\n   1326     if hasattr(temp_arg, \"_detach\"):\n\nFile ~/Documents/IFEBY310/.venv/lib/python3.12/site-packages/pyspark/errors/exceptions/captured.py:185, in capture_sql_exception.&lt;locals&gt;.deco(*a, **kw)\n    181 converted = convert_exception(e.java_exception)\n    182 if not isinstance(converted, UnknownException):\n    183     # Hide where the exception came from that shows a non-Pythonic\n    184     # JVM exception message.\n--&gt; 185     raise converted from None\n    186 else:\n    187     raise\n\nAnalysisException: [PATH_NOT_FOUND] Path does not exist: file:/home/boucheron/Documents/IFEBY310/core/notebooks/webdata.parquet."
  },
  {
    "objectID": "core/notebooks/notebook11_dive.html#transformation-of-the-data",
    "href": "core/notebooks/notebook11_dive.html#transformation-of-the-data",
    "title": "Diving deeer",
    "section": "Transformation of the data",
    "text": "Transformation of the data\nAt this step we compute a lot of extra things from the data. The aim is to build features that describe users.\n\n\nCode\ndef n_events_transformer(df):\n    xid_partition = Window.partitionBy('xid')\n    n_events = func.count(col('action')).over(xid_partition)\n    df = df.withColumn('n_events', n_events)\n    return df\n\ndef n_events_per_action_transformer(df):\n    xid_action_partition = Window.partitionBy('xid', 'action')\n    n_events_per_action = func.count(col('action')).over(xid_action_partition)\n    df = df.withColumn('n_events_per_action', n_events_per_action)\n    return df\n\ndef hour_transformer(df):\n    hour = func.hour(col('date'))\n    df = df.withColumn('hour', hour)\n    return df\n\ndef weekday_transformer(df):\n    weekday = func.date_format(col('date'), 'EEEE')\n    df = df.withColumn('weekday', weekday)\n    return df\n\ndef n_events_per_hour_transformer(df):\n    xid_hour_partition = Window.partitionBy('xid', 'hour')\n    n_events_per_hour = func.count(col('action')).over(xid_hour_partition)\n    df = df.withColumn('n_events_per_hour', n_events_per_hour)\n    return df\n\ndef n_events_per_weekday_transformer(df):\n    xid_weekday_partition = Window.partitionBy('xid', 'weekday')\n    n_events_per_weekday = func.count(col('action')).over(xid_weekday_partition)\n    df = df.withColumn('n_events_per_weekday', n_events_per_weekday)\n    return df\n\ndef n_days_since_last_event_transformer(df):\n    xid_partition = Window.partitionBy('xid')\n    max_date = func.max(col('date')).over(xid_partition)\n    n_days_since_last_event = func.datediff(func.current_date(), max_date)\n    df = df.withColumn('n_days_since_last_event',\n                       n_days_since_last_event + lit(0.1))\n    return df\n\ndef n_days_since_last_action_transformer(df):\n    xid_partition_action = Window.partitionBy('xid', 'action')\n    max_date = func.max(col('date')).over(xid_partition_action)\n    n_days_since_last_action = func.datediff(func.current_date(),\n                                                        max_date)\n    df = df.withColumn('n_days_since_last_action',\n                       n_days_since_last_action + lit(0.1))\n    return df\n\ndef n_unique_day_transformer(df):\n    xid_partition = Window.partitionBy('xid')\n    dayofyear = func.dayofyear(col('date'))\n    rank_day = func.dense_rank().over(xid_partition.orderBy(dayofyear))\n    n_unique_day = func.last(rank_day).over(xid_partition)\n    df = df.withColumn('n_unique_day', n_unique_day)\n    return df\n\ndef n_unique_hour_transformer(df):\n    xid_partition = Window.partitionBy('xid')\n    rank_hour = func.dense_rank().over(xid_partition.orderBy('hour'))\n    n_unique_hour = func.last(rank_hour).over(xid_partition)\n    df = df.withColumn('n_unique_hour', n_unique_hour)\n    return df\n\ndef n_events_per_device_transformer(df):\n    xid_device_partition = Window.partitionBy('xid', 'device')\n    n_events_per_device = func.count(func.col('device')) \\\n        .over(xid_device_partition)\n    df = df.withColumn('n_events_per_device', n_events_per_device)\n    return df\n\ndef n_unique_device_transformer(df):\n    xid_partition = Window.partitionBy('xid')\n    rank_device = func.dense_rank().over(xid_partition.orderBy('device'))\n    n_unique_device = func.last(rank_device).over(xid_partition)\n    df = df.withColumn('n_device', n_unique_device)\n    return df\n\ndef n_actions_per_category_id_transformer(df):\n    xid_category_id_partition = Window.partitionBy('xid', 'category_id',\n                                                   'action')\n    n_actions_per_category_id = func.count(func.col('action')) \\\n        .over(xid_category_id_partition)\n    df = df.withColumn('n_actions_per_category_id', n_actions_per_category_id)\n    return df\n\ndef n_unique_category_id_transformer(df):\n    xid_partition = Window.partitionBy('xid')\n    rank_category_id = func.dense_rank().over(xid_partition\\\n                                              .orderBy('category_id'))\n    n_unique_category_id = func.last(rank_category_id).over(xid_partition)\n    df = df.withColumn('n_unique_category_id', n_unique_category_id)\n    return df\n\ndef n_events_per_category_id_transformer(df):\n    xid_category_id_partition = Window.partitionBy('xid', 'category_id')\n    n_events_per_category_id = func.count(func.col('action')) \\\n        .over(xid_category_id_partition)\n    df = df.withColumn('n_events_per_category_id', n_events_per_category_id)\n    return df\n\ndef n_events_per_website_id_transformer(df):\n    xid_website_id_partition = Window.partitionBy('xid', 'website_id')\n    n_events_per_website_id = func.count(col('action'))\\\n        .over(xid_website_id_partition)\n    df = df.withColumn('n_events_per_website_id', n_events_per_website_id)\n    return df\n\n\n\n\nCode\ntransformers = [\n    hour_transformer,\n    weekday_transformer,\n    n_events_per_hour_transformer,\n    n_events_per_weekday_transformer,\n    n_days_since_last_event_transformer,\n    n_days_since_last_action_transformer,\n    n_unique_day_transformer,\n    n_unique_hour_transformer,\n    n_events_per_device_transformer,\n    n_unique_device_transformer,\n    n_actions_per_category_id_transformer,\n    n_events_per_category_id_transformer,\n    n_events_per_website_id_transformer,\n]\n\nfor transformer in transformers:\n    df = transformer(df)\n\ndf.head(n=1)\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[21], line 18\n      1 transformers = [\n      2     hour_transformer,\n      3     weekday_transformer,\n   (...)\n     14     n_events_per_website_id_transformer,\n     15 ]\n     17 for transformer in transformers:\n---&gt; 18     df = transformer(df)\n     20 df.head(n=1)\n\nNameError: name 'df' is not defined\n\n\n\n\n\nCode\nsorted(df.columns)\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[22], line 1\n----&gt; 1 sorted(df.columns)\n\nNameError: name 'df' is not defined"
  },
  {
    "objectID": "core/notebooks/notebook11_dive.html#load-step",
    "href": "core/notebooks/notebook11_dive.html#load-step",
    "title": "Diving deeer",
    "section": "Load step",
    "text": "Load step\nHere, we use all the previous computations (saved in the columns of the dataframe) to compute aggregated informations about each user.\n\n\nCode\ndef n_events_per_hour_loader(df):\n    csr = df\\\n        .select('xid', 'hour', 'n_events_per_hour')\\\n        .withColumnRenamed('n_events_per_hour', 'value')\\\n        .distinct()\n    feature_name = func.concat(lit('n_events_per_hour#'), col('hour'))\n    csr = csr\\\n        .withColumn('feature_name', feature_name)\\\n        .drop('hour')\n    return csr\n\ndef n_events_per_website_id_loader(df):\n    csr = df.select('xid', 'website_id', 'n_events_per_website_id')\\\n        .withColumnRenamed('n_events_per_hour', 'value')\\\n        .distinct()\n    feature_name = func.concat(lit('n_events_per_website_id#'),\n                               col('website_id'))\n    csr = csr\\\n        .withColumn('feature_name', feature_name)\\\n        .drop('website_id')\n    return csr\n\ndef n_events_per_hour_loader(df):\n    csr = df\\\n        .select('xid', 'hour', 'n_events_per_hour')\\\n        .withColumnRenamed('n_events_per_hour', 'value')\\\n        .distinct()\n    feature_name = func.concat(lit('n_events_per_hour#'), col('hour'))\n    csr = csr\\\n        .withColumn('feature_name', feature_name)\\\n        .drop('hour')\n    return csr\n\ndef n_events_per_weekday_loader(df):\n    csr = df\\\n        .select('xid', 'weekday', 'n_events_per_weekday')\\\n        .withColumnRenamed('n_events_per_weekday', 'value')\\\n        .distinct()\n    feature_name = func.concat(lit('n_events_per_weekday#'), col('weekday'))\n    csr = csr\\\n        .withColumn('feature_name', feature_name)\\\n        .drop('weekday')\n    return csr\n\ndef n_days_since_last_event_loader(df):\n    csr = df.select('xid',  'n_days_since_last_event')\\\n        .withColumnRenamed('n_days_since_last_event#', 'value')\\\n        .distinct()\n    feature_name = lit('n_days_since_last_event')\n    csr = csr\\\n        .withColumn('feature_name', feature_name)\n    return csr\n\ndef n_days_since_last_action_loader(df):\n    csr = df.select('xid', 'action', 'n_days_since_last_action')\\\n        .withColumnRenamed('n_days_since_last_action', 'value')\\\n        .distinct()\n    feature_name = func.concat(lit('n_days_since_last_action#'), col('action'))\n    csr = csr\\\n        .withColumn('feature_name', feature_name)\\\n        .drop('action')\n    return csr\n\ndef n_unique_day_loader(df):\n    csr = df.select('xid', 'n_unique_day')\\\n        .withColumnRenamed('n_unique_day', 'value')\\\n        .distinct()\n    feature_name = lit('n_unique_day')\n    csr = csr\\\n        .withColumn('feature_name', feature_name)\n    return csr\n\ndef n_unique_hour_loader(df):\n    csr = df.select('xid', 'n_unique_hour')\\\n        .withColumnRenamed('n_unique_hour', 'value')\\\n        .distinct()\n    feature_name = lit('n_unique_hour')\n    csr = csr\\\n        .withColumn('feature_name', feature_name)\n    return csr\n\ndef n_events_per_device_loader(df):\n    csr = df\\\n        .select('xid', 'device', 'n_events_per_device')\\\n        .withColumnRenamed('n_events_per_device', 'value')\\\n        .distinct()\n    feature_name = func.concat(lit('n_events_per_device#'), col('device'))\n    csr = csr\\\n        .withColumn('feature_name', feature_name)\\\n        .drop('device')\n    return csr\n\ndef n_unique_device_loader(df):\n    csr = df.select('xid', 'n_device')\\\n        .withColumnRenamed('n_device', 'value')\\\n        .distinct()\n    feature_name = lit('n_device')\n    csr = csr\\\n        .withColumn('feature_name', feature_name)\n    return csr\n\ndef n_events_per_category_id_loader(df):\n    csr = df.select('xid', 'category_id', 'n_events_per_category_id')\\\n        .withColumnRenamed('n_events_per_category_id', 'value')\\\n        .distinct()\n    feature_name = func.concat(lit('n_events_per_category_id#'),\n                               col('category_id'))\n    csr = csr\\\n        .withColumn('feature_name', feature_name)\\\n        .drop('category_id')\n    return csr\n\ndef n_actions_per_category_id_loader(df):\n    csr = df.select('xid', 'category_id', 'action', 'n_actions_per_category_id')\\\n        .withColumnRenamed('n_actions_per_category_id', 'value')\\\n        .distinct()\n    feature_name = func.concat(lit('n_actions_per_category_id#'),\n                               col('action'), lit('#'), \n                               col('category_id'))\n    csr = csr\\\n        .withColumn('feature_name', feature_name)\\\n        .drop('category_id')\\\n        .drop('action')\n    return csr\n\ndef n_events_per_website_id_loader(df):\n    csr = df.select('xid', 'website_id', 'n_events_per_website_id')\\\n        .withColumnRenamed('n_events_per_website_id', 'value')\\\n        .distinct()\n    feature_name = func.concat(lit('n_events_per_website_id#'),\n                               col('website_id'))\n    csr = csr\\\n        .withColumn('feature_name', feature_name)\\\n        .drop('website_id')\n    return csr\n\n\n\n\nCode\nfrom functools import reduce\n\nloaders = [\n    n_events_per_hour_loader,\n    n_events_per_website_id_loader,\n    n_events_per_hour_loader,\n    n_events_per_weekday_loader,\n    n_days_since_last_event_loader,\n    n_days_since_last_action_loader,\n    n_unique_day_loader,\n    n_unique_hour_loader,\n    n_events_per_device_loader,\n    n_unique_device_loader,\n    n_events_per_category_id_loader,\n    n_actions_per_category_id_loader,\n    n_events_per_website_id_loader,\n]\n\ndef union(df, other):\n    return df.union(other)\n\ncsr = reduce(\n    lambda df1, df2: df1.union(df2),\n    [loader(df) for loader in loaders]\n)\n\ncsr.head(n=3)\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[24], line 24\n     19 def union(df, other):\n     20     return df.union(other)\n     22 csr = reduce(\n     23     lambda df1, df2: df1.union(df2),\n---&gt; 24     [loader(df) for loader in loaders]\n     25 )\n     27 csr.head(n=3)\n\nNameError: name 'df' is not defined\n\n\n\n\n\nCode\ncsr.columns\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[25], line 1\n----&gt; 1 csr.columns\n\nNameError: name 'csr' is not defined\n\n\n\n\n\nCode\ncsr.count()\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[26], line 1\n----&gt; 1 csr.count()\n\nNameError: name 'csr' is not defined\n\n\n\n\n\nCode\n# Replace features names and xid by a unique number\nfeature_name_partition = Window().orderBy('feature_name')\nxid_partition = Window().orderBy('xid')\n\ncol_idx = func.dense_rank().over(feature_name_partition)\nrow_idx = func.dense_rank().over(xid_partition)\n\ncsr = csr.withColumn('col', col_idx)\\\n    .withColumn('row', row_idx)\n\ncsr = csr.na.drop('any')\n\ncsr.head(n=5)\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[27], line 8\n      5 col_idx = func.dense_rank().over(feature_name_partition)\n      6 row_idx = func.dense_rank().over(xid_partition)\n----&gt; 8 csr = csr.withColumn('col', col_idx)\\\n      9     .withColumn('row', row_idx)\n     11 csr = csr.na.drop('any')\n     13 csr.head(n=5)\n\nNameError: name 'csr' is not defined\n\n\n\n\n\nCode\n# Let's save the result of our hard work into a new parquet file\noutput_path = './'\noutput_file = os.path.join(output_path, 'csr.parquet')\ncsr.write.parquet(output_file, mode='overwrite')\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[28], line 4\n      2 output_path = './'\n      3 output_file = os.path.join(output_path, 'csr.parquet')\n----&gt; 4 csr.write.parquet(output_file, mode='overwrite')\n\nNameError: name 'csr' is not defined"
  },
  {
    "objectID": "core/notebooks/notebook11_dive.html#finally",
    "href": "core/notebooks/notebook11_dive.html#finally",
    "title": "Diving deeer",
    "section": "Finally !!",
    "text": "Finally !!\nWow ! That was a lot of work. Now we have a features matrix \\(X\\) and a vector of labels \\(y\\).\n\n\nCode\nX.indices\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[51], line 1\n----&gt; 1 X.indices\n\nNameError: name 'X' is not defined\n\n\n\n\n\nCode\nX.indptr\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[52], line 1\n----&gt; 1 X.indptr\n\nNameError: name 'X' is not defined\n\n\n\n\n\nCode\nX.shape, X.nnz\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[53], line 1\n----&gt; 1 X.shape, X.nnz\n\nNameError: name 'X' is not defined\n\n\n\n\n\nCode\ny.shape, y.sum()\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[54], line 1\n----&gt; 1 y.shape, y.sum()\n\nNameError: name 'y' is not defined"
  },
  {
    "objectID": "core/notebooks/notebook15_polars.html",
    "href": "core/notebooks/notebook15_polars.html",
    "title": "Polars",
    "section": "",
    "text": "Code\n!pip install polars\n\n\nRequirement already satisfied: polars in /home/boucheron/Documents/IFEBY310/.venv/lib/python3.12/site-packages (1.20.0)\nCode\nimport pyarrow as pa\nimport numpy as np\nimport polars as pl\nCode\ndf = pl.read_csv('./tips.csv')\nhttps://github.com/mattharrison/datasets\nCode\ntips\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[4], line 1\n----&gt; 1 tips\n\nNameError: name 'tips' is not defined"
  },
  {
    "objectID": "core/notebooks/notebook15_polars.html#arrow",
    "href": "core/notebooks/notebook15_polars.html#arrow",
    "title": "Polars",
    "section": "Arrow",
    "text": "Arrow\nLibrary for In Memory management of tabular data.\nPyArrow types (not numpy types?)"
  },
  {
    "objectID": "core/notebooks/notebook15_polars.html#rust",
    "href": "core/notebooks/notebook15_polars.html#rust",
    "title": "Polars",
    "section": "Rust",
    "text": "Rust\nA language."
  },
  {
    "objectID": "core/notebooks/notebook15_polars.html#polars-api",
    "href": "core/notebooks/notebook15_polars.html#polars-api",
    "title": "Polars",
    "section": "Polars API",
    "text": "Polars API\ndtypes, columns,\n\n\nCode\nimport inspect\n\n\n\n\nCode\ndf.estimated_size()\n\n\n9777\n\n\n\n\nCode\ndf.describe()\n\n\n\nshape: (9, 8)\n\n\n\nstatistic\ntotal_bill\ntip\nsex\nsmoker\nday\ntime\nsize\n\n\nstr\nf64\nf64\nstr\nstr\nstr\nstr\nf64\n\n\n\n\n\"count\"\n244.0\n244.0\n\"244\"\n\"244\"\n\"244\"\n\"244\"\n244.0\n\n\n\"null_count\"\n0.0\n0.0\n\"0\"\n\"0\"\n\"0\"\n\"0\"\n0.0\n\n\n\"mean\"\n19.785943\n2.998279\nnull\nnull\nnull\nnull\n2.569672\n\n\n\"std\"\n8.902412\n1.383638\nnull\nnull\nnull\nnull\n0.9511\n\n\n\"min\"\n3.07\n1.0\n\"Female\"\n\"No\"\n\"Fri\"\n\"Dinner\"\n1.0\n\n\n\"25%\"\n13.37\n2.0\nnull\nnull\nnull\nnull\n2.0\n\n\n\"50%\"\n17.81\n2.92\nnull\nnull\nnull\nnull\n2.0\n\n\n\"75%\"\n24.08\n3.55\nnull\nnull\nnull\nnull\n3.0\n\n\n\"max\"\n50.81\n10.0\n\"Male\"\n\"Yes\"\n\"Thur\"\n\"Lunch\"\n6.0"
  },
  {
    "objectID": "core/notebooks/notebook15_polars.html#expressions",
    "href": "core/notebooks/notebook15_polars.html#expressions",
    "title": "Polars",
    "section": "Expressions",
    "text": "Expressions\n\n\nCode\ntype(pl.col('sex'))\n\n\npolars.expr.expr.Expr\n\n\nSelect numerical columns\n\n\nCode\n(\n  df\n    .select(pl.col(pl.Float64))\n)\n\n\n\nshape: (244, 2)\n\n\n\ntotal_bill\ntip\n\n\nf64\nf64\n\n\n\n\n16.99\n1.01\n\n\n10.34\n1.66\n\n\n21.01\n3.5\n\n\n23.68\n3.31\n\n\n24.59\n3.61\n\n\n…\n…\n\n\n29.03\n5.92\n\n\n27.18\n2.0\n\n\n22.67\n2.0\n\n\n17.82\n1.75\n\n\n18.78\n3.0\n\n\n\n\n\n\n\n\nCode\n(\n  df\n    .with_columns(\n      pl.lit('spam').alias('egg')\n      )\n)\n\n\n\nshape: (244, 8)\n\n\n\ntotal_bill\ntip\nsex\nsmoker\nday\ntime\nsize\negg\n\n\nf64\nf64\nstr\nstr\nstr\nstr\ni64\nstr\n\n\n\n\n16.99\n1.01\n\"Female\"\n\"No\"\n\"Sun\"\n\"Dinner\"\n2\n\"spam\"\n\n\n10.34\n1.66\n\"Male\"\n\"No\"\n\"Sun\"\n\"Dinner\"\n3\n\"spam\"\n\n\n21.01\n3.5\n\"Male\"\n\"No\"\n\"Sun\"\n\"Dinner\"\n3\n\"spam\"\n\n\n23.68\n3.31\n\"Male\"\n\"No\"\n\"Sun\"\n\"Dinner\"\n2\n\"spam\"\n\n\n24.59\n3.61\n\"Female\"\n\"No\"\n\"Sun\"\n\"Dinner\"\n4\n\"spam\"\n\n\n…\n…\n…\n…\n…\n…\n…\n…\n\n\n29.03\n5.92\n\"Male\"\n\"No\"\n\"Sat\"\n\"Dinner\"\n3\n\"spam\"\n\n\n27.18\n2.0\n\"Female\"\n\"Yes\"\n\"Sat\"\n\"Dinner\"\n2\n\"spam\"\n\n\n22.67\n2.0\n\"Male\"\n\"Yes\"\n\"Sat\"\n\"Dinner\"\n2\n\"spam\"\n\n\n17.82\n1.75\n\"Male\"\n\"No\"\n\"Sat\"\n\"Dinner\"\n2\n\"spam\"\n\n\n18.78\n3.0\n\"Female\"\n\"No\"\n\"Thur\"\n\"Dinner\"\n2\n\"spam\"\n\n\n\n\n\n\nIf possible stay in the rust.\nTidy selection using regular expressions."
  },
  {
    "objectID": "core/notebooks/notebook15_polars.html#in-memory-or-not",
    "href": "core/notebooks/notebook15_polars.html#in-memory-or-not",
    "title": "Polars",
    "section": "In memory or not",
    "text": "In memory or not"
  },
  {
    "objectID": "core/notebooks/notebook15_polars.html#streaming",
    "href": "core/notebooks/notebook15_polars.html#streaming",
    "title": "Polars",
    "section": "Streaming",
    "text": "Streaming"
  },
  {
    "objectID": "core/notebooks/notebook15_polars.html#comparisons",
    "href": "core/notebooks/notebook15_polars.html#comparisons",
    "title": "Polars",
    "section": "Comparisons",
    "text": "Comparisons\n\nPandas\nModen\nDask\nPolars"
  },
  {
    "objectID": "core/notebooks/xciti_pandas.html",
    "href": "core/notebooks/xciti_pandas.html",
    "title": "Imports",
    "section": "",
    "text": "Code\nimport glob\n\nimport os\nimport sys\nimport re \nimport shutils\nimport logging \n\nimport pandas as pd\nimport numpy as np\n\n\n\nimport datetime\n\n# from functools import reduce\nimport itertools\n\nimport zipfile\nfrom zipfile import ZipFile\nfrom tqdm import tqdm\n\nimport pyarrow as pa\nimport comet    as co\nimport pyarrow.parquet as pq\nimport pyarrow.dataset as ds\n\n\nimport dask\n\nos.environ['PYSPARK_PYTHON'] = sys.executable\nos.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable\n\n\n\n---------------------------------------------------------------------------\nModuleNotFoundError                       Traceback (most recent call last)\nCell In[1], line 6\n      4 import sys\n      5 import re \n----&gt; 6 import shutils\n      7 import logging \n      9 import pandas as pd\n\nModuleNotFoundError: No module named 'shutils'\nCode\nfrom dask.distributed import Client\n\nclient = Client(n_workers=20, threads_per_worker=2, memory_limit=\"2GB\")\nclient\n\n\n\n---------------------------------------------------------------------------\nModuleNotFoundError                       Traceback (most recent call last)\nFile ~/Documents/IFEBY310/.venv/lib/python3.12/site-packages/dask/distributed.py:13\n     12 try:\n---&gt; 13     from distributed import *\n     14 except ImportError as e:\n\nModuleNotFoundError: No module named 'distributed'\n\nThe above exception was the direct cause of the following exception:\n\nImportError                               Traceback (most recent call last)\nCell In[2], line 1\n----&gt; 1 from dask.distributed import Client\n      3 client = Client(n_workers=20, threads_per_worker=2, memory_limit=\"2GB\")\n      4 client\n\nFile ~/Documents/IFEBY310/.venv/lib/python3.12/site-packages/dask/distributed.py:16\n     14 except ImportError as e:\n     15     if e.msg == \"No module named 'distributed'\":\n---&gt; 16         raise ImportError(_import_error_message) from e\n     17     else:\n     18         raise\n\nImportError: dask.distributed is not installed.\n\nPlease either conda or pip install distributed:\n\n  conda install dask distributed             # either conda install\n  python -m pip install \"dask[distributed]\" --upgrade    # or pip install\nCode\nlogger = logging.getLogger(__name__)\nlogging.basicConfig(filename='example.log', encoding='utf-8', level=logging.DEBUG)\nlogger.debug('This message should go to the log file')\nlogger.info('So should this')\nlogger.warning('And this, too')\nlogger.error('And non-ASCII stuff, too, like Øresund and Malmö')\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[3], line 1\n----&gt; 1 logger = logging.getLogger(__name__)\n      2 logging.basicConfig(filename='example.log', encoding='utf-8', level=logging.DEBUG)\n      3 logger.debug('This message should go to the log file')\n\nNameError: name 'logging' is not defined"
  },
  {
    "objectID": "core/notebooks/xciti_pandas.html#paths",
    "href": "core/notebooks/xciti_pandas.html#paths",
    "title": "Imports",
    "section": "Paths",
    "text": "Paths\nDownloaded zip archives are in data_dir\nExtracted csv files are in extract_dir\nParquet files are in parquet_dir\n\n\nCode\ndata_dir = '../data'\nos.path.exists(data_dir)\n\nextract_dir = os.path.join(data_dir, 'xcitibike')\nif not os.path.exists(extract_dir):\n    os.mkdir(extract_dir)\n\nparquet_dir = os.path.join(data_dir, 'pq_citibike')\nif not os.path.exists(parquet_dir):\n    os.mkdir(parquet_dir)\n\n\n\n---------------------------------------------------------------------------\nFileNotFoundError                         Traceback (most recent call last)\nCell In[4], line 6\n      4 extract_dir = os.path.join(data_dir, 'xcitibike')\n      5 if not os.path.exists(extract_dir):\n----&gt; 6     os.mkdir(extract_dir)\n      8 parquet_dir = os.path.join(data_dir, 'pq_citibike')\n      9 if not os.path.exists(parquet_dir):\n\nFileNotFoundError: [Errno 2] No such file or directory: '../data/xcitibike'"
  },
  {
    "objectID": "core/notebooks/xciti_pandas.html#extracting-archives",
    "href": "core/notebooks/xciti_pandas.html#extracting-archives",
    "title": "Imports",
    "section": "Extracting archives",
    "text": "Extracting archives\nZip archive files contain directory trees where the csv files are to be found.\n\n\nCode\ncitibike_archives_paths = sorted(glob.glob(data_dir + '/*-citibike-tripdata.zip'))\n\n\nTODO: - parallelize part of the extraction process - one thread per element in citibike_archives_paths - should be doable with dask\n\n\nCode\nfor ar_path in tqdm(citibike_archives_paths):\n    myzip = ZipFile(ar_path)\n    to_extract = [elt  for elt in myzip.namelist() if (elt.endswith(\".csv\") and not (elt.startswith('__MACOSX')))]\n    myzip.extractall(path=extract_dir,\n                     members=to_extract)\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[6], line 1\n----&gt; 1 for ar_path in tqdm(citibike_archives_paths):\n      2     myzip = ZipFile(ar_path)\n      3     to_extract = [elt  for elt in myzip.namelist() if (elt.endswith(\".csv\") and not (elt.startswith('__MACOSX')))]\n\nNameError: name 'tqdm' is not defined"
  },
  {
    "objectID": "core/notebooks/xciti_pandas.html#collecting-headers",
    "href": "core/notebooks/xciti_pandas.html#collecting-headers",
    "title": "Imports",
    "section": "Collecting headers",
    "text": "Collecting headers\nThe extracted csv files do not share the same schema and the same datetime encoding format.\nWalking through the csv files in extract_dir, allows to gather the three different column naming patterns.\nTODO: - Save the schemata with inferred types in a json file. Different typing patterns may correspond to the same column naming pattern - For each csv file spot the column naming pattern, the datetime encoding format\n\n\nCode\n# schemata_names = set()\n\n# for (root, dirs ,files) in os.walk(extract_dir, topdown=True):\n#    if dirs:\n#        continue\n#    for fn in files:\n#        if fn.endswith('.csv'):\n#            with open(os.path.join(root, fn), 'r') as fd:\n#                schemata_names.add(fd.readline())\n\n# schemata_names = [s.replace('\\n', '').split(',') for s in schemata_names]\n\n\n\n\nCode\nschemata_names = [\n    ['ride_id',\n  'rideable_type',\n  'started_at',\n  'ended_at',\n  'start_station_name',\n  'start_station_id',\n  'end_station_name',\n  'end_station_id',\n  'start_lat',\n  'start_lng',\n  'end_lat',\n  'end_lng',\n  'member_casual'],\n ['tripduration',\n  'starttime',\n  'stoptime',\n  'start station id',\n  'start station name',\n  'start station latitude',\n  'start station longitude',\n  'end station id',\n  'end station name',\n  'end station latitude',\n  'end station longitude',\n  'bikeid',\n  'usertype',\n  'birth year',\n  'gender'],\n ['Trip Duration',\n  'Start Time',\n  'Stop Time',\n  'Start Station ID',\n  'Start Station Name',\n  'Start Station Latitude',\n  'Start Station Longitude',\n  'End Station ID',\n  'End Station Name',\n  'End Station Latitude',\n  'End Station Longitude',\n  'Bike ID',\n  'User Type',\n  'Birth Year',\n  'Gender']\n]\n\n\nFor each csv file, find the column naming pattern, build a dictionary with this information.\nTODO: - should done during the first walk.\n\n\nCode\nschemata_numbers = {}\n\nfor (root, dirs ,files) in os.walk(extract_dir, topdown=True):\n    if dirs:\n        continue\n    for fn in files:\n        if fn.endswith('.csv'):        \n            with open(os.path.join(root, fn), 'r') as fd:\n                col_names = fd.readline().replace('\\n', '').split(',')\n                schemata_numbers[fn] = schemata_names.index(col_names)"
  },
  {
    "objectID": "core/notebooks/xciti_pandas.html#building-renaming-dictionaries",
    "href": "core/notebooks/xciti_pandas.html#building-renaming-dictionaries",
    "title": "Imports",
    "section": "Building renaming dictionaries",
    "text": "Building renaming dictionaries\n\nFrom 0\nNothing to do\n\n\nFrom 1\nUse\n{\n 'tripduration': 'trip_duration',\n 'starttime': 'started_at',\n 'stoptime': 'ended_at',\n 'bikeid': 'bike_id',\n 'usertype': 'user_type',\n 'start station latitude': 'start_lat',\n 'start station longitude': 'start_lng',\n 'end station latitude': 'end_lat',\n 'end station longitude': 'end_lng'\n}\nand replace  with ’_’.\n\n\nFrom 2\n{\n 'Trip Duration': 'trip_duration',\n  'Start Time': 'started_at',\n  'Stop Time': 'ended_at',\n  'Start Station Latitude': 'start_lat',\n  'Start Station Longitude': 'start_lng',\n  'End Station Latitude': 'end_lat',\n  'End Station Longitude': 'end_lng'    \n}\n\nand replace  with ’_’, use lower().\n\n\nCode\ndicts_rename = {\n    1: {\n 'tripduration': 'trip_duration',\n 'starttime': 'started_at',\n 'stoptime': 'ended_at',\n 'bikeid': 'bike_id',\n 'usertype': 'user_type',\n 'start station latitude': 'start_lat',\n 'start station longitude': 'start_lng',\n 'end station latitude': 'end_lat',\n 'end station longitude': 'end_lng'\n} ,\n  2:  {\n 'Trip Duration': 'trip_duration',\n  'Start Time': 'started_at',\n  'Stop Time': 'ended_at',\n  'Start Station Latitude': 'start_lat',\n  'Start Station Longitude': 'start_lng',\n  'End Station Latitude': 'end_lat',\n  'End Station Longitude': 'end_lng'    \n}\n}\n\n\nAnother problem.\nstart_station_id, end_station_id is not consistently formatted."
  },
  {
    "objectID": "core/notebooks/xciti_pandas.html#building-a-parquet-replica",
    "href": "core/notebooks/xciti_pandas.html#building-a-parquet-replica",
    "title": "Imports",
    "section": "Building a parquet replica",
    "text": "Building a parquet replica\nTODO: - explain why engine='pyarrow' is useful when using pd.read_csv() - clean up the renaming schemes\nDatetime hand-made parsing for non ISO compliant csv file\n\n\nCode\ndef my_parse(s):\n    \"\"\"datetime parsing for non-ISO enco\n\n    Args:\n        s (str): a datetime encoding string '%m/%d/%Y H:M[:S]'\n\n    Returns:\n        datetime: a datetime object without timezone\n    \"\"\"\n    rem = re.compile(r\"(\\d+)/(\\d+)/(\\d+) (\\d+)?(:\\d+)?(:\\d+)?\")\n\n    matches = rem.search(s).groups()\n    month, day, year, hours, mins, secs = [int(x.replace(':','')) if x else 0 for x in matches]\n\n    zdt = datetime.datetime(year, month, day, hours, mins, secs)\n    return zdt\n\n\nTODO: - parallelize this\n\n\nCode\ndef csv2pq(root, dirs, files):\n    if dirs:\n        return\n\n    for fn in files:\n        if not fn.endswith('.csv'):  \n            continue\n\n        df = pd.read_csv(\n                    os.path.join(root, fn),\n                    engine = 'pyarrow'\n            )\n                    \n        if 1==schemata_numbers[fn]: \n            df = ( \n                df\n                    .rename(columns=dicts_rename[1])\n                    .rename(mapper= lambda s : s.replace(' ', '_'), axis='columns')\n            )                \n        elif 2==schemata_numbers[fn]:\n            df = ( \n                df\n                    .rename(columns=dicts_rename[2])\n                    .rename(mapper= lambda s : s.lower().replace(' ', '_'), axis='columns')\n            )\n\n        if (str(df.dtypes.loc['ended_at'])=='object'):\n            # Format is either '%m/%d/%Y %H:%M:%S'or '%m/%d/%Y %H:%M'\n            try:\n                df['ended_at'] = pd.to_datetime(df.ended_at, format='%m/%d/%Y %H:%M:%S')\n                df['started_at'] = pd.to_datetime(df.started_at, format='%m/%d/%Y %H:%M:%S')\n            except ValueError:\n                df['ended_at'] = pd.to_datetime(df.ended_at, format='%m/%d/%Y %H:%M')\n                df['started_at'] = pd.to_datetime(df.started_at, format='%m/%d/%Y %H:%M')\n            except:\n                df['ended_at'] = df.ended_at.map(my_parse)\n                df['started_at'] = df.started_at.map(my_parse)\n\n\n        # if df.start_station_id.dtype != np.dtype('O'):\n\n        df['start_station_id'] = df.start_station_id.astype(np.dtype('O'))  \n        df['end_station_id'] = df.end_station_id.astype(np.dtype('O')) \n            \n        df['start_year'] = df.started_at.dt.year\n        df['start_month'] = df.started_at.dt.month  \n        \n        table = pa.Table.from_pandas(df)\n\n        logger.info('writing: ' + fn)\n\n        pq.write_to_dataset(\n                table,\n                parquet_dir,\n                partition_cols=[\"start_year\", \"start_month\"],\n        )    \n\n    return root \n\n\n\n\nCode\ntodo = dask.delayed([dask.delayed(csv2pq)(root, dirs ,files)\n    for (root, dirs ,files) in os.walk(extract_dir, topdown=True)\n ])\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[13], line 1\n----&gt; 1 todo = dask.delayed([dask.delayed(csv2pq)(root, dirs ,files)\n      2     for (root, dirs ,files) in os.walk(extract_dir, topdown=True)\n      3  ])\n\nNameError: name 'dask' is not defined\n\n\n\n\n\nCode\nfoo = todo.compute()\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[14], line 1\n----&gt; 1 foo = todo.compute()\n\nNameError: name 'todo' is not defined\n\n\n\n\n\nCode\n[x  for x in foo if x]\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[15], line 1\n----&gt; 1 [x  for x in foo if x]\n\nNameError: name 'foo' is not defined\n\n\n\n\n\nCode\nlist(schemata_numbers.keys())\n\n\n[]\n\n\n\n\nCode\nfor (root, dirs ,files) in tqdm(os.walk(extract_dir, topdown=True)):\n    if dirs:\n        continue\n    for fn in files:\n        if not fn.endswith('.csv'):  \n            continue\n\n        df = pd.read_csv(\n                os.path.join(root, fn),\n                engine = 'pyarrow'\n        )\n                \n        if 1==schemata_numbers[fn]: \n            df = ( \n                df\n                  .rename(columns=dicts_rename[1])\n                  .rename(mapper= lambda s : s.replace(' ', '_'), axis='columns')\n            )                \n        elif 2==schemata_numbers[fn]:\n            df = ( \n                df\n                  .rename(columns=dicts_rename[2])\n                  .rename(mapper= lambda s : s.lower().replace(' ', '_'), axis='columns')\n            )\n        \n        if (str(df.dtypes.loc['ended_at'])=='object'):\n            # Format is either '%m/%d/%Y %H:%M:%S'or '%m/%d/%Y %H:%M'\n            try:\n                df['ended_at'] = pd.to_datetime(df.ended_at, format='%m/%d/%Y %H:%M:%S')\n                df['started_at'] = pd.to_datetime(df.started_at, format='%m/%d/%Y %H:%M:%S')\n            except ValueError:\n                df['ended_at'] = pd.to_datetime(df.ended_at, format='%m/%d/%Y %H:%M')\n                df['started_at'] = pd.to_datetime(df.started_at, format='%m/%d/%Y %H:%M')\n            except:\n                df['ended_at'] = df.ended_at.map(my_parse)\n                df['started_at'] = df.started_at.map(my_parse) \n\n        \n\n        if df.start_station_id.dtype != np.dtype('O'):\n            df['start_station_id'] = df.start_station_id.astype(np.dtype('O'))  \n            df['end_station_id'] = df.end_station_id.astype(np.dtype('O')) \n         \n        df['start_year'] = df.started_at.dt.year\n        df['start_month'] = df.started_at.dt.month  \n      \n        table = pa.Table.from_pandas(df)\n\n        pa.schema(table)\n\n        pq.write_to_dataset(\n             table,\n             parquet_dir,\n             partition_cols=[\"start_year\", \"start_month\"],\n        )\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[17], line 1\n----&gt; 1 for (root, dirs ,files) in tqdm(os.walk(extract_dir, topdown=True)):\n      2     if dirs:\n      3         continue\n\nNameError: name 'tqdm' is not defined\n\n\n\n\n\nCode\ndf.start_station_id.astype(np.dtype('O'))\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[18], line 1\n----&gt; 1 df.start_station_id.astype(np.dtype('O'))\n\nNameError: name 'df' is not defined"
  },
  {
    "objectID": "core/notebooks/xciti_pandas.html#todos",
    "href": "core/notebooks/xciti_pandas.html#todos",
    "title": "Imports",
    "section": "TODOs",
    "text": "TODOs\n\nHandling schema evolution\nSchema changed between 2021 January and 2021 February\n\n\n\n\n\n\n\n\nOld Column\nNew Column\nAction\n\n\n\n\n\nride_id\nPrimary key ?\n\n\n\nride_type\ndocked_bike\n\n\ntripduration\n\nIn seconds, can be recovered from started_at/ended_at\n\n\nstarttime\nstarted_at\nNo need for microseconds before 2021 January\n\n\nstoptime\nended_at\nNo need for microseconds before 2021 January\n\n\nstart station id\nstart_station_id\nOrder mismatch, code mismatch. Before : int. After:\n\n\nstart station name\nstart_station_name\nCheck consistency\n\n\nstart station latitude\nstart_lat\nCheck consistency\n\n\nstart station longitude\nstart_lng\nCheck consistency\n\n\n\n\n\n\n\n\n\n\n\nOld Column\nNew Column\nAction\n\n\n\n\nend station id\nend_station_id\nCheck consistency\n\n\nend station name\nend_station_name\nCheck consistency\n\n\nend station latitude\nend_lat\nCheck consistency\n\n\nend station longitude\nend_lng\nCheck consistency\n\n\nbikeid\n\n\n\n\nusertype\n\nSubscriber/Customer\n\n\nbirth year\n\n\n\n\ngender\n\n0, 1\n\n\n\nmember_casual\ncasual/member\n\n\n\n\nreading side: just read start_time, end_time, start_station_id, end_station_id,\nstart_at and end_at must be translated to start_time, end_time\ntrip_duration, user_type, bike_id, member_casual\nPrepare for a dimension table for stations\n\nid\nname\nlat\nlon\nmore\n\n\n\n\nSelect from the colum names\nCan we read directly as a pyarrow table ? Yes, but Pandas is convenient for datetime manipulations, and possibly for renaming\n\n\nUsage pyarrow.unify_schemas\n\n\nParsing dates\nFor some files, timestamps are not in ISO format.\nFrom 2014-09-01 till 2016-09-.., started_at and ended_at do not abide ISO format, but %m/%d/%Y %H:%M:%S.\nTry to use pd.to_datetime(). If failure, use regular expression to parse the putative date column. Handle the optional field that way.\nBetter ask forgiveness than permission.\n\n\nCode\nroot, dirs, fn = next(os.walk(os.path.join(parquet_dir, 'start_year=2013', 'start_month=9')))\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[19], line 1\n----&gt; 1 root, dirs, fn = next(os.walk(os.path.join(parquet_dir, 'start_year=2013', 'start_month=9')))\n\nNameError: name 'parquet_dir' is not defined\n\n\n\n\n\nCode\nspam = pq.read_metadata(os.path.join(root, fn[0]))\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[20], line 1\n----&gt; 1 spam = pq.read_metadata(os.path.join(root, fn[0]))\n\nNameError: name 'pq' is not defined\n\n\n\n\n\nCode\nfrom  dask import dataframe as dd\n\n\n/home/boucheron/Documents/IFEBY310/.venv/lib/python3.12/site-packages/dask/dataframe/__init__.py:49: FutureWarning:\n\n\nDask dataframe query planning is disabled because dask-expr is not installed.\n\nYou can install it with `pip install dask[dataframe]` or `conda install dask`.\nThis will raise in a future version.\n\n\n\n\n\n\nCode\nspam = dd.read_parquet(os.path.join(parquet_dir, 'start_year=2023' ))\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[22], line 1\n----&gt; 1 spam = dd.read_parquet(os.path.join(parquet_dir, 'start_year=2023' ))\n\nNameError: name 'parquet_dir' is not defined\n\n\n\n\n\nCode\nspam.dtypes\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[23], line 1\n----&gt; 1 spam.dtypes\n\nNameError: name 'spam' is not defined\n\n\n\n\n\nCode\nfoo_path = os.path.join(parquet_dir, 'start_year=2023', 'start_month=1')\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[24], line 1\n----&gt; 1 foo_path = os.path.join(parquet_dir, 'start_year=2023', 'start_month=1')\n\nNameError: name 'parquet_dir' is not defined\n\n\n\n\n\nCode\nroot, dirs, fn = next(os.walk(foo_path))\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[25], line 1\n----&gt; 1 root, dirs, fn = next(os.walk(foo_path))\n\nNameError: name 'foo_path' is not defined\n\n\n\n\n\nCode\nparquet_file = pq.ParquetFile(os.path.join(root, fn[0]))\nschema = parquet_file.schema\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[26], line 1\n----&gt; 1 parquet_file = pq.ParquetFile(os.path.join(root, fn[0]))\n      2 schema = parquet_file.schema\n\nNameError: name 'pq' is not defined\n\n\n\n\n\nCode\nschema\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[27], line 1\n----&gt; 1 schema\n\nNameError: name 'schema' is not defined"
  },
  {
    "objectID": "cours-equipe.html",
    "href": "cours-equipe.html",
    "title": "Équipe enseignante",
    "section": "",
    "text": "S. Boucheron Professeur de Mathématiques à l’Université Paris Cité/Laboratoire de Probabilités, Statistiques et Modélisatiion LPSM."
  },
  {
    "objectID": "cours-equipe.html#cours",
    "href": "cours-equipe.html#cours",
    "title": "Équipe enseignante",
    "section": "",
    "text": "S. Boucheron Professeur de Mathématiques à l’Université Paris Cité/Laboratoire de Probabilités, Statistiques et Modélisatiion LPSM."
  },
  {
    "objectID": "cours-equipe.html#travaux-dirigés",
    "href": "cours-equipe.html#travaux-dirigés",
    "title": "Équipe enseignante",
    "section": " Travaux dirigés",
    "text": "Travaux dirigés\n\n\n\nNom\nHoraire\nSalle\n\n\n\n\nStéphane Boucheron\nVendredi 15h45 - 18h15\n2004/5 Sophie Germain\n\n\nCristina Sirangelo\nVendredi 15h45 - 18h15\n2006 Sophie Germain\n\n\nAmine Souiri\nJeudi 13h30 - 16h00\n2006 Sophie Germain\n\n\nSylvain Schmitz\n\n\n\n\nAmélie Gheerbrant\n\n\n\n\nAnatole Dahan"
  },
  {
    "objectID": "course-team.html",
    "href": "course-team.html",
    "title": "Course team",
    "section": "",
    "text": "Teachers 2024-25\n\nC. Sirangelo Professeur d’Informatique à l’Université Paris Cité/Institut de Recherche en Informatique Fondamentale.\nS. Boucheron Professeur de Mathématiques à l’Université Paris Cité/Laboratoire de Probabilités, Statistiques et Modélisatiion LPSM.\n\n\n\n\n\n\n Former contributors\n\nStéphane Gaiffas\nSothéa Has\nAmélie Gheerbrant\nVlady Ravelomanana",
    "crumbs": [
      "Information",
      "Team"
    ]
  },
  {
    "objectID": "notebooks-listings.html",
    "href": "notebooks-listings.html",
    "title": "Notebooks",
    "section": "",
    "text": "Slides provide the framework for the course. You can use them to familiarise yourself with the material for the next lesson, and/or to review previous lessons.\nIcon  points to chalkboard arguments.\n\n\n\n\n\n\n\n\n\nMode d’emploi\n\n\n\nSlides use revealjs de . They are best viewed using a browser.\nKeyboard shortcut  takes you to the Help page.\n\n\n\n\n\n\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Description\n        \n         \n          Jupyter notebook\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\nDate\n\n\nTitle\n\n\nDescription\n\n\nJupyter notebook\n\n\n\n\n\n\nJan 17, 2025\n\n\nPython Stack for Data Science\n\n\nPython\n\n\n\n\n\n\n\nJan 17, 2025\n\n\nNumpy and Scipy\n\n\nPython, Numpy, Scipy, Plotly\n\n\n   \n\n\n\n\nJan 24, 2025\n\n\nTable wranglig with Pandas\n\n\nPython, Pandas\n\n\n   \n\n\n\n\nJan 31, 2025\n\n\nTable wranglig with Dask\n\n\nPython, Dask\n\n\n \n\n\n\n\nFeb 7, 2025\n\n\nSpark Resilient Distributed Datasets\n\n\nPySpark, Spark, RDD\n\n\n   \n\n\n\n\nFeb 20, 2025\n\n\nSpark SQL\n\n\nPySpark, Spark, SQL\n\n\n   \n\n\n\n\nFeb 21, 2025\n\n\nPandas on Spark and SparklyR\n\n\nPySpark, Spark, Pandas, R\n\n\n \n\n\n\n\nMar 7, 2025\n\n\nJSON format\n\n\n \n\n\n   \n\n\n\n\nMar 14, 2025\n\n\nWeb data\n\n\n \n\n\n \n\n\n\n\nMar 21, 2025\n\n\nSpark again\n\n\n \n\n\n \n\n\n\n\n\nNo matching items",
    "crumbs": [
      "Notebooks"
    ]
  },
  {
    "objectID": "weeks-listings.html",
    "href": "weeks-listings.html",
    "title": "Journal",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Tags\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\nDate\n\n\nTitle\n\n\nTags\n\n\n\n\n\n\nJan 17, 2025\n\n\nWeek 1\n\n\nBig data, Python data stack\n\n\n\n\nJan 24, 2025\n\n\nWeek 2\n\n\nPandas\n\n\n\n\nJan 31, 2025\n\n\nWeek 3\n\n\nDask\n\n\n\n\nFeb 7, 2025\n\n\nweek 4\n\n\nSpark RDD\n\n\n\n\nFeb 21, 2025\n\n\nWeek 6\n\n\nSpark SQL\n\n\n\n\nMar 7, 2025\n\n\nWeek 7\n\n\nJson, serialization\n\n\n\n\nMar 14, 2025\n\n\nWeek 8\n\n\nFile formats, Parquet, Avro, ORC\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "Journal"
    ]
  },
  {
    "objectID": "weeks/week-2.html",
    "href": "weeks/week-2.html",
    "title": "Week 2",
    "section": "",
    "text": "Important\n\n\n\n\nRoom: Sophie Germain 0014\nFriday 24 January 2025 15h45-17h45\n Calendar"
  },
  {
    "objectID": "weeks/week-2.html#lecture-slides",
    "href": "weeks/week-2.html#lecture-slides",
    "title": "Week 2",
    "section": "Lecture : slides",
    "text": "Lecture : slides\nWe shall briefly come back to several parts of\n\n Python Data Science Stack\n\nData processing\nVisualization"
  },
  {
    "objectID": "weeks/week-2.html#notebooks",
    "href": "weeks/week-2.html#notebooks",
    "title": "Week 2",
    "section": "Notebooks",
    "text": "Notebooks\nYou shall have gone through (on your own)\n\n Jupyter notebook I : tour of Python\n html: tour of Python\n\nYou shall have gone through\n\n Jupyter notebook II : tour of numpy\n html: tour of numpy\n\nup to Section Sparse Matrices (not included)\nWe shall spend most of the lecture on\n\n Jupyter notebook III : tour of pandas\n html: tour of Pandas"
  },
  {
    "objectID": "weeks/week-2.html#references",
    "href": "weeks/week-2.html#references",
    "title": "Week 2",
    "section": "References",
    "text": "References\nYou can watch the Introduction to NumPy conference presented at EuroSciPy 2023\n\nThe Pandas book by Wes McKinney\nPandas exercises on Kaggle\n\n\nContenders to Pandas are gaining attention: Polars"
  },
  {
    "objectID": "weeks/week-2.html#logistics",
    "href": "weeks/week-2.html#logistics",
    "title": "Week 2",
    "section": "Logistics",
    "text": "Logistics\n To work the jupyter notebooks, install python 3, and modules related to jupyter: jupyter-cache, jupyter_client, jupyter_core, jupyterlab_widgets (this induces the installation of dependencies).\nhttps://jupyter.org\nDownload the jupyter notebooks from notebooks listings.\n If you do not already have an ENT account, follow instructions on Moodle to get one. You shall need this account to connect to PostGres cluster.\n\n\nBack to Agenda ⏎"
  },
  {
    "objectID": "weeks/week-4.html",
    "href": "weeks/week-4.html",
    "title": "Weeks 4-5",
    "section": "",
    "text": "Important\n\n\n\n\n 1 session during Week 4/0 session during Week 5\nFriday 4 February 2025 Sophie Germain 0014 15h45-17h45\n Calendar"
  },
  {
    "objectID": "weeks/week-4.html#lecture-slides",
    "href": "weeks/week-4.html#lecture-slides",
    "title": "Weeks 4-5",
    "section": "Lecture : slides",
    "text": "Lecture : slides\n\n Spark low level APIs: RDD  We may come back to several parts of\n Python Data Science Stack\n Dask"
  },
  {
    "objectID": "weeks/week-4.html#notebooks",
    "href": "weeks/week-4.html#notebooks",
    "title": "Weeks 4-5",
    "section": "Notebooks",
    "text": "Notebooks\nWe shall spend most of the lectures on\n\n Jupyter notebook V : Spark RDD\n html: Spark RDD\n\n\nand possibly compare with:\nDask Tutorial\nYou shall have gone through (on your own)\n\n Jupyter notebook I : tour of Python\n html: tour of Python\n\n Jupyter notebook II : tour of numpy\n html: tour of numpy\n\n Jupyter notebook III: tour of pandas\n html: tour of Pandas"
  },
  {
    "objectID": "weeks/week-4.html#references",
    "href": "weeks/week-4.html#references",
    "title": "Weeks 4-5",
    "section": "References",
    "text": "References\n\n\n\nSpark Definitive Guide\nSpark\nSpark\nData pipelines\nNext Generation Databases NoSQLand Big Data, Guy Harrison\nMastering Spark on R\n\n\n\nParquet\n‘Arrow’\nDask Docs\nDask Examples\nDask Code\nDask Blog\nPandas cheatsheet"
  },
  {
    "objectID": "weeks/week-4.html#logistics",
    "href": "weeks/week-4.html#logistics",
    "title": "Weeks 4-5",
    "section": "Logistics",
    "text": "Logistics\n pyspark\n To work the jupyter notebooks, install python 3, and modules related to jupyter: jupyter-cache, jupyter_client, jupyter_core, jupyterlab_widgets (this induces the installation of dependencies).\nhttps://jupyter.org\nDownload the jupyter notebooks from notebooks listings.\n If you do not already have an ENT account, follow instructions on Moodle to get one. You shall need this account to connect to PostGres cluster.\n\n\nBack to Agenda ⏎"
  },
  {
    "objectID": "weeks/week-6.html",
    "href": "weeks/week-6.html",
    "title": "Weeks 6",
    "section": "",
    "text": "Important\n\n\n\n2 Sessions\n\nThursday 20 February 2025 Olympes de Gouges 358 10h45-12h45\nFriday 21 February 2025 Sophie Germain 0014 15h45-17h45\n Calendar"
  },
  {
    "objectID": "weeks/week-6.html#lecture-slides",
    "href": "weeks/week-6.html#lecture-slides",
    "title": "Weeks 6",
    "section": "Lecture : slides",
    "text": "Lecture : slides\n\n Spark high level APIs: SQL\n\nWe may come back to several parts of\n\n Spark low level APIs: RDD\n Python Data Science Stack\n Dask"
  },
  {
    "objectID": "weeks/week-6.html#notebooks",
    "href": "weeks/week-6.html#notebooks",
    "title": "Weeks 6",
    "section": "Notebooks",
    "text": "Notebooks\nWe shall spend most of the lectures on\n\n Jupyter notebook VI : Spark SQL\n html: Spark SQL\n\n Jupyter notebook VII : JSON\n html: JSON\n\nand possibly compare with:\nDask Tutorial\nYou shall have gone through (on your own)\n\n Jupyter notebook V : Spark RDD\n html: Spark RDD\n\n Jupyter notebook I : tour of Python\n html: tour of Python\n\n Jupyter notebook II : tour of numpy\n html: tour of numpy\n\n Jupyter notebook III: tour of pandas\n html: tour of Pandas"
  },
  {
    "objectID": "weeks/week-6.html#references",
    "href": "weeks/week-6.html#references",
    "title": "Weeks 6",
    "section": "References",
    "text": "References\n\n\n\nSpark Definitive Guide\nSpark\nSpark\nData pipelines\nNext Generation Databases NoSQLand Big Data, Guy Harrison\nMastering Spark on R\n\n\n\nParquet\n‘Arrow’\nDask Docs\nDask Examples\nDask Code\nDask Blog\nPandas cheatsheet"
  },
  {
    "objectID": "weeks/week-6.html#logistics",
    "href": "weeks/week-6.html#logistics",
    "title": "Weeks 6",
    "section": "Logistics",
    "text": "Logistics\n pyspark\n To work the jupyter notebooks, install python 3, and modules related to jupyter: jupyter-cache, jupyter_client, jupyter_core, jupyterlab_widgets (this induces the installation of dependencies).\nhttps://jupyter.org\nDownload the jupyter notebooks from notebooks listings.\n If you do not already have an ENT account, follow instructions on Moodle to get one. You shall need this account to connect to PostGres cluster.\n\n\nBack to Agenda ⏎"
  },
  {
    "objectID": "weeks/week-8.html",
    "href": "weeks/week-8.html",
    "title": "Week 8",
    "section": "",
    "text": "Important\n\n\n\n1 Session\n\nFriday 14 March 2025 Sophie Germain 1005 13h30-15h30\n Calendar"
  },
  {
    "objectID": "weeks/week-8.html#lecture-slides",
    "href": "weeks/week-8.html#lecture-slides",
    "title": "Week 8",
    "section": "Lecture : slides",
    "text": "Lecture : slides\n\n File formats\n\nWe may come back to several parts of\n\n Json format\n Spark high level APIs: SQL\n Spark low level APIs: RDD\n Python Data Science Stack\n Dask"
  },
  {
    "objectID": "weeks/week-8.html#notebooks",
    "href": "weeks/week-8.html#notebooks",
    "title": "Week 8",
    "section": "Notebooks",
    "text": "Notebooks\nWe shall spend most of the lectures on\n\n Jupyter notebook VII : JSON\n html: JSON\n\nYou shall have gone through (on your own)\n\n Jupyter notebook VII : JSON\n html: JSON\n\n Jupyter notebook VI : Spark SQL\n Jupyter notebook V : Spark RDD\n html: Spark RDD\n\n Jupyter notebook I : tour of Python\n html: tour of Python\n\n Jupyter notebook II : tour of numpy\n html: tour of numpy\n\n Jupyter notebook III: tour of pandas\n html: tour of Pandas"
  },
  {
    "objectID": "weeks/week-8.html#references",
    "href": "weeks/week-8.html#references",
    "title": "Week 8",
    "section": "References",
    "text": "References\n\n\n\nSpark Definitive Guide\nSpark project\nSpark documentation\nData pipelines\nNext Generation Databases NoSQLand Big Data, Guy Harrison\nMastering Spark on R\n\n\n\nParquet\n‘Arrow’\nDask Docs\nDask Examples\nDask Code\nDask Blog\nPandas cheatsheet"
  },
  {
    "objectID": "weeks/week-8.html#logistics",
    "href": "weeks/week-8.html#logistics",
    "title": "Week 8",
    "section": "Logistics",
    "text": "Logistics\n pyspark\n To work the jupyter notebooks, install python 3, and modules related to jupyter: jupyter-cache, jupyter_client, jupyter_core, jupyterlab_widgets (this induces the installation of dependencies).\nhttps://jupyter.org\nDownload the jupyter notebooks from notebooks listings.\n If you do not already have an ENT account, follow instructions on Moodle to get one. You shall need this account to connect to PostGres cluster.\n\n\nBack to Agenda ⏎"
  },
  {
    "objectID": "core/slides/slides04_sparkl.html#pyspark-overview",
    "href": "core/slides/slides04_sparkl.html#pyspark-overview",
    "title": "Spark SQL",
    "section": "PySpark overview",
    "text": "PySpark overview\n\nOfficial documentation"
  },
  {
    "objectID": "core/slides/slides04_sparkl.html#overview",
    "href": "core/slides/slides04_sparkl.html#overview",
    "title": "Spark SQL",
    "section": "Overview",
    "text": "Overview\n\nSpark SQL is a library included in Spark since version 1.3\nSpark Dataframes was introduced with version\nIt provides an easier interface to process tabular data\nInstead of RDDs, we deal with DataFrames\nSince Spark 1.6, there is also the concept of Datasets, but only for Scala and Java"
  },
  {
    "objectID": "core/slides/slides04_sparkl.html#sparkcontext-and-sparksession",
    "href": "core/slides/slides04_sparkl.html#sparkcontext-and-sparksession",
    "title": "Spark SQL",
    "section": "SparkContext and SparkSession",
    "text": "SparkContext and SparkSession\n\nBefore Spark 2, there was only SparkContext and SQLContext\nAll core functionality was accessed with SparkContext\nAll SQL functionality needed the SQLContext, which can be created from an SparkContext\nWith Spark 2 came the SparkSession class\nSparkSession is the .stress[global entry-point] for everything Spark-related\n\n\n\nSparkContext was enough for handling RDDs\nPurpose of SQLContext ?\nCould we use SparkSession to handle RDDs?"
  },
  {
    "objectID": "core/slides/slides04_sparkl.html#sparkcontext-and-sparksession-1",
    "href": "core/slides/slides04_sparkl.html#sparkcontext-and-sparksession-1",
    "title": "Spark SQL",
    "section": "SparkContext and SparkSession",
    "text": "SparkContext and SparkSession\nBefore Spark 2\n&gt;&gt;&gt; from pyspark import SparkConf, SparkContext\n&gt;&gt;&gt; from pyspark.sql import SQLContext\n\n&gt;&gt;&gt; conf = SparkConf().setAppName(appName).setMaster(master)\n&gt;&gt;&gt; sc = SparkContext(conf = conf)\n&gt;&gt;&gt; sql_context = new SQLContext(sc)\n\nSince Spark 2\n\nfrom pyspark.sql import SparkSession\n\napp_name = \"Spark Dataframes\"\n\nspark = (\n  SparkSession \n    .builder \n    .appName(app_name) \n#        .master(master) \n#        .config(\"spark.some.config.option\", \"some-value\") \\\n    .getOrCreate()\n)"
  },
  {
    "objectID": "core/slides/slides04_sparkl.html#dataframe-1",
    "href": "core/slides/slides04_sparkl.html#dataframe-1",
    "title": "Spark SQL",
    "section": "DataFrame",
    "text": "DataFrame\n\nThe main entity of Spark SQL is the DataFrame\nA DataFrame is actually an RDD of Rows with a schema\nA schema gives the names of the columns and their types\nRow is a class representing a row of the DataFrame.\nIt can be used almost as a python list, with its size equal to the number of columns in the schema.\n\n\nRow-oriented or column-oriented?"
  },
  {
    "objectID": "core/slides/slides04_sparkl.html#dataframe-2",
    "href": "core/slides/slides04_sparkl.html#dataframe-2",
    "title": "Spark SQL",
    "section": "DataFrame",
    "text": "DataFrame\n\nfrom pyspark.sql import Row\n\nrow1 = Row(name=\"John\", age=21)\nrow2 = Row(name=\"James\", age=32)\nrow3 = Row(name=\"Jane\", age=18)\nrow1['name']\n\n\n\n'John'\n\n\n\ndf = spark.createDataFrame([row1, row2, row3])\ndf\n\n\n\nDataFrame[name: string, age: bigint]\n\n\n\ndf.show()\n\n\n\n+-----+---+\n| name|age|\n+-----+---+\n| John| 21|\n|James| 32|\n| Jane| 18|\n+-----+---+\n\n\n\n\nRelate Row to named tuple or dictionary\nWhat does .show() ?"
  },
  {
    "objectID": "core/slides/slides04_sparkl.html#dataframe-3",
    "href": "core/slides/slides04_sparkl.html#dataframe-3",
    "title": "Spark SQL",
    "section": "DataFrame",
    "text": "DataFrame\n\ndf.printSchema()\n\n\n\nroot\n |-- name: string (nullable = true)\n |-- age: long (nullable = true)\n\n\n\nYou can access the underlying RDD object using .rdd\n\nprint(df.rdd.toDebugString().decode(\"utf-8\"))\n\n\n\n(20) MapPartitionsRDD[10] at javaToPython at NativeMethodAccessorImpl.java:0 []\n |   MapPartitionsRDD[9] at javaToPython at NativeMethodAccessorImpl.java:0 []\n |   SQLExecutionRDD[8] at javaToPython at NativeMethodAccessorImpl.java:0 []\n |   MapPartitionsRDD[7] at javaToPython at NativeMethodAccessorImpl.java:0 []\n |   MapPartitionsRDD[4] at applySchemaToPythonRDD at NativeMethodAccessorImpl.java:0 []\n |   MapPartitionsRDD[3] at map at SerDeUtil.scala:69 []\n |   MapPartitionsRDD[2] at mapPartitions at SerDeUtil.scala:117 []\n |   PythonRDD[1] at RDD at PythonRDD.scala:53 []\n |   ParallelCollectionRDD[0] at readRDDFromFile at PythonRDD.scala:289 []\n\n\n\ndf.rdd.getNumPartitions()\n\n\n\n20"
  },
  {
    "objectID": "core/slides/slides04_sparkl.html#creating-dataframes",
    "href": "core/slides/slides04_sparkl.html#creating-dataframes",
    "title": "Spark SQL",
    "section": "Creating DataFrames",
    "text": "Creating DataFrames\n\nWe can use the method createDataFrame from the SparkSession instance\nCan be used to create a Spark DataFrame from:\n\na pandas.DataFrame object\na local python list\nan RDD\n\nFull documentation can be found in the [API docs]"
  },
  {
    "objectID": "core/slides/slides04_sparkl.html#creating-dataframes-1",
    "href": "core/slides/slides04_sparkl.html#creating-dataframes-1",
    "title": "Spark SQL",
    "section": "Creating DataFrames",
    "text": "Creating DataFrames\n\nrows = [\n        Row(name=\"John\", age=21, gender=\"male\"),\n        Row(name=\"Jane\", age=25, gender=\"female\"),\n        Row(name=\"Albert\", age=46, gender=\"male\")\n    ]\ndf = spark.createDataFrame(rows)\ndf.show()\n\n\n\n+------+---+------+\n|  name|age|gender|\n+------+---+------+\n|  John| 21|  male|\n|  Jane| 25|female|\n|Albert| 46|  male|\n+------+---+------+"
  },
  {
    "objectID": "core/slides/slides04_sparkl.html#creating-dataframes-2",
    "href": "core/slides/slides04_sparkl.html#creating-dataframes-2",
    "title": "Spark SQL",
    "section": "Creating DataFrames",
    "text": "Creating DataFrames\n\ncolumn_names = [\"name\", \"age\", \"gender\"]\nrows = [\n        [\"John\", 21, \"male\"],\n        [\"James\", 25, \"female\"],\n        [\"Albert\", 46, \"male\"]\n    ]\ndf = spark.createDataFrame(rows, column_names)\ndf.show()\n\n\n\n+------+---+------+\n|  name|age|gender|\n+------+---+------+\n|  John| 21|  male|\n| James| 25|female|\n|Albert| 46|  male|\n+------+---+------+"
  },
  {
    "objectID": "core/slides/slides04_sparkl.html#creating-dataframes-3",
    "href": "core/slides/slides04_sparkl.html#creating-dataframes-3",
    "title": "Spark SQL",
    "section": "Creating DataFrames",
    "text": "Creating DataFrames\n\ncolumn_names = [\"name\", \"age\", \"gender\"]\n\nsc = spark._sc\n\nrdd = sc.parallelize([\n        (\"John\", 21, \"male\"),\n        (\"James\", 25, \"female\"),\n        (\"Albert\", 46, \"male\")\n    ])\n\ndf = spark.createDataFrame(rdd, column_names)\ndf.show()\n\n\n\n+------+---+------+\n|  name|age|gender|\n+------+---+------+\n|  John| 21|  male|\n| James| 25|female|\n|Albert| 46|  male|\n+------+---+------+"
  },
  {
    "objectID": "core/slides/slides04_sparkl.html#schema-and-types",
    "href": "core/slides/slides04_sparkl.html#schema-and-types",
    "title": "Spark SQL",
    "section": "Schema and Types",
    "text": "Schema and Types\n\nA DataFrame always contains a schema\nThe schema defines the column names and types\nIn all previous examples, the schema was inferred\nThe schema of a DataFrame is represented by the class types.StructType [API doc]\nWhen creating a DataFrame, the schema can be either inferred or defined by the user\n\n\nfrom pyspark.sql.types import *\n\ndf.schema\n# StructType(List(StructField(name,StringType,true),\n#                 StructField(age,IntegerType,true),\n#                 StructField(gender,StringType,true)))\n\n\n\nStructType([StructField('name', StringType(), True), StructField('age', LongType(), True), StructField('gender', StringType(), True)])\n\n\n\ncheck absence of quotation\nSpark has its own collection (tree) of types. The Python counterparts are defined in pyspark.sql.types"
  },
  {
    "objectID": "core/slides/slides04_sparkl.html#creating-a-custom-schema",
    "href": "core/slides/slides04_sparkl.html#creating-a-custom-schema",
    "title": "Spark SQL",
    "section": "Creating a custom Schema",
    "text": "Creating a custom Schema\n\nfrom pyspark.sql.types import *\n\nschema = StructType([\n    StructField(\"name\", StringType(), True),\n    StructField(\"age\", IntegerType(), True),\n    StructField(\"gender\", StringType(), True)\n])\n\nrows = [(\"John\", 21, \"male\")]\ndf = spark.createDataFrame(rows, schema)\ndf.printSchema()\ndf.show()\n\n\n\nroot\n |-- name: string (nullable = true)\n |-- age: integer (nullable = true)\n |-- gender: string (nullable = true)\n\n+----+---+------+\n|name|age|gender|\n+----+---+------+\n|John| 21|  male|\n+----+---+------+"
  },
  {
    "objectID": "core/slides/slides04_sparkl.html#types-supported-by-spark-sql",
    "href": "core/slides/slides04_sparkl.html#types-supported-by-spark-sql",
    "title": "Spark SQL",
    "section": "Types supported by Spark SQL",
    "text": "Types supported by Spark SQL\n\nStringType\nIntegerType\nLongType\nFloatType\nDoubleType\nBooleanType\nDateType\nTimestampType\n...\n\nThe full list of types can be found in [API doc]"
  },
  {
    "objectID": "core/slides/slides04_sparkl.html#reading-data-from-sources",
    "href": "core/slides/slides04_sparkl.html#reading-data-from-sources",
    "title": "Spark SQL",
    "section": "Reading data from sources",
    "text": "Reading data from sources\n\nData is usually read from external sources (move the algorithms, not the data)\nSpark SQL provides connectors to read from many different sources:\n\nText files (CSV, JSON)\nDistributed tabular files (Parquet, ORC)\nIn-memory data sources (Apache Arrow)\nGeneral relational Databases (via JDBC)\nThird-party connectors to connect to many other databases\nAnd you can create your own connector for Spark (in Scala)"
  },
  {
    "objectID": "core/slides/slides04_sparkl.html#reading-data-from-sources-1",
    "href": "core/slides/slides04_sparkl.html#reading-data-from-sources-1",
    "title": "Spark SQL",
    "section": "Reading data from sources",
    "text": "Reading data from sources\n\nIn all cases, the syntax is similar:\n\nspark.read.{source}(path)\n\nSpark supports different file systems to look at the data:\n\nLocal files: file://path/to/file or just path/to/file\nHDFS (Hadoop Distributed FileSystem): hdfs://path/to/file\nAmazon S3: s3://path/to/file"
  },
  {
    "objectID": "core/slides/slides04_sparkl.html#reading-from-a-csv-file",
    "href": "core/slides/slides04_sparkl.html#reading-from-a-csv-file",
    "title": "Spark SQL",
    "section": "Reading from a CSV file",
    "text": "Reading from a CSV file\n\npath_to_csv = \"../../../../Downloads/tips.csv\"\ndf = spark.read.csv(path_to_csv)\n\n\n\n\n\ndf = (\n  spark.read\n    .format('csv')\n    .option('header', 'true')\n    .option('sep', \",\")\n    .load(path_to_csv)\n)\n\n\n\n\n\nmy_csv_options = {\n  'header': True,\n  'sep': ';',\n}\n\ndf = (\n  spark\n    .read\n    .csv(path_to_csv, **my_csv_options)\n)"
  },
  {
    "objectID": "core/slides/slides04_sparkl.html#reading-from-a-csv-file-1",
    "href": "core/slides/slides04_sparkl.html#reading-from-a-csv-file-1",
    "title": "Spark SQL",
    "section": "Reading from a CSV file",
    "text": "Reading from a CSV file\nMain options\nSome important options of the CSV reader are listed here:\n\n\n\n\n\n\n\nOption\nDescription\n\n\n\n\nsep\nThe separator character\n\n\nheader\nIf “true”, the first line contains the column names\n\n\ninferSchema\nIf “true”, the column types will be guessed from the contents\n\n\ndateFormat\nA string representing the format of the date columns\n\n\n\nThe full list of options can be found in the API Docs"
  },
  {
    "objectID": "core/slides/slides04_sparkl.html#reading-from-other-file-types",
    "href": "core/slides/slides04_sparkl.html#reading-from-other-file-types",
    "title": "Spark SQL",
    "section": "Reading from other file types",
    "text": "Reading from other file types\n## JSON file\ndf = spark.read.json(\"/path/to/file.json\")\ndf = spark.read.format(\"json\").load(\"/path/to/file.json\")\n## Parquet file (distributed tabular data)\ndf = spark.read.parquet(\"hdfs://path/to/file.parquet\")\ndf = spark.read.format(\"parquet\").load(\"hdfs://path/to/file.parquet\")\n## ORC file (distributed tabular data)\ndf = spark.read.orc(\"hdfs://path/to/file.orc\")\ndf = spark.read.format(\"orc\").load(\"hdfs://path/to/file.orc\")"
  },
  {
    "objectID": "core/slides/slides04_sparkl.html#reading-from-external-databases",
    "href": "core/slides/slides04_sparkl.html#reading-from-external-databases",
    "title": "Spark SQL",
    "section": "Reading from external databases",
    "text": "Reading from external databases\n\nWe can use JDBC drivers (Java) to read from relational databases\nExamples of databases: Oracle, PostgreSQL, MySQL, etc.\nThe java driver file must be uploaded to the cluster before trying to access\nThis operation can be very heavy. When available, specific connectors should be used\nSpecific connectors are often provided by third-party libraries"
  },
  {
    "objectID": "core/slides/slides04_sparkl.html#reading-from-external-databases-1",
    "href": "core/slides/slides04_sparkl.html#reading-from-external-databases-1",
    "title": "Spark SQL",
    "section": "Reading from external databases",
    "text": "Reading from external databases\n\nspark = (\n  SparkSession \n    .builder \n    .appName(\"Python Spark SQL basic example\") \n    .config(\"spark.jars\", \n            spark_home + \"/jars/\" + \"postgresql-42.7.2.jar\") \n    .getOrCreate()\n)\n\n\n\n\n\ndf = (\n  spark\n    .read.format(\"jdbc\") \n    .option(\"url\", \"jdbc:postgresql:dbserver\") \n    .option(\"dbtable\", \"schema.tablename\") \n    .option(\"user\", usrnm) \n    .option(\"password\", pwd) \n    .load()\n)\n\n\n\n\nor\ndf = spark.read.jdbc(\n      url=\"jdbc:postgresql:dbserver\",\n      table=\"schema.tablename\"\n      properties={\n          \"user\": \"username\",\n          \"password\": \"p4ssw0rd\"\n      }\n)\n\ndf_airlines = (\n  spark\n    .read\n    .format(\"jdbc\")\n    .options(**(dico_jdbc_pg | {'dbtable': 'nycflights.airlines'}))\n    .load()\n)"
  },
  {
    "objectID": "core/slides/slides04_sparkl.html#spark-sql-as-a-substitute-for-hiveql",
    "href": "core/slides/slides04_sparkl.html#spark-sql-as-a-substitute-for-hiveql",
    "title": "Spark SQL",
    "section": "Spark SQL as a Substitute for HiveQL",
    "text": "Spark SQL as a Substitute for HiveQL\n\n Hive (Hadoop InteractiVE)\n\nDevlopped by  dring 2000’s\nReleased 2010 as Apache project\n\n\nHiveQL: SQL-like interface to query data stored in various databases and file systems that integrate with Hadoop.\nHive on wikipedia\n\n\nApache Hive is a data warehouse software project, built on top of Apache Hadoop for providing data query and analysis.[3][4] Hive gives an SQL-like interface to query data stored in various databases and file systems that integrate with Hadoop. Traditional SQL queries must be implemented in the MapReduce Java API to execute SQL applications and queries over distributed data. Hive provides the necessary SQL abstraction to integrate SQL-like queries (HiveQL) into the underlying Java without the need to implement queries in the low-level Java API. Since most data warehousing applications work with SQL-based querying languages, Hive aids the portability of SQL-based applications to Hadoop.[5] While initially developed by Facebook, Apache Hive is used and developed by other companies such as Netflix and the Financial Industry Regulatory Authority (FINRA).[6][7] Amazon maintains a software fork of Apache Hive included in Amazon Elastic MapReduce on Amazon Web Services.[8]"
  },
  {
    "objectID": "core/slides/slides04_sparkl.html#performing-queries",
    "href": "core/slides/slides04_sparkl.html#performing-queries",
    "title": "Spark SQL",
    "section": "Performing queries",
    "text": "Performing queries\n\nSpark SQL is designed to be compatible with ANSI SQL queries\nSpark SQL allows SQL-like queries to be evaluated on Spark DataFrames (and on many other tables)\nSpark DataFrames have to be tagged as temporary views\nSpark SQL Queries can be submitted using spark.sql()\n\n Method sql for class SparkSession provides access to SQLContext"
  },
  {
    "objectID": "core/slides/slides04_sparkl.html#performing-queries-1",
    "href": "core/slides/slides04_sparkl.html#performing-queries-1",
    "title": "Spark SQL",
    "section": "Performing queries",
    "text": "Performing queries\n\ncolumn_names = [\"name\", \"age\", \"gender\"]\nrows = [\n        [\"John\", 21, \"male\"],\n        [\"Jane\", 25, \"female\"]\n    ]\ndf = spark.createDataFrame(rows, column_names)\n\ndf.show()\n\n\n\n+----+---+------+\n|name|age|gender|\n+----+---+------+\n|John| 21|  male|\n|Jane| 25|female|\n+----+---+------+\n\n\n\n\n\n## Create a temporary view from the DataFrame\ndf.createOrReplaceTempView(\"new_view\")\n\n## Define the query\nquery = \"\"\"\n  SELECT name, age \n  FROM new_view \n  WHERE gender='male'\n\"\"\"\n\nmen_df = spark.sql(query)\nmen_df.show()\n\n\n\n+----+---+\n|name|age|\n+----+---+\n|John| 21|\n+----+---+"
  },
  {
    "objectID": "core/slides/slides04_sparkl.html#using-the-api",
    "href": "core/slides/slides04_sparkl.html#using-the-api",
    "title": "Spark SQL",
    "section": "Using the API",
    "text": "Using the API\nSQL queries form an expresive feature, it’s not the best way to code a complex logic\n\nErrors are harder to find in strings\nQueries makes the code less modular\n\n\nThe Spark dataframe API offers a developper-friendly API for implementing\n\nRelational algebra \\(\\sigma, \\pi, \\bowtie, \\cup, \\cap, \\setminus\\)\nPartitionning GROUP BY\nAggregation and Window functions\n\n\n\nCompare the Spark Dataframe API with:\n dplyr, dtplyr, dbplyr in R Tidyverse\n Pandas\nChaining and/or piping enable modular query construction"
  },
  {
    "objectID": "core/slides/slides04_sparkl.html#basic-single-tables-operations-methodsverbs",
    "href": "core/slides/slides04_sparkl.html#basic-single-tables-operations-methodsverbs",
    "title": "Spark SQL",
    "section": "Basic Single Tables Operations (methods/verbs)",
    "text": "Basic Single Tables Operations (methods/verbs)\n\n\n\n\n\n\n\nOperation\nDescription\n\n\n\n\nselect\nChooses columns from the table \\(\\pi\\)\n\n\nselectExpr\nChooses columns and expressions from table \\(\\pi\\)\n\n\nwhere\nFilters rows based on a boolean rule \\(\\sigma\\)\n\n\nlimit\nLimits the number of rows LIMIT ...\n\n\norderBy\nSorts the DataFrame based on one or more columns ORDER BY ...\n\n\nalias\nChanges the name of a column AS ...\n\n\ncast\nChanges the type of a column\n\n\nwithColumn\nAdds a new column"
  },
  {
    "objectID": "core/slides/slides04_sparkl.html#select",
    "href": "core/slides/slides04_sparkl.html#select",
    "title": "Spark SQL",
    "section": "SELECT",
    "text": "SELECT\n\n## SQL query:\nquery = \"\"\"\n  SELECT name, age \n  FROM table\n\"\"\"\n\n## Using Spark SQL API:\n( \n  df.select(\"name\", \"age\")\n    .show()\n)\n\n\n\n+----+---+\n|name|age|\n+----+---+\n|John| 21|\n|Jane| 25|\n+----+---+"
  },
  {
    "objectID": "core/slides/slides04_sparkl.html#select-continued",
    "href": "core/slides/slides04_sparkl.html#select-continued",
    "title": "Spark SQL",
    "section": "SELECT (continued)",
    "text": "SELECT (continued)\nThe argument of select() is *cols where cols can be built from column names (strings), column expressions like df.age + 10, lists\n\ndf.select( df.name.alias(\"nom\"), df.age+10 ).show()\n\n\n\n+----+----------+\n| nom|(age + 10)|\n+----+----------+\n|John|        31|\n|Jane|        35|\n+----+----------+\n\n\n\n\ndf.select([c for c in df.columns if \"a\" in c]).show()\n\n\n\n+----+---+\n|name|age|\n+----+---+\n|John| 21|\n|Jane| 25|\n+----+---+"
  },
  {
    "objectID": "core/slides/slides04_sparkl.html#selectexpr",
    "href": "core/slides/slides04_sparkl.html#selectexpr",
    "title": "Spark SQL",
    "section": "selectExpr",
    "text": "selectExpr\n###  A variant of select() that accepts SQL expressions.\n&gt;&gt;&gt; df.selectExpr(\"age * 2\", \"abs(age)\").collect()"
  },
  {
    "objectID": "core/slides/slides04_sparkl.html#where",
    "href": "core/slides/slides04_sparkl.html#where",
    "title": "Spark SQL",
    "section": "WHERE",
    "text": "WHERE\n\n## In a SQL query:\nquery = \"\"\"\n  SELECT * \n  FROM table \n  WHERE age &gt; 21\n\"\"\"\n\n## Using Spark SQL API:\ndf.where(\"age &gt; 21\").show()\n\n## Alternatively:\n# df.where(df['age'] &gt; 21).show()\n# df.where(df.age &gt; 21).show()\n# df.select(\"*\").where(\"age &gt; 21\").show()\n\n\n\n+----+---+------+\n|name|age|gender|\n+----+---+------+\n|Jane| 25|female|\n+----+---+------+"
  },
  {
    "objectID": "core/slides/slides04_sparkl.html#limit",
    "href": "core/slides/slides04_sparkl.html#limit",
    "title": "Spark SQL",
    "section": "LIMIT",
    "text": "LIMIT\n\n## In a SQL query:\nquery = \"\"\"\n  SELECT * \n  FROM table \n  LIMIT 1\n\"\"\"\n\n## Using Spark SQL API:\n( \n  df.limit(1)\n    .show()\n)\n\n## Or even\ndf.select(\"*\").limit(1).show()\n\n\n\n+----+---+------+\n|name|age|gender|\n+----+---+------+\n|John| 21|  male|\n+----+---+------+\n\n+----+---+------+\n|name|age|gender|\n+----+---+------+\n|John| 21|  male|\n+----+---+------+"
  },
  {
    "objectID": "core/slides/slides04_sparkl.html#order-by",
    "href": "core/slides/slides04_sparkl.html#order-by",
    "title": "Spark SQL",
    "section": "ORDER BY",
    "text": "ORDER BY\n\n## In a SQL query:\nquery = \"\"\"\n  SELECT * \n  FROM table \n  ORDER BY name ASC\n\"\"\"\n\n## Using Spark SQL API:\ndf.orderBy(df.name.asc()).show()\n\n\n\n+----+---+------+\n|name|age|gender|\n+----+---+------+\n|Jane| 25|female|\n|John| 21|  male|\n+----+---+------+"
  },
  {
    "objectID": "core/slides/slides04_sparkl.html#alias-name-change",
    "href": "core/slides/slides04_sparkl.html#alias-name-change",
    "title": "Spark SQL",
    "section": "ALIAS (name change)",
    "text": "ALIAS (name change)\n\n## In a SQL query:\nquery = \"\"\"\n  SELECT name, age, gender AS sex \n  FROM table\n\"\"\"\n\n## Using Spark SQL API:\ndf.select(\n    df.name, \n    df.age, \n    df.gender.alias('sex')\n  ).show()\n\n\n\n+----+---+------+\n|name|age|   sex|\n+----+---+------+\n|John| 21|  male|\n|Jane| 25|female|\n+----+---+------+"
  },
  {
    "objectID": "core/slides/slides04_sparkl.html#cast-type-change",
    "href": "core/slides/slides04_sparkl.html#cast-type-change",
    "title": "Spark SQL",
    "section": "CAST (type change)",
    "text": "CAST (type change)\n\n## In a SQL query:\nquery = \"\"\"\n  SELECT name, cast(age AS float) AS age_f \n  FROM table\n\"\"\"\n\n## Using Spark SQL API:\ndf.select(\n  df.name, \n  df.age.cast(\"float\").alias(\"age_f\")\n).show()\n\n## Or\nnew_age_col = df.age.cast(\"float\").alias(\"age_f\")\n\ndf.select(df.name, new_age_col).show()\n\n\n\n+----+-----+\n|name|age_f|\n+----+-----+\n|John| 21.0|\n|Jane| 25.0|\n+----+-----+\n\n+----+-----+\n|name|age_f|\n+----+-----+\n|John| 21.0|\n|Jane| 25.0|\n+----+-----+"
  },
  {
    "objectID": "core/slides/slides04_sparkl.html#adding-new-columns",
    "href": "core/slides/slides04_sparkl.html#adding-new-columns",
    "title": "Spark SQL",
    "section": "Adding new columns",
    "text": "Adding new columns\n\n## In a SQL query:\nquery = \"SELECT *, 12*age AS age_months FROM table\"\n\n## Using Spark SQL API:\ndf.withColumn(\"age_months\", df.age * 12).show()\n\n## Or\ndf.select(\"*\", \n          (df.age * 12).alias(\"age_months\")\n  ).show()\n\n\n\n+----+---+------+----------+\n|name|age|gender|age_months|\n+----+---+------+----------+\n|John| 21|  male|       252|\n|Jane| 25|female|       300|\n+----+---+------+----------+\n\n+----+---+------+----------+\n|name|age|gender|age_months|\n+----+---+------+----------+\n|John| 21|  male|       252|\n|Jane| 25|female|       300|\n+----+---+------+----------+"
  },
  {
    "objectID": "core/slides/slides04_sparkl.html#basic-operations",
    "href": "core/slides/slides04_sparkl.html#basic-operations",
    "title": "Spark SQL",
    "section": "Basic operations",
    "text": "Basic operations\n\nThe full list of operations that can be applied to a DataFrame can be found in the [DataFrame doc]\nThe list of operations on columns can be found in the [Column docs]"
  },
  {
    "objectID": "core/slides/slides04_sparkl.html#column-functions-1",
    "href": "core/slides/slides04_sparkl.html#column-functions-1",
    "title": "Spark SQL",
    "section": "Column functions",
    "text": "Column functions\n\nOften, we need to make many transformations using one or more functions\nSpark SQL has a package called functions with many functions available for that\nSome of those functions are only for aggregations  Examples: avg, sum, etc. We will cover them later\nSome others are for column transformation or operations  Examples:\n\nsubstr, concat, … (string and regex manipulation)\ndatediff, … (timestamp and duration)\nfloor, … (numerics)\n\nThe full list is, once again, in the [API docs]"
  },
  {
    "objectID": "core/slides/slides04_sparkl.html#column-functions-2",
    "href": "core/slides/slides04_sparkl.html#column-functions-2",
    "title": "Spark SQL",
    "section": "Column functions",
    "text": "Column functions\nTo use these functions, we first need to import them:\n\nfrom pyspark.sql import functions as fn\n\n\n\n\nNote: the “as fn” part is important to avoid confusion with native Python functions such as “sum”"
  },
  {
    "objectID": "core/slides/slides04_sparkl.html#numeric-functions-examples",
    "href": "core/slides/slides04_sparkl.html#numeric-functions-examples",
    "title": "Spark SQL",
    "section": "Numeric functions examples",
    "text": "Numeric functions examples\n\nfrom pyspark.sql import functions as fn\n\ncolumns = [\"brand\", \"cost\"]\ndf = spark.createDataFrame([\n        (\"garnier\", 3.49),\n        (\"elseve\", 2.71)\n        ], columns)\n\nround_cost = fn.round(df.cost, 1)\nfloor_cost = fn.floor(df.cost)\nceil_cost = fn.ceil(df.cost)\n\ndf.withColumn('round', round_cost)\\\n        .withColumn('floor', floor_cost)\\\n        .withColumn('ceil', ceil_cost)\\\n        .show()\n\n\n\n+-------+----+-----+-----+----+\n|  brand|cost|round|floor|ceil|\n+-------+----+-----+-----+----+\n|garnier|3.49|  3.5|    3|   4|\n| elseve|2.71|  2.7|    2|   3|\n+-------+----+-----+-----+----+"
  },
  {
    "objectID": "core/slides/slides04_sparkl.html#string-functions-examples",
    "href": "core/slides/slides04_sparkl.html#string-functions-examples",
    "title": "Spark SQL",
    "section": "String functions examples",
    "text": "String functions examples\n\nfrom pyspark.sql import functions as fn\n\ncolumns = [\"first_name\", \"last_name\"]\n\ndf = spark.createDataFrame([\n        (\"John\", \"Doe\"),\n        (\"Mary\", \"Jane\")\n  ], \n  columns      \n)\n\nlast_name_initial = fn.substring(df.last_name, 0, 1)\nname = fn.concat_ws(\" \", df.first_name, last_name_initial)\ndf.withColumn(\"name\", name).show()\n\n\n\n+----------+---------+------+\n|first_name|last_name|  name|\n+----------+---------+------+\n|      John|      Doe|John D|\n|      Mary|     Jane|Mary J|\n+----------+---------+------+"
  },
  {
    "objectID": "core/slides/slides04_sparkl.html#date-functions-examples",
    "href": "core/slides/slides04_sparkl.html#date-functions-examples",
    "title": "Spark SQL",
    "section": "Date functions examples",
    "text": "Date functions examples\n\nfrom datetime import date\nfrom pyspark.sql import functions as fn\n\ndf = spark.createDataFrame([\n        (date(2015, 1, 1), date(2015, 1, 15)),\n        (date(2015, 2, 21), date(2015, 3, 8)),\n        ], [\"start_date\", \"end_date\"]\n    )\ndays_between = fn.datediff(df.end_date, df.start_date)\nstart_month = fn.month(df.start_date)\n\ndf.withColumn('days_between', days_between)\\\n        .withColumn('start_month', start_month)\\\n        .show()\n\n\n\n+----------+----------+------------+-----------+\n|start_date|  end_date|days_between|start_month|\n+----------+----------+------------+-----------+\n|2015-01-01|2015-01-15|          14|          1|\n|2015-02-21|2015-03-08|          15|          2|\n+----------+----------+------------+-----------+"
  },
  {
    "objectID": "core/slides/slides04_sparkl.html#conditional-transformations",
    "href": "core/slides/slides04_sparkl.html#conditional-transformations",
    "title": "Spark SQL",
    "section": "Conditional transformations",
    "text": "Conditional transformations\n\nIn the functions package is a special function called when\nThis function is used to create a new column which value depends on the value of other columns\notherwise is used to match “the rest”\nCombination between conditions can be done using \"&\" for “and” and \"|\" for “or”"
  },
  {
    "objectID": "core/slides/slides04_sparkl.html#examples",
    "href": "core/slides/slides04_sparkl.html#examples",
    "title": "Spark SQL",
    "section": "Examples",
    "text": "Examples\n\ndf = spark.createDataFrame([\n        (\"John\", 21, \"male\"),\n        (\"Jane\", 25, \"female\"),\n        (\"Albert\", 46, \"male\"),\n        (\"Brad\", 49, \"super-hero\")\n    ], [\"name\", \"age\", \"gender\"])\n\nsupervisor = fn.when(df.gender == 'male', 'Mr. Smith')\\\n        .when(df.gender == 'female', 'Miss Jones')\\\n        .otherwise('NA')\n\ndf.withColumn(\"supervisor\", supervisor).show()\n\n\n\n+------+---+----------+----------+\n|  name|age|    gender|supervisor|\n+------+---+----------+----------+\n|  John| 21|      male| Mr. Smith|\n|  Jane| 25|    female|Miss Jones|\n|Albert| 46|      male| Mr. Smith|\n|  Brad| 49|super-hero|        NA|\n+------+---+----------+----------+"
  },
  {
    "objectID": "core/slides/slides04_sparkl.html#functions-in-relational-database-management-systems",
    "href": "core/slides/slides04_sparkl.html#functions-in-relational-database-management-systems",
    "title": "Spark SQL",
    "section": " Functions in Relational Database Management Systems",
    "text": "Functions in Relational Database Management Systems\nCompare functions defined in pyspark.sql.functions with functions specified in ANSI SQL and defined in popular RDBMs\nPostgreSQL Documentation\n Section on Functions and Operators\n\n\n\nIn RDBMs functions serve many purposes\n\nquerying\nsystem administration\ntriggers\n…"
  },
  {
    "objectID": "core/slides/slides04_sparkl.html#user-defined-functions",
    "href": "core/slides/slides04_sparkl.html#user-defined-functions",
    "title": "Spark SQL",
    "section": "User-defined functions",
    "text": "User-defined functions\n\nWhen you need a transformation that is not available in the functions package, you can create a User Defined Function (UDF)\nWarning: the performance of this can be very very low\nSo, it should be used only when you are sure the operation cannot be done with available functions\nTo create an UDF, use functions.udf, passing a lambda or a named functions\nIt is similar to the map operation of RDDs"
  },
  {
    "objectID": "core/slides/slides04_sparkl.html#example",
    "href": "core/slides/slides04_sparkl.html#example",
    "title": "Spark SQL",
    "section": "Example",
    "text": "Example\n\nfrom pyspark.sql import functions as fn\nfrom pyspark.sql.types import StringType\n\ndf = spark.createDataFrame([(1, 3), (4, 2)], [\"first\", \"second\"])\n\ndef my_func(col_1, col_2):\n        if (col_1 &gt; col_2):\n            return \"{} is bigger than {}\".format(col_1, col_2)\n        else:\n            return \"{} is bigger than {}\".format(col_2, col_1)\n\nmy_udf = fn.udf(my_func, StringType())\n\ndf.withColumn(\"udf\", my_udf(df['first'], df['second'])).show()\n\n\n\n+-----+------+------------------+\n|first|second|               udf|\n+-----+------+------------------+\n|    1|     3|3 is bigger than 1|\n|    4|     2|4 is bigger than 2|\n+-----+------+------------------+"
  },
  {
    "objectID": "core/slides/slides04_sparkl.html#performing-joins",
    "href": "core/slides/slides04_sparkl.html#performing-joins",
    "title": "Spark SQL",
    "section": "Performing joins",
    "text": "Performing joins\n\nSpark SQL supports joins between two DataFrame\nAs in ANSI SQL, a join rule must be defined\nThe rule can either be a set of join keys (equi-join), or a conditional rule (\\(\\theta\\)-join)\nJoin with conditional rules (\\(\\theta\\)-joins) in Spark can be very heavy\nSeveral types of joins are available, default is inner\n\nSyntax for \\(\\texttt{left_df} \\bowtie_{\\texttt{cols}} \\texttt{right_df}\\) is simple:\nleft_df.join(\n  other=right_df, \n  on=cols, \n  how=join_type\n)\n\ncols contains a column name or a list of column names\njoin_type is the type of join"
  },
  {
    "objectID": "core/slides/slides04_sparkl.html#examples-1",
    "href": "core/slides/slides04_sparkl.html#examples-1",
    "title": "Spark SQL",
    "section": "Examples",
    "text": "Examples\n\nfrom datetime import date\n\nproducts = spark.createDataFrame([\n        ('1', 'mouse', 'microsoft', 39.99),\n        ('2', 'keyboard', 'logitech', 59.99),\n    ], ['prod_id', 'prod_cat', 'prod_brand', 'prod_value'])\n\npurchases = spark.createDataFrame([\n        (date(2017, 11, 1), 2, '1'),\n        (date(2017, 11, 2), 1, '1'),\n        (date(2017, 11, 5), 1, '2'),\n    ], ['date', 'quantity', 'prod_id'])\n\n# The default join type is the \"INNER\" join\npurchases.join(products, 'prod_id').show()\n\n\n\n+-------+----------+--------+--------+----------+----------+\n|prod_id|      date|quantity|prod_cat|prod_brand|prod_value|\n+-------+----------+--------+--------+----------+----------+\n|      1|2017-11-01|       2|   mouse| microsoft|     39.99|\n|      1|2017-11-02|       1|   mouse| microsoft|     39.99|\n|      2|2017-11-05|       1|keyboard|  logitech|     59.99|\n+-------+----------+--------+--------+----------+----------+"
  },
  {
    "objectID": "core/slides/slides04_sparkl.html#examples-2",
    "href": "core/slides/slides04_sparkl.html#examples-2",
    "title": "Spark SQL",
    "section": "Examples",
    "text": "Examples\n\n# We can also use a query string (not recommended)\nproducts.createOrReplaceTempView(\"products\")\npurchases.createOrReplaceTempView(\"purchases\")\n\nquery = \"\"\"\n  SELECT * \n  FROM  purchases AS prc INNER JOIN \n        products AS prd \n    ON (prc.prod_id = prd.prod_id)\n\"\"\"\n\nspark.sql(query).show()\n\n\n\n+----------+--------+-------+-------+--------+----------+----------+\n|      date|quantity|prod_id|prod_id|prod_cat|prod_brand|prod_value|\n+----------+--------+-------+-------+--------+----------+----------+\n|2017-11-01|       2|      1|      1|   mouse| microsoft|     39.99|\n|2017-11-02|       1|      1|      1|   mouse| microsoft|     39.99|\n|2017-11-05|       1|      2|      2|keyboard|  logitech|     59.99|\n+----------+--------+-------+-------+--------+----------+----------+"
  },
  {
    "objectID": "core/slides/slides04_sparkl.html#examples-3",
    "href": "core/slides/slides04_sparkl.html#examples-3",
    "title": "Spark SQL",
    "section": "Examples",
    "text": "Examples\n\nnew_purchases = spark.createDataFrame([\n        (date(2017, 11, 1), 2, '1'),\n        (date(2017, 11, 2), 1, '3'),\n    ], ['date', 'quantity', 'prod_id_x']\n)\n\njoin_rule = new_purchases.prod_id_x == products.prod_id\n\nnew_purchases.join(products, join_rule, 'left').show()\n\n\n\n+----------+--------+---------+-------+--------+----------+----------+\n|      date|quantity|prod_id_x|prod_id|prod_cat|prod_brand|prod_value|\n+----------+--------+---------+-------+--------+----------+----------+\n|2017-11-01|       2|        1|      1|   mouse| microsoft|     39.99|\n|2017-11-02|       1|        3|   NULL|    NULL|      NULL|      NULL|\n+----------+--------+---------+-------+--------+----------+----------+"
  },
  {
    "objectID": "core/slides/slides04_sparkl.html#performing-joins-some-remarks",
    "href": "core/slides/slides04_sparkl.html#performing-joins-some-remarks",
    "title": "Spark SQL",
    "section": "Performing joins: some remarks",
    "text": "Performing joins: some remarks\n\nSpark removes the duplicated column in the DataFrame it outputs after a join operation\nWhen joining using columns with nulls, Spark just skips those\n\n&gt;&gt;&gt; df1.show()               &gt;&gt;&gt; df2.show()\n+----+-----+                 +----+-----+\n|  id| name|                 |  id| dept|\n+----+-----+                 +----+-----+\n| 123|name1|                 |null|sales|\n| 456|name3|                 | 223|Legal|\n|null|name2|                 | 456|   IT|\n+----+-----+                 +----+-----+\n\n&gt;&gt;&gt; df1.join(df2, \"id\").show\n+---+-----+-----+\n| id| name| dept|\n+---+-----+-----+\n|123|name1|sales|\n|456|name3|   IT|\n+---+-----+-----+"
  },
  {
    "objectID": "core/slides/slides04_sparkl.html#join-types",
    "href": "core/slides/slides04_sparkl.html#join-types",
    "title": "Spark SQL",
    "section": "Join types",
    "text": "Join types\n\n\n\n\n\n\n\n\nSQL Join Type\nIn Spark (synonyms)\nDescription\n\n\n\n\nINNER\n\"inner\"\nData from left and right matching both ways (intersection)\n\n\nFULL OUTER\n\"outer\", \"full\", \"fullouter\"\nAll rows from left and right with extra data if present (union)\n\n\nLEFT OUTER\n\"leftouter\", \"left\"\nRows from left with extra data from right if present\n\n\nRIGHT OUTER\n\"rightouter\", \"right\"\nRows from right with extra data from left if present\n\n\nLEFT SEMI\n\"leftsemi\"\nData from left with a match with right\n\n\nLEFT ANTI\n\"leftanti\"\nData from left with no match with right\n\n\nCROSS\n\"cross\"\nCartesian product of left and right (never used)"
  },
  {
    "objectID": "core/slides/slides04_sparkl.html#join-types-1",
    "href": "core/slides/slides04_sparkl.html#join-types-1",
    "title": "Spark SQL",
    "section": "Join types",
    "text": "Join types"
  },
  {
    "objectID": "core/slides/slides04_sparkl.html#inner-join-inner",
    "href": "core/slides/slides04_sparkl.html#inner-join-inner",
    "title": "Spark SQL",
    "section": "Inner join (“inner”)",
    "text": "Inner join (“inner”)\n&gt;&gt;&gt; inner = df_left.join(df_right, \"id\", \"inner\")\n\ndf_left                df_right             \n+---+-----+            +---+-----+\n| id|value|            | id|value|\n+---+-----+            +---+-----+\n|  1|   A1|            |  3|   A3|\n|  2|   A2|            |  4| A4_1|\n|  3|   A3|            |  4|   A4|\n|  4|   A4|            |  5|   A5|\n+---+-----+            |  6|   A6|\n                       +---+-----+\ninner\n+---+-----+-----+\n| id|value|value|\n+---+-----+-----+\n|  3|   A3|   A3|\n|  4|   A4|   A4|\n|  4|   A4| A4_1|\n+---+-----+-----+"
  },
  {
    "objectID": "core/slides/slides04_sparkl.html#outer-join-outer-full-or-fullouter",
    "href": "core/slides/slides04_sparkl.html#outer-join-outer-full-or-fullouter",
    "title": "Spark SQL",
    "section": "Outer join (“outer”, “full” or “fullouter”)",
    "text": "Outer join (“outer”, “full” or “fullouter”)\n&gt;&gt;&gt; outer = df_left.join(df_right, \"id\", \"outer\")\ndf_left                df_right             \n+---+-----+            +---+-----+\n| id|value|            | id|value|\n+---+-----+            +---+-----+\n|  1|   A1|            |  3|   A3|\n|  2|   A2|            |  4| A4_1|\n|  3|   A3|            |  4|   A4|\n|  4|   A4|            |  5|   A5|\n+---+-----+            |  6|   A6|\n                       +---+-----+\nouter\n+---+-----+-----+\n| id|value|value|\n+---+-----+-----+\n|  1|   A1| null|\n|  2|   A2| null|\n|  3|   A3|   A3|\n|  4|   A4|   A4|\n|  4|   A4| A4_1|\n|  5| null|   A5|\n|  6| null|   A6|\n+---+-----+-----+"
  },
  {
    "objectID": "core/slides/slides04_sparkl.html#left-join-leftouter-or-left",
    "href": "core/slides/slides04_sparkl.html#left-join-leftouter-or-left",
    "title": "Spark SQL",
    "section": "Left join (“leftouter” or “left” )",
    "text": "Left join (“leftouter” or “left” )\n&gt;&gt;&gt; left = df_left.join(df_right, \"id\", \"left\")\n\ndf_left                df_right             \n+---+-----+            +---+-----+\n| id|value|            | id|value|\n+---+-----+            +---+-----+\n|  1|   A1|            |  3|   A3|\n|  2|   A2|            |  4| A4_1|\n|  3|   A3|            |  4|   A4|\n|  4|   A4|            |  5|   A5|\n+---+-----+            |  6|   A6|\n                       +---+-----+\nleft\n+---+-----+-----+\n| id|value|value|\n+---+-----+-----+\n|  1|   A1| null|\n|  2|   A2| null|\n|  3|   A3|   A3|\n|  4|   A4|   A4|\n|  4|   A4| A4_1|\n+---+-----+-----+"
  },
  {
    "objectID": "core/slides/slides04_sparkl.html#right-rightouter-or-right",
    "href": "core/slides/slides04_sparkl.html#right-rightouter-or-right",
    "title": "Spark SQL",
    "section": "Right (“rightouter” or “right”)",
    "text": "Right (“rightouter” or “right”)\n&gt;&gt;&gt; right = df_left.join(df_right, \"id\", \"right\")\n\ndf_left                df_right             \n+---+-----+            +---+-----+\n| id|value|            | id|value|\n+---+-----+            +---+-----+\n|  1|   A1|            |  3|   A3|\n|  2|   A2|            |  4| A4_1|\n|  3|   A3|            |  4|   A4|\n|  4|   A4|            |  5|   A5|\n+---+-----+            |  6|   A6|\n                       +---+-----+\nright\n+---+-----+-----+\n| id|value|value|\n+---+-----+-----+\n|  3|   A3|   A3|\n|  4|   A4|   A4|\n|  4|   A4| A4_1|\n|  5| null|   A5|\n|  6| null|   A6|\n+---+-----+-----+"
  },
  {
    "objectID": "core/slides/slides04_sparkl.html#left-semi-join-leftsemi",
    "href": "core/slides/slides04_sparkl.html#left-semi-join-leftsemi",
    "title": "Spark SQL",
    "section": "Left semi join (“leftsemi”)",
    "text": "Left semi join (“leftsemi”)\n&gt;&gt;&gt; left_semi = df_left.join(df_right, \"id\", \"leftsemi\")\n\ndf_left                df_right             \n+---+-----+            +---+-----+\n| id|value|            | id|value|\n+---+-----+            +---+-----+\n|  1|   A1|            |  3|   A3|\n|  2|   A2|            |  4| A4_1|\n|  3|   A3|            |  4|   A4|\n|  4|   A4|            |  5|   A5|\n+---+-----+            |  6|   A6|\n                       +---+-----+\nleft_semi\n+---+-----+\n| id|value|\n+---+-----+\n|  3|   A3|\n|  4|   A4|\n+---+-----+"
  },
  {
    "objectID": "core/slides/slides04_sparkl.html#left-anti-joint-leftanti",
    "href": "core/slides/slides04_sparkl.html#left-anti-joint-leftanti",
    "title": "Spark SQL",
    "section": "Left anti joint (“leftanti”)",
    "text": "Left anti joint (“leftanti”)\n&gt;&gt;&gt; left_anti = df_left.join(df_right, \"id\", \"leftanti\")\n\ndf_left                df_right             \n+---+-----+            +---+-----+\n| id|value|            | id|value|\n+---+-----+            +---+-----+\n|  1|   A1|            |  3|   A3|\n|  2|   A2|            |  4| A4_1|\n|  3|   A3|            |  4|   A4|\n|  4|   A4|            |  5|   A5|\n+---+-----+            |  6|   A6|\n                       +---+-----+\nleft_anti\n+---+-----+\n| id|value|\n+---+-----+\n|  1|   A1|\n|  2|   A2|\n+---+-----+"
  },
  {
    "objectID": "core/slides/slides04_sparkl.html#performing-joins-1",
    "href": "core/slides/slides04_sparkl.html#performing-joins-1",
    "title": "Spark SQL",
    "section": "Performing joins",
    "text": "Performing joins\n\nNode-to-node communication strategy\nPer node computation strategy\n\n\nSection *“How Spark Performs Joins”"
  },
  {
    "objectID": "core/slides/slides04_sparkl.html#performing-aggregations",
    "href": "core/slides/slides04_sparkl.html#performing-aggregations",
    "title": "Spark SQL",
    "section": "Performing aggregations",
    "text": "Performing aggregations\n\nMaybe the most used operations in SQL and Spark SQL\nSimilar to SQL, we use \"group by\" to perform aggregations\nWe usually can call the aggregation function just after groupBy  Namely, we use groupBy().agg()\nMany aggregation functions in pyspark.sql.functions\nSome examples:\n\nNumerical: fn.avg, fn.sum, fn.min, fn.max, etc.\nGeneral: fn.first, fn.last, fn.count, fn.countDistinct, etc."
  },
  {
    "objectID": "core/slides/slides04_sparkl.html#examples-4",
    "href": "core/slides/slides04_sparkl.html#examples-4",
    "title": "Spark SQL",
    "section": "Examples",
    "text": "Examples\n\nfrom pyspark.sql import functions as fn\n\nproducts = spark.createDataFrame([\n        ('1', 'mouse', 'microsoft', 39.99),\n        ('2', 'mouse', 'microsoft', 59.99),\n        ('3', 'keyboard', 'microsoft', 59.99),\n        ('4', 'keyboard', 'logitech', 59.99),\n        ('5', 'mouse', 'logitech', 29.99),\n    ], ['prod_id', 'prod_cat', 'prod_brand', 'prod_value'])\n\nproducts.groupBy('prod_cat').avg('prod_value').show()\n\n# Or\nproducts.groupBy('prod_cat').agg(fn.avg('prod_value')).show()\n\n\n\n+--------+-----------------+\n|prod_cat|  avg(prod_value)|\n+--------+-----------------+\n|   mouse|43.32333333333333|\n|keyboard|            59.99|\n+--------+-----------------+\n\n+--------+-----------------+\n|prod_cat|  avg(prod_value)|\n+--------+-----------------+\n|   mouse|43.32333333333333|\n|keyboard|            59.99|\n+--------+-----------------+"
  },
  {
    "objectID": "core/slides/slides04_sparkl.html#examples-5",
    "href": "core/slides/slides04_sparkl.html#examples-5",
    "title": "Spark SQL",
    "section": "Examples",
    "text": "Examples\n\nfrom pyspark.sql import functions as fn\n\nproducts.groupBy('prod_brand', 'prod_cat')\\\n        .agg(fn.avg('prod_value')).show()\n\n\n\n+----------+--------+---------------+\n|prod_brand|prod_cat|avg(prod_value)|\n+----------+--------+---------------+\n| microsoft|   mouse|          49.99|\n| microsoft|keyboard|          59.99|\n|  logitech|keyboard|          59.99|\n|  logitech|   mouse|          29.99|\n+----------+--------+---------------+"
  },
  {
    "objectID": "core/slides/slides04_sparkl.html#examples-6",
    "href": "core/slides/slides04_sparkl.html#examples-6",
    "title": "Spark SQL",
    "section": "Examples",
    "text": "Examples\n\nfrom pyspark.sql import functions as fn\n\nproducts.groupBy('prod_brand').agg(\n    fn.round(fn.avg('prod_value'), 1).alias('average'),\n    fn.ceil(fn.sum('prod_value')).alias('sum'),\n    fn.min('prod_value').alias('min')\n).show()\n\n\n\n+----------+-------+---+-----+\n|prod_brand|average|sum|  min|\n+----------+-------+---+-----+\n| microsoft|   53.3|160|39.99|\n|  logitech|   45.0| 90|29.99|\n+----------+-------+---+-----+"
  },
  {
    "objectID": "core/slides/slides04_sparkl.html#examples-7",
    "href": "core/slides/slides04_sparkl.html#examples-7",
    "title": "Spark SQL",
    "section": "Examples",
    "text": "Examples\n\n# Using an SQL query\nproducts.createOrReplaceTempView(\"products\")\n\nquery = \"\"\"\n  SELECT\n    prod_brand,\n    round(avg(prod_value), 1) AS average,\n    min(prod_value) AS min\n  FROM products\n  GROUP BY prod_brand\n\"\"\"\n\nspark.sql(query).show()\n\n\n\n+----------+-------+-----+\n|prod_brand|average|  min|\n+----------+-------+-----+\n| microsoft|   53.3|39.99|\n|  logitech|   45.0|29.99|\n+----------+-------+-----+"
  },
  {
    "objectID": "core/slides/slides04_sparkl.html#window-analytic-functions",
    "href": "core/slides/slides04_sparkl.html#window-analytic-functions",
    "title": "Spark SQL",
    "section": "Window (analytic) functions",
    "text": "Window (analytic) functions\n\nA very, very powerful feature\nThey allow to solve complex problems\nANSI SQL2003 allows for a window_clause in aggregate function calls, the addition of which makes those functions into window functions\nA good article about this feature is [here]\n\nSee also :\nhttps://www.postgresql.org/docs/current/tutorial-window.html\n\n\nA window function performs a calculation across a set of table rows that are somehow related to the current row. This is comparable to the type of calculation that can be done with an aggregate function. However, window functions do not cause rows to become grouped into a single output row like non-window aggregate calls would. Instead, the rows retain their separate identities. Behind the scenes, the window function is able to access more than just the current row of the query result."
  },
  {
    "objectID": "core/slides/slides04_sparkl.html#window-functions-1",
    "href": "core/slides/slides04_sparkl.html#window-functions-1",
    "title": "Spark SQL",
    "section": "Window functions",
    "text": "Window functions\n\nIt’s similar to aggregations, but the number of rows doesn’t change\nInstead, new columns are created, and the aggregated values are duplicated for values of the same “group”\nThere are\n\n“traditional” aggregations, such as min, max, avg, sum and\n“special” types, such as lag, lead, rank"
  },
  {
    "objectID": "core/slides/slides04_sparkl.html#numerical-window-functions",
    "href": "core/slides/slides04_sparkl.html#numerical-window-functions",
    "title": "Spark SQL",
    "section": "Numerical window functions",
    "text": "Numerical window functions\n\nfrom pyspark.sql import Window\nfrom pyspark.sql import functions as fn\n\n# First, we create the Window definition\nwindow = Window.partitionBy('prod_brand')\n\n# Then, we can use \"over\" to aggregate on this window\navg = fn.avg('prod_value').over(window)\n\n# Finally, we can it as a classical column\nproducts.withColumn('avg_brand_value', fn.round(avg, 2)).show()\n\n\n\n+-------+--------+----------+----------+---------------+\n|prod_id|prod_cat|prod_brand|prod_value|avg_brand_value|\n+-------+--------+----------+----------+---------------+\n|      4|keyboard|  logitech|     59.99|          44.99|\n|      5|   mouse|  logitech|     29.99|          44.99|\n|      1|   mouse| microsoft|     39.99|          53.32|\n|      2|   mouse| microsoft|     59.99|          53.32|\n|      3|keyboard| microsoft|     59.99|          53.32|\n+-------+--------+----------+----------+---------------+"
  },
  {
    "objectID": "core/slides/slides04_sparkl.html#numerical-window-functions-1",
    "href": "core/slides/slides04_sparkl.html#numerical-window-functions-1",
    "title": "Spark SQL",
    "section": "Numerical window functions",
    "text": "Numerical window functions\n\nfrom pyspark.sql import Window\nfrom pyspark.sql import functions as fn\n\n# The window can be defined on multiple columns\nwindow = Window.partitionBy('prod_brand', 'prod_cat')\n\navg = fn.avg('prod_value').over(window)\n\nproducts.withColumn('avg_value', fn.round(avg, 2)).show()\n\n\n\n+-------+--------+----------+----------+---------+\n|prod_id|prod_cat|prod_brand|prod_value|avg_value|\n+-------+--------+----------+----------+---------+\n|      4|keyboard|  logitech|     59.99|    59.99|\n|      5|   mouse|  logitech|     29.99|    29.99|\n|      3|keyboard| microsoft|     59.99|    59.99|\n|      1|   mouse| microsoft|     39.99|    49.99|\n|      2|   mouse| microsoft|     59.99|    49.99|\n+-------+--------+----------+----------+---------+"
  },
  {
    "objectID": "core/slides/slides04_sparkl.html#numerical-window-functions-2",
    "href": "core/slides/slides04_sparkl.html#numerical-window-functions-2",
    "title": "Spark SQL",
    "section": "Numerical window functions",
    "text": "Numerical window functions\n\nfrom pyspark.sql import Window\nfrom pyspark.sql import functions as fn\n\n# Multiple windows can be defined\nwindow1 = Window.partitionBy('prod_brand')\nwindow2 = Window.partitionBy('prod_cat')\n\navg_brand = fn.avg('prod_value').over(window1)\navg_cat = fn.avg('prod_value').over(window2)\n\nproducts \\\n  .withColumn('avg_by_brand', fn.round(avg_brand, 2)) \\\n  .withColumn('avg_by_cat', fn.round(avg_cat, 2)) \\\n  .show()\n\n\n\n+-------+--------+----------+----------+------------+----------+\n|prod_id|prod_cat|prod_brand|prod_value|avg_by_brand|avg_by_cat|\n+-------+--------+----------+----------+------------+----------+\n|      4|keyboard|  logitech|     59.99|       44.99|     59.99|\n|      3|keyboard| microsoft|     59.99|       53.32|     59.99|\n|      5|   mouse|  logitech|     29.99|       44.99|     43.32|\n|      1|   mouse| microsoft|     39.99|       53.32|     43.32|\n|      2|   mouse| microsoft|     59.99|       53.32|     43.32|\n+-------+--------+----------+----------+------------+----------+"
  },
  {
    "objectID": "core/slides/slides04_sparkl.html#lag-and-lead",
    "href": "core/slides/slides04_sparkl.html#lag-and-lead",
    "title": "Spark SQL",
    "section": "Lag and Lead",
    "text": "Lag and Lead\n\nlag and lead are special functions used over an ordered window\nThey are used to take the “previous” and “next” value within the window\nVery useful in datasets with a date column for instance"
  },
  {
    "objectID": "core/slides/slides04_sparkl.html#lag-and-lead-1",
    "href": "core/slides/slides04_sparkl.html#lag-and-lead-1",
    "title": "Spark SQL",
    "section": "Lag and Lead",
    "text": "Lag and Lead\n\npurchases = spark.createDataFrame([\n        (date(2017, 11, 1), 'mouse'),\n        (date(2017, 11, 2), 'mouse'),\n        (date(2017, 11, 4), 'keyboard'),\n        (date(2017, 11, 6), 'keyboard'),\n        (date(2017, 11, 9), 'keyboard'),\n        (date(2017, 11, 12), 'mouse'),\n        (date(2017, 11, 18), 'keyboard')\n    ], ['date', 'prod_cat'])\npurchases.show()\n\n\n\n+----------+--------+\n|      date|prod_cat|\n+----------+--------+\n|2017-11-01|   mouse|\n|2017-11-02|   mouse|\n|2017-11-04|keyboard|\n|2017-11-06|keyboard|\n|2017-11-09|keyboard|\n|2017-11-12|   mouse|\n|2017-11-18|keyboard|\n+----------+--------+"
  },
  {
    "objectID": "core/slides/slides04_sparkl.html#lag-and-lead-2",
    "href": "core/slides/slides04_sparkl.html#lag-and-lead-2",
    "title": "Spark SQL",
    "section": "Lag and Lead",
    "text": "Lag and Lead\n\nwindow = Window.partitionBy('prod_cat').orderBy('date')\n\nprev_purch = fn.lag('date', 1).over(window)\nnext_purch = fn.lead('date', 1).over(window)\n\npurchases\\\n  .withColumn('prev', prev_purch)\\\n  .withColumn('next', next_purch)\\\n  .orderBy('prod_cat', 'date')\\\n  .show()\n\n\n\n+----------+--------+----------+----------+\n|      date|prod_cat|      prev|      next|\n+----------+--------+----------+----------+\n|2017-11-04|keyboard|      NULL|2017-11-06|\n|2017-11-06|keyboard|2017-11-04|2017-11-09|\n|2017-11-09|keyboard|2017-11-06|2017-11-18|\n|2017-11-18|keyboard|2017-11-09|      NULL|\n|2017-11-01|   mouse|      NULL|2017-11-02|\n|2017-11-02|   mouse|2017-11-01|2017-11-12|\n|2017-11-12|   mouse|2017-11-02|      NULL|\n+----------+--------+----------+----------+"
  },
  {
    "objectID": "core/slides/slides04_sparkl.html#rank-denserank-and-rownumber",
    "href": "core/slides/slides04_sparkl.html#rank-denserank-and-rownumber",
    "title": "Spark SQL",
    "section": "Rank, DenseRank and RowNumber",
    "text": "Rank, DenseRank and RowNumber\n\nAnother set of useful “special” functions\nAlso used on ordered windows\nThey create a rank, or an order of the items within the window"
  },
  {
    "objectID": "core/slides/slides04_sparkl.html#rank-and-rownumber",
    "href": "core/slides/slides04_sparkl.html#rank-and-rownumber",
    "title": "Spark SQL",
    "section": "Rank and RowNumber",
    "text": "Rank and RowNumber\n\ncontestants = spark.createDataFrame([\n    ('veterans', 'John', 3000),\n    ('veterans', 'Bob', 3200),\n    ('veterans', 'Mary', 4000),\n    ('young', 'Jane', 4000),\n    ('young', 'April', 3100),\n    ('young', 'Alice', 3700),\n    ('young', 'Micheal', 4000)], \n  ['category', 'name', 'points']\n)\n\ncontestants.show()\n\n\n\n+--------+-------+------+\n|category|   name|points|\n+--------+-------+------+\n|veterans|   John|  3000|\n|veterans|    Bob|  3200|\n|veterans|   Mary|  4000|\n|   young|   Jane|  4000|\n|   young|  April|  3100|\n|   young|  Alice|  3700|\n|   young|Micheal|  4000|\n+--------+-------+------+"
  },
  {
    "objectID": "core/slides/slides04_sparkl.html#rank-and-rownumber-1",
    "href": "core/slides/slides04_sparkl.html#rank-and-rownumber-1",
    "title": "Spark SQL",
    "section": "Rank and RowNumber",
    "text": "Rank and RowNumber\n\nwindow = Window.partitionBy('category')\\\n        .orderBy(contestants.points.desc())\n\nrank = fn.rank().over(window)\ndense_rank = fn.dense_rank().over(window)\nrow_number = fn.row_number().over(window)\n\ncontestants\\\n        .withColumn('rank', rank)\\\n        .withColumn('dense_rank', dense_rank)\\\n        .withColumn('row_number', row_number)\\\n        .orderBy('category', fn.col('points').desc())\\\n        .show()\n\n\n\n+--------+-------+------+----+----------+----------+\n|category|   name|points|rank|dense_rank|row_number|\n+--------+-------+------+----+----------+----------+\n|veterans|   Mary|  4000|   1|         1|         1|\n|veterans|    Bob|  3200|   2|         2|         2|\n|veterans|   John|  3000|   3|         3|         3|\n|   young|   Jane|  4000|   1|         1|         1|\n|   young|Micheal|  4000|   1|         1|         2|\n|   young|  Alice|  3700|   3|         2|         3|\n|   young|  April|  3100|   4|         3|         4|\n+--------+-------+------+----+----------+----------+"
  },
  {
    "objectID": "core/slides/slides04_sparkl.html#writing-dataframes-1",
    "href": "core/slides/slides04_sparkl.html#writing-dataframes-1",
    "title": "Spark SQL",
    "section": "Writing dataframes",
    "text": "Writing dataframes\n\nVery similar to reading. Output formats are the same: csv, json, parquet, orc, jdbc, etc. Note that write is an action\nInstead of df.read.{source} use df.write.{target}\nMain option is mode with possible values:\n\n\"append\": append contents of this DataFrame to existing data.\n\"overwrite\": overwrite existing data\n\"error\": throw an exception if data already exists\n\"ignore\": silently ignore this operation if data already exists.\n\n\nExample\n\nproducts.write.csv('./products.csv')\nproducts.write.mode('overwrite').parquet('./produits.parquet')\nproducts.write.format('parquet').save('./produits_2.parquet')"
  },
  {
    "objectID": "core/slides/slides04_sparkl.html#query-planning-and-optimization",
    "href": "core/slides/slides04_sparkl.html#query-planning-and-optimization",
    "title": "Spark SQL",
    "section": "Query planning and optimization",
    "text": "Query planning and optimization\nA lot happens under the hood when executing an action on a DataFrame. The query goes through the following exectution stages:\n\nLogical Analysis\nCaching Replacement\nLogical Query Optimization (using rule-based and cost-based optimizations)\nPhysical Planning\nPhysical Optimization (e.g. Whole-Stage Java Code Generation or Adaptive Query Execution)\nConstructing the RDD of Internal Binary Rows (that represents the structured query in terms of Spark Core’s RDD API)\n\n\n\nhttps://jaceklaskowski.gitbooks.io/mastering-spark-sql/spark-sql.html]"
  },
  {
    "objectID": "core/slides/slides04_sparkl.html#query-planning-and-optimization-1",
    "href": "core/slides/slides04_sparkl.html#query-planning-and-optimization-1",
    "title": "Spark SQL",
    "section": "Query planning and optimization",
    "text": "Query planning and optimization\n\n\n\n\n\nhttps://jaceklaskowski.gitbooks.io/mastering-spark-sql/spark-sql.html]"
  },
  {
    "objectID": "core/slides/slides04_sparkl.html#references",
    "href": "core/slides/slides04_sparkl.html#references",
    "title": "Spark SQL",
    "section": "References",
    "text": "References\nPySpark Quickstart\nPandas on Spark User’s Guide"
  },
  {
    "objectID": "core/slides/slides07_spark-deeper-dive.html#shuffles-1",
    "href": "core/slides/slides07_spark-deeper-dive.html#shuffles-1",
    "title": "Spark deeper dive",
    "section": "Shuffles",
    "text": "Shuffles\nWhat happens when we do a reduceByKey on a RDD?\n&gt;&gt;&gt; rdd = sc.parallelize([(\"a\", 1), (\"b\", 1), (\"a\", 1)])\n&gt;&gt;&gt; rdd.reduceByKey(lambda a, b: a + b).collect()\nLet’s have a look at the Spark UI available at :\nhttp://localhost:4040/jobs/]\n\n\nRelate shuffles to wide transformations\nIn Spark parlance, in a narrow transformation, each input partition (class of the input partition) contributes to at most one output partition (one class of the output partition).\nIn a wide transformation, input partitions contribute to several or even many output partitions.\nShuffles correspond to wide transformations."
  },
  {
    "objectID": "core/slides/slides07_spark-deeper-dive.html#shuffles-2",
    "href": "core/slides/slides07_spark-deeper-dive.html#shuffles-2",
    "title": "Spark deeper dive",
    "section": "Shuffles",
    "text": "Shuffles\n  \n\nSpark has to move data from one node to another to be “grouped” with its “key”\nDoing this is called shuffling. It is never called directly, it happens behind the curtains for some other functions as reduceByKey above.\nThis might be very expensive because of latency"
  },
  {
    "objectID": "core/slides/slides07_spark-deeper-dive.html#shuffles-3",
    "href": "core/slides/slides07_spark-deeper-dive.html#shuffles-3",
    "title": "Spark deeper dive",
    "section": "Shuffles",
    "text": "Shuffles\n\nreduceByKey results in one key-value pair per key\nThis single key-value pair cannot span across multiple workers."
  },
  {
    "objectID": "core/slides/slides07_spark-deeper-dive.html#example",
    "href": "core/slides/slides07_spark-deeper-dive.html#example",
    "title": "Spark deeper dive",
    "section": "Example",
    "text": "Example\n&gt;&gt;&gt; from collections import namedtuple\n&gt;&gt;&gt; columns = [\"client_id\", \"destination\", \"price\"]\n&gt;&gt;&gt; CFFPurchase = namedtuple(\"CFFPurchase\", columns)\n&gt;&gt;&gt; CFFPurchase(100, \"Geneva\", 22.25)\nCFFPurchase(client_id=100, destination='Geneva', price=22.25)\nGoal: calculate how many trips and how much money was spent by each client"
  },
  {
    "objectID": "core/slides/slides07_spark-deeper-dive.html#example-cont.",
    "href": "core/slides/slides07_spark-deeper-dive.html#example-cont.",
    "title": "Spark deeper dive",
    "section": "Example (cont.)",
    "text": "Example (cont.)\n&gt;&gt;&gt; purchases = [\n        CFFPurchase(100, \"Geneva\", 22.25),\n        CFFPurchase(100, \"Zurich\", 42.10),\n        CFFPurchase(100, \"Fribourg\", 12.40),\n        CFFPurchase(101, \"St.Gallen\", 8.20),\n        CFFPurchase(101, \"Lucerne\", 31.60),\n        CFFPurchase(100, \"Basel\", 16.20)\n    ]\n&gt;&gt;&gt; purchases = sc.parallelize(purchases)\n&gt;&gt;&gt; purchases.collect()\n[CFFPurchase(client_id=100, destination='Geneva', price=22.25),\n CFFPurchase(client_id=100, destination='Zurich', price=42.1),\n CFFPurchase(client_id=100, destination='Fribourg', price=12.4),\n CFFPurchase(client_id=101, destination='St.Gallen', price=8.2),\n CFFPurchase(client_id=101, destination='Lucerne', price=31.6),\n CFFPurchase(client_id=100, destination='Basel', price=16.2)]\n\nHow are namedtuple translated into Spark types?"
  },
  {
    "objectID": "core/slides/slides07_spark-deeper-dive.html#example-cont.-1",
    "href": "core/slides/slides07_spark-deeper-dive.html#example-cont.-1",
    "title": "Spark deeper dive",
    "section": "Example (cont.)",
    "text": "Example (cont.)\n&gt;&gt;&gt; purchases_per_client = (purchases\n        # Pair RDD\n        .map(lambda p: (p.client_id, p.price))\n        # RDD[p.customerId, List[p.price]]\n        .groupByKey()                          \n        .map(lambda p: (p[0], (len(p[1]), sum(p[1]))))\n        .collect()\n    )\n&gt;&gt;&gt; purchases_per_client\n[(100, (4, 92.95)), (101, (2, 39.8))]\nHow would this looks on a cluster? (imagine that the dataset has millions of purchases)"
  },
  {
    "objectID": "core/slides/slides07_spark-deeper-dive.html#shuffles-4",
    "href": "core/slides/slides07_spark-deeper-dive.html#shuffles-4",
    "title": "Spark deeper dive",
    "section": "Shuffles",
    "text": "Shuffles\n\nLets say we have 3 worker nodes and our data is evenly distributed on it, so the operations above look like this:\n\n\n\nThis shuffling is very expensive because of latency\nCan we do a better job?"
  },
  {
    "objectID": "core/slides/slides07_spark-deeper-dive.html#shuffles-5",
    "href": "core/slides/slides07_spark-deeper-dive.html#shuffles-5",
    "title": "Spark deeper dive",
    "section": "Shuffles",
    "text": "Shuffles\nPerhaps we can reduce before we shuffle in order to greatly reduce the amount of data sent over network. We use reduceByKey.\n&gt;&gt;&gt; purchases_per_client = (purchases\n        .map(lambda p: (p.client_id, (1, p.price)))\n        .reduceByKey(lambda v1, v2: (v1[0] + v2[0], v1[1] + v2[1]))\n        .collect()\n    )\nThis looks like on the cluster:"
  },
  {
    "objectID": "core/slides/slides07_spark-deeper-dive.html#shuffles-6",
    "href": "core/slides/slides07_spark-deeper-dive.html#shuffles-6",
    "title": "Spark deeper dive",
    "section": "Shuffles",
    "text": "Shuffles\n\ngroupByKey (left) VS reduceByKey (right) :\n\n \n\nWith reduceByKey we shuffle considerably less amount of data\n\nBenefits of this approach:\n\nby reducing the data first, the amount of data sent during the shuffle is greatly reduced, leading to strong performance gains!\nThis is because groupByKey requires collecting all key-value pairs with the same key on the same machine while reduceByKey reduces locally before shuffling."
  },
  {
    "objectID": "core/slides/slides07_spark-deeper-dive.html#partitioning",
    "href": "core/slides/slides07_spark-deeper-dive.html#partitioning",
    "title": "Spark deeper dive",
    "section": "Partitioning",
    "text": "Partitioning\nHow does Spark know which key to put on which machine?\n\nBy default, Spark uses hash partitioning to determine which key-value pair should be sent to which machine.\n\nThe data within an RDD is split into several partitions. Some properties of partitions:\n\nPartitions never span multiple machines, data in the same partition is always on the same worker machine.\nEach machine in the cluster contains one or more partitions.\nThe number of partitions to use is configurable. By default, it is the total number of cores on all executor nodes; 6 workers with 4 cores should lead to 24 partitions."
  },
  {
    "objectID": "core/slides/slides07_spark-deeper-dive.html#partitioning-1",
    "href": "core/slides/slides07_spark-deeper-dive.html#partitioning-1",
    "title": "Spark deeper dive",
    "section": "Partitioning",
    "text": "Partitioning\nTwo kinds of partitioning are available in Spark:\n\nHash partitioning\nRange partitioning\n\nCustomizing a partitioning is only possible on a PairRDD and DataFrame, namely something with keys."
  },
  {
    "objectID": "core/slides/slides07_spark-deeper-dive.html#hash-partitioning",
    "href": "core/slides/slides07_spark-deeper-dive.html#hash-partitioning",
    "title": "Spark deeper dive",
    "section": "Hash Partitioning",
    "text": "Hash Partitioning\nGiven a Pair RDD that should be grouped, groupByKey first computes per tuple (k,v) its partition p:\np = k.hashCode() % numPartitions\nThen, all tuples in the same partition p are sent to the machine hosting p."
  },
  {
    "objectID": "core/slides/slides07_spark-deeper-dive.html#partitioning-2",
    "href": "core/slides/slides07_spark-deeper-dive.html#partitioning-2",
    "title": "Spark deeper dive",
    "section": "Partitioning",
    "text": "Partitioning\nIntuition: hash partitioning attempts to spread data evenly across partitions based on the key.\nThe other kind of partitioning is range partitioning"
  },
  {
    "objectID": "core/slides/slides07_spark-deeper-dive.html#range-partitioning",
    "href": "core/slides/slides07_spark-deeper-dive.html#range-partitioning",
    "title": "Spark deeper dive",
    "section": "Range Partitioning",
    "text": "Range Partitioning\n\nPair RDDs may contain keys that have an ordering defined, such as int, String, etc.\nFor such RDDs, range partitioning may be more efficient.\n\nUsing a range partitioner, keys are partitioned according to 2 things:\n\nan ordering for keys\na set of sorted ranges of keys\n\n(key, value) pairs with keys in the same range end up in the same partition."
  },
  {
    "objectID": "core/slides/slides07_spark-deeper-dive.html#partitioning-3",
    "href": "core/slides/slides07_spark-deeper-dive.html#partitioning-3",
    "title": "Spark deeper dive",
    "section": "Partitioning",
    "text": "Partitioning\nConsider a Pair RDD with keys: [8, 96, 240, 400, 401, 800], and a desired number of partitions of 4.\nWith hash partitioning\np = k.hashCode() % numPartitions\n  = k % 4\nleads to - Partition 0: [8, 96, 240, 400, 800] - Partition 1: [401] - Partition 2: [] - Partition 3: []\nThis results in a very unbalanced distribution which hurts performance, since the data is spread mostly on 1 node, so not very parallel."
  },
  {
    "objectID": "core/slides/slides07_spark-deeper-dive.html#partitioning-4",
    "href": "core/slides/slides07_spark-deeper-dive.html#partitioning-4",
    "title": "Spark deeper dive",
    "section": "Partitioning",
    "text": "Partitioning\nIn this case, using range partitioning can improve the distribution significantly.\n\nAssumptions: (a) keys are non-negative, (b) 800 is the biggest key\nRanges: [1-200], [201-400], [401-600], [601-800]\n\nBased on this the keys are distributed as follows:\n\nPartition 0: [8, 96]\nPartition 1: [240, 400]\nPartition 2: [401]\nPartition 3: [800]\n\nThis is much more balanced."
  },
  {
    "objectID": "core/slides/slides07_spark-deeper-dive.html#partitioning-5",
    "href": "core/slides/slides07_spark-deeper-dive.html#partitioning-5",
    "title": "Spark deeper dive",
    "section": "Partitioning",
    "text": "Partitioning\nHow do we set a partitioning for our data?\n\nOn a Pair RDD: call partitionBy, providing an explicit Partitioner (scala only, use a partitioning function in pyspark)\nOn a DataFrame: Call repartition for hash partitioning and repartitionByRange for range partitioning\nUsing transformations that return a RDD or a DataFrame with specific partitioners."
  },
  {
    "objectID": "core/slides/slides07_spark-deeper-dive.html#partitioning-a-rdd-using-partitionby",
    "href": "core/slides/slides07_spark-deeper-dive.html#partitioning-a-rdd-using-partitionby",
    "title": "Spark deeper dive",
    "section": "Partitioning a RDD using partitionBy",
    "text": "Partitioning a RDD using partitionBy\n&gt;&gt;&gt; pairs = purchases.map(lambda p: (p.client_id, p.price))\n&gt;&gt;&gt; pairs = pairs.partitionBy(3, \"client_id\")\n&gt;&gt;&gt; pairs.getNumPartitions()\n3"
  },
  {
    "objectID": "core/slides/slides07_spark-deeper-dive.html#partitioning-6",
    "href": "core/slides/slides07_spark-deeper-dive.html#partitioning-6",
    "title": "Spark deeper dive",
    "section": "Partitioning",
    "text": "Partitioning\nUsing RangePartitioner with pyspark requires\n\nSpecifying the desired number of partitions\nProviding a DataFrame with orderable keys\n\npairs = purchases.map(lambda p: (p.client_id, p.price))\npairs = spark.createDataFrame(pairs, [\"id\", \"price\"])\npairs.repartitionByRange(3, \"price\").persist()\npairs.show()\n\n\n\n\n\n\nImportant\n\n\n\nThe result of partitionBy, repartition, repartitionByRange should be persisted.\nOtherwise partitioning is repeatedly applied (with shuffling!) each time the partitioned data is used."
  },
  {
    "objectID": "core/slides/slides07_spark-deeper-dive.html#partitioning-using-transformations",
    "href": "core/slides/slides07_spark-deeper-dive.html#partitioning-using-transformations",
    "title": "Spark deeper dive",
    "section": "Partitioning using transformations",
    "text": "Partitioning using transformations\n\nPair RDDs that are result of a transformation on a partitioned Pair RDD use typically the same hash partitioner\nSome operations on RDDs automatically result in an RDD with a known partitioner - when it makes sense.\n\n\n\n\n\n\n\nExamples\n\n\n\nWhen using sortByKey, a RangePartitioner is used.\nWith groupByKey, a default hash partitioner is used."
  },
  {
    "objectID": "core/slides/slides07_spark-deeper-dive.html#operations-on-pair-rdds-that-hold-to-and-propagate-a-partitioner",
    "href": "core/slides/slides07_spark-deeper-dive.html#operations-on-pair-rdds-that-hold-to-and-propagate-a-partitioner",
    "title": "Spark deeper dive",
    "section": "Operations on Pair RDDs that hold to and propagate a partitioner:",
    "text": "Operations on Pair RDDs that hold to and propagate a partitioner:\n\ncogroup, groupWith, join, leftOuterJoin, rightOuterJoin\n[group,reduce,fold,combine]ByKey, partitionBy, sort\nmapValues, flatMapValues, filter (if parent has a partitioner)\n\nAll other operations will produce a result without partitioner!\n\nWhy?"
  },
  {
    "objectID": "core/slides/slides07_spark-deeper-dive.html#partitioning-7",
    "href": "core/slides/slides07_spark-deeper-dive.html#partitioning-7",
    "title": "Spark deeper dive",
    "section": "Partitioning",
    "text": "Partitioning\nConsider the map transformation. Given that we have a hash-partitioned Pair RDD, why loosing the partitioner in the returned RDD?\nBecause its possible for map or flatMap to change the key:\nhashPartitionedRdd.map(lambda k, v: (\"ooooops\", v))\nIf the map transformation preserved the previous partitioner, it would no longer makes sense: the keys are all same after this map\nHence, use mapValues, since it enables to do map transformations without changing the keys, thereby preserving the partitioner."
  },
  {
    "objectID": "core/slides/slides07_spark-deeper-dive.html#optimizing-with-partitioning",
    "href": "core/slides/slides07_spark-deeper-dive.html#optimizing-with-partitioning",
    "title": "Spark deeper dive",
    "section": "Optimizing with partitioning",
    "text": "Optimizing with partitioning\nWhy would we want to repartition the data?\n\nBecause it can bring substantial performance gains, especially before shuffles.\nWe saw that using reduceByKey instead of groupByKey localizes data better due to different partitioning strategies and thus reduces latency to deliver performance gains.\nBy manually repartitioning the data for the same example as before, we can improve the performance even further.\nBy using range partitioners we can optimize the use of reduceByKey in that example so that it does not involve any shuffling over the network at all!"
  },
  {
    "objectID": "core/slides/slides07_spark-deeper-dive.html#optimizing-with-partitioning-1",
    "href": "core/slides/slides07_spark-deeper-dive.html#optimizing-with-partitioning-1",
    "title": "Spark deeper dive",
    "section": "Optimizing with partitioning",
    "text": "Optimizing with partitioning\nCompared to what we did previously, we use sortByKey to produce a range partitioner for the RDD that we immediately persist.\n&gt;&gt;&gt; pairs = purchases.map(lambda p: (p.client_id, (1, p.price)))\n&gt;&gt;&gt; pairs = pairs.sortByKey().persist()\n&gt;&gt;&gt; pairs.reduceByKey(\n       lambda v1, v2: (v1[0] + v2[0], v1[1] + v2[1])\n    ).collect()\nThis typically leads to much faster computations in this case (for large RDD, not the small toy example from before)."
  },
  {
    "objectID": "core/slides/slides07_spark-deeper-dive.html#optimizing-with-broadcasting",
    "href": "core/slides/slides07_spark-deeper-dive.html#optimizing-with-broadcasting",
    "title": "Spark deeper dive",
    "section": "Optimizing with broadcasting",
    "text": "Optimizing with broadcasting\nWhen joining two DataDrames, where one is small enough to fit in memory, it is broadcasted over all the workers where the large DataFrame resides (and a hash join is performed). This has two phases:\n\nBroadcast: the smaller DataFrame is broadcasted across workers containing the large one\nHash join: a standard hash join is executed on each workder\n\nThere is therefore no shuffling involved and this can be much faster than a regular join.\nThe default threshold for broadcasting is\n&gt;&gt;&gt; spark.conf.get(\"spark.sql.autoBroadcastJoinThreshold\")\n'10485760'\nmeaning 10MB"
  },
  {
    "objectID": "core/slides/slides07_spark-deeper-dive.html#optimizing-with-broadcasting-1",
    "href": "core/slides/slides07_spark-deeper-dive.html#optimizing-with-broadcasting-1",
    "title": "Spark deeper dive",
    "section": "Optimizing with broadcasting",
    "text": "Optimizing with broadcasting\nCan be changed using\n&gt;&gt;&gt; spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", \"20971520\")\n\"spark.broadcast.compress\" can be used to configure whether to compress the data before sending it (True by default).\nIt uses the compression specified in \"spark.io.compression.codec config\" and the default is \"lz4\". We can use other compression codecs but what the hell.\nMore important: even though a DataFrame is small, sometimes Spark can’t estimate the size of it. We can enforce using a broadcast hint:\n&gt;&gt;&gt; from pyspark.sql.functions import broadcast\n&gt;&gt;&gt; largeDF.join(broadcast(smallDF))"
  },
  {
    "objectID": "core/slides/slides07_spark-deeper-dive.html#optimizing-with-broadcasting-2",
    "href": "core/slides/slides07_spark-deeper-dive.html#optimizing-with-broadcasting-2",
    "title": "Spark deeper dive",
    "section": "Optimizing with broadcasting",
    "text": "Optimizing with broadcasting\n\nIf a broadcast hint is specified, the side with the hint will be broadcasted irrespective of autoBroadcastJoinThreshold.\nIf both sides have broadcast hints, the side with a smallest estimated size will be broadcasted.\nIf there is no hint and the estimated size of DataFrame &lt; autoBroadcastJoinThreshold, that table is usually broadcasted\nSpark has a BitTorrent-like implementation to perform broadcast. Allows to avoid the driver being the bottleneck when sending data to multiple executors."
  },
  {
    "objectID": "core/slides/slides07_spark-deeper-dive.html#optimizing-with-broadcasting-3",
    "href": "core/slides/slides07_spark-deeper-dive.html#optimizing-with-broadcasting-3",
    "title": "Spark deeper dive",
    "section": "Optimizing with broadcasting",
    "text": "Optimizing with broadcasting\n\nUsually, a broadcast join performs faster than other join algorithms when the broadcast side is small enough.\nHowever, broadcasting tables is network-intensive and can cause out of memory errors or even perform worse than other joins if the broadcasted table is too large.\nBroadcast join is not supported for a full outer join. For right outer join, only left side table can be broadcasted and for other left joins only the right table can be broadcasted."
  },
  {
    "objectID": "core/slides/slides07_spark-deeper-dive.html#shuffle-hash-vs-sort-merge-joins-1",
    "href": "core/slides/slides07_spark-deeper-dive.html#shuffle-hash-vs-sort-merge-joins-1",
    "title": "Spark deeper dive",
    "section": "Shuffle hash VS sort merge joins",
    "text": "Shuffle hash VS sort merge joins\nSpark can use mainly two strategies for joining:\n\nShuffle hash join\nSort merge join\n\nSort merge join is the default join strategy, since it is very scalable and performs better than other joins most of the times.\nShuffle hash join is used as the join strategy when:\n\nspark.sql.join.preferSortMergeJoin is set to False\nthe cost to build a hash map is less than sorting the data."
  },
  {
    "objectID": "core/slides/slides07_spark-deeper-dive.html#shuffle-hash",
    "href": "core/slides/slides07_spark-deeper-dive.html#shuffle-hash",
    "title": "Spark deeper dive",
    "section": "Shuffle hash",
    "text": "Shuffle hash\nShuffle hash join has 2 phases:\n\nA shuffle phase, where data from the join tables are partitioned based on the join key. This shuffles data across the partitions. Data with the same keys end up in the same partition: the data required for joins is available in the same partition.\nHash join phase: data on each partition performs a single node hash join.\n\nThus, shuffle hash join breaks apart the big join of two tables into localized smaller chunks.\n\nShuffle is very expensive, the creation of Hash tables is also expensive and memory bound.\nShuffle hash join is not suited for joins that wouldn’t fit in memory."
  },
  {
    "objectID": "core/slides/slides07_spark-deeper-dive.html#shuffle-hash-1",
    "href": "core/slides/slides07_spark-deeper-dive.html#shuffle-hash-1",
    "title": "Spark deeper dive",
    "section": "Shuffle hash",
    "text": "Shuffle hash\n\nThe performance of shuffle hash join depends on the distribution of the keys. The greater number of unique join keys the better data distribution we get.\nMaximum amount of achievable parallelism is proportional to the number of unique keys.\n\nBy default, sort merge join is preferred over shuffle hash join. ShuffledHashJoin is still useful when:\n\nAny partition of the resulting table can fit in memory\nOne table is much smaller than the other one, so that building a hash table of the small table is smaller than sorting the large table.\n\nThis explains why this shuffle is used for broadcast joins."
  },
  {
    "objectID": "core/slides/slides07_spark-deeper-dive.html#sort-merge-join",
    "href": "core/slides/slides07_spark-deeper-dive.html#sort-merge-join",
    "title": "Spark deeper dive",
    "section": "Sort merge join",
    "text": "Sort merge join\nSort merge join is Spark’s default join strategy if:\n\nmatching join keys are sortable\nnot eligible for broadcast join or shuffle hash join.\n\nIt is very scalable and is an inheritance of Hadoop and map-reduce programs. What makes it scalable is that it can spill data to the disk and doesn’t require the entire data to fit inside the memory.\nIt has three phases:\n\nShuffle phase: the two large tables are (re)partitioned using the join key(s) across the partitions in the cluster.\nSort phase: sort the data within each partition in parallel.\nMerge phase: join the sorted and partitioned data. This is simply merging the datasets by iterating over the elements and joining the rows having the same value for the join key."
  },
  {
    "objectID": "core/slides/slides07_spark-deeper-dive.html#sort-merge-join-1",
    "href": "core/slides/slides07_spark-deeper-dive.html#sort-merge-join-1",
    "title": "Spark deeper dive",
    "section": "Sort merge join",
    "text": "Sort merge join\n\nFor ideal performance of the sort merge join, all rows with the same join key(s) are available in the same partition. This can help with the infamous partition exchange (shuffle) between workers.\nCollocated partitions can avoid unnecessary data shuffle.\nData needs to be evenly distributed in the join keys, so that they can be equally distributed across the cluster to achieve the maximum parallelism from the available partitions."
  },
  {
    "objectID": "core/slides/slides07_spark-deeper-dive.html#other-join-types",
    "href": "core/slides/slides07_spark-deeper-dive.html#other-join-types",
    "title": "Spark deeper dive",
    "section": "Other join types",
    "text": "Other join types\nThere are other join types, such as BroadcastNestedLoopJoin in weird situations where no joining keys are specified and either there is a broadcast hint or the size of a table is &lt; autoBroadcastJoinThreshold.\nIn words: don’t use these, if you see these in an execution plan or in the Spark UI, it usually means that something has been done poorly."
  },
  {
    "objectID": "core/slides/slides07_spark-deeper-dive.html#take-home-messages-on-joins",
    "href": "core/slides/slides07_spark-deeper-dive.html#take-home-messages-on-joins",
    "title": "Spark deeper dive",
    "section": "Take home messages on joins",
    "text": "Take home messages on joins\n\nSort merge join is the default join and performs well in most of the scenarios.\nIn some cases, if you are confident enough that shuffle hash join is better than sort merge join, you can disable sort merge join for those scenarios.\nTune spark.sql.autoBroadcastJoinThreshold accordingly if deemed necessary. Try to use broadcast joins wherever possible and filter out the irrelevant rows to the join key before the join to avoid unnecessary data shuffling.\nJoins without unique join keys or no join keys can often be very expensive and should be avoided."
  },
  {
    "objectID": "core/slides/slides07_spark-deeper-dive.html#how-do-i-know-when-a-shuffle-will-occur",
    "href": "core/slides/slides07_spark-deeper-dive.html#how-do-i-know-when-a-shuffle-will-occur",
    "title": "Spark deeper dive",
    "section": "How do I know when a shuffle will occur?",
    "text": "How do I know when a shuffle will occur?\nRule of thumb: a shuffle can occur when the resulting data depends on other data (can be the same or another RDD/DataFrame).\nWe can also figure out if a shuffle has been planned or executed via:\n\nThe return type of certain transformations (in scala only)\nBy looking at the Spark UI\nUsing toDebugString on a RDD to see its execution plan:\n\n&gt;&gt;&gt; print(pairs.toDebugString().decode(\"utf-8\"))\n(3) PythonRDD[157] at RDD at PythonRDD.scala:53 [Memory Serialized 1x Replicated]\n |       CachedPartitions: 3; MemorySize: 233.0 B; ExternalBlockStoreSize: 0.0 B; DiskSize: 0.0 B\n |  MapPartitionsRDD[156] at mapPartitions at PythonRDD.scala:133 [Memory Serialized 1x Replicated]\n |  ShuffledRDD[155] at partitionBy at &lt;unknown&gt;:0 [Memory Serialized 1x Replicated]\n +-(8) PairwiseRDD[154] at sortByKey at &lt;ipython-input-35-112008c310ec&gt;:2 [Memory Serialized 1x Replicated]\n    |  PythonRDD[153] at sortByKey at &lt;ipython-input-35-112008c310ec&gt;:2 [Memory Serialized 1x Replicated]\n    |  ParallelCollectionRDD[0] at parallelize at PythonRDD.scala:195 [Memory Serialized 1x Replicated]"
  },
  {
    "objectID": "core/slides/slides07_spark-deeper-dive.html#how-do-i-know-when-a-shuffle-will-occur-1",
    "href": "core/slides/slides07_spark-deeper-dive.html#how-do-i-know-when-a-shuffle-will-occur-1",
    "title": "Spark deeper dive",
    "section": "How do I know when a shuffle will occur?",
    "text": "How do I know when a shuffle will occur?\nOperations that might cause a shuffle:\n\ncogroup, groupWith, join, leftOuterJoin, rightOuterJoin, groupByKey, reduceByKey, combineByKey, distinct, intersection, repartition, coalesce\n\nWhen can we avoid shuffles using partitioning ?\n\nreduceByKey running on a pre-partitioned RDD will cause the values to be computed locally, requiring only the final reduced value to be sent to the driver.\njoin called on 2 RDDs that are pre-partitioned with the same partitioner and cached on the same machine will cause the join to be computed locally, with no shuffling across the network."
  },
  {
    "objectID": "core/slides/slides07_spark-deeper-dive.html#wide-versus-narrow-dependencies-1",
    "href": "core/slides/slides07_spark-deeper-dive.html#wide-versus-narrow-dependencies-1",
    "title": "Spark deeper dive",
    "section": "Wide versus narrow dependencies",
    "text": "Wide versus narrow dependencies\n\nWe have seen that some transformations are significantly more expensive (latency) than others\nThis is often explained by wide versus narrow dependencies, which dictate relationships between RDDs in graphs of computation, that has a lot to do with shuffling"
  },
  {
    "objectID": "core/slides/slides07_spark-deeper-dive.html#lineages",
    "href": "core/slides/slides07_spark-deeper-dive.html#lineages",
    "title": "Spark deeper dive",
    "section": "Lineages",
    "text": "Lineages\n\nComputations on RDDs are represented as a lineage graph: a DAG representing the computations done on the RDD.\nThis DAG is what Spark analyzes to do optimizations. Thanks to this, it is possible for an operation to step back and figure out how a result is derived from a particular point."
  },
  {
    "objectID": "core/slides/slides07_spark-deeper-dive.html#wide-versus-narrow-dependencies-2",
    "href": "core/slides/slides07_spark-deeper-dive.html#wide-versus-narrow-dependencies-2",
    "title": "Spark deeper dive",
    "section": "Wide versus narrow dependencies",
    "text": "Wide versus narrow dependencies\n\n\nExample\nrdd = sc.textFile(...)\n\nfiltered = (\n  rdd.map(...)\n     .filter(...)\n     .persist()\n)\n\ncount = filtered.count()\nreduced = filtered.reduce()"
  },
  {
    "objectID": "core/slides/slides07_spark-deeper-dive.html#wide-versus-narrow-dependencies-3",
    "href": "core/slides/slides07_spark-deeper-dive.html#wide-versus-narrow-dependencies-3",
    "title": "Spark deeper dive",
    "section": "Wide versus narrow dependencies",
    "text": "Wide versus narrow dependencies\n\n\nRDDs are made up of 4 parts:\n\nPartitions: atomic pieces of the dataset. One or many per worker\nDependencies: models relationship between this RDD and its partitions with the RDD(s) it was derived from (dependencies maybe modeled per partition)\nA function for computing the dataset based on its parent RDDs\nMetadata about partitioning scheme and data placement."
  },
  {
    "objectID": "core/slides/slides07_spark-deeper-dive.html#wide-versus-narrow-dependencies-4",
    "href": "core/slides/slides07_spark-deeper-dive.html#wide-versus-narrow-dependencies-4",
    "title": "Spark deeper dive",
    "section": "Wide versus narrow dependencies",
    "text": "Wide versus narrow dependencies\nRDD dependencies and shuffles\n\nWe saw a rule of thumb: a shuffle can occur when the resulting RDD depends on other elements from the same RDD or another RDD.\nIn fact, RDD dependencies encode when data must be shuffled.\n\nTransformations cause shuffles, and can have 2 kinds of dependencies:\n\nNarrow dependencies: each partition of the parent RDD is used by at most one partition of the child RDD.\n\n[parent RDD partition] --&gt; [child RDD partition]\nFast! No shuffle necessary. Optimizations like pipelining possible.\nTransformations with narrow dependencies are fast."
  },
  {
    "objectID": "core/slides/slides07_spark-deeper-dive.html#wide-versus-narrow-dependencies-5",
    "href": "core/slides/slides07_spark-deeper-dive.html#wide-versus-narrow-dependencies-5",
    "title": "Spark deeper dive",
    "section": "Wide versus narrow dependencies",
    "text": "Wide versus narrow dependencies\n\nWide dependencies: each partition of the parent RDD may be used by multiple child partitions\n\n                           ---&gt; [child RDD partition 1]\n[parent RDD partition] ---&gt; [child RDD partition 2]\n                           ---&gt; [child RDD partition 3]\nSlow! Shuffle necessary for all or some data over the network.\nTransformations with wide dependencies are slow."
  },
  {
    "objectID": "core/slides/slides07_spark-deeper-dive.html#wide-versus-narrow-dependencies-6",
    "href": "core/slides/slides07_spark-deeper-dive.html#wide-versus-narrow-dependencies-6",
    "title": "Spark deeper dive",
    "section": "Wide versus narrow dependencies",
    "text": "Wide versus narrow dependencies"
  },
  {
    "objectID": "core/slides/slides07_spark-deeper-dive.html#wide-versus-narrow-dependencies-7",
    "href": "core/slides/slides07_spark-deeper-dive.html#wide-versus-narrow-dependencies-7",
    "title": "Spark deeper dive",
    "section": "Wide versus narrow dependencies",
    "text": "Wide versus narrow dependencies\nAssume that we have a following DAG:\n]"
  },
  {
    "objectID": "core/slides/slides07_spark-deeper-dive.html#wide-versus-narrow-dependencies-8",
    "href": "core/slides/slides07_spark-deeper-dive.html#wide-versus-narrow-dependencies-8",
    "title": "Spark deeper dive",
    "section": "Wide versus narrow dependencies",
    "text": "Wide versus narrow dependencies\nWhat do the dependencies look like? Which is wide and which is narrow?\n\n\nThe B to G join is narrow because groupByKey already partitions the keys and places them appropriately in B.\nThus, join operations can be narrow or wide depending on lineage"
  },
  {
    "objectID": "core/slides/slides07_spark-deeper-dive.html#wide-versus-narrow-dependencies-9",
    "href": "core/slides/slides07_spark-deeper-dive.html#wide-versus-narrow-dependencies-9",
    "title": "Spark deeper dive",
    "section": "Wide versus narrow dependencies",
    "text": "Wide versus narrow dependencies\nTransformations with (usually) narrow dependencies:\n\nmap, mapValues, flatMap, filter, mapPartitions, mapPartitionsWithIndex\n\nTransformations with (usually) wide dependencies (might cause a shuffle):\n\ncogroup, groupWith, join, leftOuterJoin, rightOuterJoin, groupByKey, reduceByKey, combineByKey, distinct, intersection, repartition, coalesce\n\nThese list are usually correct, but as seen above for join, a correct answer depends on lineage"
  },
  {
    "objectID": "core/slides/slides07_spark-deeper-dive.html#wide-versus-narrow-dependencies-10",
    "href": "core/slides/slides07_spark-deeper-dive.html#wide-versus-narrow-dependencies-10",
    "title": "Spark deeper dive",
    "section": "Wide versus narrow dependencies",
    "text": "Wide versus narrow dependencies\nHow do we find out if an operation is wide or narrow?\n\nMonitor the job with the Spark UI and check if ShuffleRDD are used.\nUse the toDebugString method. It prints the RDD lineage along with other information relevant to scheduling. Indentations separate groups of narrow transformations that may be pipelined together with wide transformations that require shuffles. These groupings are called stages.\n\n&gt;&gt;&gt; print(pairs.toDebugString().decode(\"utf-8\"))\n(3) PythonRDD[157] at RDD at PythonRDD.scala:53 [Memory Serialized 1x Replicated]\n |       CachedPartitions: 3; MemorySize: 233.0 B; ExternalBlockStoreSize: 0.0 B; DiskSize: 0.0 B\n |  MapPartitionsRDD[156] at mapPartitions at PythonRDD.scala:133 [Memory Serialized 1x Replicated]\n |  ShuffledRDD[155] at partitionBy at &lt;unknown&gt;:0 [Memory Serialized 1x Replicated]\n +-(8) PairwiseRDD[154] at sortByKey at &lt;ipython-input-35-112008c310ec&gt;:2 [Memory Serialized 1x Replicated]\n    |  PythonRDD[153] at sortByKey at &lt;ipython-input-35-112008c310ec&gt;:2 [Memory Serialized 1x Replicated]\n    |  ParallelCollectionRDD[0] at parallelize at PythonRDD.scala:195 [Memory Serialized 1x Replicated]"
  },
  {
    "objectID": "core/slides/slides07_spark-deeper-dive.html#wide-versus-narrow-dependencies-11",
    "href": "core/slides/slides07_spark-deeper-dive.html#wide-versus-narrow-dependencies-11",
    "title": "Spark deeper dive",
    "section": "Wide versus narrow dependencies",
    "text": "Wide versus narrow dependencies\n\nCheck the dependencies (only with scala and java APIs): there is a dependencies method on RDDs. It returns a sequence of Dependency objects, which are the dependencies used by Spark’s scheduler to know how this RDD depends on other (or itself) RDDs.\n\nThe types of dependency objects that this method may return include:\n\nNarrow dependency objects: OneToOneDependency, PruneDependency, RangeDependency\nWide dependency objects: ShuffleDependency\n\nExample in scala:\nval wordsRDD = sc.parallelize(largeList)\nval pairs = wordsRdd.map(c=&gt;(c,1))\n                    .groupByKey\n                    .dependencies\n// pairs: Seq[org.apache.spark.Dependency[_]] \n//   = List(org.apache.spark.ShuffleDependency@4294a23d)"
  },
  {
    "objectID": "core/slides/slides07_spark-deeper-dive.html#wide-versus-narrow-dependencies-12",
    "href": "core/slides/slides07_spark-deeper-dive.html#wide-versus-narrow-dependencies-12",
    "title": "Spark deeper dive",
    "section": "Wide versus narrow dependencies",
    "text": "Wide versus narrow dependencies\nAlso, toDebugString is more precise with the scala API:\nval pairs = wordsRdd.map(c=&gt;(c,1))\n                    .groupByKey\n                    .toDebugString\n// pairs: String =\n// (8) ShuffledRDD[219] at groupByKey at &lt;console&gt;:38 []\n//  +-(8) MapPartitionsRDD[218] at map at &lt;console&gt;:37 []\n//     | ParallelCollectionRDD[217] at parallelize at &lt;console&gt;:36 []\nWe can see immediately that a ShuffledRDD is used"
  },
  {
    "objectID": "core/slides/slides07_spark-deeper-dive.html#lineage-allows-fault-tolerance-1",
    "href": "core/slides/slides07_spark-deeper-dive.html#lineage-allows-fault-tolerance-1",
    "title": "Spark deeper dive",
    "section": "Lineage allows fault tolerance",
    "text": "Lineage allows fault tolerance\nLineages are the key to fault tolerance in Spark\nIdeas from functional programming enable fault tolerance in Spark:\n\nRDDs are immutable\nUse higher order functions like map, flatMap, filter to do functional transformations on this immutable data\nA function for computing an RDD based on its parent RDDs is part the RDD’s representation\n\nThis is all done in Spark RDDs: a by product of these ideas is fault tolerance:\n\nWe just need to keep the information required by these 3 properties.\nRecovering from failures is simply achieved by recomputing lost partitions using the lineage graph"
  },
  {
    "objectID": "core/slides/slides07_spark-deeper-dive.html#lineage-allows-fault-tolerance-2",
    "href": "core/slides/slides07_spark-deeper-dive.html#lineage-allows-fault-tolerance-2",
    "title": "Spark deeper dive",
    "section": "Lineage allows fault tolerance",
    "text": "Lineage allows fault tolerance\nAren’t you amazed by this ?!?\n\nSpark provides fault tolerance without having to write data on disk!\nData can be rebuilt using the above information.\n\n\n\nIf a partition fails:\n\n\nSpark recomputes it to get back on track:"
  },
  {
    "objectID": "core/slides/slides07_spark-deeper-dive.html#catalyst-optimizer",
    "href": "core/slides/slides07_spark-deeper-dive.html#catalyst-optimizer",
    "title": "Spark deeper dive",
    "section": "Catalyst optimizer",
    "text": "Catalyst optimizer\nWhat is the Catalyst optimizer ?\n\n\nAn optimizer that automatically finds out the most efficient plan to execute data operations specified in the user’s program.\nIt translates transformations used to build the dataset to an optimized physical plan of execution, which is a DAG of low-level operations on RDDs.\nA precious tool for spark.sql in terms of performance. It understands the structure of the data used and of the operations made on it, so the optimizer can make some decisions helping to reduce execution time."
  },
  {
    "objectID": "core/slides/slides07_spark-deeper-dive.html#catalyst-optimizer-1",
    "href": "core/slides/slides07_spark-deeper-dive.html#catalyst-optimizer-1",
    "title": "Spark deeper dive",
    "section": "Catalyst optimizer",
    "text": "Catalyst optimizer\nLet’s first define some terminology used in the optimizer\n\n\nLogical plan: series of algebraic or language constructs, for example: SELECT, GROUP BY, UNION, etc. Usually represented as a DAG where nodes are the constructs.\nPhysical plan: similar to the logical plan, also represented by a DAG but concerning low-level operations (operations on RDDs).\nUnoptimized/optimized plans: a plan becomes optimized when the optimizer passed on it and made some optimizations, such as merging filter() methods, replacing some instructions by faster another ones, etc.\n\n\nCatalyst helps to move from an unoptimized logical query plan to an optimized physical plan"
  },
  {
    "objectID": "core/slides/slides07_spark-deeper-dive.html#catalyst-optimizer-how-it-works",
    "href": "core/slides/slides07_spark-deeper-dive.html#catalyst-optimizer-how-it-works",
    "title": "Spark deeper dive",
    "section": "Catalyst optimizer: how it works?",
    "text": "Catalyst optimizer: how it works?"
  },
  {
    "objectID": "core/slides/slides07_spark-deeper-dive.html#catalyst-optimizer-how-it-works-1",
    "href": "core/slides/slides07_spark-deeper-dive.html#catalyst-optimizer-how-it-works-1",
    "title": "Spark deeper dive",
    "section": "Catalyst optimizer: how it works?",
    "text": "Catalyst optimizer: how it works?\n\n\nTry to optimize logical plan through predefined rule-based optimizations. Some optimizations are:\n\npredicate or projection pushdown, helps to eliminate data not respecting preconditions earlier in the computation;\nrearrange filter;\nconversion of decimals operations to long integer operations;\nreplacement of some RegEx expressions by Java’s methods startsWith or contains;\nif-else clauses simplification.\n\nCreate the optimized logical plan.\nConstruct multiple physical plans from the optimized logical plan. These are also optimized, some examples are: merging different filter(), sending predicate/projection pushdown to the data source to eliminate data directly from the source."
  },
  {
    "objectID": "core/slides/slides07_spark-deeper-dive.html#catalyst-optimizer-how-it-works-2",
    "href": "core/slides/slides07_spark-deeper-dive.html#catalyst-optimizer-how-it-works-2",
    "title": "Spark deeper dive",
    "section": "Catalyst optimizer: how it works?",
    "text": "Catalyst optimizer: how it works?\n\n\nDetermine which physical plan has the lowest cost of execution and choses it as the physical plan used for the computation.\nGenerate bytecode for the best physical plan thanks to a scala feature called quasiquotes.\nOnce a physical plan is defined, it’s executed and retrieved data is put to the output DataFrame."
  },
  {
    "objectID": "core/slides/slides07_spark-deeper-dive.html#catalyst-optimizer-2",
    "href": "core/slides/slides07_spark-deeper-dive.html#catalyst-optimizer-2",
    "title": "Spark deeper dive",
    "section": "Catalyst optimizer",
    "text": "Catalyst optimizer\nLet’s understand how Catalyst optimizer works for a given query"
  },
  {
    "objectID": "core/slides/slides07_spark-deeper-dive.html#catalyst-optimizer-3",
    "href": "core/slides/slides07_spark-deeper-dive.html#catalyst-optimizer-3",
    "title": "Spark deeper dive",
    "section": "Catalyst optimizer",
    "text": "Catalyst optimizer"
  },
  {
    "objectID": "core/slides/slides07_spark-deeper-dive.html#catalyst-optimizer-4",
    "href": "core/slides/slides07_spark-deeper-dive.html#catalyst-optimizer-4",
    "title": "Spark deeper dive",
    "section": "Catalyst optimizer",
    "text": "Catalyst optimizer"
  },
  {
    "objectID": "core/slides/slides07_spark-deeper-dive.html#catalyst-optimizer-5",
    "href": "core/slides/slides07_spark-deeper-dive.html#catalyst-optimizer-5",
    "title": "Spark deeper dive",
    "section": "Catalyst optimizer",
    "text": "Catalyst optimizer"
  },
  {
    "objectID": "core/slides/slides07_spark-deeper-dive.html#catalyst-optimizer-6",
    "href": "core/slides/slides07_spark-deeper-dive.html#catalyst-optimizer-6",
    "title": "Spark deeper dive",
    "section": "Catalyst optimizer",
    "text": "Catalyst optimizer"
  },
  {
    "objectID": "core/slides/slides07_spark-deeper-dive.html#catalyst-optimizer-7",
    "href": "core/slides/slides07_spark-deeper-dive.html#catalyst-optimizer-7",
    "title": "Spark deeper dive",
    "section": "Catalyst optimizer",
    "text": "Catalyst optimizer"
  },
  {
    "objectID": "core/slides/slides07_spark-deeper-dive.html#catalyst-optimizer-8",
    "href": "core/slides/slides07_spark-deeper-dive.html#catalyst-optimizer-8",
    "title": "Spark deeper dive",
    "section": "Catalyst optimizer",
    "text": "Catalyst optimizer"
  },
  {
    "objectID": "core/slides/slides07_spark-deeper-dive.html#catalyst-optimizer-9",
    "href": "core/slides/slides07_spark-deeper-dive.html#catalyst-optimizer-9",
    "title": "Spark deeper dive",
    "section": "Catalyst optimizer",
    "text": "Catalyst optimizer"
  },
  {
    "objectID": "core/slides/slides07_spark-deeper-dive.html#catalyst-optimizer-10",
    "href": "core/slides/slides07_spark-deeper-dive.html#catalyst-optimizer-10",
    "title": "Spark deeper dive",
    "section": "Catalyst optimizer",
    "text": "Catalyst optimizer"
  },
  {
    "objectID": "core/slides/slides07_spark-deeper-dive.html#catalyst-optimizer-11",
    "href": "core/slides/slides07_spark-deeper-dive.html#catalyst-optimizer-11",
    "title": "Spark deeper dive",
    "section": "Catalyst optimizer",
    "text": "Catalyst optimizer"
  },
  {
    "objectID": "core/slides/slides07_spark-deeper-dive.html#catalyst-optimizer-12",
    "href": "core/slides/slides07_spark-deeper-dive.html#catalyst-optimizer-12",
    "title": "Spark deeper dive",
    "section": "Catalyst optimizer",
    "text": "Catalyst optimizer"
  },
  {
    "objectID": "core/slides/slides07_spark-deeper-dive.html#catalyst-optimizer-13",
    "href": "core/slides/slides07_spark-deeper-dive.html#catalyst-optimizer-13",
    "title": "Spark deeper dive",
    "section": "Catalyst optimizer",
    "text": "Catalyst optimizer\n\n\nBy performing these transformations, Catalyst improves the execution times of relational queries and mitigates the importance of semantics\nCatalyst makes use of some powerful functional programming features from Scala to allow developers to concisely specify complex relational optimizations.\nCatalyst helps but only when it can: explicit schemas, precise function calls, clever order of operations can only help Catalyst."
  },
  {
    "objectID": "core/slides/slides08_spark-tips.html#tip-1.-use-dataframes-instead-of-rdds",
    "href": "core/slides/slides08_spark-tips.html#tip-1.-use-dataframes-instead-of-rdds",
    "title": "Spark tips",
    "section": "Tip 1. Use DataFrames instead of RDDs",
    "text": "Tip 1. Use DataFrames instead of RDDs\n\nInstead of using the RDD API\n\nrdd = sc.textFile(\"/path/to/file.txt\")\n\nUse the DataFrame API\n\ndf = spark.read.textFile(\"/path/to/file.txt\")\n\nThe DataFrame API uses the * Catalyst* optimizer to improve the execution plan of your Spark Job\nThe low-level RDD API does not\nMost of the recent Spark advances are towards an improvement of the SQL"
  },
  {
    "objectID": "core/slides/slides08_spark-tips.html#tip-2.-avoid-using-regular-expressions",
    "href": "core/slides/slides08_spark-tips.html#tip-2.-avoid-using-regular-expressions",
    "title": "Spark tips",
    "section": "Tip 2. Avoid using regular expressions",
    "text": "Tip 2. Avoid using regular expressions\n\nJava Regex is great to parse data in an expected structure\nBut, unfortunately, it is generally a slow process when processing millions of rows\nIncreasing a little bit the parsing of rows increases a lot the entire job\nIf possible, avoid using Regex’s and try to load your data in a more structured format\n\n\nhttps://docs.python.org/3/library/re.html\nhttps://docs.python.org/fr/3/howto/regex.html"
  },
  {
    "objectID": "core/slides/slides08_spark-tips.html#tip-3.-joins-largest-dataset-on-the-left",
    "href": "core/slides/slides08_spark-tips.html#tip-3.-joins-largest-dataset-on-the-left",
    "title": "Spark tips",
    "section": "Tip 3. Joins: largest dataset on the left",
    "text": "Tip 3. Joins: largest dataset on the left\n\nWhen joining two datasets where one is smaller than the other, you must put the largest on the left\n\njoinedDF = largeDF.join(smallDF, on=\"id\")\n\nThe data specified on the left is static on the executors while the data on the right is transfered between the executors\nSomething like\n\njoinedDF = smallDF.join(largeDF, on=\"id\")\ncan be much longer or even fail if largeDF is large"
  },
  {
    "objectID": "core/slides/slides08_spark-tips.html#tip-4.-joins-use-broadcast-joining",
    "href": "core/slides/slides08_spark-tips.html#tip-4.-joins-use-broadcast-joining",
    "title": "Spark tips",
    "section": "Tip 4. Joins: use broadcast joining",
    "text": "Tip 4. Joins: use broadcast joining\n\nOften, we need to join a huge dataframe with a small one\nUse broadcast joins for joining small datasets to larger ones\n\nfrom pyspark.sql.functions import broadcast\n\njoinedDF = largeDF.join(broadcast(smallDF), on=\"id\")\n\nUsually leads to much faster joins since is allows to avoid shuffles"
  },
  {
    "objectID": "core/slides/slides08_spark-tips.html#tip-5.-use-caching-when-repeating-queries",
    "href": "core/slides/slides08_spark-tips.html#tip-5.-use-caching-when-repeating-queries",
    "title": "Spark tips",
    "section": "Tip 5. Use caching when repeating queries",
    "text": "Tip 5. Use caching when repeating queries\n\nIf you are constantly using the same DataFrame on multiple queries, you can use caching or persistence:\n\ndf = (\n  spark\n    .read\n    .textFile(\"/path/to/file.txt\")\n    .cache()\n)\n\nBut avoid overusing this. Depending on caching strategy (in-memory then swap to disk), cache can end up being slower than reading\nStorage space used for caching means less space for processing\nCaching can cost more than reading the DataFrame (e.g. only few columns are useful, predictate pushdown)"
  },
  {
    "objectID": "core/slides/slides08_spark-tips.html#tip-6.-compute-statistics-of-tables",
    "href": "core/slides/slides08_spark-tips.html#tip-6.-compute-statistics-of-tables",
    "title": "Spark tips",
    "section": "Tip 6. COMPUTE STATISTICS of tables",
    "text": "Tip 6. COMPUTE STATISTICS of tables\n\nBefore querying a table, it can be helpful to compute the statistics of those tables so that Catalyst can find a better plan to process it:\n\nquery = \"ANALYZE TABLE db.table COMPUTE STATISTICS\"\nspark.sql(query)\n\nHowever, Spark does not always get everything it needs just from the above broad COMPUTE STATISTICS call"
  },
  {
    "objectID": "core/slides/slides08_spark-tips.html#tip-6.-compute-statistics-of-tables-1",
    "href": "core/slides/slides08_spark-tips.html#tip-6.-compute-statistics-of-tables-1",
    "title": "Spark tips",
    "section": "Tip 6. COMPUTE STATISTICS of tables",
    "text": "Tip 6. COMPUTE STATISTICS of tables\n\nAlso helps to check specific columns so that Catalyst can better check those columns\nIt’s recommended to COMPUTE STATISTICS for any columns involved in filtering and joining :\n\nquery = \"ANALYZE TABLE db.table COMPUTE STATISTICS\"\n            \" FOR COLUMNS joinColumn, filterColumn\"\n\nspark.sql(query)"
  },
  {
    "objectID": "core/slides/slides08_spark-tips.html#tip-7.-shuffles-know-your-data",
    "href": "core/slides/slides08_spark-tips.html#tip-7.-shuffles-know-your-data",
    "title": "Spark tips",
    "section": "Tip 7. Shuffles: know your data",
    "text": "Tip 7. Shuffles: know your data\n\nShuffle is the transportation of data between workers across a Spark cluster’s network\nIt’s central for operations where a reorganization of data is required, referred to as wide dependencies (wide vs narrow dependencies)\nThis kind of operation usually is the bottleneck of your Spark application\nTo use Spark well, you need to know what you shuffle, and for this it’s essential that you know your data"
  },
  {
    "objectID": "core/slides/slides08_spark-tips.html#tip-8.-shuffles-beware-of-skews",
    "href": "core/slides/slides08_spark-tips.html#tip-8.-shuffles-beware-of-skews",
    "title": "Spark tips",
    "section": "Tip 8. Shuffles: beware of skews",
    "text": "Tip 8. Shuffles: beware of skews\n\nSkew is an imbalance in the distribution of your data\nIf you fail to account for how your data is distributed, you may find that Spark naively places an overwhelming majority of rows on one executor, and a fraction on all the rest\nThis is skew, and it will kill your application\nWhether by causing out of memory errors, network timeouts, or exponentially long running processes that will never terminate"
  },
  {
    "objectID": "core/slides/slides08_spark-tips.html#tip-9.-partitions-change-the-default",
    "href": "core/slides/slides08_spark-tips.html#tip-9.-partitions-change-the-default",
    "title": "Spark tips",
    "section": "Tip 9. Partitions: change the default",
    "text": "Tip 9. Partitions: change the default\n\nIt’s absolutely essential to model the number of partitions around the kind of stuff you’re solving\nThe default value for spark.sql.shuffle.partitions is 200. It controls the number of partitions used by shuffles (= number of partitions in the resulting DataFrame of RDD).\nNumber of shuffle partitions does not change with different data size. For small data, 200 is overkill, for large data, it does not effectively use the all resources.\nRule of thumb: set this configuration to the number of cores you have available across all your executors\n\n(\n  spark\n    .conf\n    .set(\"spark.sql.shuffle.partitions\", 42)\n)"
  },
  {
    "objectID": "core/slides/slides08_spark-tips.html#tip-10.-partitions-well-distributed-columns",
    "href": "core/slides/slides08_spark-tips.html#tip-10.-partitions-well-distributed-columns",
    "title": "Spark tips",
    "section": "Tip 10. Partitions: well-distributed columns",
    "text": "Tip 10. Partitions: well-distributed columns\n\nA powerful way to control Spark shuffles is to partition your data intelligently\nPartitioning on the right column (or set of columns) helps to balance the amount of data mapped across the cluster network in order to perform actions\nPartitioning on a unique ID is generally a good strategy, but don’t partition on sparsely filled columns (with many NAs) or columns that over-represent particular values"
  },
  {
    "objectID": "core/slides/slides08_spark-tips.html#tip-11.-joins-again-highly-flammable",
    "href": "core/slides/slides08_spark-tips.html#tip-11.-joins-again-highly-flammable",
    "title": "Spark tips",
    "section": "Tip 11. Joins again: highly flammable",
    "text": "Tip 11. Joins again: highly flammable\n\nJoins are shuffle offenders. Dangers of SQL joining are amplified by the scale enabled by Spark\nEven joining medium sized data can cause an explosion if there are repeated join values on both sides of your join\nMillion rows datasets with “pseudo unique” keys can explode into a billions rows join!\nJoin columns with null values usually means massive skew and an explosive join\nA solution is to pre-fill empty cells to arbitrary balanced values (e.g. uniform random values) before running a join"
  },
  {
    "objectID": "core/slides/slides08_spark-tips.html#tip-12.-is-your-data-real-yet",
    "href": "core/slides/slides08_spark-tips.html#tip-12.-is-your-data-real-yet",
    "title": "Spark tips",
    "section": "Tip 12. Is your data real yet?",
    "text": "Tip 12. Is your data real yet?\n\nDon’t forget that operations in Spark are divided between transformations and actions. Transformations are lazy operations allowing Spark to optimize your query\nTransformations set up a DataFrame for changes (adding a column, joining it to another, etc.) but will not execute these until an action is performed.\nThis can result in surprising results: imagine that you create an id column using monotonically_increasing_id, and then join on that column. If you do not place an action in between, your values have not been materialized. The result will be non-deterministic!"
  },
  {
    "objectID": "core/slides/slides08_spark-tips.html#tip-13.-checkpointing-is-your-friend",
    "href": "core/slides/slides08_spark-tips.html#tip-13.-checkpointing-is-your-friend",
    "title": "Spark tips",
    "section": "Tip 13. Checkpointing is your friend",
    "text": "Tip 13. Checkpointing is your friend\n\nCheckpointing means saving data to disk and reloading it back in, which is redundant anywhere else besides Spark.\nIt triggers an action on any waiting transformations, and truncates the Spark query plan for the checkpointed data.\nThis action shows up in your Spark UI, indicating where you are in your job.\nIt can help to conserve resources, since it can release memory that would otherwise be cached for downstream access.\nCheckpointed data is also a valuable source for data-debugging."
  },
  {
    "objectID": "core/slides/slides08_spark-tips.html#tip-14.-check-your-runtime-with-monitoring",
    "href": "core/slides/slides08_spark-tips.html#tip-14.-check-your-runtime-with-monitoring",
    "title": "Spark tips",
    "section": "Tip 14. Check your runtime with monitoring",
    "text": "Tip 14. Check your runtime with monitoring\n\nSpark UI is your friend, and so are other monitoring tools that let you know how your run is going in real-time.\nThe Spark UI contains information on the job level, the stage level, and the executor level. You can see if the volume of data going to each partition or each executor makes sense, if some part of your job is taking too much time.\nSuch a monitoring tool allowing to view your total memory and CPU usage across executors is essential for resource planning and “autopsies” on failed jobs."
  },
  {
    "objectID": "core/slides/slides08_spark-tips.html#tip-15.-csv-reading-is-brittle",
    "href": "core/slides/slides08_spark-tips.html#tip-15.-csv-reading-is-brittle",
    "title": "Spark tips",
    "section": "Tip 15. CSV reading is brittle",
    "text": "Tip 15. CSV reading is brittle\n\nNaively reading CSVs in Spark can result in silent escape-character errors\n\ndf = spark.read.csv(\"quote-happy.csv\")\n\nYour DataFrame seems happy: no runtime exceptions, and you can execute operations on the DataFrame\nBut after careful debugging, you realize that at some point in the data, everything has shifted over one or several columns!\nTo be safe, you can include escape and quote options in your reads. Even better: use Parquet instead of CSV files!"
  },
  {
    "objectID": "core/slides/slides08_spark-tips.html#tip-16.-parquet-is-your-friend",
    "href": "core/slides/slides08_spark-tips.html#tip-16.-parquet-is-your-friend",
    "title": "Spark tips",
    "section": "Tip 16. Parquet is your friend",
    "text": "Tip 16. Parquet is your friend\n\nRead/Write operations are order of magnitude more efficient with Parquet than with uncompressed CSV files\nParquet is “columnar”: reads only the columns required for a sql query and skip over those that are not requested.\nAnd also predicate pushdown operations on filtering operations: run queries only on relevant subsets of the values.\nSwitching from CSV to Parquet is the first thing you can do to improve performance.\nIf you are generating Parquet files from another format (using PyArrow, Pandas, etc.) be conscious that creating a single parquet file gives up a major benefit of the format: you need to partition it!"
  },
  {
    "objectID": "core/slides/slides08_spark-tips.html#tip-17.-problems-with-udfs",
    "href": "core/slides/slides08_spark-tips.html#tip-17.-problems-with-udfs",
    "title": "Spark tips",
    "section": "Tip 17. Problems with UDFs",
    "text": "Tip 17. Problems with UDFs\nUDF = User Defined Function = something very convenient\n&gt;&gt;&gt; from pyspark.sql import functions as F, types as T\n\n&gt;&gt;&gt; data = [{'a': 1, 'b': 0}, {'a': 10, 'b': 3}]\n&gt;&gt;&gt; df = spark.createDataFrame(data)\n\n&gt;&gt;&gt; def calculate_a_b_ratio(a, b):\n&gt;&gt;&gt;     if b &gt; 0:\n&gt;&gt;&gt;         return a / b\n&gt;&gt;&gt;     return 0.\n\n&gt;&gt;&gt; udf_ratio_calculation = F.udf(calculate_a_b_ratio, T.FloatType())\n\n&gt;&gt;&gt; df = df.withColumn('a_b_ratio_float', udf_ratio_calculation('a', 'b'))\n&gt;&gt;&gt; df.show()\n+---+---+---------------+\n|  a|  b|a_b_ratio_float|\n+---+---+---------------+\n|  1|  0|            0.0|\n| 10|  3|      3.3333333|\n+---+---+---------------+"
  },
  {
    "objectID": "core/slides/slides08_spark-tips.html#tip-17.-problems-with-udfs-1",
    "href": "core/slides/slides08_spark-tips.html#tip-17.-problems-with-udfs-1",
    "title": "Spark tips",
    "section": "Tip 17. Problems with UDFs",
    "text": "Tip 17. Problems with UDFs\nUDF are Excruciatingly slow with pyspark and spark won’t complain if the return type is incorrect and just return nulls\n&gt;&gt;&gt; udf_ratio_calculation = F.udf(calculate_a_b_ratio, T.DecimalType())\n&gt;&gt;&gt; df = df.withColumn('a_b_ratio_dec', udf_ratio_calculation('a', 'b'))\n&gt;&gt;&gt; df.show()\n+---+---+---------------+-------------+\n|  a|  b|a_b_ratio_float|a_b_ratio_dec|\n+---+---+---------------+-------------+\n|  1|  0|            0.0|         null|\n| 10|  3|      3.3333333|         null|\n+---+---+---------------+-------------+\n&gt;&gt;&gt; udf_ratio_calculation = F.udf(calculate_a_b_ratio, T.BooleanType())\n&gt;&gt;&gt; df = df.withColumn('a_b_ratio_bool', udf_ratio_calculation('a', 'b'))\n&gt;&gt;&gt; df.show()\n+---+---+---------------+-------------+--------------+\n|  a|  b|a_b_ratio_float|a_b_ratio_dec|a_b_ratio_bool|\n+---+---+---------------+-------------+--------------+\n|  1|  0|            0.0|         null|          null|\n| 10|  3|      3.3333333|         null|          null|\n+---+---+---------------+-------------+--------------+"
  },
  {
    "objectID": "core/slides/slides08_spark-tips.html#tip-18.-use-all-of-the-resources",
    "href": "core/slides/slides08_spark-tips.html#tip-18.-use-all-of-the-resources",
    "title": "Spark tips",
    "section": "Tip 18. Use all of the resources",
    "text": "Tip 18. Use all of the resources\n\nSpark driver memory and executor memory are set by default to 1 Go.\nIt is in general very useful to take a look at the many configuration parameters and their defaults:\n\nhttps://spark.apache.org/docs/latest/configuration.html\n\nMany things there that can influence your spark application\nWhen running locally, adjust spark.driver.memory to something that’s reasonable for your system, e.g. \"8g\"\nWhen running on a cluster, you might also want to tweak the spark.executor.memory (though it depends on your cluster and its configuration)."
  },
  {
    "objectID": "core/slides/slides08_spark-tips.html#tip-18.-use-all-of-the-resources-1",
    "href": "core/slides/slides08_spark-tips.html#tip-18.-use-all-of-the-resources-1",
    "title": "Spark tips",
    "section": "Tip 18. Use all of the resources",
    "text": "Tip 18. Use all of the resources\nfrom pyspark import SparkConf\nfrom pyspark.sql import SparkSession\n\nconf = SparkConf()\nconf.set('spark.executor.memory', '16g')\nconf.set('spark.driver.memory', '8g')\n\nspark_session = SparkSession.builder \\\n        .config(conf=conf) \\\n        .appName('Name') \\\n        .getOrCreate()"
  },
  {
    "objectID": "core/slides/slides08_spark-tips.html#interpret-error-messages-1",
    "href": "core/slides/slides08_spark-tips.html#interpret-error-messages-1",
    "title": "Spark tips",
    "section": "Interpret error messages",
    "text": "Interpret error messages\n\nError messages don’t mean what they say\nTakes quite a while to understand that Spark complains about one thing, when the problem is somewhere else\n\"Connection reset by peer\" often means that you have skews and one particular worker has run out of memory\n\"java.net.SocketTimeoutException: Write timed out\" can mean that the number of partitions too high, so that the filesystem is too slow to handle the number of simultaneous writes attempted by Spark"
  },
  {
    "objectID": "core/slides/slides08_spark-tips.html#interpret-error-messages-2",
    "href": "core/slides/slides08_spark-tips.html#interpret-error-messages-2",
    "title": "Spark tips",
    "section": "Interpret error messages",
    "text": "Interpret error messages\n\n\"Total size of serialized results[...] is bigger than spark.driver.maxResultSize\" can mean that the number of partitions is too high and results can’t fit onto a particular worker\n\"Column a is not a member of table b\": you have a sql join error. Try your job locally on a small sample to avoid reverse engineering of such errors\nSometimes you get a true \"out of memory\" error. You can increase the size of individual workers, but before you do that, ask yourself, is the data well distributed ?"
  },
  {
    "objectID": "core/slides/slides08_spark-tips.html#interpret-error-messages-3",
    "href": "core/slides/slides08_spark-tips.html#interpret-error-messages-3",
    "title": "Spark tips",
    "section": "Interpret error messages",
    "text": "Interpret error messages\n\n\"ClassNotFoundException\": usually when you are trying to connect your application to an external a database. Here is an example"
  },
  {
    "objectID": "core/slides/slides08_spark-tips.html#interpret-error-messages-4",
    "href": "core/slides/slides08_spark-tips.html#interpret-error-messages-4",
    "title": "Spark tips",
    "section": "Interpret error messages",
    "text": "Interpret error messages\n\nMeans that Spark cannot find the necessary jar driver to connect to the database\nNeed to provide the correct jars to your application using the spark configuration or as a command line argument\n\nfrom pyspark import SparkConf\nfrom pyspark.sql import SparkSession\n\njars = \"/full/path/to/postgres.jar,/full/path/to/other/jar\"\nconf = SparkConf()\nconf.set(\"spark.jars\", jars)\n\nspark = (\n  SparkSession\n    .builder\n    .config(conf=conf)\n    .appName('test')\n    .getOrCreate()\n)\nor\nspark-submit --jars /full/path/to/postgres.jar,/full/path/to/other/jar ..."
  },
  {
    "objectID": "core/slides/slides08_spark-tips.html#interpret-error-messages-5",
    "href": "core/slides/slides08_spark-tips.html#interpret-error-messages-5",
    "title": "Spark tips",
    "section": "Interpret error messages",
    "text": "Interpret error messages\n\nAll the jars must be accessible to all nodes and not local to the driver.\nThis error might also mean a Spark version mismatch between the cluster components\nMake sure there is no space between the commas in the list of jars."
  },
  {
    "objectID": "core/slides/slides08_spark-tips.html#interpret-error-messages-6",
    "href": "core/slides/slides08_spark-tips.html#interpret-error-messages-6",
    "title": "Spark tips",
    "section": "Interpret error messages",
    "text": "Interpret error messages\nTrying to connect to a database: \"java.sql.SQLException: No suitable driver\""
  },
  {
    "objectID": "core/slides/slides08_spark-tips.html#interpret-error-messages-7",
    "href": "core/slides/slides08_spark-tips.html#interpret-error-messages-7",
    "title": "Spark tips",
    "section": "Interpret error messages",
    "text": "Interpret error messages\nError happens while trying to save to a database: \"java.lang.NullPointerException\""
  },
  {
    "objectID": "core/slides/slides08_spark-tips.html#interpret-error-messages-8",
    "href": "core/slides/slides08_spark-tips.html#interpret-error-messages-8",
    "title": "Spark tips",
    "section": "Interpret error messages",
    "text": "Interpret error messages\nThis errors usually mean that we forgot to set the driver, \"org.postgresql.Driver\" for Postgres:\ndf = spark.read.format('jdbc').options(\n    url= 'db_url',\n    driver='org.postgresql.Driver',  # &lt;-- here\n    dbtable='table_name',\n    user='user',\n    password='password'\n).load()\nand also make sure that the drivers’ jars are set."
  },
  {
    "objectID": "core/slides/slides08_spark-tips.html#interpret-error-messages-9",
    "href": "core/slides/slides08_spark-tips.html#interpret-error-messages-9",
    "title": "Spark tips",
    "section": "Interpret error messages",
    "text": "Interpret error messages\nHorrible error : 'NoneType' object has no attribute '_jvm'\n\n…mainly comes from two mistakes"
  },
  {
    "objectID": "core/slides/slides08_spark-tips.html#interpret-error-messages-10",
    "href": "core/slides/slides08_spark-tips.html#interpret-error-messages-10",
    "title": "Spark tips",
    "section": "Interpret error messages",
    "text": "Interpret error messages\n\nYou are using pyspark functions without having an active spark session\n\nfrom pyspark.sql import SparkSession, functions as fn\n\nclass A(object):\n    def __init__(self):\n        self.calculations = fn.col('a') / fn.col('b')\n...\n# Instantiating A without an active spark session \n# will give you this error\na = A()"
  },
  {
    "objectID": "core/slides/slides08_spark-tips.html#interpret-error-messages-11",
    "href": "core/slides/slides08_spark-tips.html#interpret-error-messages-11",
    "title": "Spark tips",
    "section": "Interpret error messages",
    "text": "Interpret error messages\n\nYou are using pyspark functions within a UDF:\n\n# Create a dataframe\ndata = [{'a': 1, 'b': 0}, {'a': 10, 'b': 3}]\ndf = spark.createDataFrame(data)\n\n# Define a simple function that returns a / b\ndef calculate_a_b_max(a, b):\n    return F.max([a, b])\n\n# and a udf for this function - notice the return datatype\nudf_max_calculation = F.udf(calculate_a_b_ratio, T.FloatType())\n\ndf = df.withColumn('a_b_max', udf_max_calculation('a', 'b'))\n\ndf.show()\nWe CANNOT use pyspark functions inside a udf: a UDF operates on a row per row basis while pyspark functions on a column basis."
  },
  {
    "objectID": "core/slides/spark-standalone.html#section",
    "href": "core/slides/spark-standalone.html#section",
    "title": "Spark standalone",
    "section": "",
    "text": "The current working directory is where spark-3.5.0-bin-hadoop3 has been installed\n\n\n\n.\n├── bin\n├── conf\n├── data\n├── examples\n├── jars\n├── kubernetes\n├── LICENSE\n├── licenses\n├── logs\n├── NOTICE\n├── python\n├── R\n├── README.md\n├── RELEASE\n├── sbin\n├── work\n└── yarn"
  },
  {
    "objectID": "core/slides/spark-standalone.html#start-the-standalone-master-server",
    "href": "core/slides/spark-standalone.html#start-the-standalone-master-server",
    "title": "Spark standalone",
    "section": "Start the standalone master server",
    "text": "Start the standalone master server\n\n./sbin/start-master.sh\n\n\n\n\n\nWWW UI\n--webui-port 8080\nMonitoring of\n\nworkers\nrunning applications\ncompleted applications\n\n\n\nSpark Master at spark://&lt;host&gt;:&lt;port&gt;"
  },
  {
    "objectID": "core/slides/spark-standalone.html#starting-a-worker",
    "href": "core/slides/spark-standalone.html#starting-a-worker",
    "title": "Spark standalone",
    "section": "Starting a worker",
    "text": "Starting a worker\n\n./sbin/start-worker.sh spark://boucheron-Precision-5480:7077"
  },
  {
    "objectID": "core/slides/spark-standalone.html#back-to-the-web-ui",
    "href": "core/slides/spark-standalone.html#back-to-the-web-ui",
    "title": "Spark standalone",
    "section": "Back to the Web UI",
    "text": "Back to the Web UI"
  },
  {
    "objectID": "core/slides/spark-standalone.html#starting-workers",
    "href": "core/slides/spark-standalone.html#starting-workers",
    "title": "Spark standalone",
    "section": "Starting workers",
    "text": "Starting workers"
  },
  {
    "objectID": "core/slides/spark-standalone.html#options",
    "href": "core/slides/spark-standalone.html#options",
    "title": "Spark standalone",
    "section": "Options",
    "text": "Options\n\n--host\n\n--port default 7077\n\n--wbeui-port default 8080"
  },
  {
    "objectID": "core/slides/spark-standalone.html#connecting-pyspark-to-the-cluster",
    "href": "core/slides/spark-standalone.html#connecting-pyspark-to-the-cluster",
    "title": "Spark standalone",
    "section": "Connecting pyspark to the cluster",
    "text": "Connecting pyspark to the cluster\nTo run an application on the Spark cluster, simply pass the spark://IP:PORT URL of the master as to the SparkContext constructor.\nTo run an interactive Spark shell against the cluster, run the following command:\n\n./bin/pyspark --master spark://boucheron-Precision-5480:7077"
  },
  {
    "objectID": "core/slides/spark-standalone.html#xxx",
    "href": "core/slides/spark-standalone.html#xxx",
    "title": "Spark standalone",
    "section": "xxx",
    "text": "xxx"
  },
  {
    "objectID": "core/slides/lec-1.html#forking-the-course-repository",
    "href": "core/slides/lec-1.html#forking-the-course-repository",
    "title": "Lecture 1",
    "section": "Forking the course repository",
    "text": "Forking the course repository"
  },
  {
    "objectID": "core/slides/lec-1.html#feedback",
    "href": "core/slides/lec-1.html#feedback",
    "title": "Lecture 1",
    "section": "Feedback",
    "text": "Feedback"
  },
  {
    "objectID": "core/slides/lec-1.html#useful-docker-images",
    "href": "core/slides/lec-1.html#useful-docker-images",
    "title": "Lecture 1",
    "section": "Useful Docker images",
    "text": "Useful Docker images"
  },
  {
    "objectID": "core/slides/lec-1.html#running-an-image",
    "href": "core/slides/lec-1.html#running-an-image",
    "title": "Lecture 1",
    "section": "Running an image",
    "text": "Running an image"
  },
  {
    "objectID": "core/slides/lec-1.html#jupyter",
    "href": "core/slides/lec-1.html#jupyter",
    "title": "Lecture 1",
    "section": "Jupyter",
    "text": "Jupyter"
  },
  {
    "objectID": "core/slides/lec-1.html#numpy-scipy",
    "href": "core/slides/lec-1.html#numpy-scipy",
    "title": "Lecture 1",
    "section": "Numpy, Scipy",
    "text": "Numpy, Scipy"
  },
  {
    "objectID": "core/slides/lec-1.html#pandas",
    "href": "core/slides/lec-1.html#pandas",
    "title": "Lecture 1",
    "section": "Pandas",
    "text": "Pandas"
  },
  {
    "objectID": "core/slides/lec-1.html#plotly-altair",
    "href": "core/slides/lec-1.html#plotly-altair",
    "title": "Lecture 1",
    "section": "Plotly, Altair",
    "text": "Plotly, Altair"
  },
  {
    "objectID": "core/slides/lec-1.html#pyspark",
    "href": "core/slides/lec-1.html#pyspark",
    "title": "Lecture 1",
    "section": "Pyspark",
    "text": "Pyspark"
  },
  {
    "objectID": "core/slides/lec-1.html#libraries",
    "href": "core/slides/lec-1.html#libraries",
    "title": "Lecture 1",
    "section": "Libraries",
    "text": "Libraries"
  },
  {
    "objectID": "core/slides/lec-1.html#rstudio",
    "href": "core/slides/lec-1.html#rstudio",
    "title": "Lecture 1",
    "section": "rstudio",
    "text": "rstudio"
  },
  {
    "objectID": "core/slides/lec-1.html#tidyverse",
    "href": "core/slides/lec-1.html#tidyverse",
    "title": "Lecture 1",
    "section": "tidyverse",
    "text": "tidyverse"
  },
  {
    "objectID": "core/slides/lec-1.html#sparklyr",
    "href": "core/slides/lec-1.html#sparklyr",
    "title": "Lecture 1",
    "section": "sparklyr",
    "text": "sparklyr"
  },
  {
    "objectID": "core/slides/lec-1.html#github-accounts",
    "href": "core/slides/lec-1.html#github-accounts",
    "title": "Lecture 1",
    "section": "Github accounts",
    "text": "Github accounts"
  },
  {
    "objectID": "core/slides/lec-1.html#inviting-to-a-private-project",
    "href": "core/slides/lec-1.html#inviting-to-a-private-project",
    "title": "Lecture 1",
    "section": "Inviting to a private project",
    "text": "Inviting to a private project"
  },
  {
    "objectID": "core/slides/slides06_file-formats.html#file-formats-1",
    "href": "core/slides/slides06_file-formats.html#file-formats-1",
    "title": "File formats",
    "section": "File formats",
    "text": "File formats\n\nYou will need to choose the right format for your data\nThe right format typically depends on the use-case"
  },
  {
    "objectID": "core/slides/slides06_file-formats.html#why-different-file-formats",
    "href": "core/slides/slides06_file-formats.html#why-different-file-formats",
    "title": "File formats",
    "section": "Why different file formats ?",
    "text": "Why different file formats ?\n\nA huge bottleneck for big data applications is time spent to find data in a particular location and time spent to write it back to another location\nEven more complicated with large datasets with evolving schemas, or storage constraints\nSeveral Hadoop file formats evolved to ease these issues across a number of use cases"
  },
  {
    "objectID": "core/slides/slides06_file-formats.html#file-formats-trade-offs",
    "href": "core/slides/slides06_file-formats.html#file-formats-trade-offs",
    "title": "File formats",
    "section": "File formats (trade-offs)",
    "text": "File formats (trade-offs)\nChoosing an appropriate file format has the following potential benefits\n\nFaster reads or faster writes\nSplittable files\nSchema evolution support (schema changes over time)\nAdvanced compression support\n\nSome file formats are designed for general use\nOthers for more specific use cases\nSome with specific data characteristics in mind\n\nWhat is the meaning of Schema? May depend on format."
  },
  {
    "objectID": "core/slides/slides06_file-formats.html#main-file-formats",
    "href": "core/slides/slides06_file-formats.html#main-file-formats",
    "title": "File formats",
    "section": "Main file formats",
    "text": "Main file formats\n\n    \n\nWe shall talk about the core concepts and use-cases for the following popular data formats:\n\nAvro : https://avro.apache.org\nORC : https://parquet.apache.org\nParquet : https://orc.apache.org"
  },
  {
    "objectID": "core/slides/slides06_file-formats.html#avro-principles",
    "href": "core/slides/slides06_file-formats.html#avro-principles",
    "title": "File formats",
    "section": "Avro: Principles",
    "text": "Avro: Principles\n\n\n\nAvro is a row-based data format and data serialization system released by the Hadoop working group in 2009\nData schema is stored as JSON in the header. Rest of the data stored in a binary format to make it compact and efficient\nAvro is language-neutral and can be used by many languages (for now C, C++, ..., Python, and R)\nOne shining point of Avro: robust support for schema evolution\n\n\n\n\n\nAvro is used in streaming applications\nOn Hadoop portal, Avro is described as a data serialization system\nIn the old days, Avro used to be an aircraft manufacturer\n\nhttps://en.wikipedia.org/wiki/Avro\n\n\nConfluent Platform works with any data format you prefer, but we added some special facilities for Avro because of its popularity. In the rest of this document I’ll go through some of the reasons why.\n\n\nAvro has a JSON like data model, but can be represented as either JSON or in a compact binary form. It comes with a very sophisticated schema description language that describes data.\n\n\nWe think Avro is the best choice for a number of reasons:\n\n\nIt has a direct mapping to and from JSON\nIt has a very compact format. The bulk of JSON, repeating every field name with every single record, is what makes JSON inefficient for high-volume usage.\nIt is very fast.\nIt has great bindings for a wide variety of programming languages so you can generate Java objects that make working with event data easier, but it does not require code generation so tools can be written generically for any data stream.\nIt has a rich, extensible schema language defined in pure JSON\nIt has the best notion of compatibility for evolving your data over time.\nThough it may seem like a minor thing handling this kind of metadata turns out to be one of the most critical and least appreciated aspects in keeping data high quality and easily useable at organizational scale."
  },
  {
    "objectID": "core/slides/slides06_file-formats.html#avro-rationale",
    "href": "core/slides/slides06_file-formats.html#avro-rationale",
    "title": "File formats",
    "section": "Avro: rationale",
    "text": "Avro: rationale\n\nAvro provides rich data structures: can create a record that contains an array, an enumerated type and a sub-record\n\nIdeal candidate to store data in a data lake since:\n\nData is usually read as a whole in a data lake for further processing by downstream systems\nDownstream systems can retrieve schemas easily from files (no need to store the schemas separately).\nAny source schema change is easily handled\n\n\n\n\ndata lake\ndata warehouse\ndatabse\n\nSpot the differences"
  },
  {
    "objectID": "core/slides/slides06_file-formats.html#avro-organization",
    "href": "core/slides/slides06_file-formats.html#avro-organization",
    "title": "File formats",
    "section": "Avro: organization",
    "text": "Avro: organization"
  },
  {
    "objectID": "core/slides/slides06_file-formats.html#parquet-history-and-principles",
    "href": "core/slides/slides06_file-formats.html#parquet-history-and-principles",
    "title": "File formats",
    "section": "Parquet: History and Principles",
    "text": "Parquet: History and Principles\n\n\n\nParquet is an open-source file format for Hadoop created by Cloudera and Twitter  in 2013\nIt stores nested data structures in a flat columnar format.\nCompared to traditional row-oriented approaches, Parquet is more efficient in terms of storage and performance\nIt is especially good for queries that need read a small subset of columns from a data file with many columns : only the required columns are read (optimized I/O)\n\n\n\n\n\n\n\nmeaning of nested data structure"
  },
  {
    "objectID": "core/slides/slides06_file-formats.html#parquet-row-wise-vs-columnar-storage-format",
    "href": "core/slides/slides06_file-formats.html#parquet-row-wise-vs-columnar-storage-format",
    "title": "File formats",
    "section": "Parquet: Row-wise VS columnar storage format",
    "text": "Parquet: Row-wise VS columnar storage format\nIf you have a dataframe like this\n+----+-------+----------+\n| ID | Name  | Product  | \n+----+-------+----------+\n| 1  | name1 | product1 |\n| 2  | name2 | product2 |\n| 3  | name3 | product3 |\n+----+-------+----------+\nIn row-wise storage format records are contiguous in the file:\n1 name1 product1 2 name2 product2 3 name3 product3 \nWhile in the columnar storage format, columns are stored together:\n1 2 3 name1 name2 name3 product1 product2 product3"
  },
  {
    "objectID": "core/slides/slides06_file-formats.html#parquet-organization",
    "href": "core/slides/slides06_file-formats.html#parquet-organization",
    "title": "File formats",
    "section": "Parquet: organization",
    "text": "Parquet: organization\n\nThis makes columnar storage more efficient when querying a few columns from the table\nNo need to read whole records, but only the required columns\nA unique feature of Parquet is that even nested fields can be read individually without the need to read all the fields\nParquet uses record shredding and an assembly algorithm to store nested structures in a columnar fashion\n\n\nExamples of nested fields"
  },
  {
    "objectID": "core/slides/slides06_file-formats.html#parquet-organization-continued",
    "href": "core/slides/slides06_file-formats.html#parquet-organization-continued",
    "title": "File formats",
    "section": "Parquet: organization (continued)",
    "text": "Parquet: organization (continued)"
  },
  {
    "objectID": "core/slides/slides06_file-formats.html#parquet-organization-lexikon",
    "href": "core/slides/slides06_file-formats.html#parquet-organization-lexikon",
    "title": "File formats",
    "section": "Parquet: organization (lexikon)",
    "text": "Parquet: organization (lexikon)\nThe main entities in a Parquet file are the following:\n\nRow group\n\na horizontal partitioning of the data into rows. A row group consists of a column chunk for each column in the dataset\n\nColumn chunk\n\na chunk of the data for a particular column. These column chunks live in a particular row group and are guaranteed to be contiguous in the file\n\nPage\n\ncolumn chunks are divided up into pages written back to back. The pages share a common header and readers can skip the page they are not interested in"
  },
  {
    "objectID": "core/slides/slides06_file-formats.html#about-parquet",
    "href": "core/slides/slides06_file-formats.html#about-parquet",
    "title": "File formats",
    "section": "About Parquet",
    "text": "About Parquet"
  },
  {
    "objectID": "core/slides/slides06_file-formats.html#parquet-headers-and-footers",
    "href": "core/slides/slides06_file-formats.html#parquet-headers-and-footers",
    "title": "File formats",
    "section": "Parquet: headers and footers",
    "text": "Parquet: headers and footers\n\nThe header just contains a magic number “PAR1” (4-byte) that identifies the file as Parquet format file\n\nThe footer contains:\n\nFile metadata: all the locations of all the column metadata start locations. Readers first read the file metadata to find the column chunks they need. Column chunks are then read sequentially. It also includes the format version, the schema, and any extra key-value pairs.\nlength of file metadata (4-byte)\nmagic number “PAR1” (4-byte)"
  },
  {
    "objectID": "core/slides/slides06_file-formats.html#orc-principles",
    "href": "core/slides/slides06_file-formats.html#orc-principles",
    "title": "File formats",
    "section": "ORC: principles",
    "text": "ORC: principles\n\n\n\nORC stands for Optimized Row Columnar file format. Created by Hortonworks in 2013 in order to speed up Hive \nORC file format provides a highly efficient way to store data\nIt is a raw columnar data format highly optimized for reading, writing, and processing data in Hive\nIt stores data in a compact way and enables skipping quickly irrelevant parts\n\n\n\n\n\n\n\nA few words about Hive\nHive official site"
  },
  {
    "objectID": "core/slides/slides06_file-formats.html#about-orc-organization",
    "href": "core/slides/slides06_file-formats.html#about-orc-organization",
    "title": "File formats",
    "section": "About ORC: organization",
    "text": "About ORC: organization\n\nORC stores collections of rows in one file. Within the collection, row data is stored in a columnar format\nAn ORC file contains groups of row data called stripes, along with auxiliary information in a file footer. At the end of the file a postscript holds compression parameters and the size of the compressed footer\nThe default stripe size is 250 MB. Large stripe sizes enable large, efficient reads from HDFS\nThe file footer contains a list of stripes in the file, the number of rows per stripe, and each column’s data type. It also contains column-level aggregates count, min, max, and sum"
  },
  {
    "objectID": "core/slides/slides06_file-formats.html#about-orc-onctinued",
    "href": "core/slides/slides06_file-formats.html#about-orc-onctinued",
    "title": "File formats",
    "section": "About ORC (onctinued)",
    "text": "About ORC (onctinued)\n\n\n\nIndex data include min and max values for each column and the row’s positions within each column\nORC indexes are used only for the selection of stripes and row groups and not for answering queries\n\n\n\n\n\nORC file structure"
  },
  {
    "objectID": "core/slides/slides06_file-formats.html#about-orc",
    "href": "core/slides/slides06_file-formats.html#about-orc",
    "title": "File formats",
    "section": "About ORC",
    "text": "About ORC\nORC file format has many advantages such as:\n\nHive type support including DateTime, decimal, and the complex types (struct, list, map and union)\nConcurrent reads of the same file\nAbility to split files without scanning for markers\nEstimate an upper bound on heap memory allocation based on the information in the file footer."
  },
  {
    "objectID": "core/slides/slides06_file-formats.html#avro-versus-parquet",
    "href": "core/slides/slides06_file-formats.html#avro-versus-parquet",
    "title": "File formats",
    "section": "Avro versus Parquet",
    "text": "Avro versus Parquet\n\nAvro is a row-based storage format whereas Parquet is a columnar based storage format\nParquet is much better for analytical querying i.e. reads and querying are much more efficient than writing.\nWrite operations in Avro are better than in Parquet.\nAvro is more mature than Parquet for schema evolution: Parquet supports only schema append while Avro supports more things, such as adding or modifying columns\nParquet is ideal for querying a subset of columns in a multi-column table. Avro is ideal for operations where all the columns are needed (such as in a ETL workflow)"
  },
  {
    "objectID": "core/slides/slides06_file-formats.html#orc-vs-parquet",
    "href": "core/slides/slides06_file-formats.html#orc-vs-parquet",
    "title": "File formats",
    "section": "ORC vs Parquet",
    "text": "ORC vs Parquet\n\nParquet is more capable of storing nested data\nORC is more capable of predicate pushdown (SQL queries on a data file are better optimized, chunks of data can be skipped directly while reading)\nORC is more compression efficient"
  },
  {
    "objectID": "core/slides/slides06_file-formats.html#in-summary",
    "href": "core/slides/slides06_file-formats.html#in-summary",
    "title": "File formats",
    "section": "In summary…",
    "text": "In summary…"
  },
  {
    "objectID": "core/slides/slides06_file-formats.html#r-ead-write-intensive-query-pattern",
    "href": "core/slides/slides06_file-formats.html#r-ead-write-intensive-query-pattern",
    "title": "File formats",
    "section": "R ead / write intensive & query pattern",
    "text": "R ead / write intensive & query pattern\n\nRow-based file formats are overall better for storing write-intensive data because appending new records is easier\nIf only a small subset of columns is queried frequently, columnar formats will be better since only those needed columns will be accessed and transmitted (whereas row formats need to pull all the columns)"
  },
  {
    "objectID": "core/slides/slides06_file-formats.html#c-ompression",
    "href": "core/slides/slides06_file-formats.html#c-ompression",
    "title": "File formats",
    "section": "C ompression",
    "text": "C ompression\n\nCompression is one of the key aspects to consider since compression helps reduce the resources required to store and transmit data\nColumnar formats are better than row-based formats in terms of compression because storing the same type of values together allows more efficient compression\nIn columnar formats, a different and efficient encoding is utilized for each column\nORC has the best compression rate of all three, thanks to its stripes"
  },
  {
    "objectID": "core/slides/slides06_file-formats.html#s-chema-evolution",
    "href": "core/slides/slides06_file-formats.html#s-chema-evolution",
    "title": "File formats",
    "section": "S chema Evolution",
    "text": "S chema Evolution\n\nOne challenge in big data is the frequent change of data schema: e.g. adding/dropping columns and changing columns names\nIf you know that the schema of the data will change several times, the best choice is Avro\nAvro data schema is in JSON and Avro is able to keep data compact even when many different schemas exist\n\nSee Schema merging for parquet files"
  },
  {
    "objectID": "core/slides/slides06_file-formats.html#n-ested-columns",
    "href": "core/slides/slides06_file-formats.html#n-ested-columns",
    "title": "File formats",
    "section": "N ested Columns",
    "text": "N ested Columns\n\nIf you have a lot of complex nested columns in your dataset and often only query a subset of columns or subcolumns, Parquet is the best choice\nParquet allows to access and retrieve subcolumns without pulling the rest of the nested column"
  },
  {
    "objectID": "core/slides/slides06_file-formats.html#f-ramework-support",
    "href": "core/slides/slides06_file-formats.html#f-ramework-support",
    "title": "File formats",
    "section": "F ramework support",
    "text": "F ramework support\n\nYou have consider the framework you are using when choosing a data format\nData formats perform differently depending on where they are used\nORC works best with Hive  (it was designed for it)\nSpark provides great support for processing Parquet formats.\nAvro is often a good choice for Kafka (streaming applications)\n\nBut… you can use an try all formats with any framework"
  },
  {
    "objectID": "core/slides/slides02_python_ds_stack.html#features-of-python-1",
    "href": "core/slides/slides02_python_ds_stack.html#features-of-python-1",
    "title": "Python Data Science Stack",
    "section": "Features of Python\n",
    "text": "Features of Python\n\n\nHigh-level data types (tuples, dict, list, set, etc.)\nStandard libraries with batteries included\n\nString services,\nRegular expressions\nDatetime\n…\n\n\nLibraries for scientific computing\nEasy and efficient I/O, many file formats\nOS, threading, multiprocessing\nNetworking, email, html, webserver, scrapping\nCan be extended with C/C++ and easily accelerated (cython, numba, pypy)\nTons of external libraries"
  },
  {
    "objectID": "core/slides/slides02_python_ds_stack.html#features-of-python-2",
    "href": "core/slides/slides02_python_ds_stack.html#features-of-python-2",
    "title": "Python Data Science Stack",
    "section": "Features of Python\n",
    "text": "Features of Python"
  },
  {
    "objectID": "core/slides/slides02_python_ds_stack.html#the-stackoverflow-2023-survey",
    "href": "core/slides/slides02_python_ds_stack.html#the-stackoverflow-2023-survey",
    "title": "Python Data Science Stack",
    "section": "The stackoverflow 2023 survey\n",
    "text": "The stackoverflow 2023 survey"
  },
  {
    "objectID": "core/slides/slides02_python_ds_stack.html#python-popularity-growth",
    "href": "core/slides/slides02_python_ds_stack.html#python-popularity-growth",
    "title": "Python Data Science Stack",
    "section": "\nPython popularity growth",
    "text": "Python popularity growth"
  },
  {
    "objectID": "core/slides/slides02_python_ds_stack.html#python-popularity-growth-1",
    "href": "core/slides/slides02_python_ds_stack.html#python-popularity-growth-1",
    "title": "Python Data Science Stack",
    "section": "\nPython popularity growth",
    "text": "Python popularity growth"
  },
  {
    "objectID": "core/slides/slides02_python_ds_stack.html#numpy",
    "href": "core/slides/slides02_python_ds_stack.html#numpy",
    "title": "Python Data Science Stack",
    "section": "Numpy",
    "text": "Numpy\n\n\n\n\n\n\n\n\nnumpy is all about multi-dimensional arrays and matrices\nhigh-level computation such as\n\nlinear algebra: numpy.linalg\n\nrandom number generation:numpy.random\n\n\n\nFast but not optimized for multi-threaded architectures\nNot for distributed multi-machine settings"
  },
  {
    "objectID": "core/slides/slides02_python_ds_stack.html#scipy",
    "href": "core/slides/slides02_python_ds_stack.html#scipy",
    "title": "Python Data Science Stack",
    "section": "Scipy",
    "text": "Scipy\n\n\n\n\n\n\n\n\nscipy extends numpy with extra modules:\n\noptimization,\nintegration,\nFFT, signal and image processing\n…\n\n\nSparse matrix formats in scipy.sparse"
  },
  {
    "objectID": "core/slides/slides02_python_ds_stack.html#pandas",
    "href": "core/slides/slides02_python_ds_stack.html#pandas",
    "title": "Python Data Science Stack",
    "section": "Pandas",
    "text": "Pandas\n\n\n\n\n\n\n\n\npandas builds upon numpy to provide a high-performance, easy-to-use DataFrame object, with high-level data processing\nEasy I/O with most data format : csv, json, hdf5, feather, parquet, etc.\n\nSQL semantics: select, filter, join, groupby, agg, , where, etc.\nVery large general-purpose library for data processing, not distributed, medium scale data only\n\n\n\n\n\n\n\n\n\n\nLinks\n\n\n\nPandas online book\nPandas homepage\nPolars homepage\nPolars versus Pandas"
  },
  {
    "objectID": "core/slides/slides02_python_ds_stack.html#dask",
    "href": "core/slides/slides02_python_ds_stack.html#dask",
    "title": "Python Data Science Stack",
    "section": "Dask",
    "text": "Dask\n\n\n\n\n\n\n\n\ndask is roughly a distributed and parallel pandas\n\nSame API has pandas !\nTask scheduling, lazy evaluation, distributed dataframes\nStill young and far behind spark, but can be useful\nEasier than spark, full Python (no JVM)\n\n\n\n\n\n\n\n\n\n\nLinks\n\n\n\nDask homepage"
  },
  {
    "objectID": "core/slides/slides02_python_ds_stack.html#pyspark",
    "href": "core/slides/slides02_python_ds_stack.html#pyspark",
    "title": "Python Data Science Stack",
    "section": "Pyspark",
    "text": "Pyspark\n\n\n\n\n\n\n\n\npyspark is the python API to spark, a big data processing framework\nWe will use it a lot in this course\nNative API to spark is scala: pyspark can be slower (much slower if you are not careful)\n\n\n\n\n\n\n\n\n\n\nLinks\n\n\n\nPyspark documentation\nSpark Apache Project"
  },
  {
    "objectID": "core/slides/slides02_python_ds_stack.html#sqlalchemy",
    "href": "core/slides/slides02_python_ds_stack.html#sqlalchemy",
    "title": "Python Data Science Stack",
    "section": "SQLAlchemy",
    "text": "SQLAlchemy\n\n\n\n\n\n\nObject Relational Model (ORM)\nODBC\n\n\n\n\n\n\n\n\n\nLinks\n\n\n\nSQl Alchemy homepage\npsycopg2\npsycopg documentation"
  },
  {
    "objectID": "core/slides/slides02_python_ds_stack.html#pyarrow",
    "href": "core/slides/slides02_python_ds_stack.html#pyarrow",
    "title": "Python Data Science Stack",
    "section": "Pyarrow",
    "text": "Pyarrow\n\n\n\n\n\n\n\nThe universal columnar format and multi-language toolbox for fast data interchange and in-memory analytics\n\n\nApache Arrow defines a language-independent columnar memory format for flat and hierarchical data, organized for efficient analytic operations on modern hardware like CPUs and GPUs. The Arrow memory format also supports zero-copy reads for lightning-fast data access without serialization overhead.\n\n\n\n\n\n\n\n\n\n\nLinks\n\n\n\nApache Arrow Project Homepage\nPyarrow documentation"
  },
  {
    "objectID": "core/slides/slides02_python_ds_stack.html#matplotlib",
    "href": "core/slides/slides02_python_ds_stack.html#matplotlib",
    "title": "Python Data Science Stack",
    "section": "Matplotlib",
    "text": "Matplotlib\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmatplotlib provides versatile 2D plotting capabilities\n\nscientific computing\ndata visualization\n\n\nLarge and customizable library\nThe historical one, somewhat low-level when plotting things related to data\n\n\n\n\n\n\n\n\n\nLinks\n\n\nMatplotlib Homepage"
  },
  {
    "objectID": "core/slides/slides02_python_ds_stack.html#plotly",
    "href": "core/slides/slides02_python_ds_stack.html#plotly",
    "title": "Python Data Science Stack",
    "section": "Plotly",
    "text": "Plotly\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAn interactive visualization library for web browsers based on javascript graphic library d3.js\n\nWith a clean and simple python interface, can be used in a jupyter notebook\nInteractions enabled by default (zoom, etc.) and fast rendering\nVery good looking plots with good default parameters\n\n\n\n\n\n\n\n\n\nLinks\n\n\n\nPlotly homepage"
  },
  {
    "objectID": "core/slides/slides02_python_ds_stack.html#altair",
    "href": "core/slides/slides02_python_ds_stack.html#altair",
    "title": "Python Data Science Stack",
    "section": "Altair",
    "text": "Altair\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVega-Altair: Declarative Visualization in Python\nVega-Altair is a declarative visualization library for Python. Its simple, friendly and consistent API, built on top of the powerful Vega-Lite grammar, empowers you to spend less time writing code and more time exploring your data.\n\n\n\n\n\n\n\n\n\n\nLinks\n\n\n\nAltair homepage\nVega-Lite: A Grammar of Interactive Graphics"
  },
  {
    "objectID": "core/slides/slides02_python_ds_stack.html#dash",
    "href": "core/slides/slides02_python_ds_stack.html#dash",
    "title": "Python Data Science Stack",
    "section": "Dash",
    "text": "Dash\n\n\n\n\n\n\n\n\nLinks\n\n\nDash homepage"
  },
  {
    "objectID": "core/slides/slides02_python_ds_stack.html#shiny",
    "href": "core/slides/slides02_python_ds_stack.html#shiny",
    "title": "Python Data Science Stack",
    "section": "Shiny",
    "text": "Shiny\n\n\n\n\n\n\n\n\nLinks\n\n\nShiny homepage"
  },
  {
    "objectID": "core/slides/slides02_python_ds_stack.html#pure-python-interfaces",
    "href": "core/slides/slides02_python_ds_stack.html#pure-python-interfaces",
    "title": "Python Data Science Stack",
    "section": "Pure Python interfaces",
    "text": "Pure Python interfaces\n\n\n\n\nWays to use all these tools\n\nWrite a script script.py and use python directly in a CLI : python script.py\nUse the ipython interactive shell"
  },
  {
    "objectID": "core/slides/slides02_python_ds_stack.html#interfaces-jupyter",
    "href": "core/slides/slides02_python_ds_stack.html#interfaces-jupyter",
    "title": "Python Data Science Stack",
    "section": "Interfaces : Jupyter",
    "text": "Interfaces : Jupyter\n\n\n\n\n\nUse jupyter: a web application that allows to create and run documents, called notebooks (with .ipynb extension)\nNotebooks can contain code, equations, visualizations, text, etc. (literate programming)\nEach notebook has a kernel running a python/R,Julia, … thread\nA problem: a ipynb file is a json document. Leads to bad code diff, a problem with git versioning\n\n\n\n\n\n\n\n\n\n\nLinks\n\n\n\njupyter\njupyterlab\npolynote"
  },
  {
    "objectID": "core/slides/slides02_python_ds_stack.html#quarto",
    "href": "core/slides/slides02_python_ds_stack.html#quarto",
    "title": "Python Data Science Stack",
    "section": "Quarto",
    "text": "Quarto"
  },
  {
    "objectID": "core/slides/slides02_python_ds_stack.html#interfaceside-vs-code-and-other-editors",
    "href": "core/slides/slides02_python_ds_stack.html#interfaceside-vs-code-and-other-editors",
    "title": "Python Data Science Stack",
    "section": "Interfaces/IDE : VS Code (and other editors)",
    "text": "Interfaces/IDE : VS Code (and other editors)"
  },
  {
    "objectID": "core/slides/slides02_python_ds_stack.html#reticulate",
    "href": "core/slides/slides02_python_ds_stack.html#reticulate",
    "title": "Python Data Science Stack",
    "section": "Reticulate",
    "text": "Reticulate\n\n\nReticulate embeds a Python session within your R session, enabling seamless, high-performance interoperability. If you are an R developer that uses Python for some of your work or a member of data science team that uses both languages, reticulate can dramatically streamline your workflow!\n\n\n\n\n\n\n\n\n\n\nLinks\n\n\n\nReticulate homepage"
  },
  {
    "objectID": "core/slides/slides02_python_ds_stack.html#py2r",
    "href": "core/slides/slides02_python_ds_stack.html#py2r",
    "title": "Python Data Science Stack",
    "section": "Py2R",
    "text": "Py2R\n\n\nPython has several well-written packages for statistics and data science, but CRAN, R’s central repository, contains thousands of packages implementing sophisticated statistical algorithms that have been field-tested over many years. Thanks to the rpy2 package, Pythonistas can take advantage of the great work already done by the R community. rpy2 provides an interface that allows you to run R in Python processes. Users can move between languages and use the best of both programming languages.\n\n\n\nrpy2 homepage"
  },
  {
    "objectID": "core/slides/slides02_python_ds_stack.html#statistics",
    "href": "core/slides/slides02_python_ds_stack.html#statistics",
    "title": "Python Data Science Stack",
    "section": "Statistics",
    "text": "Statistics\n\n\nstatsmodels"
  },
  {
    "objectID": "core/slides/slides02_python_ds_stack.html#machine-learning",
    "href": "core/slides/slides02_python_ds_stack.html#machine-learning",
    "title": "Python Data Science Stack",
    "section": "Machine learning",
    "text": "Machine learning\n\nscikit-learn\nxgboost\nlightgbm\nvowpalwabbit\n…"
  },
  {
    "objectID": "core/slides/slides02_python_ds_stack.html#deep-learning",
    "href": "core/slides/slides02_python_ds_stack.html#deep-learning",
    "title": "Python Data Science Stack",
    "section": "Deep learning",
    "text": "Deep learning\n\nkeras\ntensorflow\npytorch\n…"
  },
  {
    "objectID": "core/slides/slides02_python_ds_stack.html#getting-faster",
    "href": "core/slides/slides02_python_ds_stack.html#getting-faster",
    "title": "Python Data Science Stack",
    "section": "Getting faster",
    "text": "Getting faster\n\n\nnumba, cython, cupy"
  },
  {
    "objectID": "core/slides/slides02_python_ds_stack.html#and",
    "href": "core/slides/slides02_python_ds_stack.html#and",
    "title": "Python Data Science Stack",
    "section": "And …",
    "text": "And …\n\nPython APIs for most databases and clouds\nProcessing and plotting tools for Geospatial data\nImage processing\nWeb development, web scrapping\n\namong many many many other things…"
  },
  {
    "objectID": "core/slides/slides03_sparkrdd.html#principles",
    "href": "core/slides/slides03_sparkrdd.html#principles",
    "title": "Apache and RDD",
    "section": "Principles",
    "text": "Principles\nSpark computing framework deals with many complex issues: fault tolerance, slow machines, big datasets, etc.\n\nIt follows the next guideline\nHere is an operation, run it on all the data.\n\n\n\n\n\n\nNote\n\n\n\nI do not care where it runs\nFeel free to run it twice on different nodes\n\n\n\n\n\n\nJobs are divided in tasks that are executed by the workers\n\n\n\n\n\n\nNote\n\n\n\nHow do we deal with failure? Launch another task!\n\nHow do we deal with stragglers? Launch another task!  … and kill the original task"
  },
  {
    "objectID": "core/slides/slides03_sparkrdd.html#a-picture",
    "href": "core/slides/slides03_sparkrdd.html#a-picture",
    "title": "Apache and RDD",
    "section": "A picture",
    "text": "A picture\n\nSpark Cluster Overview"
  },
  {
    "objectID": "core/slides/slides03_sparkrdd.html#job",
    "href": "core/slides/slides03_sparkrdd.html#job",
    "title": "Apache and RDD",
    "section": "Job",
    "text": "Job\nA job in Spark represents a complete computation triggered by an action in the application code.\nWhen you invoke an action (such as collect(), saveAsTextFile(), etc.) on a Spark RDD, DataFrame, or Dataset, it triggers the execution of one or more jobs.\n\nEach job consists of one or more stages, where each stage represents a set of tasks that can be executed in parallel.\nJobs in Spark are created by transformations that have no dependency on each other, meaning each stage can execute independently."
  },
  {
    "objectID": "core/slides/slides03_sparkrdd.html#task",
    "href": "core/slides/slides03_sparkrdd.html#task",
    "title": "Apache and RDD",
    "section": "Task",
    "text": "Task\nA task is the smallest unit of work in Spark and represents the execution of a computation on a single partition of data.\n\nTasks are created for each partition of the RDD, DataFrame, or Dataset involved in the computation.\n\n\nSpark’s execution engine assigns tasks to individual executor nodes in the cluster for parallel execution.\n\n\nTasks are executed within the context of a specific stage, and each task typically operates on a subset of the data distributed across the cluster.\n\n\nThe number of tasks within a stage depends on the number of partitions of the input data and the degree of parallelism configured for the Spark application.\n\n\nIn summary, a job represents the entire computation triggered by an action, composed of one or more stages, each of which is divided into smaller units of work called tasks.\n\n\nTasks operate on individual partitions of the data in parallel to achieve efficient and scalable distributed computation in Spark."
  },
  {
    "objectID": "core/slides/slides03_sparkrdd.html#api",
    "href": "core/slides/slides03_sparkrdd.html#api",
    "title": "Apache and RDD",
    "section": "API",
    "text": "API\nAn API allows a user to interact with the software\nSpark is implemented in Scala and runs on the JVM (Java Virtual Machine)\n\nMultiple Application Programming Interfaces (APIs):\n\nScala (JVM)\nJava (JVM)\n Python\n R\n\n\n\nThis course uses primarily the Python API. Easier to learn than Scala and Java\n\n\n\n\n\n\n\nAbout the R APIs\n\n\nSee Mastering Spark in R"
  },
  {
    "objectID": "core/slides/slides03_sparkrdd.html#digression-on-acronym-api-application-programming-interface",
    "href": "core/slides/slides03_sparkrdd.html#digression-on-acronym-api-application-programming-interface",
    "title": "Apache and RDD",
    "section": "Digression on acronym API (Application Programming Interface)",
    "text": "Digression on acronym API (Application Programming Interface)\nSee https://en.wikipedia.org/wiki/API for more on this acronym\n In Python language, look at interface and corresponding chapter Interfaces, Protocols and ABCs in Fluent Python\n\n For R there are in fact two APIs, or two packages that offer a Spark API\n\nsparklyr\nSparkR\n\nSee Mastering Spark with R by Javier Luraschi, Kevin Kuo, Edgar Ruiz"
  },
  {
    "objectID": "core/slides/slides03_sparkrdd.html#architecture",
    "href": "core/slides/slides03_sparkrdd.html#architecture",
    "title": "Apache and RDD",
    "section": "Architecture",
    "text": "Architecture\n\n\nWhen you interact with Spark through its API, you send instructions to the Driver\n\nThe Driver is the central coordinator\nIt communicates with distributed workers called executors\nCreates a logical directed acyclic graph (DAG) of operations\nMerges operations that can be merged\nSplits the operations in tasks (smallest unit of work in Spark)\nSchedules the tasks and send them to the executors\nTracks data and tasks\n\n\n\n\n\n\n\nSpark Cluster Overview\n\n\nExample\n\nExample of DAG: map(f) - map(g) - filter(h) - reduce(l)\nmap(f o g)"
  },
  {
    "objectID": "core/slides/slides03_sparkrdd.html#sparkcontext-versus-sparksession",
    "href": "core/slides/slides03_sparkrdd.html#sparkcontext-versus-sparksession",
    "title": "Apache and RDD",
    "section": "SparkContext versus SparkSession",
    "text": "SparkContext versus SparkSession\nSparkContext and SparkSession serve different purposes\n\nSparkContext was the main entry point for Spark applications in first versions of Apache Spark.\nSparkContext represented the connection to a Spark cluster, allowing the application to interact with the cluster manager.\nSparkContext was responsible for coordinating and managing the execution of jobs and tasks.\nSparkContext provided APIs for creating RDDs (Resilient Distributed Datasets), which were the primary abstraction in Spark for representing distributed data."
  },
  {
    "objectID": "core/slides/slides03_sparkrdd.html#sparkcontext-object",
    "href": "core/slides/slides03_sparkrdd.html#sparkcontext-object",
    "title": "Apache and RDD",
    "section": "SparkContext object",
    "text": "SparkContext object\nYour python session interacts with the driver through a SparkContext object\n\nIn the Spark interactive shell  An object of class SparkContext is automatically created in the session and named sc\nIn a jupyter notebook  Create a SparkContext object using:\n\n&gt;&gt;&gt; from pyspark import SparkConf, SparkContext\n\n&gt;&gt;&gt; conf = (\n  SparkConf()\n  .setAppName(appName)\n  .setMaster(master)\n)\n&gt;&gt;&gt; sc = SparkContext(conf=conf)"
  },
  {
    "objectID": "core/slides/slides03_sparkrdd.html#sparksession",
    "href": "core/slides/slides03_sparkrdd.html#sparksession",
    "title": "Apache and RDD",
    "section": " SparkSession",
    "text": "SparkSession\nIn Spark 2.0 and later versions, SparkContext is still available but is not the primary entry point.\nInstead, SparkSession is preferred.\nSparkSession was introduced in Spark 2.0 as a higher-level abstraction that encapsulates SparkContext, SQLContext, and HiveContext.\nSparkSession provides a unified entry point for Spark functionality, integrating Structured APIs:\n\nSQL,\nDataFrame,\nDataset\n\nand the traditional RDD-based APIs."
  },
  {
    "objectID": "core/slides/slides03_sparkrdd.html#what-sparksession",
    "href": "core/slides/slides03_sparkrdd.html#what-sparksession",
    "title": "Apache and RDD",
    "section": "What SparkSession?",
    "text": "What SparkSession?\nSparkSession is designed to make it easier to work with structured data (like data stored in tables or files with a schema) using Spark’s DataFrame and Dataset APIs.\n\nSparkSession also provides built-in support for reading data from various sources (like Parquet, JSON, JDBC, etc.) into DataFrames and writing DataFrames back to different formats.\n\n\nAdditionally, SparkSession simplifies the configuration of Spark properties and provides a Spark SQL CLI and a Spark Shell with SQL and DataFrame support.\n\n\n\n\n\n\n\n\nNote\n\n\nSparkSession internally creates and manages a SparkContext, so when you create a SparkSession, you don’t need to create a SparkContext separately."
  },
  {
    "objectID": "core/slides/slides03_sparkrdd.html#section-3",
    "href": "core/slides/slides03_sparkrdd.html#section-3",
    "title": "Apache and RDD",
    "section": "",
    "text": "SparkContext is lower-level and primarily focused on managing the execution of Spark jobs and interacting with the cluster\nSparkSession provides a higher-level, more user-friendly interface for working with structured data and integrates various Spark functionalities, including SQL, DataFrame, and Dataset APIs."
  },
  {
    "objectID": "core/slides/slides03_sparkrdd.html#rdds-and-running-model",
    "href": "core/slides/slides03_sparkrdd.html#rdds-and-running-model",
    "title": "Apache and RDD",
    "section": "RDDs and running model",
    "text": "RDDs and running model\nSpark programs are written in terms of operations on RDDs\n\nRDD stands for Resilient Distributed Dataset \nAn immutable distributed collection of objects spread across the cluster disks or memory\nRDDs can contain any type of Python, Java, or Scala objects, including user-defined classes\nParallel transformations and actions can be applied to RDDs\nRDDs are automatically rebuilt on machine failure"
  },
  {
    "objectID": "core/slides/slides03_sparkrdd.html#creating-a-rdd",
    "href": "core/slides/slides03_sparkrdd.html#creating-a-rdd",
    "title": "Apache and RDD",
    "section": "Creating a RDD",
    "text": "Creating a RDD\nFrom an iterable object iterator1 (e.g. a Python list, etc.):\nlines = sc.parallelize(iterator)\nFrom a text file:\nlines = sc.textFile(\"/path/to/file.txt\")\nwhere lines is the resulting RDD, and sc the spark context\n\n\n\n\n\n\nRemarks\n\n\n\nparallelize not really used in practice\nIn real life: load data from external storage\nExternal storage is often HDFS (Hadoop Distributed File System)\nCan read most formats (json, csv, xml, parquet, orc, etc.)\n\n\n\n\nSee Chapter 17 Iterators, Generators, … in Fluent Python"
  },
  {
    "objectID": "core/slides/slides03_sparkrdd.html#operations-on-rdd",
    "href": "core/slides/slides03_sparkrdd.html#operations-on-rdd",
    "title": "Apache and RDD",
    "section": "Operations on RDD",
    "text": "Operations on RDD\nTwo families of operations can be performed on RDDs\n\n\nTransformations  Operations on RDDs which return a new RDD  Lazy evaluation\n\n\n\n\nActions  Operations on RDDs that return some other data type  Triggers computations\n\n\n\n\n\n\n What is lazy evaluation ?"
  },
  {
    "objectID": "core/slides/slides03_sparkrdd.html#section-4",
    "href": "core/slides/slides03_sparkrdd.html#section-4",
    "title": "Apache and RDD",
    "section": "",
    "text": "When a transformation is called on a RDD:\n\nThe operation is not immediately performed\nSpark internally records that this operation has been requested\nComputations are triggered only if an action requires the result of this transformation at some point"
  },
  {
    "objectID": "core/slides/slides03_sparkrdd.html#transformations-1",
    "href": "core/slides/slides03_sparkrdd.html#transformations-1",
    "title": "Apache and RDD",
    "section": "Transformations",
    "text": "Transformations\nThe most important transformation is map\n\n\n\ntransformation\ndescription\n\n\n\n\nmap(f)\napply a function f to each element of the RDD\n\n\n\n\nHere is an example:\n&gt;&gt;&gt; rdd = sc.parallelize([2, 3, 4])\n&gt;&gt;&gt; (\n  rdd\n    .map(lambda x: list(range(1, x)))\n    .collect()\n)\n[[1], [1, 2], [1, 2, 3]]\n\n\n\nWe have to call collect (an action) otherwise nothing happens\nOnce again, transformation map is lazily evaluated \n\n\n\n\n\n\n\n\n\nNote\n\n\n\nIn Python, three options for passing functions into Spark\n\nfor short functions: lambda expressions (anonymous functions)\ntop-level functions\nlocally/user defined functions with def"
  },
  {
    "objectID": "core/slides/slides03_sparkrdd.html#transformations-2",
    "href": "core/slides/slides03_sparkrdd.html#transformations-2",
    "title": "Apache and RDD",
    "section": "Transformations",
    "text": "Transformations\nPassing functions to map:\n\nInvolves serialization with pickle\nSpark sends the entire pickled function to worker nodes\n\n\n\n\n\n\n\nWarning\n\n\nIf the function is an object method:\n\nThe whole object is pickled since the method contains references to the object (self) and references to attributes of the object\nThe whole object can be large\nThe whole object may not be serializable with pickle\n\n\n\n\n\n\nGo to notebook05_sparkrdd.ipynb"
  },
  {
    "objectID": "core/slides/slides03_sparkrdd.html#pythons-corner",
    "href": "core/slides/slides03_sparkrdd.html#pythons-corner",
    "title": "Apache and RDD",
    "section": "Python’s corner",
    "text": "Python’s corner\n\n[it for it in map(lambda x : list(range(1, x)), [1, 2, 3])]"
  },
  {
    "objectID": "core/slides/slides03_sparkrdd.html#serialization",
    "href": "core/slides/slides03_sparkrdd.html#serialization",
    "title": "Apache and RDD",
    "section": " Serialization",
    "text": "Serialization\n\nConverting an object from its in-memory structure to a binary or text-oriented format for storage or transmission, in a way that allows the future reconstruction of a clone of the object on the same system or on a different one.\n\n\n\nThe pickle module supports serialization of arbitrary Python objects to a binary format\n\nfrom Fluent Python by Ramalho\n\n\n\nSee also cloudpickle"
  },
  {
    "objectID": "core/slides/slides03_sparkrdd.html#transformations-continued",
    "href": "core/slides/slides03_sparkrdd.html#transformations-continued",
    "title": "Apache and RDD",
    "section": "Transformations (continued)",
    "text": "Transformations (continued)\nflatMap\n\n\n\n\n\n\n\ntransformation\ndescription\n\n\n\n\nflatMap(f)\napply f to each element of the RDD, then flattens the results\n\n\n\n\n\n\n\n\n\n\nExample\n\n\n&gt;&gt;&gt; rdd = sc.parallelize([2, 3, 4, 5])\n&gt;&gt;&gt; (\n  rdd\n    .flatMap(lambda x: range(1, x))\n    .collect()\n)\n[1, 1, 2, 1, 2, 3, 1, 2, 3, 4]"
  },
  {
    "objectID": "core/slides/slides03_sparkrdd.html#pythons-corner-is-there-any-flatmap-function",
    "href": "core/slides/slides03_sparkrdd.html#pythons-corner-is-there-any-flatmap-function",
    "title": "Apache and RDD",
    "section": "Python’s corner: is there any flatMap() function?",
    "text": "Python’s corner: is there any flatMap() function?\nNested list comprehensions\n\n[o for it in map(lambda x : list(range(1, x)), [1, 2, 3, 4])   for o in it]\n\n[1, 1, 2, 1, 2, 3]\n\n\n\n\nimport itertools\n\n[o for o in itertools.chain.from_iterable(map(lambda x : list(range(1, x)), [1, 2, 3, 4]))]\n\n[1, 1, 2, 1, 2, 3]\n\n\n\n\n\nflatten = itertools.chain.from_iterable\n\n[o for o in  flatten(map(lambda x : list(range(1, x)), [1, 2, 3, 4]))]\n\n[1, 1, 2, 1, 2, 3]\n\n\n\nFrom https://discuss.python.org/t/add-built-in-flatmap-function-to-functools/21137\nhttps://docs.python.org/3/library/itertools.html"
  },
  {
    "objectID": "core/slides/slides03_sparkrdd.html#transformations-continued-1",
    "href": "core/slides/slides03_sparkrdd.html#transformations-continued-1",
    "title": "Apache and RDD",
    "section": "Transformations (continued)",
    "text": "Transformations (continued)\nfilter allows to filter an RDD\n\n\n\n\n\n\n\ntransformation\ndescription\n\n\n\n\nfilter(f)\nReturn an RDD consisting of only elements that pass the condition f passed to filter()\n\n\n\n\n\n\n\n\n\n\nExample\n\n\n&gt;&gt;&gt; rdd = sc.parallelize(range(10))\n&gt;&gt;&gt; rdd.filter(lambda x: x % 2 == 0).collect()\n[0, 2, 4, 6, 8]"
  },
  {
    "objectID": "core/slides/slides03_sparkrdd.html#pythons-corner-1",
    "href": "core/slides/slides03_sparkrdd.html#pythons-corner-1",
    "title": "Apache and RDD",
    "section": "Python’s corner",
    "text": "Python’s corner\nUsing list comprehensions\n\nlll = list(range(10))\nspam = lambda x: x % 2 == 0\n\n[o  for o in lll if spam(o)]\n\n[0, 2, 4, 6, 8]\n\n\n\nTweaking filterfalse from itertools\n\n[o for o in itertools.filterfalse(lambda x : not x% 2==0, lll)]\n\n[0, 2, 4, 6, 8]"
  },
  {
    "objectID": "core/slides/slides03_sparkrdd.html#transformations-distinct-and-sample",
    "href": "core/slides/slides03_sparkrdd.html#transformations-distinct-and-sample",
    "title": "Apache and RDD",
    "section": "Transformations: distinct and sample",
    "text": "Transformations: distinct and sample\n\n\n\n\n\n\n\ntransformation\ndescription\n\n\n\n\ndistinct()\nRemoves duplicates\n\n\nsample(withReplacement, fraction, [seed])\nSample an RDD, with or without replacement\n\n\n\n\n\n\n\n\n\n\nExample\n\n\n&gt;&gt;&gt; rdd = sc.parallelize([1, 1, 4, 2, 1, 3, 3])\n&gt;&gt;&gt; rdd.distinct().collect()\n[1, 2, 3, 4]"
  },
  {
    "objectID": "core/slides/slides03_sparkrdd.html#pythons-corner-2",
    "href": "core/slides/slides03_sparkrdd.html#pythons-corner-2",
    "title": "Apache and RDD",
    "section": "Python’s corner",
    "text": "Python’s corner"
  },
  {
    "objectID": "core/slides/slides03_sparkrdd.html#transformations-3",
    "href": "core/slides/slides03_sparkrdd.html#transformations-3",
    "title": "Apache and RDD",
    "section": "Transformations",
    "text": "Transformations\nWe have also pseudo-set-theoretical operations\n\n\n\n\n\n\n\ntransformation\ndescription\n\n\n\n\nunion(otherRdd)\nReturns union with otherRdd\n\n\ninstersection(otherRdd)\nReturns intersection with otherRdd\n\n\nsubtract(otherRdd)\nReturn each value in self that is not contained in otherRdd.\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nIf there are duplicates in the input RDD, the result of union() will contain duplicates (fixed with distinct())\nintersection() removes all duplicates (including duplicates from a single RDD)\nPerformance of intersection() is much worse than union() since it requires a shuffle to identify common elements\nsubtract also requires a shuffle"
  },
  {
    "objectID": "core/slides/slides03_sparkrdd.html#transformations-4",
    "href": "core/slides/slides03_sparkrdd.html#transformations-4",
    "title": "Apache and RDD",
    "section": "Transformations",
    "text": "Transformations\nWe have also pseudo-set-theoretical operations\n\n\n\n\n\n\n\ntransformation\ndescription\n\n\n\n\nunion(otherRdd)\nReturns union with otherRdd\n\n\ninstersection(otherRdd)\nReturns intersection with otherRdd\n\n\nsubtract(otherRdd)\nReturn each value in self that is not contained in otherRdd.\n\n\n\n\n\n\n\n\n\n\nExample with union and distinct\n\n\n&gt;&gt;&gt; rdd1 = sc.parallelize(range(5))\n&gt;&gt;&gt; rdd2 = sc.parallelize(range(3, 9))\n&gt;&gt;&gt; rdd3 = rdd1.union(rdd2)\n&gt;&gt;&gt; rdd3.collect()\n[0, 1, 2, 3, 4, 3, 4, 5, 6, 7, 8]\n&gt;&gt;&gt; rdd3.distinct().collect()\n[0, 1, 2, 3, 4, 5, 6, 7, 8]\n\n\n\n\n\n\nHow does Spark decide whether two RDD items are equal?"
  },
  {
    "objectID": "core/slides/slides03_sparkrdd.html#pythons-corner-3",
    "href": "core/slides/slides03_sparkrdd.html#pythons-corner-3",
    "title": "Apache and RDD",
    "section": "Python’s corner",
    "text": "Python’s corner\n\n# %%\nspam = list(range(5)) + list(range(3, 9))\n[o for o in set(spam)]\n\n\n\n\n\n\nHow does Python decide whether two objects are equal/identical?\nSee also all_unique() from more_itertools"
  },
  {
    "objectID": "core/slides/slides03_sparkrdd.html#about-shuffles",
    "href": "core/slides/slides03_sparkrdd.html#about-shuffles",
    "title": "Apache and RDD",
    "section": " About shuffles",
    "text": "About shuffles\n\nCertain operations trigger a shuffle\nIt is Spark’s mechanism for redistributing data so as to modify the partitioning\nIt involves moving data across executors and machines, making shuffle a complex and costly operation\nMore on shuffles later"
  },
  {
    "objectID": "core/slides/slides03_sparkrdd.html#performance-impact",
    "href": "core/slides/slides03_sparkrdd.html#performance-impact",
    "title": "Apache and RDD",
    "section": " Performance Impact",
    "text": "Performance Impact\n\nA shuffle involves\n\ndisk I/O,\ndata serialization\nnetwork I/O.\n\n\n\n\nTo organize data for the shuffle, Spark generates sets of tasks:\n\nmap tasks to organize the data and\nreduce tasks to aggregate it\n\n\n\n\n\nThis vocabulary comes from MapReduce and does not directly relate to Spark’s map and reduce operations."
  },
  {
    "objectID": "core/slides/slides03_sparkrdd.html#transformations-5",
    "href": "core/slides/slides03_sparkrdd.html#transformations-5",
    "title": "Apache and RDD",
    "section": "Transformations",
    "text": "Transformations\nAnother pseudo set operation\n\n\n\n\n\n\n\ntransformation\ndescription\n\n\n\n\ncartesian(otherRdd)\nReturn the Cartesian product of this RDD and another one\n\n\n\n\n\n\n\n\n\n\nExample\n\n\n&gt;&gt;&gt; rdd1 = sc.parallelize([1, 2])\n&gt;&gt;&gt; rdd2 = sc.parallelize([\"a\", \"b\"])\n&gt;&gt;&gt; rdd1.cartesian(rdd2).collect()\n[(1, 'a'), (1, 'b'), (2, 'a'), (2, 'b')]\n\n\n\n cartesian() is very expensive for large RDDs\n\n\n\nLet’s go to notebook05_sparkrdd.ipynb"
  },
  {
    "objectID": "core/slides/slides03_sparkrdd.html#actions-1",
    "href": "core/slides/slides03_sparkrdd.html#actions-1",
    "title": "Apache and RDD",
    "section": "Actions",
    "text": "Actions\n\n\ncollect() brings the RDD back to the driver\n\n\n\ntransformation\ndescription\n\n\n\n\ncollect()\nReturn all elements from the RDD\n\n\n\nExample\n&gt;&gt;&gt; rdd = sc.parallelize([1, 2, 3, 3])\n&gt;&gt;&gt; rdd.collect()\n[1, 2, 3, 3]\n\n\n\n\n\n Be sure that the retrieved data fits in the driver memory !\nUseful when developping and working on small data for testing\n We’ll use it a lot here, but we don’t use it in real-world problems"
  },
  {
    "objectID": "core/slides/slides03_sparkrdd.html#actions-2",
    "href": "core/slides/slides03_sparkrdd.html#actions-2",
    "title": "Apache and RDD",
    "section": "Actions",
    "text": "Actions\nCounts matter!\n\n\n\n\n\n\n\ntransformation\ndescription\n\n\n\n\ncount()\nReturn the number of elements in the RDD\n\n\ncountByValue()\nReturn the count of each unique value in the RDD as a dictionary of {value: count} pairs.\n\n\n\n\n\n\n\n\n\nExample\n\n\n&gt;&gt;&gt; rdd = sc.parallelize([1, 3, 1, 2, 2, 2])\n&gt;&gt;&gt; rdd.count()\n6\n&gt;&gt;&gt; rdd.countByValue()\ndefaultdict(int, {1: 2, 3: 1, 2: 3})"
  },
  {
    "objectID": "core/slides/slides03_sparkrdd.html#pythons-corner-4",
    "href": "core/slides/slides03_sparkrdd.html#pythons-corner-4",
    "title": "Apache and RDD",
    "section": "Python’s corner",
    "text": "Python’s corner\n\nspam = [1, 3, 1, 2, 2, 2]\n\nlen(spam)\n\nfrom collections import Counter\n\nCounter(spam)\n\nCounter({2: 3, 1: 2, 3: 1})\n\n\n\n\nhttps://docs.python.org/3/library/collections.html#collections.Counter"
  },
  {
    "objectID": "core/slides/slides03_sparkrdd.html#actions-cherry-picking",
    "href": "core/slides/slides03_sparkrdd.html#actions-cherry-picking",
    "title": "Apache and RDD",
    "section": "Actions: cherry-picking",
    "text": "Actions: cherry-picking\nHow to get some (but not all) values in an RDD ?\n\n\n\n\n\n\n\naction\ndescription\n\n\n\n\ntake(n)\nReturn n elements from the RDD (deterministic)\n\n\ntop(n)\nReturn first n elements from the RDD (descending order)\n\n\ntakeOrdered(num, key=None)\nGet the N elements from a RDD ordered in ascending order or as specified by the optional key function.\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\ntake(n) returns n elements from the RDD and attempts to minimize the number of partitions it accesses\n the result may be a biased collection\ncollect and take may return the elements in an order you don’t expect"
  },
  {
    "objectID": "core/slides/slides03_sparkrdd.html#pythons-corner-5",
    "href": "core/slides/slides03_sparkrdd.html#pythons-corner-5",
    "title": "Apache and RDD",
    "section": "Python’s corner",
    "text": "Python’s corner\n\nlist(itertools.islice(list(range(10)), 3))\n\n[0, 1, 2]"
  },
  {
    "objectID": "core/slides/slides03_sparkrdd.html#actions-3",
    "href": "core/slides/slides03_sparkrdd.html#actions-3",
    "title": "Apache and RDD",
    "section": "Actions",
    "text": "Actions\nHow to get some values in an RDD?\n\n\n\n\n\n\n\naction\ndescription\n\n\n\n\ntake(n)\nReturn n elements from the RDD (deterministic)\n\n\ntop(n)\nReturn first n elements from the RDD (decending order)\n\n\ntakeOrdered(num, key=None)\nGet the $N $elements from a RDD ordered in ascending order or as specified by the optional key function.\n\n\n\n\n\n\n\n\n\n\nExample\n\n\n&gt;&gt;&gt; rdd = sc.parallelize([(3, 'a'), (1, 'b'), (2, 'd')])\n&gt;&gt;&gt; rdd.takeOrdered(2)\n[(1, 'b'), (2, 'd')]\n&gt;&gt;&gt; rdd.takeOrdered(2, key=lambda x: x[1])\n[(3, 'a'), (1, 'b')]"
  },
  {
    "objectID": "core/slides/slides03_sparkrdd.html#actions-reductions",
    "href": "core/slides/slides03_sparkrdd.html#actions-reductions",
    "title": "Apache and RDD",
    "section": "Actions: reduction(s)",
    "text": "Actions: reduction(s)\n\n\n\n\n\n\n\n\naction\ndescription\n\n\n\n\nreduce(f)\nReduces the elements of this RDD using the specified commutative and associative binary operator f.\n\n\nfold(zeroValue, op)\nSame as reduce() but with the provided zero value.\n\n\n\n\n\n\nop(x, y) is allowed to modify x and return it as its result value to avoid object allocation; however, it should not modify y.\nreduce applies some operation to pairs of elements until there is just one left. Throws an exception for empty collections.\nfold has initial zero-value: defined for empty collections."
  },
  {
    "objectID": "core/slides/slides03_sparkrdd.html#actions-reductions-1",
    "href": "core/slides/slides03_sparkrdd.html#actions-reductions-1",
    "title": "Apache and RDD",
    "section": "Actions: reduction(s)",
    "text": "Actions: reduction(s)\n\n\n\n\n\n\n\n\naction\ndescription\n\n\n\n\nreduce(f)\nReduces the elements of this RDD using the specified commutative and associative binary operator f.\n\n\nfold(zeroValue, op)\nSame as reduce() but with the provided zero value.\n\n\n\n\n\n\n\n\n\n\n\nExample\n\n\n&gt;&gt;&gt; rdd = sc.parallelize([1, 2, 3])\n&gt;&gt;&gt; rdd.reduce(lambda a, b: a + b)\n6\n&gt;&gt;&gt; rdd.fold(0, lambda a, b: a + b)\n6"
  },
  {
    "objectID": "core/slides/slides03_sparkrdd.html#actions-reductions-2",
    "href": "core/slides/slides03_sparkrdd.html#actions-reductions-2",
    "title": "Apache and RDD",
    "section": "Actions: reduction(s)",
    "text": "Actions: reduction(s)\n\n\n\n\n\n\n\n\naction\ndescription\n\n\n\n\nreduce(f)\nReduces the elements of this RDD using the specified commutative and associative binary operator f.\n\n\nfold(zeroValue, op)\nSame as reduce() but with the provided zero value.\n\n\n\n\n\n\n\n\n\n\n\nWarning\n\n\nWith fold, solutions can depend on the number of partitions\n&gt;&gt;&gt; rdd = sc.parallelize([1, 2, 4], 2) # RDD with 2 partitions\n&gt;&gt;&gt; rdd.fold(2.5, lambda a, b: a + b)\n14.5\n\nRDD has 2 partition: say [1, 2] and [4]\nSum in the partitions: 2.5 + (1 + 2) = 5.5 and 2.5 + (4) = 6.5\nSum over partitions: 2.5 + (5.5 + 6.5) = 14.5"
  },
  {
    "objectID": "core/slides/slides03_sparkrdd.html#actions-reductions-3",
    "href": "core/slides/slides03_sparkrdd.html#actions-reductions-3",
    "title": "Apache and RDD",
    "section": "Actions: reduction(s)",
    "text": "Actions: reduction(s)\n\n\n\n\n\n\n\n\naction\ndescription\n\n\n\n\nreduce(f)\nReduces the elements of this RDD using the specified commutative and associative binary operator f.\n\n\nfold(zeroValue, op)\nSame as reduce() but with the provided zero value.\n\n\n\n\n\n\n\n\n\n\n\nWarning\n\n\nSolutions can depend on the number of partitions\n&gt;&gt;&gt; rdd = sc.parallelize([1, 2, 3], 5) # RDD with 5 partitions\n&gt;&gt;&gt; rdd.fold(2, lambda a, b: a + b)\n\n\n\n\n\n\n\n\n\nNote\n\n\nBack to Jupyter notebook V : Spark RDD"
  },
  {
    "objectID": "core/slides/slides03_sparkrdd.html#actions-reductions-4",
    "href": "core/slides/slides03_sparkrdd.html#actions-reductions-4",
    "title": "Apache and RDD",
    "section": "Actions: reduction(s)",
    "text": "Actions: reduction(s)\n\n\n\n\n\n\n\n\naction\ndescription\n\n\n\n\nreduce(f)\nReduces the elements of this RDD using the specified commutative and associative binary operator f.\n\n\nfold(zeroValue, op)\nSame as reduce() but with the provided zero value.\n\n\n\n\n\n\n\n\n\n\n\nWarning\n\n\nSolutions can depend on the number of partitions\n&gt;&gt;&gt; rdd = sc.parallelize([1, 2, 3], 5) # RDD with 5 partitions\n&gt;&gt;&gt; rdd.fold(2, lambda a, b: a + b)\n18\n\nYes, even if there is less partitions than elements !\n18 = 2 * 5 + (1+2+3) + 2"
  },
  {
    "objectID": "core/slides/slides03_sparkrdd.html#pythons-corner-6",
    "href": "core/slides/slides03_sparkrdd.html#pythons-corner-6",
    "title": "Apache and RDD",
    "section": "Python’s corner",
    "text": "Python’s corner\n\nfrom functools import reduce\n\nreduce(lambda a, b: a + b,  [1, 2, 3])\n1reduce(lambda a, b: a + b,  [1, 2, 3], 2)\n\n\n1\n\ninitial argument used to initialize the accumulator. The default is 0\n\n\n\n\n8"
  },
  {
    "objectID": "core/slides/slides03_sparkrdd.html#actions-aggregate",
    "href": "core/slides/slides03_sparkrdd.html#actions-aggregate",
    "title": "Apache and RDD",
    "section": "Actions : aggregate",
    "text": "Actions : aggregate\n\n\n\n\n\n\n\n\naction\ndescription\n\n\n\n\naggregate(zero, seqOp, combOp)\nSimilar to reduce() but used to return a different type\n\n\n\n\n\nAggregates the elements of each partition, and then the results for all the partitions, given aggregation functions and zero value.\n\nseqOp(acc, val): function to combine the elements of a partition from the RDD (val) with an accumulator (acc).  The result type may differ from the RDD type (if any)\ncombOp: function that merges the accumulators of two partitions\nIn both functions, the first argument can be modified while the second cannot"
  },
  {
    "objectID": "core/slides/slides03_sparkrdd.html#actions-aggregate-1",
    "href": "core/slides/slides03_sparkrdd.html#actions-aggregate-1",
    "title": "Apache and RDD",
    "section": "Actions : aggregate",
    "text": "Actions : aggregate\n\n\n\n\n\n\n\n\naction\ndescription\n\n\n\n\naggregate(zero, seqOp, combOp)\nSimilar to reduce() but used to return a different type\n\n\n\n\n\n\n\n\n\n\n\nExample\n\n\n&gt;&gt;&gt; seqOp = lambda x, y: (x[0] + y, x[1] + 1)\n&gt;&gt;&gt; combOp = lambda x, y: (x[0] + y[0], x[1] + y[1])\n&gt;&gt;&gt; sc.parallelize([1, 2, 3, 4]).aggregate((0, 0), seqOp, combOp)\n(10, 4)\n&gt;&gt;&gt; ( \n      sc.parallelize([])\n        .aggregate((0, 0), seqOp, combOp)\n)\n(0, 0)\n\n\n\n\n\n\nBack to Jupyter notebook V : Spark RDD"
  },
  {
    "objectID": "core/slides/slides03_sparkrdd.html#actions-4",
    "href": "core/slides/slides03_sparkrdd.html#actions-4",
    "title": "Apache and RDD",
    "section": "Actions",
    "text": "Actions\nThe foreach action\n\n\n\naction\ndescription\n\n\n\n\nforeach(f)\nApply a function f to each element of a RDD\n\n\n\n\n\nPerforms an action on all of the elements in the RDD without returning any result to the driver.\nExample : insert records into a database with f\n\n\n\n The foreach() action performs computations on each element of the RDD without bringing it back to the driver"
  },
  {
    "objectID": "core/slides/slides03_sparkrdd.html#lazy-evaluation-and-persistence",
    "href": "core/slides/slides03_sparkrdd.html#lazy-evaluation-and-persistence",
    "title": "Apache and RDD",
    "section": "Lazy evaluation and persistence",
    "text": "Lazy evaluation and persistence\n\nSpark RDDs are lazily evaluated\nEach time an action is called on a RDD, this RDD and all its dependencies are recomputed\nIf you plan to reuse a RDD multiple times, you should use persistence\n\n\n\n\n\n\n\nNote\n\n\n\nLazy evaluation helps spark to reduce the number of passes over the data it has to make by grouping operations together\nNo substantial benefit to writing a single complex map instead of chaining together many simple operations\nUsers are free to organize their program into smaller, more manageable operations"
  },
  {
    "objectID": "core/slides/slides03_sparkrdd.html#persistence-1",
    "href": "core/slides/slides03_sparkrdd.html#persistence-1",
    "title": "Apache and RDD",
    "section": "Persistence",
    "text": "Persistence\nHow to use persistence ?\n\n\n\n\n\n\n\nmethod\ndescription\n\n\n\n\ncache()\nPersist the RDD in memory\n\n\npersist(storageLevel)\nPersist the RDD according to storageLevel\n\n\n\n\n These methods must be called before the action, and do not trigger the actual computation"
  },
  {
    "objectID": "core/slides/slides03_sparkrdd.html#usage-of-storagelevel",
    "href": "core/slides/slides03_sparkrdd.html#usage-of-storagelevel",
    "title": "Apache and RDD",
    "section": "Usage of storageLevel",
    "text": "Usage of storageLevel\npyspark.StorageLevel(\n  useDisk, useMemory, useOffHeap, deserialized, replication=1\n)"
  },
  {
    "objectID": "core/slides/slides03_sparkrdd.html#shades-of-persistence",
    "href": "core/slides/slides03_sparkrdd.html#shades-of-persistence",
    "title": "Apache and RDD",
    "section": "Shades of persistence",
    "text": "Shades of persistence\n\nWhat does persistence in memory mean?\nMake StorageLevel explicit\nAny difference between cache() and persist() with useMemory?\nWhy do we call persistence caching?"
  },
  {
    "objectID": "core/slides/slides03_sparkrdd.html#options-for-persistence",
    "href": "core/slides/slides03_sparkrdd.html#options-for-persistence",
    "title": "Apache and RDD",
    "section": "Options for persistence",
    "text": "Options for persistence\n\nOptions for persistence\n\n\n\n\n\n\n\nargument\ndescription\n\n\n\n\nuseDisk\nAllow caching to use disk if True\n\n\nuseMemory\nAllow caching to use memory if True\n\n\nuseOffHeap\nStore data outside of JVM heap if True. Useful if using some in-memory storage system (such a Tachyon)\n\n\ndeserialized\nCache data without serialization if True\n\n\nreplication\nNumber of replications of the cached data\n\n\n\n\nreplication: If you are caching data that is expensive to compute, you can use replication. If one machine fails, data does not need to be recomputed."
  },
  {
    "objectID": "core/slides/slides03_sparkrdd.html#options-for-persistence-1",
    "href": "core/slides/slides03_sparkrdd.html#options-for-persistence-1",
    "title": "Apache and RDD",
    "section": "Options for persistence",
    "text": "Options for persistence\n\nOptions for persistence\n\n\n\n\n\n\n\nargument\ndescription\n\n\n\n\nuseDisk\nAllow caching to use disk if True\n\n\nuseMemory\nAllow caching to use memory if True\n\n\nuseOffHeap\nStore data outside of JVM heap if True. Useful if using some in-memory storage system (such a Tachyon)\n\n\ndeserialized\nCache data without serialization if True\n\n\nreplication\nNumber of replications of the cached data\n\n\n\n\ndeserialized :\n\nSerialization consists in converting data to some binary format\nTo the best of our knowledge, PySpark only support serialized caching (using pickle)"
  },
  {
    "objectID": "core/slides/slides03_sparkrdd.html#options-for-persistence-2",
    "href": "core/slides/slides03_sparkrdd.html#options-for-persistence-2",
    "title": "Apache and RDD",
    "section": "Options for persistence",
    "text": "Options for persistence\n\nOptions for persistence\n\n\n\n\n\n\n\nargument\ndescription\n\n\n\n\nuseDisk\nAllow caching to use disk if True\n\n\nuseMemory\nAllow caching to use memory if True\n\n\nuseOffHeap\nStore data outside of JVM heap if True. Useful if using some in-memory storage system (such a Tachyon)\n\n\ndeserialized\nCache data without serialization if True\n\n\nreplication\nNumber of replications of the cached data\n\n\n\n\n\nuseOffHeap\n\n\n\n\n\nData cached in the JVM heap by default\nVery interesting alternative in-memory solutions such as tachyon\nDon’t forget that spark is scala running on the JVM"
  },
  {
    "objectID": "core/slides/slides03_sparkrdd.html#back-to-options-for-persistence",
    "href": "core/slides/slides03_sparkrdd.html#back-to-options-for-persistence",
    "title": "Apache and RDD",
    "section": "Back to options for persistence",
    "text": "Back to options for persistence\nStorageLevel(useDisk, useMemory, useOffHeap, deserialized, replication)\nYou can use these constants:\nDISK_ONLY = StorageLevel(True, False, False, False, 1)\nDISK_ONLY_2 = StorageLevel(True, False, False, False, 2)\nMEMORY_AND_DISK = StorageLevel(True, True, False, True, 1)\nMEMORY_AND_DISK_2 = StorageLevel(True, True, False, True, 2)\nMEMORY_AND_DISK_SER = StorageLevel(True, True, False, False, 1)\nMEMORY_AND_DISK_SER_2 = StorageLevel(True, True, False, False, 2)\nMEMORY_ONLY = StorageLevel(False, True, False, True, 1)\nMEMORY_ONLY_2 = StorageLevel(False, True, False, True, 2)\nMEMORY_ONLY_SER = StorageLevel(False, True, False, False, 1)\nMEMORY_ONLY_SER_2 = StorageLevel(False, True, False, False, 2)\nOFF_HEAP = StorageLevel(False, False, True, False, 1)\nand simply call for instance\nrdd.persist(MEMORY_AND_DISK)"
  },
  {
    "objectID": "core/slides/slides03_sparkrdd.html#persistence-2",
    "href": "core/slides/slides03_sparkrdd.html#persistence-2",
    "title": "Apache and RDD",
    "section": "Persistence",
    "text": "Persistence\nWhat if you attempt to cache too much data to fit in memory ?\nSpark will automatically evict old partitions using a Least Recently Used (LRU) cache policy:\n\nFor the memory-only storage levels, it will recompute these partitions the next time they are accessed\nFor the memory-and-disk ones, it will write them out to disk\n\nUse unpersist() to RDDs to manually remove them from the cache"
  },
  {
    "objectID": "core/slides/slides03_sparkrdd.html#reminder-about-passing-functions",
    "href": "core/slides/slides03_sparkrdd.html#reminder-about-passing-functions",
    "title": "Apache and RDD",
    "section": "Reminder: about passing functions ",
    "text": "Reminder: about passing functions \n\n\n\n\n\n\n\nWarning\n\n\nWhen passing functions, you can inadvertently serialize the object containing the function.\n\n\n\n\nIf you pass a function that:\n\nis the member of an object (a method)\ncontains references to fields in an object\n\nthen Spark sends the entire object to worker nodes, which can be much larger than the bit of information you need\n\n\n\n\n\n\nCaution\n\n\nThis can cause your program to fail, if your class contains objects that Python can’t pickle"
  },
  {
    "objectID": "core/slides/slides03_sparkrdd.html#about-passing-functions",
    "href": "core/slides/slides03_sparkrdd.html#about-passing-functions",
    "title": "Apache and RDD",
    "section": "About passing functions",
    "text": "About passing functions\nPassing a function with field references (don’t do this !  )\nclass SearchFunctions(object):\n  \n  def __init__(self, query):\n      self.query = query\n\n  def isMatch(self, s):\n      return self.query in s\n\n  def getMatchesFunctionReference(self, rdd):\n      # Problem: references all of \"self\" in \"self.isMatch\"\n      return rdd.filter(self.isMatch)\n\n  def getMatchesMemberReference(self, rdd):\n      # Problem: references all of \"self\" in \"self.query\"\n      return rdd.filter(lambda x: self.query in x)\n\n\n\n\n\n\nTip\n\n\nInstead, just extract the fields you need from your object into a local variable and pass that in"
  },
  {
    "objectID": "core/slides/slides03_sparkrdd.html#about-passing-functions-1",
    "href": "core/slides/slides03_sparkrdd.html#about-passing-functions-1",
    "title": "Apache and RDD",
    "section": "About passing functions",
    "text": "About passing functions\nPython function passing without field references\nclass WordFunctions(object):\n  ...\n\ndef getMatchesNoReference(self, rdd):\n  # Safe: extract only the field we need into a local variable\n  query = self.query\n  return rdd.filter(lambda x: query in x)\n\nMuch better!"
  },
  {
    "objectID": "core/slides/slides03_sparkrdd.html#pair-rdd-key-value-pairs-1",
    "href": "core/slides/slides03_sparkrdd.html#pair-rdd-key-value-pairs-1",
    "title": "Apache and RDD",
    "section": "Pair RDD: key-value pairs",
    "text": "Pair RDD: key-value pairs\nIt’s roughly a RDD where each element is a tuple with two elements: a key and a value\n\n\nFor numerous tasks, such as aggregations tasks, storing information as (key, value) pairs into RDD is very convenient\nSuch RDDs are called PairRDD\nPair RDDs expose new operations such as grouping together data with the same key, and grouping together two different RDDs"
  },
  {
    "objectID": "core/slides/slides03_sparkrdd.html#creating-a-pair-rdd",
    "href": "core/slides/slides03_sparkrdd.html#creating-a-pair-rdd",
    "title": "Apache and RDD",
    "section": "Creating a pair RDD",
    "text": "Creating a pair RDD\nCalling map with a function returning a tuple with two elements\n&gt;&gt;&gt; rdd = sc.parallelize([[1, \"a\", 7], [2, \"b\", 13], [2, \"c\", 17]])\n&gt;&gt;&gt; rdd = rdd.map(lambda x: (x[0], x[1:]))\n&gt;&gt;&gt; rdd.collect()\n[(1, ['a', 7]), (2, ['b', 13]), (2, ['c', 17])]"
  },
  {
    "objectID": "core/slides/slides03_sparkrdd.html#warning-5",
    "href": "core/slides/slides03_sparkrdd.html#warning-5",
    "title": "Apache and RDD",
    "section": " Warning",
    "text": "Warning\nAll elements of a PairRDD must be tuples with two elements (the key and the value)\n&gt;&gt;&gt; rdd = sc.parallelize([[1, \"a\", 7], [2, \"b\", 13], [2, \"c\", 17]])\n&gt;&gt;&gt; rdd.keys().collect()\n[1, 2, 2]\n&gt;&gt;&gt; rdd.values().collect()\n['a', 'b', 'c']\n\nFor things to work as expected you must do\n&gt;&gt;&gt; rdd = sc.parallelize([[1, \"a\", 7], [2, \"b\", 13], [2, \"c\", 17]])\\\n      .map(lambda x: (x[0], x[1:]))\n&gt;&gt;&gt; rdd.keys().collect()\n[1, 2, 2]\n&gt;&gt;&gt; rdd.values().collect()\n[['a', 7], ['b', 13], ['c', 17]]"
  },
  {
    "objectID": "core/slides/slides03_sparkrdd.html#transformations-for-a-single-pairrdd",
    "href": "core/slides/slides03_sparkrdd.html#transformations-for-a-single-pairrdd",
    "title": "Apache and RDD",
    "section": "Transformations for a single PairRDD",
    "text": "Transformations for a single PairRDD\n\n\n\n\n\n\n\n\ntransformation\ndescription\n\n\n\n\nkeys()\nReturn an RDD containing the keys\n\n\nvalues()\nReturn an RDD containing the values\n\n\nsortByKey()\nReturn an RDD sorted by the key\n\n\nmapValues(f)\nApply a function f to each value of a pair RDD without changing the key\n\n\nflatMapValues(f)\nPass each value in the key-value pair RDD through a flatMap function f without changing the keys"
  },
  {
    "objectID": "core/slides/slides03_sparkrdd.html#transformations-for-a-single-pairrdd-1",
    "href": "core/slides/slides03_sparkrdd.html#transformations-for-a-single-pairrdd-1",
    "title": "Apache and RDD",
    "section": "Transformations for a single PairRDD",
    "text": "Transformations for a single PairRDD\n\n\n\n\n\n\n\n\ntransformation\ndescription\n\n\n\n\nkeys()\nReturn an RDD containing the keys\n\n\nvalues()\nReturn an RDD containing the values\n\n\nsortByKey()\nReturn an RDD sorted by the key\n\n\nmapValues(f)\nApply a function f to each value of a pair RDD without changing the key\n\n\nflatMapValues(f)\nPass each value in the key-value pair RDD through a flatMap function f without changing the keys\n\n\n\n\n\nExample with mapValues\n&gt;&gt;&gt; rdd = sc.parallelize([(\"a\", \"x y z\"), (\"b\", \"p r\")])\n&gt;&gt;&gt; rdd.mapValues(lambda v: v.split(' ')).collect()\n[('a', ['x', 'y', 'z']), ('b', ['p', 'r'])]"
  },
  {
    "objectID": "core/slides/slides03_sparkrdd.html#transformations-for-a-single-pairrdd-2",
    "href": "core/slides/slides03_sparkrdd.html#transformations-for-a-single-pairrdd-2",
    "title": "Apache and RDD",
    "section": "Transformations for a single PairRDD",
    "text": "Transformations for a single PairRDD\n\n\n\n\n\n\n\n\ntransformation\ndescription\n\n\n\n\nkeys()\nReturn an RDD containing the keys\n\n\nvalues()\nReturn an RDD containing the values\n\n\nsortByKey()\nReturn an RDD sorted by the key\n\n\nmapValues(f)\nApply a function f to each value of a pair RDD without changing the key\n\n\nflatMapValues(f)\nPass each value in the key-value pair RDD through a flatMap function f without changing the keys\n\n\n\n\n\nExample with flatMapValues\n&gt;&gt;&gt; texts = sc.parallelize([(\"a\", \"x y z\"), (\"b\", \"p r\")])\n&gt;&gt;&gt; tokenize = lambda x: x.split(\" \")\n&gt;&gt;&gt; texts.flatMapValues(tokenize).collect()\n[('a', 'x'), ('a', 'y'), ('a', 'z'), ('b', 'p'), ('b', 'r')]"
  },
  {
    "objectID": "core/slides/slides03_sparkrdd.html#transformations-for-a-single-pairrdd-keyed",
    "href": "core/slides/slides03_sparkrdd.html#transformations-for-a-single-pairrdd-keyed",
    "title": "Apache and RDD",
    "section": "Transformations for a single PairRDD (keyed)",
    "text": "Transformations for a single PairRDD (keyed)\n\n\n\n\n\n\n\n\ntransformation\ndescription\n\n\n\n\ngroupByKey()\nGroup values with the same key\n\n\nreduceByKey(f)\nMerge the values for each key using an associative reduce function f.\n\n\nfoldByKey(f)\nMerge the values for each key using an associative reduce function f.\n\n\ncombineByKey(createCombiner, mergeValue, mergeCombiners, [partitioner])\nGeneric function to combine the elements for each key using a custom set of aggregation functions."
  },
  {
    "objectID": "core/slides/slides03_sparkrdd.html#transformations-for-a-single-pairrdd-keyed-1",
    "href": "core/slides/slides03_sparkrdd.html#transformations-for-a-single-pairrdd-keyed-1",
    "title": "Apache and RDD",
    "section": "Transformations for a single PairRDD (keyed)",
    "text": "Transformations for a single PairRDD (keyed)\n\n\n\n\n\n\n\n\ntransformation\ndescription\n\n\n\n\ngroupByKey()\nGroup values with the same key\n\n\nreduceByKey(f)\nMerge the values for each key using an associative reduce function f.\n\n\nfoldByKey(f)\nMerge the values for each key using an associative reduce function f.\n\n\ncombineByKey(createCombiner, mergeValue, mergeCombiners, [partitioner])\nGeneric function to combine the elements for each key using a custom set of aggregation functions.\n\n\n\n\n\nExample with groupByKey\n&gt;&gt;&gt; rdd = sc.parallelize([\n        (\"a\", 1), (\"b\", 1), (\"a\", 1), \n        (\"b\", 3), (\"c\", 42)\n        ])\n&gt;&gt;&gt; rdd.groupByKey().mapValues(list).collect()\n[('c', [42]), ('b', [1, 3]), ('a', [1, 1])]"
  },
  {
    "objectID": "core/slides/slides03_sparkrdd.html#groupbykey-internals",
    "href": "core/slides/slides03_sparkrdd.html#groupbykey-internals",
    "title": "Apache and RDD",
    "section": "groupByKey() internals",
    "text": "groupByKey() internals\n\nGrouping locally\n Shuffling\nPartitionning\nRelation to reduceByKey()"
  },
  {
    "objectID": "core/slides/slides03_sparkrdd.html#transformations-for-a-single-pairrdd-keyed-2",
    "href": "core/slides/slides03_sparkrdd.html#transformations-for-a-single-pairrdd-keyed-2",
    "title": "Apache and RDD",
    "section": "Transformations for a single PairRDD (keyed)",
    "text": "Transformations for a single PairRDD (keyed)\n\n\n\n\n\n\n\n\ntransformation\ndescription\n\n\n\n\ngroupByKey()\nGroup values with the same key\n\n\nreduceByKey(f)\nMerge the values for each key using an associative reduce function f.\n\n\nfoldByKey(f)\nMerge the values for each key using an associative reduce function f.\n\n\ncombineByKey(createCombiner, mergeValue, mergeCombiners, [partitioner])\nGeneric function to combine the elements for each key using a custom set of aggregation functions.\n\n\n\n\n\nExample with reduceByKey\n&gt;&gt;&gt; rdd = sc.parallelize([(\"a\", 1), (\"b\", 1), (\"a\", 1)])\n&gt;&gt;&gt; rdd.reduceByKey(lambda a, b: a + b).collect()\n[('a', 2), ('b', 1)]\n\nThe reducing occurs first locally (within partitions)\nThen, a shuffle is performed with the local results to reduce globally"
  },
  {
    "objectID": "core/slides/slides03_sparkrdd.html#reducebykey-in-picture",
    "href": "core/slides/slides03_sparkrdd.html#reducebykey-in-picture",
    "title": "Apache and RDD",
    "section": "ReduceByKey in picture",
    "text": "ReduceByKey in picture"
  },
  {
    "objectID": "core/slides/slides03_sparkrdd.html#transformations-for-a-single-pairrdd-keyed-3",
    "href": "core/slides/slides03_sparkrdd.html#transformations-for-a-single-pairrdd-keyed-3",
    "title": "Apache and RDD",
    "section": "Transformations for a single PairRDD (keyed)",
    "text": "Transformations for a single PairRDD (keyed)\n\n\n\n\n\n\n\n\ntransformation\ndescription\n\n\n\n\ngroupByKey()\nGroup values with the same key\n\n\nreduceByKey(f)\nMerge the values for each key using an associative reduce function f.\n\n\nfoldByKey(f)\nMerge the values for each key using an associative reduce function f.\n\n\ncombineByKey(createCombiner, mergeValue, mergeCombiners, [partitioner])\nGeneric function to combine the elements for each key using a custom set of aggregation functions.\n\n\n\n\n\ncombineByKey Transforms an RDD[(K, V)] into another RDD of type RDD[(K, C)] for a combined type C that can be different from V\n\n\nThe user must define\n\ncreateCombiner : which turns a V into a C\nmergeValue : to merge a V into a C\nmergeCombiners : to combine two C’s into a single one"
  },
  {
    "objectID": "core/slides/slides03_sparkrdd.html#transformations-for-a-single-pairrdd-keyed-4",
    "href": "core/slides/slides03_sparkrdd.html#transformations-for-a-single-pairrdd-keyed-4",
    "title": "Apache and RDD",
    "section": "Transformations for a single PairRDD (keyed)",
    "text": "Transformations for a single PairRDD (keyed)\n\n\n\n\n\n\n\n\ntransformation\ndescription\n\n\n\n\ngroupByKey()\nGroup values with the same key\n\n\nreduceByKey(f)\nMerge the values for each key using an associative reduce function f.\n\n\nfoldByKey(f)\nMerge the values for each key using an associative reduce function f.\n\n\ncombineByKey(createCombiner, mergeValue, mergeCombiners, [partitioner])\nGeneric function to combine the elements for each key using a custom set of aggregation functions.\n\n\n\n\n\nIn this example\n\ncreateCombiner : converts the value to str\nmergeValue : concatenates two str\nmergeCombiners : concatenates two str\n\n&gt;&gt;&gt; rdd = sc.parallelize([('a', 1), ('b', 2), ('a', 13)])\n&gt;&gt;&gt; def add(a, b):\n        return a + str(b)\n&gt;&gt;&gt; rdd.combineByKey(str, add, add).collect()\n[('a', '113'), ('b', '2')]"
  },
  {
    "objectID": "core/slides/slides03_sparkrdd.html#transformations-for-two-pairrdd",
    "href": "core/slides/slides03_sparkrdd.html#transformations-for-two-pairrdd",
    "title": "Apache and RDD",
    "section": "Transformations for two PairRDD",
    "text": "Transformations for two PairRDD\n\n\n\n\n\n\n\ntransformation\ndescription\n\n\n\n\nsubtractByKey(other)\nRemove elements with a key present in the other RDD.\n\n\njoin(other)\nInner join with other RDD.\n\n\nrightOuterJoin(other)\nRight join with other RDD.\n\n\nleftOuterJoin(other)\nLeft join with other RDD.\n\n\n\n\nRight join: the key must be present in the first RDD\nLeft join: the key must be present in the other RDD"
  },
  {
    "objectID": "core/slides/slides03_sparkrdd.html#transformations-for-two-pairrdd-1",
    "href": "core/slides/slides03_sparkrdd.html#transformations-for-two-pairrdd-1",
    "title": "Apache and RDD",
    "section": "Transformations for two PairRDD",
    "text": "Transformations for two PairRDD\n\nJoin operations are mainly used through the high-level API: DataFrame objects and the spark.sql API\nWe will use them a lot with the high-level API (DataFrame from spark.sql)\n\n\n\nBack to Jupyter notebook V : Spark RDD"
  },
  {
    "objectID": "core/slides/slides03_sparkrdd.html#actions-for-a-single-pairrdd",
    "href": "core/slides/slides03_sparkrdd.html#actions-for-a-single-pairrdd",
    "title": "Apache and RDD",
    "section": "Actions for a single PairRDD",
    "text": "Actions for a single PairRDD\n\n\n\n\n\n\n\naction\ndescription\n\n\n\n\ncountByKey()\nCount the number of elements for each key.\n\n\nlookup(key)\nReturn all the values associated with the provided key.\n\n\ncollectAsMap()\nReturn the key-value pairs in this RDD to the master as a Python dictionary.\n\n\n\n\n\nBack to Jupyter notebook V : Spark RDD"
  },
  {
    "objectID": "core/slides/slides03_sparkrdd.html#data-partitionning",
    "href": "core/slides/slides03_sparkrdd.html#data-partitionning",
    "title": "Apache and RDD",
    "section": "Data partitionning",
    "text": "Data partitionning\n\nSome operations on PairRDDs, such as join, require to scan the data more than once\nPartitionning the RDDs in advance can reduce network communications\nWhen a key-oriented dataset is reused several times, partitionning can improve performance\nIn Spark: you can choose which keys will appear on the same node, but no explicit control of which worker node each key goes to."
  },
  {
    "objectID": "core/slides/slides03_sparkrdd.html#data-partitionning-1",
    "href": "core/slides/slides03_sparkrdd.html#data-partitionning-1",
    "title": "Apache and RDD",
    "section": "Data partitionning",
    "text": "Data partitionning\nIn practice, you can specify the number of partitions with\nrdd.partitionBy(100)\n\nYou can also use a custom partition function hash such that hash(key) returns a hash value\nimport urlparse\n\n&gt;&gt;&gt; def hash_domain(url):\n        # Returns a hash associated to the domain of a website\n        return hash(urlparse.urlparse(url).netloc)\n\nrdd.partitionBy(20, hash_domain) # Create 20 partitions\nTo have finer control on partitionning, you must use the Scala API."
  },
  {
    "objectID": "core/slides/slides03_sparkrdd.html#section-7",
    "href": "core/slides/slides03_sparkrdd.html#section-7",
    "title": "Apache and RDD",
    "section": "",
    "text": "Partitionning tweaking\nShuffles monitoring"
  },
  {
    "objectID": "core/slides/slides05_json-format.html#what-is-json",
    "href": "core/slides/slides05_json-format.html#what-is-json",
    "title": "SPARK & JSON",
    "section": "What is JSON ?",
    "text": "What is JSON ?\n\nJavaScript Object Notation (JSON) is a lightweight data-interchange format based on the syntax of JavaScript objects\nIt is a text-based, human-readable, language-independent format for representing structured object data for easy transmission or saving\nJSON objects can also be stored in files — typically a text file with a .json extension\nJSON is used for two-way data transmission between a web-server and a client, but it is also often used as a semi-structured data format\nIts syntax closely resembles JavaScript objects, but JSON can be used independently of JavaScript\n\n\nhttps://career.guru99.com/top-19-json-interview-questions/\n\nJSON is a data exchange format. JSON means JavaScript Object Notation; it is language and platform independent."
  },
  {
    "objectID": "core/slides/slides05_json-format.html#handling-json",
    "href": "core/slides/slides05_json-format.html#handling-json",
    "title": "SPARK & JSON",
    "section": "Handling JSON?",
    "text": "Handling JSON?\n\nMost languages have libraries to manipulate JSON\nIn  we shall use JSON data in python using the json module from the standard library\n has several JSON packages to handle JSON. For example jsonlite"
  },
  {
    "objectID": "core/slides/slides05_json-format.html#lexikon",
    "href": "core/slides/slides05_json-format.html#lexikon",
    "title": "SPARK & JSON",
    "section": "Lexikon",
    "text": "Lexikon\n\nJSON objects should be thought of as strings or a sequences (or series) of bytes complying with the JSON syntax\n\n\n\nSerialization: convert an object (for example a dict) to a JSON representation. The object is encoded for easy storage and/or transmission\n\n\n\n\nDeserialization: the reverse transformation of serialization. Involves decoding data in JSON format to native data types that can be manipulated\n\n\n\n\nStrings and Bytes"
  },
  {
    "objectID": "core/slides/slides05_json-format.html#why-json",
    "href": "core/slides/slides05_json-format.html#why-json",
    "title": "SPARK & JSON",
    "section": "Why JSON ?",
    "text": "Why JSON ?\n\n.stress[Much smaller representation than XML] (its predecessor) in client-server communication: faster data transfers\nJSON exists as a sequence of bytes: very useful to transmit (stream) data over a network\nJSON is reader-friendly since it is ultimately text and simultaneously machine-friendly\nJSON has an expressive syntax for representing arrays, objects, numbers and booleans/logicals"
  },
  {
    "objectID": "core/slides/slides05_json-format.html#working-with-built-in-datatypes",
    "href": "core/slides/slides05_json-format.html#working-with-built-in-datatypes",
    "title": "SPARK & JSON",
    "section": "Working with built-in datatypes",
    "text": "Working with built-in datatypes\nThe json module ()\n\nencodes Python objects as JSON strings using instances of class json.JSONEncoder\ndecodes JSON strings into Python objects using instances of class json.JSONDecoder\n\n\n\n\n\n\n\nWarning\n\n\nThe JSON encoder only handles native Python data types (str, int, float, bool, list, tuple and dict)"
  },
  {
    "objectID": "core/slides/slides05_json-format.html#dumps-and-dump",
    "href": "core/slides/slides05_json-format.html#dumps-and-dump",
    "title": "SPARK & JSON",
    "section": "Dumps() and Dump()",
    "text": "Dumps() and Dump()\nThe json module provides two very handy methods for serialization :\n\n\n\n\n\n\n\nFunction\nDescription\n\n\n\n\ndumps()\nserializes an object to a JSON formatted string\n\n\ndump()\nserializes an object to a JSON formatted stream (which supports writing to a file)."
  },
  {
    "objectID": "core/slides/slides05_json-format.html#serialization-of-built-in-datatypes",
    "href": "core/slides/slides05_json-format.html#serialization-of-built-in-datatypes",
    "title": "SPARK & JSON",
    "section": "Serialization of built-in datatypes",
    "text": "Serialization of built-in datatypes\njson.dumps() and json.dump() use the following mapping conventions for built-in datatypes :\n\n\n\n\n\nPython\nJSON\n\n\n\n\ndict\nobject\n\n\nlist, tuple\narray\n\n\nstr\nstring\n\n\nint, float\nnumber\n\n\nTrue\ntrue\n\n\nFalse\nfalse\n\n\nNone\nnull\n\n\n\n. . .\n\n\n\n\n\n\nWarning\n\n\nlist and tuple are mapped to the same json type.\nint and float are mapped to the same json type"
  },
  {
    "objectID": "core/slides/slides05_json-format.html#serialization-example",
    "href": "core/slides/slides05_json-format.html#serialization-example",
    "title": "SPARK & JSON",
    "section": "Serialization example",
    "text": "Serialization example\n\n\nSerialize a Python object into a JSON formatted string using json.dumps()\n\nimport json\n\nspam = json.dumps({\n  \"name\": \"Foo Bar\",\n  \"age\": 78,\n  \"friends\": [\"Jane\",\"John\"],\n  \"balance\": 345.80,\n  \"other_names\":(\"Doe\",\"Joe\"),\n  \"active\": True,\n  \"spouse\": None\n  }, \n  sort_keys=True, \n  indent=4\n)\n\n\n\n\n\ntype(spam)\n\n\n\nstr\n\n\n\n\n\n\n\n\n\nRemember:\n\n\nJSON.dumps() converts a Python object into a JSON formatted text.\n\n\n\n\n\n\nprint(spam)\n\n\n\n{\n    \"active\": true,\n    \"age\": 78,\n    \"balance\": 345.8,\n    \"friends\": [\n        \"Jane\",\n        \"John\"\n    ],\n    \"name\": \"Foo Bar\",\n    \"other_names\": [\n        \"Doe\",\n        \"Joe\"\n    ],\n    \"spouse\": null\n}\n\n\nPretty printing options\n\nsort_keys=True: sort the keys of the JSON object\nindent=4: indent using 4 spaces"
  },
  {
    "objectID": "core/slides/slides05_json-format.html#dumping-a-date",
    "href": "core/slides/slides05_json-format.html#dumping-a-date",
    "title": "SPARK & JSON",
    "section": "Dumping a date",
    "text": "Dumping a date\nA Python date object is not serializable.\n\nfrom datetime import date\ntd = date.today()\n\n\n\n\n&gt;&gt;&gt; js.dumps(td)\n...\nTypeError: Object of type date is not JSON serializable\nBut it can be converted into serializable types.\n\njson.dumps(td.isoformat())\n\n\n\n\n\njson.dumps(td.isocalendar())\n\n\n\n\n\njson.dumps(td.timetuple())"
  },
  {
    "objectID": "core/slides/slides05_json-format.html#serialization-example-1",
    "href": "core/slides/slides05_json-format.html#serialization-example-1",
    "title": "SPARK & JSON",
    "section": "Serialization example",
    "text": "Serialization example\njson.dump() allows to write the output stream to a file\n\nwith open('user.json','w') as file:\n        json.dump({\n            \"name\": \"Foo Bar\",\n            \"age\": 78,\n            \"friends\": [\"Jane\",\"John\"],\n            \"balance\": 345.80,\n            \"other_names\": (\"Doe\",\"Joe\"),\n            \"active\": True,\n            \"spouse\": None\n          }, \n          file, \n          sort_keys=True, indent=4\n        )\n\n\n\n\n\nThis writes a user.json file to disk with similar content as in the previous example\n\n!ls -l *.json"
  },
  {
    "objectID": "core/slides/slides05_json-format.html#deserializing-built-in-datatypes",
    "href": "core/slides/slides05_json-format.html#deserializing-built-in-datatypes",
    "title": "SPARK & JSON",
    "section": "Deserializing built-in datatypes",
    "text": "Deserializing built-in datatypes\nSimilarly to serialization, the json module exposes two methods for deserialization:\n\n\n\n\n\n\n\nFunction\nDescription\n\n\n\n\nloads()\ndeserializes a JSON document to a Python object\n\n\nload()\ndeserializes a JSON formatted stream (which supports reading from a file) to a Python object"
  },
  {
    "objectID": "core/slides/slides05_json-format.html#deserializing-built-in-datatypes-1",
    "href": "core/slides/slides05_json-format.html#deserializing-built-in-datatypes-1",
    "title": "SPARK & JSON",
    "section": "Deserializing built-in datatypes",
    "text": "Deserializing built-in datatypes\nThe decoder converts JSON encoded data into native Python data types as in the table below:\n\n\n\nJSON\nPython\n\n\n\n\nobject\ndict\n\n\narray\nlist\n\n\nstring\nstr\n\n\nnumber (int)\nint\n\n\nnumber (real)\nfloat\n\n\ntrue\nTrue\n\n\nfalse\nFalse\n\n\nnull\nNone"
  },
  {
    "objectID": "core/slides/slides05_json-format.html#deserialization-example",
    "href": "core/slides/slides05_json-format.html#deserialization-example",
    "title": "SPARK & JSON",
    "section": "Deserialization example",
    "text": "Deserialization example\nPass a JSON string to the json.loads() method :\n\nspam = json.loads('{\"active\": true, \"age\": 78, \"balance\": 345.8, \"friends\": [\"Jane\",\"John\"], \"name\": \"Foo Bar\", \"other_names\": [\"Doe\",\"Joe\"],\"spouse\":null}')\n\n\n\n\n\nwe obtain a dictionary as output:\n\nspam"
  },
  {
    "objectID": "core/slides/slides05_json-format.html#deserialization-example-1",
    "href": "core/slides/slides05_json-format.html#deserialization-example-1",
    "title": "SPARK & JSON",
    "section": "Deserialization example",
    "text": "Deserialization example\nWe can also read from the user.json file we created before:\n\nwith open('user.json', 'r') as file:\n  user_data = json.load(file)\n\nuser_data\n\n\n\n\n\nWe obtain the same dict. This is simple and fast."
  },
  {
    "objectID": "core/slides/slides05_json-format.html#serialize-and-deserialize-custom-objects",
    "href": "core/slides/slides05_json-format.html#serialize-and-deserialize-custom-objects",
    "title": "SPARK & JSON",
    "section": "Serialize and deserialize custom objects",
    "text": "Serialize and deserialize custom objects\n\nUsing JSON, we serialized and deserialized objects containing only encapsulated built-in types\nWe can also work a little bit to serialize custom objects\nLet’s go to notebook07_json-format.ipynb"
  },
  {
    "objectID": "core/slides/slides05_json-format.html#using-json-data-with-spark-1",
    "href": "core/slides/slides05_json-format.html#using-json-data-with-spark-1",
    "title": "SPARK & JSON",
    "section": "Using JSON data with Spark",
    "text": "Using JSON data with Spark\nTypically achieved using\nspark.read.json(filepath, multiLine=True)\n\n\nPretty simple\nbut usually requires extra cleaning or schema flattening\n\n(Almost) Everything is explained in the notebook :\n\nnotebook07_json-format.ipynb\n\n\n\nJSON reader and writer allows us save and read Spark dataframes with composite types."
  },
  {
    "objectID": "core/slides/slides05_json-format.html#obtaininig-json-objects-from-an-api",
    "href": "core/slides/slides05_json-format.html#obtaininig-json-objects-from-an-api",
    "title": "SPARK & JSON",
    "section": "Obtaininig JSON objects from an API",
    "text": "Obtaininig JSON objects from an API\n\nA common use of JSON is to collect JSON data from a web server as a file or HTTP request, and convert the JSON data to a Python/R/Spark object."
  },
  {
    "objectID": "core/slides/slides05_json-format.html#what-is-a-json-object",
    "href": "core/slides/slides05_json-format.html#what-is-a-json-object",
    "title": "SPARK & JSON",
    "section": " What is a JSON object?",
    "text": "What is a JSON object?\n\nAn object can be defined as an unordered set of name/value pairs. An object in JSON starts with {left brace} and finish or ends with {right brace}. Every name is followed by: (colon) and the name/value pairs are parted by, (comma)."
  },
  {
    "objectID": "core/slides/slides05_json-format.html#json-syntax",
    "href": "core/slides/slides05_json-format.html#json-syntax",
    "title": "SPARK & JSON",
    "section": " JSON syntax",
    "text": "JSON syntax\nJSON syntax is a subset of the JavaScript object notation syntax.\n\nData is in name/value pairs\nData is separated by comma ,\nCurly brackets {} hold objects\nSquare bracket [] holds arrays"
  },
  {
    "objectID": "core/slides/slides05_json-format.html#json-and-types",
    "href": "core/slides/slides05_json-format.html#json-and-types",
    "title": "SPARK & JSON",
    "section": " JSON and types",
    "text": "JSON and types\nJSON types:\n\nNumber,\nArray,\nBoolean,\nString\nObject\nNull"
  },
  {
    "objectID": "core/slides/slides05_json-format.html#json-versus-pickle",
    "href": "core/slides/slides05_json-format.html#json-versus-pickle",
    "title": "SPARK & JSON",
    "section": "json versus pickle",
    "text": "json versus pickle\nTwo competing serialization modules?\n\n\nPickle is Python bound\nPickle handles (almost) everything that can be defined in Python\nOther computing environments have to develop bypasses to handle pickle dumps.\n\n\n\n\njson is used by widely different languages and systems\njson is readable\njson is less prone to malicious code injection"
  },
  {
    "objectID": "core/slides/slides05_json-format.html#json-dialects-spatial-data",
    "href": "core/slides/slides05_json-format.html#json-dialects-spatial-data",
    "title": "SPARK & JSON",
    "section": "Json dialects : spatial data",
    "text": "Json dialects : spatial data\nJSON objects are used extensively to handle spatial or textual data.\nJSON objects are used by spatial extensions of Pandas and Spark.\n\nGeoJSON is a format for encoding a variety of geographic data structures. GeoJSON supports the following geometry types: Point, LineString, Polygon, MultiPoint, MultiLineString, and MultiPolygon. Geometric objects with additional properties are Feature objects. Sets of features are contained by FeatureCollection objects.\n\n\n\n{\n  \"type\": \"FeatureCollection\",\n  \"features\": [\n    {\n      \"type\": \"Feature\",\n      \"properties\": {},\n      \"geometry\": {\n        \"coordinates\": [\n          2.381584638521815,\n          48.82906361931293\n        ],\n        \"type\": \"Point\"\n      }\n    }\n  ]\n}\n\n\n\n\nSee Loading GeoJSON using Spark"
  },
  {
    "objectID": "core/slides/slides05_json-format.html#semi-structured-data-and-nlp",
    "href": "core/slides/slides05_json-format.html#semi-structured-data-and-nlp",
    "title": "SPARK & JSON",
    "section": "Semi-structured data and NLP",
    "text": "Semi-structured data and NLP\nNatural Language Processing (NLP) handles corpora of texts (called documents), annotates the documents, parses the documents into sentences and tokens, performs syntactic analysis (POS tagging), and eventually enables topic modeling, sentiment analysis, automatic translation, and other machine learning tasks.\nCorpus annotation can be performed using spark-nlp a package developped by the John Snow Labs to offer NLP above Spark SQL and Spark MLLib.\nAnnotation starts by applying a DocumentAssembler() transformation to a corpus. This introduces columns with composite types"
  },
  {
    "objectID": "core/slides/slides05_json-format.html#document-assembling",
    "href": "core/slides/slides05_json-format.html#document-assembling",
    "title": "SPARK & JSON",
    "section": "Document Assembling",
    "text": "Document Assembling\n\n&gt;&gt;&gt; result = documentAssembler.transform(data)\n&gt;&gt;&gt; result.select(\"document\").show(truncate=False)\n+----------------------------------------------------------------------------------------------+\n|document                                                                                      |\n+----------------------------------------------------------------------------------------------+\n|[[document, 0, 51, Spark NLP is an open-source text processing library., [sentence -&gt; 0], []]]|\n+----------------------------------------------------------------------------------------------+\n&gt;&gt;&gt; result.select(\"document\").printSchema()\nroot\n|-- document: array (nullable = True)\n|    |-- element: struct (containsNull = True)\n|    |    |-- annotatorType: string (nullable = True)\n|    |    |-- begin: integer (nullable = False)\n|    |    |-- end: integer (nullable = False)\n|    |    |-- result: string (nullable = True)\n|    |    |-- metadata: map (nullable = True)\n|    |    |    |-- key: string\n|    |    |    |-- value: string (valueContainsNull = True)\n|    |    |-- embeddings: array (nullable = True)\n|    |    |    |-- element: float (containsNull = False)\nColumn document is of type ArrayType(). The basetype of document column is of StructType() (element), the element contains subfields of primitive type, but alo a field of type map (MapType()) and a field of type StructType()."
  },
  {
    "objectID": "core/slides/slides05_json-format.html#json-path",
    "href": "core/slides/slides05_json-format.html#json-path",
    "title": "SPARK & JSON",
    "section": "JSON path",
    "text": "JSON path\nThe SQL/JSON path language: specify the items to be retrieved from JSON data\n\nPath expressions\nEvaluation\nResult\n\nDifferent dialects\n\njsonpath see PostgreSQL\njmespath see JMES Path\n..."
  },
  {
    "objectID": "core/slides/slides05_json-format.html#examples",
    "href": "core/slides/slides05_json-format.html#examples",
    "title": "SPARK & JSON",
    "section": "Examples",
    "text": "Examples\n\n…\n…\n…"
  },
  {
    "objectID": "core/slides/slides13_dask_internals.html#local-backends",
    "href": "core/slides/slides13_dask_internals.html#local-backends",
    "title": "Dask internals",
    "section": "Local backends",
    "text": "Local backends\n\nimport dask"
  },
  {
    "objectID": "core/slides/slides13_dask_internals.html#single-process-backend",
    "href": "core/slides/slides13_dask_internals.html#single-process-backend",
    "title": "Dask internals",
    "section": "Single process backend",
    "text": "Single process backend"
  },
  {
    "objectID": "core/slides/slides13_dask_internals.html#multi-threaded-backend",
    "href": "core/slides/slides13_dask_internals.html#multi-threaded-backend",
    "title": "Dask internals",
    "section": "Multi-threaded backend",
    "text": "Multi-threaded backend\n\n\n\n\n\n\nNote\n\n\n\nAvoids serialization\nAvoids interprocess communication costs\n\n\n\n\n\ndask.config.set(scheduler='threads')\n\n\n\n&lt;dask.config.set at 0x711a7c6cbb90&gt;"
  },
  {
    "objectID": "core/slides/slides13_dask_internals.html#multi-process-backend",
    "href": "core/slides/slides13_dask_internals.html#multi-process-backend",
    "title": "Dask internals",
    "section": "Multi-process backend",
    "text": "Multi-process backend\n\n\n\n\n\n\nNote\n\n\n\nAvoids Python’s Global Interpreter Lock (GIL)\n\n\n\n\n\ndic_config = {\n  \"multiprocessing.context\": \"forkserver\",\n  \"scheduler\": \"processes\"\n}\n\ndask.config.set(\n  **dic_config\n)\n\n\n\n&lt;dask.config.set at 0x711a5cbbbc50&gt;\n\n\n\n\nThe distributed scheduler can interact with the local cluster"
  },
  {
    "objectID": "core/slides/slides13_dask_internals.html#distributed-backends",
    "href": "core/slides/slides13_dask_internals.html#distributed-backends",
    "title": "Dask internals",
    "section": "Distributed backends",
    "text": "Distributed backends\n\n\n\n\n\n\nNote\n\n\n\nDask has one distributed scheduler backend\nThe distributed scheduler backend can interact with several clusters"
  },
  {
    "objectID": "core/slides/slides13_dask_internals.html#cluster-examples",
    "href": "core/slides/slides13_dask_internals.html#cluster-examples",
    "title": "Dask internals",
    "section": "Cluster examples",
    "text": "Cluster examples\n\nKubernetes\nCoiled\nYarn"
  },
  {
    "objectID": "core/slides/slides13_dask_internals.html#caveat-fault-tolerance",
    "href": "core/slides/slides13_dask_internals.html#caveat-fault-tolerance",
    "title": "Dask internals",
    "section": "Caveat : fault tolerance",
    "text": "Caveat : fault tolerance\n\n\n\n\n\n\nWarning\n\n\nDask Client is not fault tolerant"
  },
  {
    "objectID": "core/slides/slides13_dask_internals.html#pipinstall-worker-plug-in",
    "href": "core/slides/slides13_dask_internals.html#pipinstall-worker-plug-in",
    "title": "Dask internals",
    "section": "PipInstall worker plug-in",
    "text": "PipInstall worker plug-in\n\nfrom dask import distributed\n\n( \n  distributed\n    .diagnostics\n    .plugin\n    .PipInstall([[\"bs4\"]])\n)\n\n\n\n&lt;distributed.diagnostics.plugin.PipInstall at 0x711a243b4d10&gt;"
  },
  {
    "objectID": "core/slides/slides13_dask_internals.html#preinstalling-libraries",
    "href": "core/slides/slides13_dask_internals.html#preinstalling-libraries",
    "title": "Dask internals",
    "section": "Preinstalling libraries",
    "text": "Preinstalling libraries"
  },
  {
    "objectID": "core/slides/slides13_dask_internals.html#ui",
    "href": "core/slides/slides13_dask_internals.html#ui",
    "title": "Dask internals",
    "section": "UI",
    "text": "UI\nWhen connecting to a cluster through a client, UI runs on scheduler node.\nclient.dashboard_link"
  },
  {
    "objectID": "core/slides/slides13_dask_internals.html#pickling-in-python",
    "href": "core/slides/slides13_dask_internals.html#pickling-in-python",
    "title": "Dask internals",
    "section": "Pickling in Python",
    "text": "Pickling in Python"
  },
  {
    "objectID": "core/slides/slides13_dask_internals.html#cloudpickle",
    "href": "core/slides/slides13_dask_internals.html#cloudpickle",
    "title": "Dask internals",
    "section": "Cloudpickle",
    "text": "Cloudpickle"
  },
  {
    "objectID": "core/slides/slides01_introduction.html#who-are-we",
    "href": "core/slides/slides01_introduction.html#who-are-we",
    "title": "Big data technologies",
    "section": "Who are we ?",
    "text": "Who are we ?\n\n\n\n\n\n\nStéphane Boucheron\nLPSM\nStatistics \n\nhttps://stephane-v-boucheron.fr\n\n\n\n\n\n\nCristina Sirangelo\nIRIF\nData Science, Databases \n\nhttps://www.irif.fr/~amelie/"
  },
  {
    "objectID": "core/slides/slides01_introduction.html#course-logistics-1",
    "href": "core/slides/slides01_introduction.html#course-logistics-1",
    "title": "Big data technologies",
    "section": "Course logistics",
    "text": "Course logistics\n\n24 hours = 2 hours \\(\\times\\) 12 weeks : classes + hands-on\nAgenda\n\nAbout the hands-on\n\nHands-on and homeworks using Jupyter/Quarto notebooks\nUsing a Docker image  built for the course\n Hands-on must be carried out using your own laptop"
  },
  {
    "objectID": "core/slides/slides01_introduction.html#course-logistics-2",
    "href": "core/slides/slides01_introduction.html#course-logistics-2",
    "title": "Big data technologies",
    "section": "Course logistics",
    "text": "Course logistics\n\n course : https://s-v-b.github.io/IFEBY310\n Bookmark it !\n Follow the steps described on the tools page:\n\n\nhttps://s-v-b.github.io/IFEBY310/tools\n\n\nUse"
  },
  {
    "objectID": "core/slides/slides01_introduction.html#course-evaluation",
    "href": "core/slides/slides01_introduction.html#course-evaluation",
    "title": "Big data technologies",
    "section": "Course evaluation",
    "text": "Course evaluation\n\nEvaluation using homeworks and a final project\nFind a friend: all work done by pairs of students\nAll your work goes in your private git repository and nowhere else: no emails !\nAll your homework will be using quarto files"
  },
  {
    "objectID": "core/slides/slides01_introduction.html#why-docker-what-is-it",
    "href": "core/slides/slides01_introduction.html#why-docker-what-is-it",
    "title": "Big data technologies",
    "section": "Why docker ? What is it ?",
    "text": "Why docker ? What is it ?\n\nDon’t mess with your python env. and configuration files\nEverything in embedded in a container (better than a Virtual Machine)\nA container is an instance of an image\n\nSame image = same environment for everybody\nSame image = no {version, dependencies, install} problems\nIt is an entreprise standard used everywhere now!"
  },
  {
    "objectID": "core/slides/slides01_introduction.html#docker-1",
    "href": "core/slides/slides01_introduction.html#docker-1",
    "title": "Big data technologies",
    "section": "\n docker\n",
    "text": "docker\n\n\nHave a look at https://s-v-b.github.io/IFEBY310/tools\nHave a look at the Dockerfile to explain a little bit how the image is built\nPerform a quick demo on how to use the docker image\n\n\n\n\n\n\n\nAnd that’s it for logistics !"
  },
  {
    "objectID": "core/slides/slides01_introduction.html#big-data-1",
    "href": "core/slides/slides01_introduction.html#big-data-1",
    "title": "Big data technologies",
    "section": "Big data",
    "text": "Big data\n\nMoore’s Law: computing power doubled every two years between 1975 and 2012\nNowadays, less than two years and a half\nRapid growth of datasets: internet activity, social networks, genomics, physics, censor networks, IOT, …\nData size trends: doubles every year according to IDC executive summary\nData deluge: Today, data is growing faster than computing power"
  },
  {
    "objectID": "core/slides/slides01_introduction.html#question",
    "href": "core/slides/slides01_introduction.html#question",
    "title": "Big data technologies",
    "section": "Question \n",
    "text": "Question \n\n\nHow do we catch up to process the data deluge and to learn from it ?"
  },
  {
    "objectID": "core/slides/slides01_introduction.html#order-of-magnitudes",
    "href": "core/slides/slides01_introduction.html#order-of-magnitudes",
    "title": "Big data technologies",
    "section": "Order of magnitudes",
    "text": "Order of magnitudes\nbit\nA bit is a value of either a 1 or 0 (on or off)\nbyte (B)\nA byte is made of 8 bits\n\n1 character, e.g., “a”, is one byte\n\nKilobyte (KB)\nA kilobyte is \\(1024 =2^{10}\\) bytes\n\n\n2 or 3 paragraphs of ASCII text"
  },
  {
    "objectID": "core/slides/slides01_introduction.html#some-more-comparisons",
    "href": "core/slides/slides01_introduction.html#some-more-comparisons",
    "title": "Big data technologies",
    "section": "Some more comparisons",
    "text": "Some more comparisons\nMegabyte (MB)\nA megabyte is \\(1 048 576=2^{20}\\) B or \\(1 024\\) KB\n\n\n873 pages of plain text\n\n4 books (200 pages or 240 000 characters)\n\nGigabyte (GB)\nA gigabyte is \\(1 073 741 824=2^{30}\\) B, \\(1 024\\) MB or \\(1 048 576\\) KB\n\n\n894 784 pages of plain text (1 200 characters)\n\n4 473 books (200 pages or 240 000 characters)\n\n640 web pages (with 1.6 MB average file size)\n\n341 digital pictures (with 3 MB average file size)\n\n256 MP3 audio files (with 4 MB average file size)\n\n1,5 650 MB CD"
  },
  {
    "objectID": "core/slides/slides01_introduction.html#even-more",
    "href": "core/slides/slides01_introduction.html#even-more",
    "title": "Big data technologies",
    "section": "Even more",
    "text": "Even more\nTerabyte (TB)\nA terabyte is \\(1 099 511 627 776=2^{40}\\) B, 1 024 GB or 1 048 576 MB.\n\n\n916 259 689 pages of plain text (1 200 characters)\n\n4 581 298 books (200 pages or 240 000 characters)\n\n655 360 web pages (with 1.6 MB average file size)\n\n349 525 digital pictures (with 3 MB average file size)\n\n262 144 MP3 audio files (with 4 MB average file size)\n\n1 613 650 MB CD’s\n\n233 4.38 GB DVDs\n\n40 25 GB Blu-ray discs"
  },
  {
    "objectID": "core/slides/slides01_introduction.html#the-deluge",
    "href": "core/slides/slides01_introduction.html#the-deluge",
    "title": "Big data technologies",
    "section": "The deluge",
    "text": "The deluge\nPetabyte (PB)\nA petabyte is 1 024 TB, 1 048 576 GB or 1 073 741 824 MB\n\\[1125899906842624 = 2^{50} \\quad\\text{Bytes}\\]\n\n\n938 249 922 368 pages of plain text (1 200 characters)\n\n4 691 249 611 books (200 pages or 240 000 characters)\n\n671 088 640 web pages (with 1.6 MB average file size)\n\n357 913 941 digital pictures (with 3 MB average file size)\n\n268 435 456 MP3 audio files (with 4 MB average file size)\n\n1 651 910 650 MB CD’s\n\n239 400 4.38 GB DVDs\n\n41 943 25 GB Blu-ray discs\n\nExabyte, etc.\n\n1 EB = 1 exabyte = 1 024 PB\n1 ZB = 1 zettabyte = 1 024 EB"
  },
  {
    "objectID": "core/slides/slides01_introduction.html#some-figures",
    "href": "core/slides/slides01_introduction.html#some-figures",
    "title": "Big data technologies",
    "section": "Some figures",
    "text": "Some figures\nYou have every single second1:\n\nAt least 8,000 tweets sent\n900+ photos posted on Instagram\nThousands of Skype calls made\nOver 70,000 Google searches performed\nAround 80,000 YouTube videos viewed\nOver 2 million emails sent\nhttps://www.internetlivestats.com"
  },
  {
    "objectID": "core/slides/slides01_introduction.html#some-figures-1",
    "href": "core/slides/slides01_introduction.html#some-figures-1",
    "title": "Big data technologies",
    "section": "Some figures",
    "text": "Some figures\nThere are1:\n\n\n5 billion web pages as of mid-2019 (indexed web)\n\nand we expected2:\n\n\n4.8 ZB of annual IP traffic in 2022\n\nNote that\n\n\n1 ZB \\(\\approx\\) 36 000 years of HD video\nNetflix’s entire catalog is \\(\\approx\\) 3.5 years of HD video\nhttps://www.worldwidewebsize.comCisco’s Visual Networking Index"
  },
  {
    "objectID": "core/slides/slides01_introduction.html#some-figures-2",
    "href": "core/slides/slides01_introduction.html#some-figures-2",
    "title": "Big data technologies",
    "section": "Some figures",
    "text": "Some figures\nMore figures :\n\nfacebook daily logs: 60TB\n1000 genomes project: 200TB\nGoogle web index: 10+ PB\nCost of 1TB of storage: ~$35\nTime to read 1TB from disk: 3 hours if 100MB/s"
  },
  {
    "objectID": "core/slides/slides01_introduction.html#latency-numbers",
    "href": "core/slides/slides01_introduction.html#latency-numbers",
    "title": "Big data technologies",
    "section": "Latency numbers",
    "text": "Latency numbers\n\n\n\n\n\n\n\n\n\nMemory type\nLatency(ns)\nLatency(us)\n(ms)\n\n\n\n\nL1 cache reference\n0.5 ns\n\n\n\n\n\nL2 cache reference\n7 ns\n\n\n14x L1 cache\n\n\nMain memory reference\n100 ns\n\n\n20x L2, 200x L1\n\n\nCompress 1K bytes with Zippy/Snappy\n3,000 ns\n3 us\n\n\n\n\nSend 1K bytes over 1 Gbps network\n10,000 ns\n10 us\n\n\n\n\nRead 4K randomly from SSD*\n150,000 ns\n150 us\n\n~1GB/sec SSD\n\n\nRead 1 MB sequentially from memory\n250,000 ns\n250 us\n\n\n\n\nRound trip within same datacenter\n500,000 ns\n500 us\n\n\n\n\nRead 1 MB sequentially from SSD*\n1,000,000 ns\n1,000 us\n1 ms\n~1GB/sec SSD, 4X memory\n\n\nDisk seek\n10,000,000 ns\n10,000 us\n10 ms\n20x datacenter roundtrip\n\n\nRead 1 MB sequentially from disk\n20,000,000 ns\n20,000 us\n20 ms\n80x memory, 20x SSD\n\n\nSend packet US -&gt; Europe -&gt; US\n150,000,000 ns\n150,000 us\n150 ms\n600x memory"
  },
  {
    "objectID": "core/slides/slides01_introduction.html#section-2",
    "href": "core/slides/slides01_introduction.html#section-2",
    "title": "Big data technologies",
    "section": "",
    "text": "traceroute to mathscinet.ams.org (104.238.176.204), 64 hops max\n  1   192.168.10.1  3,149ms  1,532ms  1,216ms \n  2   192.168.0.254  1,623ms  1,397ms  1,309ms \n  3   78.196.1.254  2,571ms  2,120ms  2,371ms \n  4   78.255.140.126  2,813ms  2,621ms  2,200ms \n  5   78.254.243.86  2,626ms  2,528ms  2,517ms \n  6   78.254.253.42  2,517ms  4,129ms  2,671ms \n  7   78.254.242.54  2,535ms  2,258ms  2,350ms \n  8   *  *  * \n  9   195.66.224.191  12,231ms  11,718ms  12,486ms \n 10   *  *  * \n 11   63.218.14.58  26,213ms  19,264ms  18,949ms \n 12   63.218.231.106  29,135ms  22,078ms  17,954ms"
  },
  {
    "objectID": "core/slides/slides01_introduction.html#latency-numbers-1",
    "href": "core/slides/slides01_introduction.html#latency-numbers-1",
    "title": "Big data technologies",
    "section": "Latency numbers",
    "text": "Latency numbers\n\nReading 1MB from disk = 100 x reading 1MB from memory\nSending packet from US to Europe to US = 1 000 000 x main memory reference\n\nGeneral tendency\nTrue in general, not always:\n\nmemory operations : fastest\ndisk operations : slow\nnetwork operations : slowest"
  },
  {
    "objectID": "core/slides/slides01_introduction.html#latency-numbers-httpswww.eecs.berkeley.edurcsresearchinteractive_latency.html",
    "href": "core/slides/slides01_introduction.html#latency-numbers-httpswww.eecs.berkeley.edurcsresearchinteractive_latency.html",
    "title": "Big data technologies",
    "section": "Latency numbers 1(https://www.eecs.berkeley.edu/~rcs/research/interactive_latency.html)",
    "text": "Latency numbers 1(https://www.eecs.berkeley.edu/~rcs/research/interactive_latency.html)\n\n\n\nhttps://www.eecs.berkeley.edu/~rcs/research/interactive_latency.html"
  },
  {
    "objectID": "core/slides/slides01_introduction.html#latency-numbers-for-mortals",
    "href": "core/slides/slides01_introduction.html#latency-numbers-for-mortals",
    "title": "Big data technologies",
    "section": "Latency numbers for mortals",
    "text": "Latency numbers for mortals\nMultiply all durations by a billion \\(10^9\\)\n\n\n\n\n\n\n\nMemory type\nLatency\nHuman duration\n\n\n\nL1 cache reference\n0.5 s\nOne heart beat (0.5 s)\n\n\nL2 cache reference\n7 s\nLong yawn\n\n\nMain memory reference\n100 s\nBrushing your teeth\n\n\nSend 2K bytes over 1 Gbps network\n5.5 hr\nFrom lunch to end of work day\n\n\nSSD random read\n1.7 days\nA normal weekend\n\n\nRead 1 MB sequentially from memory\n2.9 days\nA long weekend\n\n\nRound trip within same datacenter\n5.8 days\nA medium vacation\n\n\nRead 1 MB sequentially from SSD\n11.6 days\nWaiting for almost 2 weeks for a delivery\n\n\nDisk seek\n16.5 weeks\nA semester in university\n\n\nRead 1 MB sequentially from disk\n7.8 months\nAlmost producing a new human being\n\n\nSend packet US -&gt; Europe -&gt; US\n4.8 years\nAverage time it takes to complete a bachelor’s degree"
  },
  {
    "objectID": "core/slides/slides01_introduction.html#challenges-with-big-datasets",
    "href": "core/slides/slides01_introduction.html#challenges-with-big-datasets",
    "title": "Big data technologies",
    "section": "Challenges with big datasets",
    "text": "Challenges with big datasets\n\nLarge data don’t fit on a single hard-drive\nOne large (and expensive) machine can’t process or store all the data\nFor computations how do we stream data from the disk to the different layers of memory ?\nConcurrent accesses to the data: disks cannot be read in parallel"
  },
  {
    "objectID": "core/slides/slides01_introduction.html#solutions",
    "href": "core/slides/slides01_introduction.html#solutions",
    "title": "Big data technologies",
    "section": "Solutions",
    "text": "Solutions\n\nCombine several machines containing hard drives and processors on a network\nUsing commodity hardware: cheap, common architecture i.e. processor + RAM + disk\nScalability = more machines on the network\nPartition the data across the machines"
  },
  {
    "objectID": "core/slides/slides01_introduction.html#challenges-1",
    "href": "core/slides/slides01_introduction.html#challenges-1",
    "title": "Big data technologies",
    "section": "Challenges",
    "text": "Challenges\nDealing with distributed computations adds software complexity\n\nScheduling\n\nHow to split the work across machines? Must exploit and optimize data locality since moving data is very expensive\n\nReliability\n\nHow to handle failures? Commodity (cheap) hardware fails more often. @Google [1%, 5%] HD failure/year and 0.2% DIMM failure/year\n\nUneven performance of machines\n\nsome nodes are slower than others"
  },
  {
    "objectID": "core/slides/slides01_introduction.html#section-3",
    "href": "core/slides/slides01_introduction.html#section-3",
    "title": "Big data technologies",
    "section": "",
    "text": "Problems sketched in\n\n\nNext Generation Dabases describes the challenges faces by database industry between 1995 and 2015, that is during the onset of the data deluge"
  },
  {
    "objectID": "core/slides/slides01_introduction.html#solutions-1",
    "href": "core/slides/slides01_introduction.html#solutions-1",
    "title": "Big data technologies",
    "section": "Solutions",
    "text": "Solutions\n\nSchedule, manage and coordinate threads and resources using appropriate software\nLocks to limit access to resources\nReplicate data for faster reading and reliability"
  },
  {
    "objectID": "core/slides/slides01_introduction.html#is-it-hpc",
    "href": "core/slides/slides01_introduction.html#is-it-hpc",
    "title": "Big data technologies",
    "section": "Is it HPC ?",
    "text": "Is it HPC ?\n\nHigh Performance Computing (HPC)\nParallel computing\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nFor HPC, scaling up means using a bigger machine\n\nHuge performance increase for medium scale problems\n\nVery expensive, specialized machines, lots of processors and memory\n\n\n\n\n\n\n\n\n\n\n\n\n\nNo"
  },
  {
    "objectID": "core/slides/slides01_introduction.html#and",
    "href": "core/slides/slides01_introduction.html#and",
    "title": "Big data technologies",
    "section": "\n and \n",
    "text": "and \n\n\nGoogle committed to a number of key tenants when designing its data center architecture. Most significantly —and at the time, uniquely— Google committed to massively parallelizing and distributing processing across very large numbers of commodity servers.\n\n\nGoogle also adopted a “Jedis build their own lightsabers” attitude: very little third party —and virtually no commercial— software would be found in the Google architecture.\n\n\nBuild was considered better than buy at Google.\n\nFrom Next Generation Dabases"
  },
  {
    "objectID": "core/slides/slides01_introduction.html#the-big-data-universe",
    "href": "core/slides/slides01_introduction.html#the-big-data-universe",
    "title": "Big data technologies",
    "section": "The Big Data universe",
    "text": "The Big Data universe\nMany technologies combining software and cloud computing"
  },
  {
    "objectID": "core/slides/slides01_introduction.html#the-big-data-universe-still-expanding",
    "href": "core/slides/slides01_introduction.html#the-big-data-universe-still-expanding",
    "title": "Big data technologies",
    "section": "The Big Data universe (still expanding)",
    "text": "The Big Data universe (still expanding)\nOften used with/for with Machine Learning (or AI)"
  },
  {
    "objectID": "core/slides/slides01_introduction.html#tools",
    "href": "core/slides/slides01_introduction.html#tools",
    "title": "Big data technologies",
    "section": "Tools \n",
    "text": "Tools \n\n\nSoftwares such as HadoopMR (Hadoop Map Reduce) and more recently Spark and Dask cope with these challenges\nThey are distributed computational engines: softwares that ease the development of distributed algorithms\n\nThey run on clusters (several machine on a network), managed by a resource manager such as :\n\n\n Yarn : https://hadoop.apache.org/docs/current/hadoop-yarn/hadoop-yarn-site/YARN.html\n\nKubernetes : https://kubernetes.io\n\n\nA resource manager ensures that the tasks running on the cluster do not try to use the same resources all at once"
  },
  {
    "objectID": "core/slides/slides01_introduction.html#apache-spark-1",
    "href": "core/slides/slides01_introduction.html#apache-spark-1",
    "title": "Big data technologies",
    "section": "Apache Spark\n",
    "text": "Apache Spark\n\nThe course will focus mainly on Spark for big data processing\n\n\nhttps://spark.apache.org\n\n\n\nSpark is an enterprise standard  (cf https://spark.apache.org/powered-by.html)\nOne of the most used big data processing framework\n\nOpen source\n\nThe predecessor of Spark is Hadoop\nSee Chapter 2 in Next Generation Dabases\nGuy Harrison"
  },
  {
    "objectID": "core/slides/slides01_introduction.html#hadoop",
    "href": "core/slides/slides01_introduction.html#hadoop",
    "title": "Big data technologies",
    "section": "Hadoop",
    "text": "Hadoop\n\nHadoop has a simple API and good fault tolerance (tolerance to nodes failing midway through a processing job)\nThe cost is lots of data shuffling across the network\nWith intermediate computations written to disk over the network which we know is very time expensive\n\nIt is made of three components:\n\nHDFS (Highly Distributed File System) inspired from GoogleFileSystem, see https://ai.google/research/pubs/pub51\nYARN (Yet Another Ressource Negociator) for processing management.\nMapReduce inspired from Google for processing again.https://research.google.com/archive/mapreduce.html"
  },
  {
    "objectID": "core/slides/slides01_introduction.html#section-4",
    "href": "core/slides/slides01_introduction.html#section-4",
    "title": "Big data technologies",
    "section": "",
    "text": "The Hadoop 1.0 architecture is powerful and easy to understand, but it is limited to MapReduce workloads and it provides limited flexibility with regard to scheduling and resource allocation.\n\n\nIn the Hadoop 2.0 architecture, YARN (Yet Another Resource Negotiator or, recursively, YARN Application Resource Negotiator) improves scalability and flexibility by splitting the roles of the Task Tracker into two processes.\n\n\nA Resource Manager controls access to the clusters resources (memory, CPU, etc.) while the Application Manager (one per job) controls task execution.\n\nGuy Harrison. Next Generation Database"
  },
  {
    "objectID": "core/slides/slides01_introduction.html#mapreduces-wordcount-example",
    "href": "core/slides/slides01_introduction.html#mapreduces-wordcount-example",
    "title": "Big data technologies",
    "section": "MapReduce’s wordcount example",
    "text": "MapReduce’s wordcount example"
  },
  {
    "objectID": "core/slides/slides01_introduction.html#spark",
    "href": "core/slides/slides01_introduction.html#spark",
    "title": "Big data technologies",
    "section": "Spark",
    "text": "Spark\nAdvantages of Spark over HadoopMR ?\n\n\nIn-memory storage: use RAM for fast iterative computations\n\nLower overhead for starting jobs\n\nSimple and expressive with Scala, Python, R, Java APIs\n\nHigher level libraries with SparkSQL, SparkStreaming, etc.\n\nDisadvantages of Spark over HadoopMR ?\n\n\nSpark requires servers with more CPU and more memory\n\nBut still much cheaper than HPC\n\nSpark is much faster than Hadoop\n\n\nHadoop uses disk and network\n\n\nSpark tries to use memory as much as possible for operations while minimizing network use"
  },
  {
    "objectID": "core/slides/slides01_introduction.html#spark-versus-hadoop",
    "href": "core/slides/slides01_introduction.html#spark-versus-hadoop",
    "title": "Big data technologies",
    "section": "\nSpark versus Hadoop\n",
    "text": "Spark versus Hadoop\n\n\n\n\n\n\n\n\n\nHadoopMR\nSpark\n\n\n\nStorage\nDisk\nin-memory or disk\n\n\nOperations\nMap, reduce\nMap, reduce, join, sample, …\n\n\nExecution model\nBatch\nBatch, interactive, streaming \n\n\nProgramming environments\nJava\nScala, Java, Python, R"
  },
  {
    "objectID": "core/slides/slides01_introduction.html#spark-and-hadoop-comparison",
    "href": "core/slides/slides01_introduction.html#spark-and-hadoop-comparison",
    "title": "Big data technologies",
    "section": "\nSpark and Hadoop comparison",
    "text": "Spark and Hadoop comparison\nFor logistic regression training (a simple classification algorithm which requires several passes on a dataset)"
  },
  {
    "objectID": "core/slides/slides01_introduction.html#the-spark-stack",
    "href": "core/slides/slides01_introduction.html#the-spark-stack",
    "title": "Big data technologies",
    "section": "The Spark stack",
    "text": "The Spark stack"
  },
  {
    "objectID": "core/slides/slides01_introduction.html#the-spark-stack-1",
    "href": "core/slides/slides01_introduction.html#the-spark-stack-1",
    "title": "Big data technologies",
    "section": "The Spark stack",
    "text": "The Spark stack"
  },
  {
    "objectID": "core/slides/slides01_introduction.html#spark-can-run-everywhere",
    "href": "core/slides/slides01_introduction.html#spark-can-run-everywhere",
    "title": "Big data technologies",
    "section": "\nSpark can run “everywhere”",
    "text": "Spark can run “everywhere”"
  },
  {
    "objectID": "core/slides/slides01_introduction.html#section-5",
    "href": "core/slides/slides01_introduction.html#section-5",
    "title": "Big data technologies",
    "section": "",
    "text": "https://mesos.apache.org: Apache Mesos abstracts CPU, memory, storage, and other compute resources away from machines (physical or virtual), enabling fault-tolerant and elastic distributed systems to easily be built and run effectively. Mesos is built using the same principles as the Linux kernel, only at a different level of abstraction. The Mesos kernel runs on every machine and provides applications (e.g., Hadoop, Spark, Kafka, Elasticsearch) with API’s for resource management and scheduling across entire datacenter and cloud environments.\nhttps://kubernetes.io Kubernetes, also known as K8s, is an open-source system for automating deployment, scaling, and management of containerized applications."
  },
  {
    "objectID": "core/slides/slides01_introduction.html#very-tentative-agenda-for-the-course",
    "href": "core/slides/slides01_introduction.html#very-tentative-agenda-for-the-course",
    "title": "Big data technologies",
    "section": "Very tentative agenda for the course",
    "text": "Very tentative agenda for the course\nWeeks 1, 2 and 3  The Python data-science stack for medium-scale problems\nWeeks 4 and 5  Introduction to spark and its low-level API\nWeeks 6, 7 and 8 Spark’s high level API: .stress[spark.sql]. Data from different formats and sources\nWeek 9  Run a job on a cluster with spark-submit, monitoring, mistakes and debugging\nWeeks 10, 11, 12  Introduction to spark applications and spark-streaming"
  },
  {
    "objectID": "core/slides/slides01_introduction.html#main-tools-for-the-course-tentative",
    "href": "core/slides/slides01_introduction.html#main-tools-for-the-course-tentative",
    "title": "Big data technologies",
    "section": "Main tools for the course (tentative…)",
    "text": "Main tools for the course (tentative…)\nInfrastructure\n\n\n\nPython stack\n\n\n\nData Visualization"
  },
  {
    "objectID": "core/slides/slides01_introduction.html#main-tools-for-the-course-tentative-1",
    "href": "core/slides/slides01_introduction.html#main-tools-for-the-course-tentative-1",
    "title": "Big data technologies",
    "section": "Main tools for the course (tentative…)",
    "text": "Main tools for the course (tentative…)\nBig data processing\n\n\n\nData storage / formats / querying"
  },
  {
    "objectID": "core/slides/slides01_introduction.html#learning-resources",
    "href": "core/slides/slides01_introduction.html#learning-resources",
    "title": "Big data technologies",
    "section": "Learning resources",
    "text": "Learning resources\n\nSpark Documentation Website http://spark.apache.org/docs/latest/\nAPI docs http://spark.apache.org/docs/latest/api/scala/index.html http://spark.apache.org/docs/latest/api/python/\nDatabricks learning notebooks https://databricks.com/resources\nStackOverflow https://stackoverflow.com/tags/apache-spark https://stackoverflow.com/tags/pyspark\nMore advanced http://books.japila.pl/apache-spark-internals/\nMisc. Next Generation Databases: NoSQLand Big Data by Guy HarrisonData Pipelines Pocket Reference by J. Densmore"
  },
  {
    "objectID": "core/slides/slides01_introduction.html#learning-resources-1",
    "href": "core/slides/slides01_introduction.html#learning-resources-1",
    "title": "Big data technologies",
    "section": "Learning Resources",
    "text": "Learning Resources\n\n\n\n\nBook: Spark The Definitive Guide http://shop.oreilly.com/product/0636920034957.do https://github.com/databricks/Spark-The-Definitive-Guide\n\n\n\n\n\n\nAbove all"
  },
  {
    "objectID": "core/slides/slides01_introduction.html#data-centers-1",
    "href": "core/slides/slides01_introduction.html#data-centers-1",
    "title": "Big data technologies",
    "section": "Data centers",
    "text": "Data centers\nWonder what a datacenter looks like ?\n\nHave a look at http://www.google.com/about/datacenters"
  },
  {
    "objectID": "core/slides/slides01_introduction.html#data-centers-2",
    "href": "core/slides/slides01_introduction.html#data-centers-2",
    "title": "Big data technologies",
    "section": "Data centers",
    "text": "Data centers\nWonder what a datacenter looks like ?"
  },
  {
    "objectID": "core/slides/slides01_introduction.html#data-centers-3",
    "href": "core/slides/slides01_introduction.html#data-centers-3",
    "title": "Big data technologies",
    "section": "Data centers",
    "text": "Data centers\nWonder what a datacenter looks like ?"
  },
  {
    "objectID": "core/slides/spark-presentation.html#from-the-archive",
    "href": "core/slides/spark-presentation.html#from-the-archive",
    "title": "Spark Presentation",
    "section": "From the archive",
    "text": "From the archive\n\nSpark project\n\nlaunched in 2010 by M. Zaharia (UC Berkeley) et al.\n\n\n\n\nSpark 1.0.0 released (May 30, 2014):\n\n… This release expands Spark’s standard libraries, introducing a new SQL package (Spark SQL) that lets users integrate SQL queries into existing Spark workflows. MLlib, Spark’s machine learning library, is expanded with sparse vector support and several new algorithms. The GraphX and Streaming libraries also introduce new features and optimizations. Spark’s core engine adds support for secured YARN clusters, a unified tool for submitting Spark applications, and several performance and stability improvements.\n\n\n\n\n\nSpark 2.0.0 released (July 26, 2016)\n\nThe major updates are API usability, SQL 2003 support, performance improvements, structured streaming, R UDF support, as well as operational improvements."
  },
  {
    "objectID": "core/slides/spark-presentation.html#from-the-archive-continued",
    "href": "core/slides/spark-presentation.html#from-the-archive-continued",
    "title": "Spark Presentation",
    "section": "From the archive (continued)",
    "text": "From the archive (continued)\n\nSpark 3.0.0 released (June 18, 2020)\n\n… This year is Spark’s 10-year anniversary as an open source project. Since its initial release in 2010, Spark has grown to be one of the most active open source projects. Nowadays, Spark is the de facto unified engine for big data processing, data science, machine learning and data analytics workloads.\n\n\n\nSpark SQL is the top active component in this release. 46% of the resolved tickets are for Spark SQL. These enhancements benefit all the higher-level libraries, including structured streaming and MLlib, and higher level APIs, including SQL and DataFrames. Various related optimizations are added in this release. In TPC-DS 30TB benchmark, Spark 3.0 is roughly two times faster than Spark 2.4.\nPython is now the most widely used language on Spark. PySpark has more than 5 million monthly downloads on PyPI, the Python Package Index. This release improves its functionalities and usability, including the pandas UDF API redesign with Python type hints, new pandas UDF types, and more Pythonic error handling.\nHere are the feature highlights in Spark 3.0: adaptive query execution; dynamic partition pruning; ANSI SQL compliance; significant improvements in pandas APIs; new UI for structured streaming; up to 40x speedups for calling R user-defined functions; accelerator-aware scheduler; and SQL reference documentation.\nFrom https://spark.apache.org/releases/spark-release-3-0-0.html\n\n“Présentation de l’outil SPARK. Cette séance offrira tout d’abord un aperçu général et théorique du développement de cette technologie, un rappel historique et des solutions existantes, la scalabilité, entre autres. Un second volet de la présentation se concentrera sur l’application de cette technologie dans la recherche, avec une démonstration pratique illustrant son utilisation.”"
  },
  {
    "objectID": "core/slides/spark-presentation.html#why-spark",
    "href": "core/slides/spark-presentation.html#why-spark",
    "title": "Spark Presentation",
    "section": "Why Spark?",
    "text": "Why Spark?\n\n\n\n\nScalability\nBeyond OLTP: OLAP (and BI)\nFrom Data Mining to Big Data\nFrom Datawarehouses to Datalakes\n\n\n\n\nMapReduce \n\n\n\n\n\nApache Hadoop\n\n\n\n\nHive  \n\nBefore 2010, de facto big data SQL API\nHelped propel Hadoop to industry\n\n\n\n\n\n\n\n\n1995-2005 Beyond ACID SQL"
  },
  {
    "objectID": "core/slides/spark-presentation.html#sparksession",
    "href": "core/slides/spark-presentation.html#sparksession",
    "title": "Spark Presentation",
    "section": "Sparksession",
    "text": "Sparksession\n\nfrom pyspark.sql import SparkSession\n\nspark = (\n    SparkSession \n        .builder \n        .appName(\"Presentation\") \n        .getOrCreate()\n)"
  },
  {
    "objectID": "core/slides/spark-presentation.html#spark-core",
    "href": "core/slides/spark-presentation.html#spark-core",
    "title": "Spark Presentation",
    "section": "Spark core",
    "text": "Spark core\n\nImplements the RDD (Resilient Distributed Datasets)\n\n\n\nSpark project was launched to implement the RDD concept presented by Zaharia et al at the end of the 2000’\n\n\n\n\nIn words, RDDs behave like distributed, fault-tolerant, Python collections (list or dict)\n\n\n\n\nRDDs areo immutable, they can be transformed using map like operations, transformed RDDs can be reduced, and the result can be collected to the driver process"
  },
  {
    "objectID": "core/slides/spark-presentation.html#spark-sql-and-hive-hadoop-interactive",
    "href": "core/slides/spark-presentation.html#spark-sql-and-hive-hadoop-interactive",
    "title": "Spark Presentation",
    "section": "Spark SQL and HIVE (Hadoop InteractiVE)",
    "text": "Spark SQL and HIVE (Hadoop InteractiVE)\nSpark SQL relies on Hive SQL’s conventions and functions\nSince release 2.0, Spark offers a native SQL parser that supports ANSI-SQL and HiveQL\nWorks for analysts, data engineers, data scientists\n\n\n\n\n\n\n\nSpark-SQL is geared towards OLAP not OLTP"
  },
  {
    "objectID": "core/slides/spark-presentation.html#rows",
    "href": "core/slides/spark-presentation.html#rows",
    "title": "Spark Presentation",
    "section": "Rows",
    "text": "Rows\nSpark dataframes are RDDs (collections of Rows)\n\nfrom pyspark.sql import Row\n\nrow1 = Row(name=\"John\", age=21)\nrow2 = Row(name=\"James\", age=32)\nrow3 = Row(name=\"Jane\", age=18)\n\nrow1['name']\n\nrows = [row1, row2, row3]\ncolumn_names = [\"Name\", \"Age\"]\n\ndf = spark.createDataFrame(rows, column_names)\n\ndf.show()"
  },
  {
    "objectID": "core/slides/spark-presentation.html#schema",
    "href": "core/slides/spark-presentation.html#schema",
    "title": "Spark Presentation",
    "section": "Schema",
    "text": "Schema\n\ndf.printSchema()"
  },
  {
    "objectID": "core/slides/spark-presentation.html#from-dataframes-to-rdds",
    "href": "core/slides/spark-presentation.html#from-dataframes-to-rdds",
    "title": "Spark Presentation",
    "section": "From dataframes to RDDs\n",
    "text": "From dataframes to RDDs\n\n\nprint(df.rdd.toDebugString().decode(\"utf-8\"))\n\n\n\n\n\ndf.rdd.getNumPartitions()"
  },
  {
    "objectID": "core/slides/spark-presentation.html#spark-dataframe-api",
    "href": "core/slides/spark-presentation.html#spark-dataframe-api",
    "title": "Spark Presentation",
    "section": "Spark dataframe API",
    "text": "Spark dataframe API\nThe Spark dataframe API offers a developper-friendly API for implementing\n\nRelational algebra \\(\\sigma, \\pi, \\bowtie, \\cup, \\cap, \\setminus\\)\n\nPartitionning GROUP BY\n\nAggregation and Window functions\n\n\nCompare the Spark Dataframe API with:\n dplyr, dtplyr, dbplyr in R Tidyverse\n Pandas\n Pandas on Spark\nChaining and/or piping enable modular query construction"
  },
  {
    "objectID": "core/slides/spark-presentation.html#basic-single-tables-operations-methodsverbs",
    "href": "core/slides/spark-presentation.html#basic-single-tables-operations-methodsverbs",
    "title": "Spark Presentation",
    "section": "Basic Single Tables Operations (methods/verbs)",
    "text": "Basic Single Tables Operations (methods/verbs)\n\n\n\n\n\n\nOperation\nDescription\n\n\n\nselect\nChooses columns from the table \\(\\pi\\)\n\n\n\nselectExpr\nChooses columns and expressions from table \\(\\pi\\)\n\n\n\nwhere\nFilters rows based on a boolean rule \\(\\sigma\\)\n\n\n\nlimit\nLimits the number of rows LIMIT ...\n\n\n\norderBy\nSorts the DataFrame based on one or more columns ORDER BY ...\n\n\n\nalias\nChanges the name of a column AS ...\n\n\n\ncast\nChanges the type of a column\n\n\nwithColumn\nAdds a new column"
  },
  {
    "objectID": "core/slides/spark-presentation.html#toy-example",
    "href": "core/slides/spark-presentation.html#toy-example",
    "title": "Spark Presentation",
    "section": "Toy example",
    "text": "Toy example\n\ncolumn_names = [\"name\", \"age\", \"gender\"]\nrows = [\n        [\"John\", 21, \"male\"],\n        [\"Jane\", 25, \"female\"]\n    ]\ndf = spark.createDataFrame(rows, column_names)\n\ndf.show()"
  },
  {
    "objectID": "core/slides/spark-presentation.html#querying-sql-style",
    "href": "core/slides/spark-presentation.html#querying-sql-style",
    "title": "Spark Presentation",
    "section": "Querying SQL style",
    "text": "Querying SQL style\n\n## Create a temporary view from the DataFrame\ndf.createOrReplaceTempView(\"new_view\")\n\n## Define the query\nquery = \"\"\"\n  SELECT name, age \n  FROM new_view \n  WHERE gender='male'\n\"\"\"\n\nmen_df = spark.sql(query)\nmen_df.show()"
  },
  {
    "objectID": "core/slides/spark-presentation.html#select",
    "href": "core/slides/spark-presentation.html#select",
    "title": "Spark Presentation",
    "section": "Select",
    "text": "Select\nThe argument of select() is *cols where cols can be built from column names (strings), column expressions like df.age + 10, lists\n\ndf.select(df.name.alias(\"nom\"), df.age+10 ).show()\n\n\n\n\n\ndf.select([c for c in df.columns if \"a\" in c]).show()"
  },
  {
    "objectID": "core/slides/spark-presentation.html#adding-new-columns",
    "href": "core/slides/spark-presentation.html#adding-new-columns",
    "title": "Spark Presentation",
    "section": "Adding new columns",
    "text": "Adding new columns\n\n## In a SQL query:\nquery = \"SELECT *, 12*age AS age_months FROM table\"\n\n## Using Spark SQL API:\ndf.withColumn(\"age_months\", df.age * 12).show()\n\n## Or\ndf.select(\"*\", \n          (df.age * 12).alias(\"age_months\")\n  ).show()"
  },
  {
    "objectID": "core/slides/spark-presentation.html#basic-operations",
    "href": "core/slides/spark-presentation.html#basic-operations",
    "title": "Spark Presentation",
    "section": "Basic operations",
    "text": "Basic operations\n\nThe full list of operations that can be applied to a DataFrame can be found in the [DataFrame doc]\nThe list of operations on columns can be found in the [Column docs]"
  },
  {
    "objectID": "core/slides/spark-presentation.html#sparkr-and-sparklyr",
    "href": "core/slides/spark-presentation.html#sparkr-and-sparklyr",
    "title": "Spark Presentation",
    "section": "\nsparkR and sparklyr\n",
    "text": "sparkR and sparklyr\n\n\n\n\nsparkR is the official Spark API for R users\n\n\n\n\n\nsparklyr (released 2016) is the de facto Spark API for tidyverse"
  },
  {
    "objectID": "core/slides/spark-presentation.html#a-glimpse-at-sparklyr",
    "href": "core/slides/spark-presentation.html#a-glimpse-at-sparklyr",
    "title": "Spark Presentation",
    "section": "A glimpse at Sparklyr",
    "text": "A glimpse at Sparklyr\n\nSpark dataframes can be handled through dplyr pipelines\n#| code-line-numbers: |4|5|6|7\nsc &lt;- spark_connect(master=\"local\", version=\"3.5\")\nwh &lt;- copy_to(sc, whiteside)\n\nwh |&gt; \n    group_by(Insul) |&gt; \n    mutate(Fn=(1+n()-min_rank(desc(Temp)))/n()) |&gt; \n    arrange(Insul, Temp)\n\n\n# Source:     spark&lt;?&gt; [?? x 4]\n# Groups:     Insul\n# Ordered by: Insul, Temp\n   Insul  Temp   Gas     Fn\n   &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n 1 After  -0.7   4.8 0.0333\n 2 After   0.8   4.6 0.0667\n 3 After   1     4.7 0.1   \n 4 After   1.4   4   0.133 \n 5 After   1.5   4.2 0.167 \n 6 After   1.6   4.2 0.2   \n 7 After   2.3   4.1 0.233 \n 8 After   2.5   4   0.3   \n 9 After   2.5   3.5 0.3   \n10 After   3.1   3.2 0.333 \n# ℹ more rows\n# ℹ Use `print(n = ...)` to see more rows"
  },
  {
    "objectID": "core/slides/spark-presentation.html#under-the-hood",
    "href": "core/slides/spark-presentation.html#under-the-hood",
    "title": "Spark Presentation",
    "section": "Under the hood",
    "text": "Under the hood\n&gt; wh |&gt;  \n    summarise(x=quantile(Temp,.25)) |&gt; \n    show_query()\n\n&lt;SQL&gt;\nSELECT PERCENTILE_CONT(0.25) WITHIN GROUP (ORDER BY `Temp`) AS `x`\nFROM `whiteside`\n\n\ndplyr queries are translated into Spark/Hive SQL\n quantile() is a base R function, it is matched to the Spark/Hive percentile() function\nsparklyr aims at avoiding sending R functions/objects across the cluster"
  },
  {
    "objectID": "core/slides/spark-presentation.html#in-words",
    "href": "core/slides/spark-presentation.html#in-words",
    "title": "Spark Presentation",
    "section": "In words",
    "text": "In words\n\n… sparklyr translates dplyr functions such as arrange into a SQL query plan that is used by SparkSQL. This is not the case with SparkR, which has functions for SparkSQL tables and Spark DataFrames.\n\n\n\n… Databricks does not recommended combining SparkR and sparklyr APIs in the same script, notebook, or job."
  },
  {
    "objectID": "core/slides/slides09_dask.html#bird-eye-big-picture",
    "href": "core/slides/slides09_dask.html#bird-eye-big-picture",
    "title": "Dask",
    "section": "Bird-eye Big Picture",
    "text": "Bird-eye Big Picture\n\nDask in picture"
  },
  {
    "objectID": "core/slides/slides09_dask.html#section",
    "href": "core/slides/slides09_dask.html#section",
    "title": "Dask",
    "section": "",
    "text": "Overview - dask’s place in the universe.\nDelayed - the single-function way to parallelize general python code.\nDataframe - parallelized operations on many pandas dataframes spread across your cluster"
  },
  {
    "objectID": "core/slides/slides09_dask.html#flavours-of-big-data",
    "href": "core/slides/slides09_dask.html#flavours-of-big-data",
    "title": "Dask",
    "section": "Flavours of (big) data",
    "text": "Flavours of (big) data\n\n\n\n\n\n\n\n\n\nType\nTypical size\nFeatures\nTool\n\n\n\n\nSmall data\nFew GigaBytes\nFits in RAM\nPandas\n\n\nMedium data\nLess than 2 Terabytes\nDoes not fit in RAM, fits on hard drive\nDask\n\n\nLarge data\nPetabytes\nDoes not fit on hard drive\nSpark"
  },
  {
    "objectID": "core/slides/slides09_dask.html#sources",
    "href": "core/slides/slides09_dask.html#sources",
    "title": "Dask",
    "section": "Sources",
    "text": "Sources\nDask Tutorial\nDask FAQ"
  },
  {
    "objectID": "core/slides/slides09_dask.html#trends",
    "href": "core/slides/slides09_dask.html#trends",
    "title": "Dask",
    "section": "Trends",
    "text": "Trends\n\nDask adoption metrics"
  },
  {
    "objectID": "core/slides/slides09_dask.html#delayed-in-a-nutshell",
    "href": "core/slides/slides09_dask.html#delayed-in-a-nutshell",
    "title": "Dask",
    "section": "Delayed (in a nutshell)",
    "text": "Delayed (in a nutshell)\n\nThe single-function way to parallelize general python code"
  },
  {
    "objectID": "core/slides/slides09_dask.html#imports",
    "href": "core/slides/slides09_dask.html#imports",
    "title": "Dask",
    "section": "Imports",
    "text": "Imports\n\nimport dask\n\ndask.config.set(scheduler='threads')\ndask.config.set({'dataframe.query-planning': True})\n\n\n\n&lt;dask.config.set at 0x7756d013e900&gt;\n\n\n\nimport dask.dataframe as dd\nimport dask.bag as db\n\n\n\n\n\nfrom dask import delayed\nimport dask.threaded\n\nfrom dask.distributed import Client\nfrom dask.diagnostics import ProgressBar\nfrom dask.diagnostics import Profiler, ResourceProfiler, CacheProfiler"
  },
  {
    "objectID": "core/slides/slides09_dask.html#localcluster",
    "href": "core/slides/slides09_dask.html#localcluster",
    "title": "Dask",
    "section": "LocalCluster",
    "text": "LocalCluster\nDask can set itself up easily in your Python session if you create a LocalCluster object, which sets everything up for you.\n\n# from dask.distributed import LocalCluster\n\n# cluster = LocalCluster()\n# client = cluster.get_client()"
  },
  {
    "objectID": "core/slides/slides09_dask.html#normal-dask-work",
    "href": "core/slides/slides09_dask.html#normal-dask-work",
    "title": "Dask",
    "section": "Normal Dask work …",
    "text": "Normal Dask work …\nAlternatively, you can skip this part, and Dask will operate within a thread pool contained entirely with your local process."
  },
  {
    "objectID": "core/slides/slides09_dask.html#delaying-pyhton-tasks",
    "href": "core/slides/slides09_dask.html#delaying-pyhton-tasks",
    "title": "Dask",
    "section": "Delaying Pyhton tasks",
    "text": "Delaying Pyhton tasks"
  },
  {
    "objectID": "core/slides/slides09_dask.html#a-job-i",
    "href": "core/slides/slides09_dask.html#a-job-i",
    "title": "Dask",
    "section": "A job (I)",
    "text": "A job (I)\n\ndef inc(x):\n  return x + 1\n\ndef double(x):\n  return x * 2\n\ndef add(x, y):\n  return x + y"
  },
  {
    "objectID": "core/slides/slides09_dask.html#a-job-ii-piecing-elements-together",
    "href": "core/slides/slides09_dask.html#a-job-ii-piecing-elements-together",
    "title": "Dask",
    "section": "A job (II): piecing elements together",
    "text": "A job (II): piecing elements together\n\ndata = [1, 2, 3, 4, 5]\n\noutput = []\n\nfor x in data:\n1  a = inc(x)\n2  b = double(x)\n3  c = add(a, b)\n  output.append(c)\n  \ntotal = sum(output)\n  \ntotal \n\n\n1\n\nIncrement x\n\n2\n\nMultiply x by 2\n\n3\n\nc == (x+1) + 2*x == 3*x+1\n\n\n\n\n\n\n50"
  },
  {
    "objectID": "core/slides/slides09_dask.html#delaying-existing-functions",
    "href": "core/slides/slides09_dask.html#delaying-existing-functions",
    "title": "Dask",
    "section": "Delaying existing functions",
    "text": "Delaying existing functions\n\noutput = []\n\nfor x in data:\n1  a = dask.delayed(inc)(x)\n  b = dask.delayed(double)(x) \n  c = dask.delayed(add)(a, b) \n  output.append(c)\n  \n2total = dask.delayed(sum)(output)\n  \ntotal\n\n\n1\n\nDecorating inc using dask.delayed()\n\n2\n\nDecorating sum()\n\n\n\n\n\n\nDelayed('sum-b042864a-ca75-4d5d-9921-92cf16c401d2')\n\n\n\n1total.compute()\n\n\n1\n\nCollecting the results\n\n\n\n\n\n\n50"
  },
  {
    "objectID": "core/slides/slides09_dask.html#another-way-of-using-decorators",
    "href": "core/slides/slides09_dask.html#another-way-of-using-decorators",
    "title": "Dask",
    "section": "Another way of using decorators",
    "text": "Another way of using decorators\n\n1@dask.delayed\ndef inc(x):\n  return x + 1\n\n@dask.delayed\ndef double(x):\n  return x * 2\n\n@dask.delayed\ndef add(x, y):\n  return x + y\n\ndata = [1, 2, 3, 4, 5]\n\n2output = []\nfor x in data:\n  a = inc(x)\n  b = double(x)\n  c = add(a, b)\n  output.append(c)\n  \ntotal = dask.delayed(sum)(output)\ntotal\n3total.compute()\n\n\n1\n\nDecorating the definition\n\n2\n\nReusing the Python code\n\n3\n\nCollecting results\n\n\n\n\n\n\n50"
  },
  {
    "objectID": "core/slides/slides09_dask.html#visualizing-the-task-graph",
    "href": "core/slides/slides09_dask.html#visualizing-the-task-graph",
    "title": "Dask",
    "section": "Visualizing the task graph",
    "text": "Visualizing the task graph\n\n\ntotal.visualize()"
  },
  {
    "objectID": "core/slides/slides09_dask.html#another-job",
    "href": "core/slides/slides09_dask.html#another-job",
    "title": "Dask",
    "section": "Another job",
    "text": "Another job\n\nDATA = []\n\n@dask.delayed\ndef inc(x):\n  return x + 1\n\n@dask.delayed\ndef add_data(x):\n  DATA.append(x)\n\n@dask.delayed\ndef sum_data(x):\n  return sum(DATA) + x\n\na = inc(1)\nb = add_data(a)\nc = inc(3)\nd = add_data(c)\ne = inc(5)\nf = sum_data(e)\nf.compute()\n\n\n\n6"
  },
  {
    "objectID": "core/slides/slides09_dask.html#a-flawed-task-graph",
    "href": "core/slides/slides09_dask.html#a-flawed-task-graph",
    "title": "Dask",
    "section": "A flawed task graph",
    "text": "A flawed task graph\n\n\nf.visualize()"
  },
  {
    "objectID": "core/slides/slides09_dask.html#fixing",
    "href": "core/slides/slides09_dask.html#fixing",
    "title": "Dask",
    "section": "Fixing",
    "text": "Fixing\n\n\n\nfrom dask.graph_manipulation import bind\n\ng = bind(sum_data, [b, d])(e)\n\ng.compute()\n\n\n\n12\n\n\n\nThe result of the evaluation of sum_data() depends not only on its argument, hence on the Delayed e, but also on the side effects of add_data(), that is on the Delayed b and d\nNote that not only the DAG was wrong but the result obtained above was not the intended result.\n\n\n\n\ng.visualize()"
  },
  {
    "objectID": "core/slides/slides09_dask.html#section-1",
    "href": "core/slides/slides09_dask.html#section-1",
    "title": "Dask",
    "section": "",
    "text": "By default, Dask Delayed uses the threaded scheduler in order to avoid data transfer costs\n\n\nConsider using multi-processing scheduler or dask.distributed scheduler on a local machine or on a cluster if your code does not release the GIL well (computations that are dominated by pure Python code, or computations wrapping external code and holding onto it)."
  },
  {
    "objectID": "core/slides/slides09_dask.html#importing-the-usual-suspects",
    "href": "core/slides/slides09_dask.html#importing-the-usual-suspects",
    "title": "Dask",
    "section": "Importing the usual suspects",
    "text": "Importing the usual suspects\n\nimport numpy as np\n1import pandas as pd\n\n2import dask.dataframe as dd\nimport dask.array as da\nimport dask.bag as db\n\n\n1\n\nStandard dataframes in Python\n\n2\n\nParallelized and distributed dataframes in Python"
  },
  {
    "objectID": "core/slides/slides09_dask.html#bird-eye-view",
    "href": "core/slides/slides09_dask.html#bird-eye-view",
    "title": "Dask",
    "section": "Bird-eye view",
    "text": "Bird-eye view"
  },
  {
    "objectID": "core/slides/slides09_dask.html#section-2",
    "href": "core/slides/slides09_dask.html#section-2",
    "title": "Dask",
    "section": "",
    "text": "Dask Dataframes parallelize the popular pandas library, providing:\n\n\n\n\nLarger-than-memory execution for single machines, allowing you to process data that is larger than your available RAM\n\n\n\n\n\n\nParallel execution for faster processing\n\n\n\n\n\n\nDistributed computation for terabyte-sized datasets\n\n\n\n\n\nDask Dataframes are similar to Apache Spark, but use the familiar pandas API and memory model\n\n\nOne Dask dataframe is simply a coordinated collection of pandas dataframes on different computers"
  },
  {
    "objectID": "core/slides/slides09_dask.html#section-3",
    "href": "core/slides/slides09_dask.html#section-3",
    "title": "Dask",
    "section": "",
    "text": "Dask DataFrame helps you process large tabular data by parallelizing Pandas, either on your laptop for larger-than-memory computing, or on a distributed cluster of computers.\n\n\n\n\nColumn of four squares collectively labeled as a Dask DataFrame with a single constituent square labeled as a pandas DataFrame\n\n\n\nJust pandas: Dask DataFrames are a collection of many pandas DataFrames.\n\n\nThe API is the same1. The execution is the same \n\n\nLarge scale: Works on 100 GiB on a laptop, or 100 TiB on a cluster.\n\n\nEasy to use: Pure Python, easy to set up and debug.\n\n\nDask DataFrames coordinate many pandas DataFrames/Series arranged along the index. A Dask DataFrame is partitioned row-wise, grouping rows by index value for efficiency. These pandas objects may live on disk or on other machines.\n\nThe Dask Dataframe API is a subset of the Pandas API"
  },
  {
    "objectID": "core/slides/slides09_dask.html#creating-a-dask-dataframe",
    "href": "core/slides/slides09_dask.html#creating-a-dask-dataframe",
    "title": "Dask",
    "section": "Creating a dask dataframe",
    "text": "Creating a dask dataframe\n\n\nindex = pd.date_range(\"2021-09-01\", \n                      periods=2400, \n                      freq=\"1H\")\n\ndf = pd.DataFrame({\n  \"a\": np.arange(2400), \n  \"b\": list(\"abcaddbe\" * 300)}, \n  index=index)\n  \n1ddf = dd.from_pandas(df, npartitions=20)\n\n2ddf.head()\n\n\n\n1\n\nIn Dask, proper partitioning is a key performance issue\n\n2\n\nThe dataframe API is (almost) the same as in Pandas!\n\n\n\n\n\n\n\n\n\n\n\na\nb\n\n\n\n\n2021-09-01 00:00:00\n0\na\n\n\n2021-09-01 01:00:00\n1\nb\n\n\n2021-09-01 02:00:00\n2\nc\n\n\n2021-09-01 03:00:00\n3\na\n\n\n2021-09-01 04:00:00\n4\nd\n\n\n\n\n\n\n\n\n\n\npandas programmers just need to learn the key differences when working with distributed computing systems to make the Dask transition easily."
  },
  {
    "objectID": "core/slides/slides09_dask.html#inside-the-dataframe",
    "href": "core/slides/slides09_dask.html#inside-the-dataframe",
    "title": "Dask",
    "section": "Inside the dataframe",
    "text": "Inside the dataframe\n\n\n\nA sketch of the interplay between index and partitioning\n\n\nddf.divisions\n\n\n(Timestamp('2021-09-01 00:00:00'),\n Timestamp('2021-09-06 00:00:00'),\n Timestamp('2021-09-11 00:00:00'),\n Timestamp('2021-09-16 00:00:00'),\n Timestamp('2021-09-21 00:00:00'),\n Timestamp('2021-09-26 00:00:00'),\n Timestamp('2021-10-01 00:00:00'),\n Timestamp('2021-10-06 00:00:00'),\n Timestamp('2021-10-11 00:00:00'),\n Timestamp('2021-10-16 00:00:00'),\n Timestamp('2021-10-21 00:00:00'),\n Timestamp('2021-10-26 00:00:00'),\n Timestamp('2021-10-31 00:00:00'),\n Timestamp('2021-11-05 00:00:00'),\n Timestamp('2021-11-10 00:00:00'),\n Timestamp('2021-11-15 00:00:00'),\n Timestamp('2021-11-20 00:00:00'),\n Timestamp('2021-11-25 00:00:00'),\n Timestamp('2021-11-30 00:00:00'),\n Timestamp('2021-12-05 00:00:00'),\n Timestamp('2021-12-09 23:00:00'))\n\n\nA dataframe has a task graph\n\nddf.visualize()\n\n\n\n\n\n\n\n\n\n\nTODO\n\n\n\nWhat’s in a partition?\n\n1ddf.partitions[1]\n\n\n1\n\nThis is the second class of the partition\n\n\n\n\n\n\nDask DataFrame Structure:\n\n\n\n\n\na\nb\n\n\nnpartitions=1\n\n\n\n\n\n\n2021-09-06\nint64\nstring\n\n\n2021-09-11\n...\n...\n\n\n\n\nDask Name: partitions, 2 expressions\n\n\nSlicing\n\n1ddf[\"2021-10-01\":\"2021-10-09 5:00\"]\n\n\n1\n\nLike slicing NumPy arrays or pandas DataFrame.\n\n\n\n\n\n\nDask DataFrame Structure:\n\n\n\n\n\na\nb\n\n\nnpartitions=2\n\n\n\n\n\n\n2021-10-01 00:00:00.000000000\nint64\nstring\n\n\n2021-10-06 00:00:00.000000000\n...\n...\n\n\n2021-10-09 05:00:59.999999999\n...\n...\n\n\n\n\nDask Name: loc, 2 expressions"
  },
  {
    "objectID": "core/slides/slides09_dask.html#dask-dataframes-contd",
    "href": "core/slides/slides09_dask.html#dask-dataframes-contd",
    "title": "Dask",
    "section": "Dask dataframes (cont’d)",
    "text": "Dask dataframes (cont’d)\n\nDask DataFrames coordinate many Pandas DataFrames/Series arranged along an index.\n\n\nWe define a Dask DataFrame object with the following components:\n\n\n\n\nA Dask graph with a special set of keys designating partitions, such as (‘x’, 0), (‘x’, 1), …\n\n\n\n\n\n\nA name to identify which keys in the Dask graph refer to this DataFrame, such as ‘x’\n\n\n\n\n\n\nAn empty Pandas object containing appropriate metadata (e.g. column names, dtypes, etc.)\n\n\n\n\n\n\nA sequence of partition boundaries along the index called divisions"
  },
  {
    "objectID": "core/slides/slides09_dask.html#methods",
    "href": "core/slides/slides09_dask.html#methods",
    "title": "Dask",
    "section": "Methods",
    "text": "Methods\n\n\n\n( \n  ddf.a\n    .mean()\n)\n\n\n\n&lt;dask_expr.expr.Scalar: expr=df['a'].mean(), dtype=float64&gt;\n\n\n\n( \n  ddf.a\n    .mean()\n    .compute()\n)\n\n\n\nnp.float64(1199.5)\n\n\n\n(\n  ddf\n    .b\n    .unique()\n)\n\n\n\nDask Series Structure:\nnpartitions=20\n    string\n       ...\n     ...  \n       ...\n       ...\nDask Name: unique, 3 expressions\nExpr=Unique(frame=df['b'])"
  },
  {
    "objectID": "core/slides/slides09_dask.html#reading-and-writing-from-parquet",
    "href": "core/slides/slides09_dask.html#reading-and-writing-from-parquet",
    "title": "Dask",
    "section": "Reading and writing from parquet",
    "text": "Reading and writing from parquet\nfname = 'fhvhv_tripdata_2022-11.parquet'\ndpath = '../../../../Downloads/'\n\nglobpath = 'fhvhv_tripdata_20*-*.parquet'\n\n!ls -l ../../../../Downloads/fhvhv_tripdata_20*-*.parquet\n\nimport os\n\nos.path.expanduser('~' + '/Documents')\n\n\n\n'/home/boucheron/Documents'\n\n\n%%time \n\ndata = dd.read_parquet(\n  os.path.join(dpath, globpath),\n  categories= ['PULocationID',\n               'DOLocationID'], \n  engine='auto'\n)\ntype(data)\n#| eval: false\ndf = data.to_dask_dataframe()\n\ndf.info()\ndf._meta.dtypes\n\ndf.npartitions\n\n\n\n\n\ndf.head()\n\n\n\n\n\ntype(df)\n\n\n\n\n\ndf._meta.dtypes\n\n\n\n\n\ndf._meta_nonempty\n\n\n\n\n\ndf.info()\n\n\n\n\n\ndf.divisions\n\n\n\n\n\ndf.describe(include=\"all\")"
  },
  {
    "objectID": "core/slides/slides09_dask.html#partitioning-and-saving-to-parquet",
    "href": "core/slides/slides09_dask.html#partitioning-and-saving-to-parquet",
    "title": "Dask",
    "section": "Partitioning and saving to parquet",
    "text": "Partitioning and saving to parquet\n\nimport pyarrow as pa\n\nschm = pa.Schema.from_pandas(df._meta)\n\nschm\n\n\n\n\n\ndf.PULocationID.unique().compute()\n\n\n\n\n\ndf.to_parquet( \n  'fhvhv_tripdata_2022-11',\n  partition_on= ['PULocationID'],\n  engine='pyarrow', \n  schema = schm\n  )\n\n\n\n\n\ndf.info(memory_usage=True)"
  },
  {
    "objectID": "core/slides/slides09_dask.html#section-4",
    "href": "core/slides/slides09_dask.html#section-4",
    "title": "Dask",
    "section": "",
    "text": "After you have generated a task graph, it is the scheduler’s job to execute it (see Scheduling).\n\n\nBy default, for the majority of Dask APIs, when you call compute() on a Dask object, Dask uses the thread pool on your computer (a.k.a threaded scheduler) to run computations in parallel. This is true for Dask Array, Dask DataFrame, and Dask Delayed. The exception being Dask Bag which uses the multiprocessing scheduler by default.\n\n\nIf you want more control, use the distributed scheduler instead. Despite having “distributed” in it’s name, the distributed scheduler works well on both single and multiple machines. Think of it as the “advanced scheduler”."
  },
  {
    "objectID": "core/slides/slides09_dask.html#section-5",
    "href": "core/slides/slides09_dask.html#section-5",
    "title": "Dask",
    "section": "",
    "text": "Dask schedulers come with diagnostics to help you understand the performance characteristics of your computations\n\n\nBy using these diagnostics and with some thought, we can often identify the slow parts of troublesome computations\n\n\nThe single-machine and distributed schedulers come with different diagnostic tools\n\n\nThese tools are deeply integrated into each scheduler, so a tool designed for one will not transfer over to the other"
  },
  {
    "objectID": "core/slides/slides09_dask.html#dask-query-optimization",
    "href": "core/slides/slides09_dask.html#dask-query-optimization",
    "title": "Dask",
    "section": "Dask query optimization",
    "text": "Dask query optimization\nDemo"
  },
  {
    "objectID": "core/slides/slides09_dask.html#visualize-task-graphs",
    "href": "core/slides/slides09_dask.html#visualize-task-graphs",
    "title": "Dask",
    "section": "Visualize task graphs",
    "text": "Visualize task graphs"
  },
  {
    "objectID": "core/slides/slides09_dask.html#single-threaded-scheduler-and-a-normal-python-profiler",
    "href": "core/slides/slides09_dask.html#single-threaded-scheduler-and-a-normal-python-profiler",
    "title": "Dask",
    "section": "Single threaded scheduler and a normal Python profiler",
    "text": "Single threaded scheduler and a normal Python profiler"
  },
  {
    "objectID": "core/slides/slides09_dask.html#diagnostics-for-the-single-machine-scheduler",
    "href": "core/slides/slides09_dask.html#diagnostics-for-the-single-machine-scheduler",
    "title": "Dask",
    "section": "Diagnostics for the single-machine scheduler",
    "text": "Diagnostics for the single-machine scheduler"
  },
  {
    "objectID": "core/slides/slides09_dask.html#diagnostics-for-the-distributed-scheduler-and-dashboard",
    "href": "core/slides/slides09_dask.html#diagnostics-for-the-distributed-scheduler-and-dashboard",
    "title": "Dask",
    "section": "Diagnostics for the distributed scheduler and dashboard",
    "text": "Diagnostics for the distributed scheduler and dashboard"
  },
  {
    "objectID": "core/slides/slides09_dask.html#reference",
    "href": "core/slides/slides09_dask.html#reference",
    "title": "Dask",
    "section": " Reference",
    "text": "Reference\n\nDocs\nExamples\nCode\nBlog\nTutorial"
  },
  {
    "objectID": "core/slides/slides09_dask.html#ask-for-help",
    "href": "core/slides/slides09_dask.html#ask-for-help",
    "title": "Dask",
    "section": " Ask for help",
    "text": "Ask for help\n\ndask tag on Stack Overflow, for usage questions\ngithub issues for bug reports and feature requests\ngitter chat for general, non-bug, discussion"
  },
  {
    "objectID": "core/slides/slides09_dask.html#books",
    "href": "core/slides/slides09_dask.html#books",
    "title": "Dask",
    "section": " Books",
    "text": "Books\n\nScaling Python with Dask\nData Science with Python and Dask\n[Dask Definitive Guide (to appear 2025)]"
  },
  {
    "objectID": "core/slides/slides09_dask.html#blogs",
    "href": "core/slides/slides09_dask.html#blogs",
    "title": "Dask",
    "section": "Blogs",
    "text": "Blogs"
  },
  {
    "objectID": "core/slides/slides09_dask.html#loading-a-parquet-file",
    "href": "core/slides/slides09_dask.html#loading-a-parquet-file",
    "title": "Dask",
    "section": "Loading a Parquet file",
    "text": "Loading a Parquet file\n\ndpath = '/home/boucheron/Dropbox/MMD-2021/DATA/ny_corpus_prq/'\n\nglobpath = '*/*.parquet'\n\ndata = dd.read_parquet(\n  os.path.join(dpath, globpath),\n  engine='auto'\n)\n\n\n\n\n\n\ndata.info\n\n\n&lt;bound method DataFrame.info of Dask DataFrame Structure:\n                 title   topic    text             date\nnpartitions=77                                         \n                string  string  string  category[known]\n                   ...     ...     ...              ...\n...                ...     ...     ...              ...\n                   ...     ...     ...              ...\n                   ...     ...     ...              ...\nDask Name: read_parquet, 1 expression\nExpr=ReadParquetFSSpec(92994fd)&gt;"
  },
  {
    "objectID": "core/slides/slides09_dask.html#section-6",
    "href": "core/slides/slides09_dask.html#section-6",
    "title": "Dask",
    "section": "",
    "text": "( \n  data\n    .groupby(\"topic\")\n    .count()\n)\n\n\n\nDask DataFrame Structure:\n\n\n\n\n\ntitle\ntext\ndate\n\n\nnpartitions=1\n\n\n\n\n\n\n\n\nint64\nint64\nint64\n\n\n\n...\n...\n...\n\n\n\n\nDask Name: count, 2 expressions"
  },
  {
    "objectID": "core/slides/slides09_dask.html#section-7",
    "href": "core/slides/slides09_dask.html#section-7",
    "title": "Dask",
    "section": "",
    "text": "ddf = dd.read_parquet(\n    \"s3://dask-data/nyc-taxi/nyc-2015.parquet/part.*.parquet\",\n    columns=[\n      \"passenger_count\", \n      \"tip_amount\"],\n    storage_options={\"anon\": True},\n)\n\n\n\n\n\nresult = (\n  ddf\n    .groupby(\"passenger_count\")\n    .tip_amount\n    .mean()\n#    .compute()\n)\n\nresult\n\n\n\nDask Series Structure:\nnpartitions=1\n    float64\n        ...\nDask Name: getitem, 4 expressions\nExpr=((ReadParquetFSSpec(117185e)[['passenger_count', 'tip_amount']]).mean(observed=False, chunk_kwargs={'numeric_only': False}, aggregate_kwargs={'numeric_only': False}, _slice='tip_amount'))['tip_amount']\n\n\n\nimport dask.dataframe as dd\nfrom dask.distributed import Client\n\n\n\n\n\nclient = Client()\nclient\n\n\n\n\n     \n    \n        Client\n        Client-23ac4916-e27e-11ef-a64c-300505fc3398\n        \n\n\n\nConnection method: Cluster object\nCluster type: distributed.LocalCluster\n\n\nDashboard: http://127.0.0.1:8787/status\n\n\n\n\n\n\n        \n\n        \n            \n            Cluster Info\n            \n    \n    \n    \n        LocalCluster\n        62d0ccd4\n        \n\n\n\nDashboard: http://127.0.0.1:8787/status\nWorkers: 5\n\n\nTotal threads: 20\nTotal memory: 30.96 GiB\n\n\nStatus: running\nUsing processes: True\n\n\n\n\n\n        \n            \n                Scheduler Info\n            \n\n            \n    \n         \n        \n            Scheduler\n            Scheduler-4a341215-7814-4fde-a35c-f90f4492536c\n            \n\n\n\nComm: tcp://127.0.0.1:45181\nWorkers: 5\n\n\nDashboard: http://127.0.0.1:8787/status\nTotal threads: 20\n\n\nStarted: Just now\nTotal memory: 30.96 GiB\n\n\n\n\n        \n    \n\n    \n        \n            Workers\n        \n\n        \n        \n             \n            \n            \n                \n                    Worker: 0\n                \n                \n\n\n\nComm: tcp://127.0.0.1:34133\nTotal threads: 4\n\n\nDashboard: http://127.0.0.1:39875/status\nMemory: 6.19 GiB\n\n\nNanny: tcp://127.0.0.1:46805\n\n\n\nLocal directory: /tmp/dask-scratch-space/worker-31p6doxl\n\n\n\n\n            \n            \n        \n        \n        \n             \n            \n            \n                \n                    Worker: 1\n                \n                \n\n\n\nComm: tcp://127.0.0.1:41389\nTotal threads: 4\n\n\nDashboard: http://127.0.0.1:37229/status\nMemory: 6.19 GiB\n\n\nNanny: tcp://127.0.0.1:43601\n\n\n\nLocal directory: /tmp/dask-scratch-space/worker-m9q13h76\n\n\n\n\n            \n            \n        \n        \n        \n             \n            \n            \n                \n                    Worker: 2\n                \n                \n\n\n\nComm: tcp://127.0.0.1:34985\nTotal threads: 4\n\n\nDashboard: http://127.0.0.1:41965/status\nMemory: 6.19 GiB\n\n\nNanny: tcp://127.0.0.1:34219\n\n\n\nLocal directory: /tmp/dask-scratch-space/worker-1yjf2_ei\n\n\n\n\n            \n            \n        \n        \n        \n             \n            \n            \n                \n                    Worker: 3\n                \n                \n\n\n\nComm: tcp://127.0.0.1:35209\nTotal threads: 4\n\n\nDashboard: http://127.0.0.1:33913/status\nMemory: 6.19 GiB\n\n\nNanny: tcp://127.0.0.1:42609\n\n\n\nLocal directory: /tmp/dask-scratch-space/worker-_sj1rvoz\n\n\n\n\n            \n            \n        \n        \n        \n             \n            \n            \n                \n                    Worker: 4\n                \n                \n\n\n\nComm: tcp://127.0.0.1:42041\nTotal threads: 4\n\n\nDashboard: http://127.0.0.1:45571/status\nMemory: 6.19 GiB\n\n\nNanny: tcp://127.0.0.1:35539\n\n\n\nLocal directory: /tmp/dask-scratch-space/worker-tgi7deul"
  },
  {
    "objectID": "core/slides/spark-nlp.html#spark-nlp",
    "href": "core/slides/spark-nlp.html#spark-nlp",
    "title": "Spark NLP",
    "section": "Spark NLP",
    "text": "Spark NLP\nSpark NLP provides an example of an application in the Apache Spark Ecosystem\n\nSpark NLP relies on the Spark SQL Lib and Spark Dataframes (high level APIs) and also on the Spark ML Lib.\n\n\nSpark NLP borrows ideas from existing NLP softwares and adapts the known techniques to the Spark principles\n…\nNLP deals with many applications of machine learning\n\nAutomatic translation (see deepl.com)\nTopic modeling (text clustering)\nSentiment Analysis\nLLMs\n…"
  },
  {
    "objectID": "core/slides/spark-nlp.html#two-flavors-of-nlp-libraries",
    "href": "core/slides/spark-nlp.html#two-flavors-of-nlp-libraries",
    "title": "Spark NLP",
    "section": "Two flavors of NLP libraries",
    "text": "Two flavors of NLP libraries\n\nFunctionality Libraries nltk.org\nAnnotation Libraries spaCy’s site"
  },
  {
    "objectID": "core/slides/spark-nlp.html#spacy-and-spark",
    "href": "core/slides/spark-nlp.html#spacy-and-spark",
    "title": "Spark NLP",
    "section": "spaCy and Spark?",
    "text": "spaCy and Spark?\nA databricks notebook discusses possible interactions between spaCy and Spark on a use case:\n\n\nGet the tweets (the texts) into a Spark dataframe using spark.sql()\n\nConvert the Spark dataframe to a numpy array\nStream all tweets in batches using nlp.pipe()\n\nGo through the processed tweets and take copy everything we need in a large array object\nConvert back the large array object into a Spark dataframe\nSave the dataframe as table, so we can query the whole thing withh SQL again\n\n\n\n\n\n\n\n\n\nNo hint at parallelizing spaCy’s annotation process"
  },
  {
    "objectID": "core/slides/spark-nlp.html#spacy-v2-current-v3.7",
    "href": "core/slides/spark-nlp.html#spacy-v2-current-v3.7",
    "title": "Spark NLP",
    "section": "spaCy v2 (current v3.7)",
    "text": "spaCy v2 (current v3.7)\n\nspaCy v2 now fully supports the Pickle protocol, making it easy to use spaCy with Apache Spark.\n\nspaCy v2 documentation"
  },
  {
    "objectID": "core/slides/spark-nlp.html#imports-sparknlp-and-others",
    "href": "core/slides/spark-nlp.html#imports-sparknlp-and-others",
    "title": "Spark NLP",
    "section": "Imports sparknlp and others",
    "text": "Imports sparknlp and others\n\n# Import Spark NLP\nfrom sparknlp.base import *\nfrom sparknlp.annotator import *\nfrom sparknlp.pretrained import PretrainedPipeline\nimport sparknlp"
  },
  {
    "objectID": "core/slides/spark-nlp.html#initiate-spark-session",
    "href": "core/slides/spark-nlp.html#initiate-spark-session",
    "title": "Spark NLP",
    "section": "Initiate Spark session",
    "text": "Initiate Spark session\nAssuming standalone mode on a laptop. master runs on localhost\n\nspark = SparkSession.builder \\\n            .appName(\"Spark NLP\") \\\n#            .master(\"spark://localhost:7077\") \\\n            .config(\"spark.driver.memory\", \"16G\") \\\n            .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") \\\n            .config(\"spark.kryoserializer.buffer.max\", \"2000M\") \\\n            .config(\"spark.driver.maxResultSize\", \"0\") \\\n            .config(\"spark.jars.packages\", \"com.johnsnowlabs.nlp:spark-nlp_2.12:5.2.3\") \\\n            .getOrCreate()\n\n\n\n\n\n\nsparknlp.version()\n\n\n\n\n…\n\nspark"
  },
  {
    "objectID": "core/slides/spark-nlp.html#toy-big-data",
    "href": "core/slides/spark-nlp.html#toy-big-data",
    "title": "Spark NLP",
    "section": "Toy (big) data",
    "text": "Toy (big) data\n\nfr_articles = [\n  (\"Le dimanche 11 juillet 2021, Chiellini a utilisé le mot Kiricocho lorsque Saka s'est approché du ballon pour le penalty.\",),\n  (\"La prochaine Coupe du monde aura lieu en novembre 2022.\",),\n  (\"À Noël 800, Charlemagne se fit couronner empereur à Rome.\",),\n  (\"Le Marathon de Paris a lieu le premier dimanche d'avril 2024\",)\n]\n\n\n\n\n\n\narticles_cols = [\"text\"]\n\ndf = spark.createDataFrame(\n  data=fr_articles, \n  schema=articles_cols)\n\ndf.printSchema()"
  },
  {
    "objectID": "core/slides/spark-nlp.html#pipelines",
    "href": "core/slides/spark-nlp.html#pipelines",
    "title": "Spark NLP",
    "section": "Pipelines",
    "text": "Pipelines\n\ndocument_assembler = DocumentAssembler() \\\n            .setInputCol(\"text\") \\\n            .setOutputCol(\"document\")\n\n\n\n\n\n\n\nColumn document contains the ‘text’ to be annotated as well as some possible metadata.\nStarting point of any annotation process\nSpark NLP relies on Saprk SQL for storing, moving, data.\n\n\n\n\n\ndate_matcher = DateMatcher() \\\n            .setInputCols(['document']) \\\n            .setOutputCol(\"date\") \\\n            .setOutputFormat(\"MM/dd/yyyy\") \\\n            .setSourceLanguage(\"fr\")\n\n\n\n\n\n\n\n\nSpark NLP adopts an original way of storing annotations\nSpark NLP creates columns for annotations\nSpark NLP stores annotation in Spark dataframes\nAnnotators are\n\nTranformers\nEstimators\nModels"
  },
  {
    "objectID": "core/slides/spark-nlp.html#transformationaction",
    "href": "core/slides/spark-nlp.html#transformationaction",
    "title": "Spark NLP",
    "section": "Transformation/Action",
    "text": "Transformation/Action\n\nassembled = ( \n  document_assembler.transform(df)\n)\n\n\n\n\n\n(\n date_matcher\n  .transform(assembled)\n  .select('date')\n  .show(10, False)\n)"
  },
  {
    "objectID": "core/slides/spark-nlp.html#more",
    "href": "core/slides/spark-nlp.html#more",
    "title": "Spark NLP",
    "section": "More",
    "text": "More\n\nfr_articles.append((\"Nous nous sommes rencontrés le 13/05/2018 puis le 18/05/2020.\",))\n\nfr_articles.append((\"Nous nous sommes rencontrés il y a 2 jours et il m'a dit qu'il nous rendrait visite la semaine prochaine.\",))\n\n\n\n\n\ndf = spark.createDataFrame(\n  data=fr_articles, \n  schema=articles_cols)\n\ndf.printSchema()\ndf.show()\n\n\n\n\n\nassembled = ( \n  document_assembler.transform(df)\n)\n\n\n\n\n\n(\n date_matcher\n  .transform(assembled)\n  .select('date')\n  .show(10, False)\n)"
  },
  {
    "objectID": "core/slides/spark-nlp.html#another-annotator",
    "href": "core/slides/spark-nlp.html#another-annotator",
    "title": "Spark NLP",
    "section": "Another annotator",
    "text": "Another annotator\n\ndate_matcher_bis = MultiDateMatcher() \\\n            .setInputCols(['document']) \\\n            .setOutputCol(\"date\") \\\n            .setOutputFormat(\"MM/dd/yyyy\") \\\n            .setSourceLanguage(\"fr\")\n\n\n\n\n\n(\n  date_matcher_bis\n    .transform(assembled)\n    .select(\"date\")\n    .show(10, False)\n)"
  },
  {
    "objectID": "core/slides/spark-nlp.html#sql-lib-and-dataframes",
    "href": "core/slides/spark-nlp.html#sql-lib-and-dataframes",
    "title": "Spark NLP",
    "section": "SQL Lib and Dataframes",
    "text": "SQL Lib and Dataframes"
  },
  {
    "objectID": "core/slides/spark-nlp.html#ml-lib-transformers-and-estimators",
    "href": "core/slides/spark-nlp.html#ml-lib-transformers-and-estimators",
    "title": "Spark NLP",
    "section": "ML Lib, Transformers and Estimators",
    "text": "ML Lib, Transformers and Estimators"
  },
  {
    "objectID": "core/slides/spark-nlp.html#getting-a-corpus-etl",
    "href": "core/slides/spark-nlp.html#getting-a-corpus-etl",
    "title": "Spark NLP",
    "section": "Getting a corpus : ETL",
    "text": "Getting a corpus : ETL\n\npattern = 'URL: http://www.nytimes.com/(?P&lt;zedate&gt;[0-9]{4}/[0-9]{2}/[0-9]{2})/.*'\ntitle = 'URL: http://www.nytimes.com/[0-9]{4}/[0-9]{2}/[0-9]{2}/(.*)'\nreg_date = re.compile(pattern)\nreg_title = re.compile(title)\n\n\n\n\n\nnypath = Path('../data/nytimes_news_articles.txt')\ncorpus_list = list()\n\n\n\n\n\nwith open(nypath, encoding='UTF-8')  as fd:\n    doc, document = None, None\n    while l := fd.readline():        \n        if m := reg_date.match(l):\n            if doc is not None:\n                corpus_list.append((*document, doc))\n                doc, document = None, None\n            ymd = date(*[int(n) for n in m.groups()[0].split('/')])\n            title = (\n                reg_title.match(l)\n                  .groups()[0]\n                  .split('/')\n            )\n            document =  (ymd, title[-1], '/'.join(title[:-1]))\n            doc = ''\n        else: doc = doc + l\n    else:\n        if doc is not None:\n            corpus_list.append((*document, doc))\n\n\n\n\n\ndf_texts = spark.createDataFrame(corpus_list,\n                      schema= StructType([\n    StructField('date', DateType(), False),\n    StructField('title', StringType(), False),\n    StructField('topic', StringType(), False),\n    StructField('text', StringType(), True)\n]))\n\n\n\n\n\ndf_texts.printSchema()\ndf_texts.count()"
  },
  {
    "objectID": "core/slides/spark-nlp.html#saving",
    "href": "core/slides/spark-nlp.html#saving",
    "title": "Spark NLP",
    "section": "Saving",
    "text": "Saving\nLocally\n\ndf_texts.write.parquet('../data/ny_corpus_pq')\n\n\n\n\n\nspam = spark.read.parquet('../data/ny_corpus_pq')\n\nspam.printSchema()\n\n\n\n\n\nspam.rdd.getNumPartitions()"
  },
  {
    "objectID": "core/slides/spark-nlp.html#section",
    "href": "core/slides/spark-nlp.html#section",
    "title": "Spark NLP",
    "section": "",
    "text": "corpus_assembled = ( \n  document_assembler.transform(df_texts)\n)\n\n\n\n\n\ncorpus_assembled.printSchema()\n\n\n\n\n\n(\n  date_matcher_bis\n    .transform(corpus_assembled)\n    .select(\"title\", \"date\")\n    .show(10, False)\n)\n\n\n\n\n\n\n\n\n\n\nWarning\n\n\nExtracted dates should be taken with a grain of salt"
  },
  {
    "objectID": "core/slides/spark-nlp.html#public-pipelines",
    "href": "core/slides/spark-nlp.html#public-pipelines",
    "title": "Spark NLP",
    "section": "Public pipelines",
    "text": "Public pipelines\n\nfrom sparknlp.pretrained import PretrainedPipeline\nexplain_document_pipeline = PretrainedPipeline(\"explain_document_ml\")"
  },
  {
    "objectID": "core/slides/spark-nlp.html#chaining-annotators",
    "href": "core/slides/spark-nlp.html#chaining-annotators",
    "title": "Spark NLP",
    "section": "Chaining annotators",
    "text": "Chaining annotators\n\nsentenceDetector = SentenceDetector() \\\n    .setInputCols([\"document\"]) \\\n    .setOutputCol(\"sentence\")\nregexTokenizer = Tokenizer() \\\n    .setInputCols([\"sentence\"]) \\\n    .setOutputCol(\"token\")\n\n\n\n\n\nfinisher = Finisher() \\\n    .setInputCols([\"token\"]) \\\n    .setIncludeMetadata(True)\n\n\n\n\n\npipeline = Pipeline().setStages([\n    document_assembler,\n    sentenceDetector,\n    regexTokenizer,\n    finisher\n])"
  },
  {
    "objectID": "core/slides/spark-nlp.html#fitting-and-transforming",
    "href": "core/slides/spark-nlp.html#fitting-and-transforming",
    "title": "Spark NLP",
    "section": "Fitting and transforming",
    "text": "Fitting and transforming\n\nspam = ( \n  pipeline.fit(df_texts)\n    .transform(df_texts)\n    .select(\"finished_token\")\n    .collect()\n)"
  },
  {
    "objectID": "core/slides/spark-nlp.html#a-customized-pipeline",
    "href": "core/slides/spark-nlp.html#a-customized-pipeline",
    "title": "Spark NLP",
    "section": "A customized pipeline",
    "text": "A customized pipeline\n\nstemmer = (\n  Stemmer()\n    .setInputCols(['token'])\n    .setOutputCol('stem')\n)\n\n\n\n\n\nlemmatizer = (\n  LemmatizerModel.pretrained()\n    .setInputCols(['token'])\n    .setOutputCol('lemma')\n)\n\n\n\n\n\n\n\n\n\n\nWarning\n\n\nlemma_antbnc download started this may take some time.\nApproximate size to download 907.6 KB\n[ / ]lemma_antbnc download started this may take some time.\nApproximate size to download 907.6 KB\n[ / ]Download done! Loading the resource.\n[ — ]\n\n[OK!]\n\n\n\n\nposTagger = PerceptronModel.pretrained() \\\n    .setInputCols([\"document\", \"token\"]) \\\n    .setOutputCol(\"pos\")\n\n\n\n\n\n\n\n\n\n\nWarning\n\n\npos_anc download started this may take some time.\nApproximate size to download 3.9 MB\n[ — ]pos_anc download started this may take some time.\nApproximate size to download 3.9 MB\n[ \\ ]Download done! Loading the resource.\n[Stage 34:===========================================&gt;              (3 + 1) / 4]\n[ | ]\n                                                                \n[OK!]\n\n\n\n\nfinisher = (\n  Finisher()\n    .setInputCols([\n      'token', \n#      'stem', \n#      'lemma', \n      'pos'])\n    .setIncludeMetadata(False)\n    .setOutputAsArray(True)\n)"
  },
  {
    "objectID": "core/slides/spark-nlp.html#section-1",
    "href": "core/slides/spark-nlp.html#section-1",
    "title": "Spark NLP",
    "section": "",
    "text": "pipeline = (\n  Pipeline()\n    .setStages([\n      document_assembler,\n      sentenceDetector,\n      regexTokenizer,\n      posTagger, \n      finisher\n    ])\n)\n\n\n\n\n\nspam = ( \n  pipeline.fit(df_texts)\n    .transform(df_texts)\n    .selectExpr(\"*\")\n    .collect()\n)"
  },
  {
    "objectID": "core/slides/spark-nlp.html#topic-modelling",
    "href": "core/slides/spark-nlp.html#topic-modelling",
    "title": "Spark NLP",
    "section": "Topic modelling",
    "text": "Topic modelling"
  },
  {
    "objectID": "core/slides/spark-nlp.html#tf-idf",
    "href": "core/slides/spark-nlp.html#tf-idf",
    "title": "Spark NLP",
    "section": "TF-IDF",
    "text": "TF-IDF"
  },
  {
    "objectID": "core/slides/spark-nlp.html#latent-dirichlet-allocation",
    "href": "core/slides/spark-nlp.html#latent-dirichlet-allocation",
    "title": "Spark NLP",
    "section": "Latent Dirichlet Allocation",
    "text": "Latent Dirichlet Allocation"
  },
  {
    "objectID": "core/slides/spark-nlp.html#execution-modes",
    "href": "core/slides/spark-nlp.html#execution-modes",
    "title": "Spark NLP",
    "section": "Execution modes",
    "text": "Execution modes\n\nstandalone\nclient\ncluster"
  },
  {
    "objectID": "core/slides/apache-arrow.html#bird-eye-perspective",
    "href": "core/slides/apache-arrow.html#bird-eye-perspective",
    "title": "Appache Arrow",
    "section": "Bird-eye perspective",
    "text": "Bird-eye perspective"
  },
  {
    "objectID": "core/slides/apache-arrow.html#why-arrow",
    "href": "core/slides/apache-arrow.html#why-arrow",
    "title": "Appache Arrow",
    "section": "Why Arrow?",
    "text": "Why Arrow?"
  },
  {
    "objectID": "core/slides/apache-arrow.html#hadoop-distributed-file-system",
    "href": "core/slides/apache-arrow.html#hadoop-distributed-file-system",
    "title": "Appache Arrow",
    "section": "Hadoop Distributed File System",
    "text": "Hadoop Distributed File System"
  },
  {
    "objectID": "core/slides/apache-arrow.html#section-1",
    "href": "core/slides/apache-arrow.html#section-1",
    "title": "Appache Arrow",
    "section": "…",
    "text": "…"
  },
  {
    "objectID": "core/slides/apache-arrow.html#section-2",
    "href": "core/slides/apache-arrow.html#section-2",
    "title": "Appache Arrow",
    "section": "…",
    "text": "…"
  },
  {
    "objectID": "core/slides/slides06_arrow.html#what",
    "href": "core/slides/slides06_arrow.html#what",
    "title": "Apache Arrow",
    "section": "What ?",
    "text": "What ?\nApache Arrow project was announced in 2016\n\nApache Arrow defines a language-independent columnar memory format for flat and hierarchical data, organized for efficient analytic operations on modern hardware like CPUs and GPUs. The Arrow memory format also supports zero-copy reads for lightning-fast data access without serialization overhead.\n\nSee Arrow website\nApache Arrow complements columnar formats for files (on disk) like Parquet and ORC (also sponsored by Apache)"
  },
  {
    "objectID": "core/slides/slides06_arrow.html#in-memory-analytics",
    "href": "core/slides/slides06_arrow.html#in-memory-analytics",
    "title": "Apache Arrow",
    "section": "In Memory Analytics",
    "text": "In Memory Analytics"
  },
  {
    "objectID": "core/slides/slides06_arrow.html#why",
    "href": "core/slides/slides06_arrow.html#why",
    "title": "Apache Arrow",
    "section": "Why ?",
    "text": "Why ?\nThe columnar format has some key features:\n\nData adjacency for sequential access (scans)\n\\(O(1)\\) (constant-time) random access\nSIMD and vectorization-friendly\nRelocatable without pointer swizzling, allowing for true zero-copy access in shared memory\n\n\nThe Arrow columnar format provides analytical performance and data locality guarantees in exchange for comparatively more expensive mutation operations\n\n\nSee also Harrisson for columnar databases\n\nColumnar compression\nColumnar write penalty\n\n\nThe column stores perform poorly during single-row modifications"
  },
  {
    "objectID": "core/slides/slides06_arrow.html#specification",
    "href": "core/slides/slides06_arrow.html#specification",
    "title": "Apache Arrow",
    "section": "Specification",
    "text": "Specification\nThe Arrow Columnar Format includes\n\na language-agnostic in-memory data structure specification\nmetadata serialization\na protocol for serialization and generic data transport"
  },
  {
    "objectID": "core/slides/slides06_arrow.html#lexikon",
    "href": "core/slides/slides06_arrow.html#lexikon",
    "title": "Apache Arrow",
    "section": "Lexikon",
    "text": "Lexikon"
  },
  {
    "objectID": "core/slides/slides06_arrow.html#physical-layout",
    "href": "core/slides/slides06_arrow.html#physical-layout",
    "title": "Apache Arrow",
    "section": "Physical layout",
    "text": "Physical layout"
  },
  {
    "objectID": "core/slides/slides06_arrow.html#padding",
    "href": "core/slides/slides06_arrow.html#padding",
    "title": "Apache Arrow",
    "section": "Padding",
    "text": "Padding"
  },
  {
    "objectID": "core/slides/slides06_arrow.html#null-values",
    "href": "core/slides/slides06_arrow.html#null-values",
    "title": "Apache Arrow",
    "section": "Null values",
    "text": "Null values\n\nAll array types, with the exception of union types (more on these later), utilize a dedicated memory buffer, known as the validity (or “null”) bitmap, to encode the nullness or non-nullness of each value slot.\nThe validity bitmap must be large enough to have at least 1 bit for each array slot."
  },
  {
    "objectID": "core/slides/slides06_arrow.html#metadata-serialization",
    "href": "core/slides/slides06_arrow.html#metadata-serialization",
    "title": "Apache Arrow",
    "section": "Metadata serialization",
    "text": "Metadata serialization\nUses  Flatbuffers"
  },
  {
    "objectID": "core/slides/slides06_arrow.html#importing-pyarrow",
    "href": "core/slides/slides06_arrow.html#importing-pyarrow",
    "title": "Apache Arrow",
    "section": "Importing pyarrow",
    "text": "Importing pyarrow\n\nimport pyarrow as pa"
  },
  {
    "objectID": "core/slides/slides06_arrow.html#arrow-arrays",
    "href": "core/slides/slides06_arrow.html#arrow-arrays",
    "title": "Apache Arrow",
    "section": "Arrow arrays",
    "text": "Arrow arrays"
  },
  {
    "objectID": "core/slides/slides06_arrow.html#arrow-tables",
    "href": "core/slides/slides06_arrow.html#arrow-tables",
    "title": "Apache Arrow",
    "section": "Arrow tables",
    "text": "Arrow tables"
  },
  {
    "objectID": "core/slides/slides06_arrow.html#arrow-to-and-from-parquet",
    "href": "core/slides/slides06_arrow.html#arrow-to-and-from-parquet",
    "title": "Apache Arrow",
    "section": "Arrow to and from Parquet",
    "text": "Arrow to and from Parquet"
  },
  {
    "objectID": "core/slides/slides06_arrow.html#arrow-to-and-from-pandas",
    "href": "core/slides/slides06_arrow.html#arrow-to-and-from-pandas",
    "title": "Apache Arrow",
    "section": "Arrow to and from Pandas",
    "text": "Arrow to and from Pandas\nArrow, Numpy, Pandas offer sequential structures (1-dimensional Numpy arrays, Pandas Series, …) and tables. Arrow offers tools to convert from Arrow format to Pandas and back\n\nThe equivalent to a pandas DataFrame in Arrow is a Table. Both consist of a set of named columns of equal length.\n\n\n\nWhile pandas only supports flat columns, the Table also provides nested columns, thus it can represent more data than a DataFrame, so a full conversion is not always possible.\n\n\nimport pyarrow as pa\nimport pandas as pd\n\ndf = pd.DataFrame({\"a\": [1, 2, 3]})\n# Convert from pandas to Arrow\ntable = pa.Table.from_pandas(df)\n# Convert back to pandas\ndf_new = table.to_pandas()\n# Infer Arrow schema from pandas\nschema = pa.Schema.from_pandas(df)\n\n\n\n\n\n\n\n\nArrow doc\nPandas doc"
  },
  {
    "objectID": "core/slides/slides06_arrow.html#handling-pandas-indexes",
    "href": "core/slides/slides06_arrow.html#handling-pandas-indexes",
    "title": "Apache Arrow",
    "section": "Handling pandas Indexes",
    "text": "Handling pandas Indexes\n\nMethods like pyarrow.Table.from_pandas() have a preserve_index option which defines how to preserve (store) or not to preserve (to not store) the data in the index member of the corresponding pandas object. This data is tracked using schema-level metadata in the internal arrow::Schema object.\n\nThe default of preserve_index is None, which behaves as follows:\n\nRangeIndex is stored as metadata-only, not requiring any extra storage.\nOther index types are stored as one or more physical data columns in the resulting Table\n\nTo not store the index at all pass preserve_index=False. Since storing a RangeIndex can cause issues in some limited scenarios (such as storing multiple DataFrame objects in a Parquet file), to force all index data to be serialized in the resulting table, pass preserve_index=True."
  },
  {
    "objectID": "core/slides/slides06_arrow.html#arrow-pandas-and-categorical-datatypes",
    "href": "core/slides/slides06_arrow.html#arrow-pandas-and-categorical-datatypes",
    "title": "Apache Arrow",
    "section": "Arrow, Pandas, and categorical datatypes",
    "text": "Arrow, Pandas, and categorical datatypes\n\nPandas categorical columns are converted to Arrow dictionary arrays, a special array type optimized to handle repeated and limited number of possible values."
  },
  {
    "objectID": "core/slides/slides06_arrow.html#arrow-database-connectivity",
    "href": "core/slides/slides06_arrow.html#arrow-database-connectivity",
    "title": "Apache Arrow",
    "section": "Arrow database connectivity",
    "text": "Arrow database connectivity"
  },
  {
    "objectID": "core/slides/slides06_arrow.html#from-the-apache-arrow-blog",
    "href": "core/slides/slides06_arrow.html#from-the-apache-arrow-blog",
    "title": "Apache Arrow",
    "section": "From the Apache Arrow blog",
    "text": "From the Apache Arrow blog\n\nThe Apache Arrow PMC is pleased to announce the donation of the Comet project, a native Spark SQL Accelerator built on Apache Arrow DataFusion.\n\n\nComet is an Apache Spark plugin that uses Apache Arrow DataFusion to accelerate Spark workloads. It is designed as a drop-in replacement for Spark’s JVM based SQL execution engine and offers significant performance improvements for some workloads.\n\n\nWith Comet, users interact with the same Spark ecosystem, tools and APIs such as Spark SQL. Queries still run through Spark’s query optimizer and planner. However, the execution is delegated to Comet, which is significantly faster and more resource efficient than a JVM based implementation.\n\n\nThere exist several Spark Accelerators like RAPIDS (for GPUs) Photon (from Databricks). Comet is one of them"
  },
  {
    "objectID": "core/slides/slides06_arrow.html#arrow-to-and-from-spark",
    "href": "core/slides/slides06_arrow.html#arrow-to-and-from-spark",
    "title": "Apache Arrow",
    "section": "Arrow to and from Spark",
    "text": "Arrow to and from Spark"
  },
  {
    "objectID": "core/slides/slides06_arrow.html#pyspark-and-comet",
    "href": "core/slides/slides06_arrow.html#pyspark-and-comet",
    "title": "Apache Arrow",
    "section": "Pyspark and Comet",
    "text": "Pyspark and Comet\nRead the Docs"
  }
]