[
  {
    "objectID": "weeks/week-9.html",
    "href": "weeks/week-9.html",
    "title": "Week 9",
    "section": "",
    "text": "Important\n\n\n\n\nFriday 21 March 2025 Sophie Germain 1005 13h30-15h30\n Calendar"
  },
  {
    "objectID": "weeks/week-9.html#lecture-slides",
    "href": "weeks/week-9.html#lecture-slides",
    "title": "Week 9",
    "section": "Lecture : slides",
    "text": "Lecture : slides\n\n Deeper dive\n\nWe may come back to several parts of\n\n Json format\n Spark high level APIs: SQL\n Spark low level APIs: RDD\n Python Data Science Stack\n Dask\n File formats"
  },
  {
    "objectID": "weeks/week-9.html#notebooks",
    "href": "weeks/week-9.html#notebooks",
    "title": "Week 9",
    "section": "Notebooks",
    "text": "Notebooks\nIf time allows, we shall go back to\n\n Jupyter notebook VII : JSON\n html: JSON\n\nYou shall have gone through (on your own)\n\n Jupyter notebook VII : JSON\n html: JSON\n\n Jupyter notebook VI : Spark SQL\n Jupyter notebook V : Spark RDD\n html: Spark RDD\n\n Jupyter notebook I : tour of Python\n html: tour of Python\n\n Jupyter notebook II : tour of numpy\n html: tour of numpy\n\n Jupyter notebook III: tour of pandas\n html: tour of Pandas"
  },
  {
    "objectID": "weeks/week-9.html#references",
    "href": "weeks/week-9.html#references",
    "title": "Week 9",
    "section": "References",
    "text": "References\n\n\n\nSpark Definitive Guide\nSpark project\nSpark documentation\nData pipelines\nNext Generation Databases NoSQLand Big Data, Guy Harrison\nMastering Spark on R\n\n\n\nParquet\n‘Arrow’\nDask Docs\nDask Examples\nDask Code\nDask Blog\nPandas cheatsheet"
  },
  {
    "objectID": "weeks/week-9.html#logistics",
    "href": "weeks/week-9.html#logistics",
    "title": "Week 9",
    "section": "Logistics",
    "text": "Logistics\n pyspark\n To work the jupyter notebooks, install python 3, and modules related to jupyter: jupyter-cache, jupyter_client, jupyter_core, jupyterlab_widgets (this induces the installation of dependencies).\nhttps://jupyter.org\nDownload the jupyter notebooks from notebooks listings.\n If you do not already have an ENT account, follow instructions on Moodle to get one. You shall need this account to connect to PostGres cluster.\n\n\nBack to Agenda ⏎"
  },
  {
    "objectID": "weeks/week-7.html",
    "href": "weeks/week-7.html",
    "title": "Weeks 7",
    "section": "",
    "text": "Important\n\n\n\n1 Session\n\nFriday 7 March 2025 Sophie Germain 1005 13h30-15h30\n Calendar"
  },
  {
    "objectID": "weeks/week-7.html#lecture-slides",
    "href": "weeks/week-7.html#lecture-slides",
    "title": "Weeks 7",
    "section": "Lecture : slides",
    "text": "Lecture : slides\n\n Json format\n\nWe may come back to several parts of\n\n Spark high level APIs: SQL\n Spark low level APIs: RDD\n Python Data Science Stack\n Dask"
  },
  {
    "objectID": "weeks/week-7.html#notebooks",
    "href": "weeks/week-7.html#notebooks",
    "title": "Weeks 7",
    "section": "Notebooks",
    "text": "Notebooks\nWe shall spend most of the lectures on\n\n Jupyter notebook VII : JSON\n html: JSON\n\nYou shall have gone through (on your own)\n\n Jupyter notebook VI : Spark SQL\n Jupyter notebook V : Spark RDD\n html: Spark RDD\n\n Jupyter notebook I : tour of Python\n html: tour of Python\n\n Jupyter notebook II : tour of numpy\n html: tour of numpy\n\n Jupyter notebook III: tour of pandas\n html: tour of Pandas"
  },
  {
    "objectID": "weeks/week-7.html#references",
    "href": "weeks/week-7.html#references",
    "title": "Weeks 7",
    "section": "References",
    "text": "References\n\n\n\nSpark Definitive Guide\nSpark project\nSpark documentation\nData pipelines\nNext Generation Databases NoSQLand Big Data, Guy Harrison\nMastering Spark on R\n\n\n\nParquet\n‘Arrow’\nDask Docs\nDask Examples\nDask Code\nDask Blog\nPandas cheatsheet"
  },
  {
    "objectID": "weeks/week-7.html#logistics",
    "href": "weeks/week-7.html#logistics",
    "title": "Weeks 7",
    "section": "Logistics",
    "text": "Logistics\n pyspark\n To work the jupyter notebooks, install python 3, and modules related to jupyter: jupyter-cache, jupyter_client, jupyter_core, jupyterlab_widgets (this induces the installation of dependencies).\nhttps://jupyter.org\nDownload the jupyter notebooks from notebooks listings.\n If you do not already have an ENT account, follow instructions on Moodle to get one. You shall need this account to connect to PostGres cluster.\n\n\nBack to Agenda ⏎"
  },
  {
    "objectID": "weeks/week-4.html",
    "href": "weeks/week-4.html",
    "title": "Weeks 4-5",
    "section": "",
    "text": "Important\n\n\n\n\n 1 session during Week 4/0 session during Week 5\nFriday 4 February 2025 Sophie Germain 0014 15h45-17h45\n Calendar"
  },
  {
    "objectID": "weeks/week-4.html#lecture-slides",
    "href": "weeks/week-4.html#lecture-slides",
    "title": "Weeks 4-5",
    "section": "Lecture : slides",
    "text": "Lecture : slides\n\n Spark low level APIs: RDD  We may come back to several parts of\n Python Data Science Stack\n Dask"
  },
  {
    "objectID": "weeks/week-4.html#notebooks",
    "href": "weeks/week-4.html#notebooks",
    "title": "Weeks 4-5",
    "section": "Notebooks",
    "text": "Notebooks\nWe shall spend most of the lectures on\n\n Jupyter notebook V : Spark RDD\n html: Spark RDD\n\n\nand possibly compare with:\nDask Tutorial\nYou shall have gone through (on your own)\n\n Jupyter notebook I : tour of Python\n html: tour of Python\n\n Jupyter notebook II : tour of numpy\n html: tour of numpy\n\n Jupyter notebook III: tour of pandas\n html: tour of Pandas"
  },
  {
    "objectID": "weeks/week-4.html#references",
    "href": "weeks/week-4.html#references",
    "title": "Weeks 4-5",
    "section": "References",
    "text": "References\n\n\n\nSpark Definitive Guide\nSpark\nSpark\nData pipelines\nNext Generation Databases NoSQLand Big Data, Guy Harrison\nMastering Spark on R\n\n\n\nParquet\n‘Arrow’\nDask Docs\nDask Examples\nDask Code\nDask Blog\nPandas cheatsheet"
  },
  {
    "objectID": "weeks/week-4.html#logistics",
    "href": "weeks/week-4.html#logistics",
    "title": "Weeks 4-5",
    "section": "Logistics",
    "text": "Logistics\n pyspark\n To work the jupyter notebooks, install python 3, and modules related to jupyter: jupyter-cache, jupyter_client, jupyter_core, jupyterlab_widgets (this induces the installation of dependencies).\nhttps://jupyter.org\nDownload the jupyter notebooks from notebooks listings.\n If you do not already have an ENT account, follow instructions on Moodle to get one. You shall need this account to connect to PostGres cluster.\n\n\nBack to Agenda ⏎"
  },
  {
    "objectID": "weeks/week-2.html",
    "href": "weeks/week-2.html",
    "title": "Week 2",
    "section": "",
    "text": "Important\n\n\n\n\nRoom: Sophie Germain 0014\nFriday 24 January 2025 15h45-17h45\n Calendar"
  },
  {
    "objectID": "weeks/week-2.html#lecture-slides",
    "href": "weeks/week-2.html#lecture-slides",
    "title": "Week 2",
    "section": "Lecture : slides",
    "text": "Lecture : slides\nWe shall briefly come back to several parts of\n\n Python Data Science Stack\n\nData processing\nVisualization"
  },
  {
    "objectID": "weeks/week-2.html#notebooks",
    "href": "weeks/week-2.html#notebooks",
    "title": "Week 2",
    "section": "Notebooks",
    "text": "Notebooks\nYou shall have gone through (on your own)\n\n Jupyter notebook I : tour of Python\n html: tour of Python\n\nYou shall have gone through\n\n Jupyter notebook II : tour of numpy\n html: tour of numpy\n\nup to Section Sparse Matrices (not included)\nWe shall spend most of the lecture on\n\n Jupyter notebook III : tour of pandas\n html: tour of Pandas"
  },
  {
    "objectID": "weeks/week-2.html#references",
    "href": "weeks/week-2.html#references",
    "title": "Week 2",
    "section": "References",
    "text": "References\nYou can watch the Introduction to NumPy conference presented at EuroSciPy 2023\n\nThe Pandas book by Wes McKinney\nPandas exercises on Kaggle\n\n\nContenders to Pandas are gaining attention: Polars"
  },
  {
    "objectID": "weeks/week-2.html#logistics",
    "href": "weeks/week-2.html#logistics",
    "title": "Week 2",
    "section": "Logistics",
    "text": "Logistics\n To work the jupyter notebooks, install python 3, and modules related to jupyter: jupyter-cache, jupyter_client, jupyter_core, jupyterlab_widgets (this induces the installation of dependencies).\nhttps://jupyter.org\nDownload the jupyter notebooks from notebooks listings.\n If you do not already have an ENT account, follow instructions on Moodle to get one. You shall need this account to connect to PostGres cluster.\n\n\nBack to Agenda ⏎"
  },
  {
    "objectID": "weeks/week-10.html",
    "href": "weeks/week-10.html",
    "title": "Week 10",
    "section": "",
    "text": "Important\n\n\n\n\nFriday 28 March Sophie Germain 1005 13h30-15h30\n Calendar"
  },
  {
    "objectID": "weeks/week-10.html#lecture-slides",
    "href": "weeks/week-10.html#lecture-slides",
    "title": "Week 10",
    "section": "Lecture : slides",
    "text": "Lecture : slides\n\n Spark tips\n Deeper dive\n\nWe may come back to several parts of\n\n Json format\n Spark high level APIs: SQL\n Spark low level APIs: RDD\n Python Data Science Stack\n Dask\n File formats"
  },
  {
    "objectID": "weeks/week-10.html#notebooks",
    "href": "weeks/week-10.html#notebooks",
    "title": "Week 10",
    "section": "Notebooks",
    "text": "Notebooks\nIf time allows, we shall go back to\n\n Jupyter notebook VII : JSON\n html: JSON\n\nYou shall have gone through (on your own)\n\n Jupyter notebook VII : JSON\n html: JSON\n\n Jupyter notebook VI : Spark SQL\n Jupyter notebook V : Spark RDD\n html: Spark RDD\n\n Jupyter notebook I : tour of Python\n html: tour of Python\n\n Jupyter notebook II : tour of numpy\n html: tour of numpy\n\n Jupyter notebook III: tour of pandas\n html: tour of Pandas"
  },
  {
    "objectID": "weeks/week-10.html#references",
    "href": "weeks/week-10.html#references",
    "title": "Week 10",
    "section": "References",
    "text": "References\n\n\n\nSpark Definitive Guide\nSpark project\nSpark documentation\nData pipelines\nNext Generation Databases NoSQLand Big Data, Guy Harrison\nMastering Spark on R\n\n\n\nParquet\n‘Arrow’\nDask Docs\nDask Examples\nDask Code\nDask Blog\nPandas cheatsheet"
  },
  {
    "objectID": "weeks/week-10.html#logistics",
    "href": "weeks/week-10.html#logistics",
    "title": "Week 10",
    "section": "Logistics",
    "text": "Logistics\n pyspark\n To work the jupyter notebooks, install python 3, and modules related to jupyter: jupyter-cache, jupyter_client, jupyter_core, jupyterlab_widgets (this induces the installation of dependencies).\nhttps://jupyter.org\nDownload the jupyter notebooks from notebooks listings.\n If you do not already have an ENT account, follow instructions on Moodle to get one. You shall need this account to connect to PostGres cluster.\n\n\nBack to Agenda ⏎"
  },
  {
    "objectID": "weeks-listings.html",
    "href": "weeks-listings.html",
    "title": "Journal",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Tags\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\nDate\n\n\nTitle\n\n\nTags\n\n\n\n\n\n\nJan 17, 2025\n\n\nWeek 1\n\n\nBig data, Python data stack\n\n\n\n\nJan 24, 2025\n\n\nWeek 2\n\n\nPandas\n\n\n\n\nJan 31, 2025\n\n\nWeek 3\n\n\nDask\n\n\n\n\nFeb 7, 2025\n\n\nweek 4\n\n\nSpark RDD\n\n\n\n\nFeb 21, 2025\n\n\nWeek 6\n\n\nSpark SQL\n\n\n\n\nMar 7, 2025\n\n\nWeek 7\n\n\nJson, serialization\n\n\n\n\nMar 14, 2025\n\n\nWeek 8\n\n\nFile formats, Parquet, Avro, ORC\n\n\n\n\nMar 21, 2025\n\n\nWeek 9\n\n\nSpark, Shuffle, Joins, Dependencies, Lineage graph, Catalyst\n\n\n\n\nMar 28, 2025\n\n\nWeek 10\n\n\nSpark, Shuffle, Joins, Dependencies, Lineage graph, Catalyst\n\n\n\n\nApr 3, 2025\n\n\nWeek 11\n\n\nSpark, Shuffle, Joins, Dependencies, Lineage graph, Catalyst\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "Journal"
    ]
  },
  {
    "objectID": "notebooks-listings.html",
    "href": "notebooks-listings.html",
    "title": "Notebooks",
    "section": "",
    "text": "Slides provide the framework for the course. You can use them to familiarise yourself with the material for the next lesson, and/or to review previous lessons.\nIcon  points to chalkboard arguments.\n\n\n\n\n\n\n\n\n\nMode d’emploi\n\n\n\nSlides use revealjs de . They are best viewed using a browser.\nKeyboard shortcut  takes you to the Help page.\n\n\n\n\n\n\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Description\n        \n         \n          Jupyter notebook\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\nDate\n\n\nTitle\n\n\nDescription\n\n\nJupyter notebook\n\n\n\n\n\n\nJan 17, 2025\n\n\nPython Stack for Data Science\n\n\nPython\n\n\n\n\n\n\n\nJan 17, 2025\n\n\nNumpy and Scipy\n\n\nPython, Numpy, Scipy, Plotly\n\n\n   \n\n\n\n\nJan 24, 2025\n\n\nTable wranglig with Pandas\n\n\nPython, Pandas\n\n\n   \n\n\n\n\nJan 31, 2025\n\n\nTable wranglig with Dask\n\n\nPython, Dask\n\n\n \n\n\n\n\nFeb 7, 2025\n\n\nSpark Resilient Distributed Datasets\n\n\nPySpark, Spark, RDD\n\n\n   \n\n\n\n\nFeb 20, 2025\n\n\nSpark SQL\n\n\nPySpark, Spark, SQL\n\n\n   \n\n\n\n\nFeb 21, 2025\n\n\nPandas on Spark and SparklyR\n\n\nPySpark, Spark, Pandas, R\n\n\n \n\n\n\n\nMar 7, 2025\n\n\nJSON format\n\n\nJSON, serialization\n\n\n   \n\n\n\n\nMar 14, 2025\n\n\nWeb data\n\n\nSpark, Window functions, Feature engineering\n\n\n   \n\n\n\n\nMar 21, 2025\n\n\nSpark again\n\n\nSpark, tips\n\n\n \n\n\n\n\n\nNo matching items",
    "crumbs": [
      "Notebooks"
    ]
  },
  {
    "objectID": "course-team.html",
    "href": "course-team.html",
    "title": "Course team",
    "section": "",
    "text": "Teachers 2024-25\n\nC. Sirangelo Professeur d’Informatique à l’Université Paris Cité/Institut de Recherche en Informatique Fondamentale.\nS. Boucheron Professeur de Mathématiques à l’Université Paris Cité/Laboratoire de Probabilités, Statistiques et Modélisatiion LPSM.\n\n\n\n\n\n\n Former contributors\n\nStéphane Gaiffas\nSothéa Has\nAmélie Gheerbrant\nVlady Ravelomanana",
    "crumbs": [
      "Information",
      "Team"
    ]
  },
  {
    "objectID": "cours-equipe.html",
    "href": "cours-equipe.html",
    "title": "Équipe enseignante",
    "section": "",
    "text": "S. Boucheron Professeur de Mathématiques à l’Université Paris Cité/Laboratoire de Probabilités, Statistiques et Modélisatiion LPSM."
  },
  {
    "objectID": "cours-equipe.html#cours",
    "href": "cours-equipe.html#cours",
    "title": "Équipe enseignante",
    "section": "",
    "text": "S. Boucheron Professeur de Mathématiques à l’Université Paris Cité/Laboratoire de Probabilités, Statistiques et Modélisatiion LPSM."
  },
  {
    "objectID": "cours-equipe.html#travaux-dirigés",
    "href": "cours-equipe.html#travaux-dirigés",
    "title": "Équipe enseignante",
    "section": " Travaux dirigés",
    "text": "Travaux dirigés\n\n\n\nNom\nHoraire\nSalle\n\n\n\n\nStéphane Boucheron\nVendredi 15h45 - 18h15\n2004/5 Sophie Germain\n\n\nCristina Sirangelo\nVendredi 15h45 - 18h15\n2006 Sophie Germain\n\n\nAmine Souiri\nJeudi 13h30 - 16h00\n2006 Sophie Germain\n\n\nSylvain Schmitz\n\n\n\n\nAmélie Gheerbrant\n\n\n\n\nAnatole Dahan"
  },
  {
    "objectID": "core/notebooks/xciti_pandas.html",
    "href": "core/notebooks/xciti_pandas.html",
    "title": "Imports",
    "section": "",
    "text": "Code\nimport glob\n\nimport os\nimport sys\nimport re \nimport shutils\nimport logging \n\nimport pandas as pd\nimport numpy as np\n\n\n\nimport datetime\n\n# from functools import reduce\nimport itertools\n\nimport zipfile\nfrom zipfile import ZipFile\nfrom tqdm import tqdm\n\nimport pyarrow as pa\nimport comet    as co\nimport pyarrow.parquet as pq\nimport pyarrow.dataset as ds\n\n\nimport dask\n\nos.environ['PYSPARK_PYTHON'] = sys.executable\nos.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable\n\n\n\n---------------------------------------------------------------------------\nModuleNotFoundError                       Traceback (most recent call last)\nCell In[1], line 6\n      4 import sys\n      5 import re \n----&gt; 6 import shutils\n      7 import logging \n      9 import pandas as pd\n\nModuleNotFoundError: No module named 'shutils'\nCode\nfrom dask.distributed import Client\n\nclient = Client(n_workers=20, threads_per_worker=2, memory_limit=\"2GB\")\nclient\n\n\n\n     \n    \n        Client\n        Client-2f14011f-108d-11f0-a00f-ac91a1bd3e89\n        \n\n\n\nConnection method: Cluster object\nCluster type: distributed.LocalCluster\n\n\nDashboard: http://127.0.0.1:8787/status\n\n\n\n\n\n\n        \n\n        \n            \n            Cluster Info\n            \n    \n    \n    \n        LocalCluster\n        8f43b2e9\n        \n\n\n\nDashboard: http://127.0.0.1:8787/status\nWorkers: 20\n\n\nTotal threads: 40\nTotal memory: 37.25 GiB\n\n\nStatus: running\nUsing processes: True\n\n\n\n\n\n        \n            \n                Scheduler Info\n            \n\n            \n    \n         \n        \n            Scheduler\n            Scheduler-366d5cb6-f3c8-46eb-96af-857815f764ba\n            \n\n\n\nComm: tcp://127.0.0.1:45971\nWorkers: 20\n\n\nDashboard: http://127.0.0.1:8787/status\nTotal threads: 40\n\n\nStarted: Just now\nTotal memory: 37.25 GiB\n\n\n\n\n        \n    \n\n    \n        \n            Workers\n        \n\n        \n        \n             \n            \n            \n                \n                    Worker: 0\n                \n                \n\n\n\nComm: tcp://127.0.0.1:36845\nTotal threads: 2\n\n\nDashboard: http://127.0.0.1:44839/status\nMemory: 1.86 GiB\n\n\nNanny: tcp://127.0.0.1:41431\n\n\n\nLocal directory: /tmp/dask-scratch-space/worker-o8f81ir2\n\n\n\n\n            \n            \n        \n        \n        \n             \n            \n            \n                \n                    Worker: 1\n                \n                \n\n\n\nComm: tcp://127.0.0.1:45341\nTotal threads: 2\n\n\nDashboard: http://127.0.0.1:43575/status\nMemory: 1.86 GiB\n\n\nNanny: tcp://127.0.0.1:37293\n\n\n\nLocal directory: /tmp/dask-scratch-space/worker-sswqfpbn\n\n\n\n\n            \n            \n        \n        \n        \n             \n            \n            \n                \n                    Worker: 2\n                \n                \n\n\n\nComm: tcp://127.0.0.1:37523\nTotal threads: 2\n\n\nDashboard: http://127.0.0.1:33189/status\nMemory: 1.86 GiB\n\n\nNanny: tcp://127.0.0.1:41099\n\n\n\nLocal directory: /tmp/dask-scratch-space/worker-gj947tlp\n\n\n\n\n            \n            \n        \n        \n        \n             \n            \n            \n                \n                    Worker: 3\n                \n                \n\n\n\nComm: tcp://127.0.0.1:42315\nTotal threads: 2\n\n\nDashboard: http://127.0.0.1:40491/status\nMemory: 1.86 GiB\n\n\nNanny: tcp://127.0.0.1:44249\n\n\n\nLocal directory: /tmp/dask-scratch-space/worker-7l9bdnbh\n\n\n\n\n            \n            \n        \n        \n        \n             \n            \n            \n                \n                    Worker: 4\n                \n                \n\n\n\nComm: tcp://127.0.0.1:35927\nTotal threads: 2\n\n\nDashboard: http://127.0.0.1:40743/status\nMemory: 1.86 GiB\n\n\nNanny: tcp://127.0.0.1:35647\n\n\n\nLocal directory: /tmp/dask-scratch-space/worker-_rrp5yai\n\n\n\n\n            \n            \n        \n        \n        \n             \n            \n            \n                \n                    Worker: 5\n                \n                \n\n\n\nComm: tcp://127.0.0.1:42877\nTotal threads: 2\n\n\nDashboard: http://127.0.0.1:35367/status\nMemory: 1.86 GiB\n\n\nNanny: tcp://127.0.0.1:39593\n\n\n\nLocal directory: /tmp/dask-scratch-space/worker-9ocnuao3\n\n\n\n\n            \n            \n        \n        \n        \n             \n            \n            \n                \n                    Worker: 6\n                \n                \n\n\n\nComm: tcp://127.0.0.1:35611\nTotal threads: 2\n\n\nDashboard: http://127.0.0.1:36999/status\nMemory: 1.86 GiB\n\n\nNanny: tcp://127.0.0.1:45113\n\n\n\nLocal directory: /tmp/dask-scratch-space/worker-6v198iid\n\n\n\n\n            \n            \n        \n        \n        \n             \n            \n            \n                \n                    Worker: 7\n                \n                \n\n\n\nComm: tcp://127.0.0.1:39531\nTotal threads: 2\n\n\nDashboard: http://127.0.0.1:38171/status\nMemory: 1.86 GiB\n\n\nNanny: tcp://127.0.0.1:33651\n\n\n\nLocal directory: /tmp/dask-scratch-space/worker-2e0p38oo\n\n\n\n\n            \n            \n        \n        \n        \n             \n            \n            \n                \n                    Worker: 8\n                \n                \n\n\n\nComm: tcp://127.0.0.1:41649\nTotal threads: 2\n\n\nDashboard: http://127.0.0.1:35337/status\nMemory: 1.86 GiB\n\n\nNanny: tcp://127.0.0.1:40221\n\n\n\nLocal directory: /tmp/dask-scratch-space/worker-qe9ic5bb\n\n\n\n\n            \n            \n        \n        \n        \n             \n            \n            \n                \n                    Worker: 9\n                \n                \n\n\n\nComm: tcp://127.0.0.1:38397\nTotal threads: 2\n\n\nDashboard: http://127.0.0.1:33565/status\nMemory: 1.86 GiB\n\n\nNanny: tcp://127.0.0.1:44057\n\n\n\nLocal directory: /tmp/dask-scratch-space/worker-6glu35ck\n\n\n\n\n            \n            \n        \n        \n        \n             \n            \n            \n                \n                    Worker: 10\n                \n                \n\n\n\nComm: tcp://127.0.0.1:46189\nTotal threads: 2\n\n\nDashboard: http://127.0.0.1:32953/status\nMemory: 1.86 GiB\n\n\nNanny: tcp://127.0.0.1:34913\n\n\n\nLocal directory: /tmp/dask-scratch-space/worker-p36vtsl0\n\n\n\n\n            \n            \n        \n        \n        \n             \n            \n            \n                \n                    Worker: 11\n                \n                \n\n\n\nComm: tcp://127.0.0.1:42589\nTotal threads: 2\n\n\nDashboard: http://127.0.0.1:34365/status\nMemory: 1.86 GiB\n\n\nNanny: tcp://127.0.0.1:44305\n\n\n\nLocal directory: /tmp/dask-scratch-space/worker-0gs2wyzl\n\n\n\n\n            \n            \n        \n        \n        \n             \n            \n            \n                \n                    Worker: 12\n                \n                \n\n\n\nComm: tcp://127.0.0.1:45655\nTotal threads: 2\n\n\nDashboard: http://127.0.0.1:37385/status\nMemory: 1.86 GiB\n\n\nNanny: tcp://127.0.0.1:46011\n\n\n\nLocal directory: /tmp/dask-scratch-space/worker-pyec8e9x\n\n\n\n\n            \n            \n        \n        \n        \n             \n            \n            \n                \n                    Worker: 13\n                \n                \n\n\n\nComm: tcp://127.0.0.1:36835\nTotal threads: 2\n\n\nDashboard: http://127.0.0.1:40751/status\nMemory: 1.86 GiB\n\n\nNanny: tcp://127.0.0.1:43667\n\n\n\nLocal directory: /tmp/dask-scratch-space/worker-waflaoji\n\n\n\n\n            \n            \n        \n        \n        \n             \n            \n            \n                \n                    Worker: 14\n                \n                \n\n\n\nComm: tcp://127.0.0.1:43599\nTotal threads: 2\n\n\nDashboard: http://127.0.0.1:36101/status\nMemory: 1.86 GiB\n\n\nNanny: tcp://127.0.0.1:44219\n\n\n\nLocal directory: /tmp/dask-scratch-space/worker-4z_4gip0\n\n\n\n\n            \n            \n        \n        \n        \n             \n            \n            \n                \n                    Worker: 15\n                \n                \n\n\n\nComm: tcp://127.0.0.1:38823\nTotal threads: 2\n\n\nDashboard: http://127.0.0.1:41181/status\nMemory: 1.86 GiB\n\n\nNanny: tcp://127.0.0.1:42207\n\n\n\nLocal directory: /tmp/dask-scratch-space/worker-mpxi4w0z\n\n\n\n\n            \n            \n        \n        \n        \n             \n            \n            \n                \n                    Worker: 16\n                \n                \n\n\n\nComm: tcp://127.0.0.1:43063\nTotal threads: 2\n\n\nDashboard: http://127.0.0.1:35175/status\nMemory: 1.86 GiB\n\n\nNanny: tcp://127.0.0.1:36171\n\n\n\nLocal directory: /tmp/dask-scratch-space/worker-fg39drd1\n\n\n\n\n            \n            \n        \n        \n        \n             \n            \n            \n                \n                    Worker: 17\n                \n                \n\n\n\nComm: tcp://127.0.0.1:40425\nTotal threads: 2\n\n\nDashboard: http://127.0.0.1:39361/status\nMemory: 1.86 GiB\n\n\nNanny: tcp://127.0.0.1:34799\n\n\n\nLocal directory: /tmp/dask-scratch-space/worker-zzdqp_p4\n\n\n\n\n            \n            \n        \n        \n        \n             \n            \n            \n                \n                    Worker: 18\n                \n                \n\n\n\nComm: tcp://127.0.0.1:44039\nTotal threads: 2\n\n\nDashboard: http://127.0.0.1:44757/status\nMemory: 1.86 GiB\n\n\nNanny: tcp://127.0.0.1:45465\n\n\n\nLocal directory: /tmp/dask-scratch-space/worker-0qtm78nz\n\n\n\n\n            \n            \n        \n        \n        \n             \n            \n            \n                \n                    Worker: 19\n                \n                \n\n\n\nComm: tcp://127.0.0.1:45865\nTotal threads: 2\n\n\nDashboard: http://127.0.0.1:36217/status\nMemory: 1.86 GiB\n\n\nNanny: tcp://127.0.0.1:42857\n\n\n\nLocal directory: /tmp/dask-scratch-space/worker-teyqaqft\nCode\nlogger = logging.getLogger(__name__)\nlogging.basicConfig(filename='example.log', encoding='utf-8', level=logging.DEBUG)\nlogger.debug('This message should go to the log file')\nlogger.info('So should this')\nlogger.warning('And this, too')\nlogger.error('And non-ASCII stuff, too, like Øresund and Malmö')\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[3], line 1\n----&gt; 1 logger = logging.getLogger(__name__)\n      2 logging.basicConfig(filename='example.log', encoding='utf-8', level=logging.DEBUG)\n      3 logger.debug('This message should go to the log file')\n\nNameError: name 'logging' is not defined"
  },
  {
    "objectID": "core/notebooks/xciti_pandas.html#paths",
    "href": "core/notebooks/xciti_pandas.html#paths",
    "title": "Imports",
    "section": "Paths",
    "text": "Paths\nDownloaded zip archives are in data_dir\nExtracted csv files are in extract_dir\nParquet files are in parquet_dir\n\n\nCode\ndata_dir = '../data'\nos.path.exists(data_dir)\n\nextract_dir = os.path.join(data_dir, 'xcitibike')\nif not os.path.exists(extract_dir):\n    os.mkdir(extract_dir)\n\nparquet_dir = os.path.join(data_dir, 'pq_citibike')\nif not os.path.exists(parquet_dir):\n    os.mkdir(parquet_dir)\n\n\n\n---------------------------------------------------------------------------\nFileNotFoundError                         Traceback (most recent call last)\nCell In[4], line 6\n      4 extract_dir = os.path.join(data_dir, 'xcitibike')\n      5 if not os.path.exists(extract_dir):\n----&gt; 6     os.mkdir(extract_dir)\n      8 parquet_dir = os.path.join(data_dir, 'pq_citibike')\n      9 if not os.path.exists(parquet_dir):\n\nFileNotFoundError: [Errno 2] No such file or directory: '../data/xcitibike'"
  },
  {
    "objectID": "core/notebooks/xciti_pandas.html#extracting-archives",
    "href": "core/notebooks/xciti_pandas.html#extracting-archives",
    "title": "Imports",
    "section": "Extracting archives",
    "text": "Extracting archives\nZip archive files contain directory trees where the csv files are to be found.\n\n\nCode\ncitibike_archives_paths = sorted(glob.glob(data_dir + '/*-citibike-tripdata.zip'))\n\n\nTODO: - parallelize part of the extraction process - one thread per element in citibike_archives_paths - should be doable with dask\n\n\nCode\nfor ar_path in tqdm(citibike_archives_paths):\n    myzip = ZipFile(ar_path)\n    to_extract = [elt  for elt in myzip.namelist() if (elt.endswith(\".csv\") and not (elt.startswith('__MACOSX')))]\n    myzip.extractall(path=extract_dir,\n                     members=to_extract)\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[6], line 1\n----&gt; 1 for ar_path in tqdm(citibike_archives_paths):\n      2     myzip = ZipFile(ar_path)\n      3     to_extract = [elt  for elt in myzip.namelist() if (elt.endswith(\".csv\") and not (elt.startswith('__MACOSX')))]\n\nNameError: name 'tqdm' is not defined"
  },
  {
    "objectID": "core/notebooks/xciti_pandas.html#collecting-headers",
    "href": "core/notebooks/xciti_pandas.html#collecting-headers",
    "title": "Imports",
    "section": "Collecting headers",
    "text": "Collecting headers\nThe extracted csv files do not share the same schema and the same datetime encoding format.\nWalking through the csv files in extract_dir, allows to gather the three different column naming patterns.\nTODO: - Save the schemata with inferred types in a json file. Different typing patterns may correspond to the same column naming pattern - For each csv file spot the column naming pattern, the datetime encoding format\n\n\nCode\n# schemata_names = set()\n\n# for (root, dirs ,files) in os.walk(extract_dir, topdown=True):\n#    if dirs:\n#        continue\n#    for fn in files:\n#        if fn.endswith('.csv'):\n#            with open(os.path.join(root, fn), 'r') as fd:\n#                schemata_names.add(fd.readline())\n\n# schemata_names = [s.replace('\\n', '').split(',') for s in schemata_names]\n\n\n\n\nCode\nschemata_names = [\n    ['ride_id',\n  'rideable_type',\n  'started_at',\n  'ended_at',\n  'start_station_name',\n  'start_station_id',\n  'end_station_name',\n  'end_station_id',\n  'start_lat',\n  'start_lng',\n  'end_lat',\n  'end_lng',\n  'member_casual'],\n ['tripduration',\n  'starttime',\n  'stoptime',\n  'start station id',\n  'start station name',\n  'start station latitude',\n  'start station longitude',\n  'end station id',\n  'end station name',\n  'end station latitude',\n  'end station longitude',\n  'bikeid',\n  'usertype',\n  'birth year',\n  'gender'],\n ['Trip Duration',\n  'Start Time',\n  'Stop Time',\n  'Start Station ID',\n  'Start Station Name',\n  'Start Station Latitude',\n  'Start Station Longitude',\n  'End Station ID',\n  'End Station Name',\n  'End Station Latitude',\n  'End Station Longitude',\n  'Bike ID',\n  'User Type',\n  'Birth Year',\n  'Gender']\n]\n\n\nFor each csv file, find the column naming pattern, build a dictionary with this information.\nTODO: - should done during the first walk.\n\n\nCode\nschemata_numbers = {}\n\nfor (root, dirs ,files) in os.walk(extract_dir, topdown=True):\n    if dirs:\n        continue\n    for fn in files:\n        if fn.endswith('.csv'):        \n            with open(os.path.join(root, fn), 'r') as fd:\n                col_names = fd.readline().replace('\\n', '').split(',')\n                schemata_numbers[fn] = schemata_names.index(col_names)"
  },
  {
    "objectID": "core/notebooks/xciti_pandas.html#building-renaming-dictionaries",
    "href": "core/notebooks/xciti_pandas.html#building-renaming-dictionaries",
    "title": "Imports",
    "section": "Building renaming dictionaries",
    "text": "Building renaming dictionaries\n\nFrom 0\nNothing to do\n\n\nFrom 1\nUse\n{\n 'tripduration': 'trip_duration',\n 'starttime': 'started_at',\n 'stoptime': 'ended_at',\n 'bikeid': 'bike_id',\n 'usertype': 'user_type',\n 'start station latitude': 'start_lat',\n 'start station longitude': 'start_lng',\n 'end station latitude': 'end_lat',\n 'end station longitude': 'end_lng'\n}\nand replace  with ’_’.\n\n\nFrom 2\n{\n 'Trip Duration': 'trip_duration',\n  'Start Time': 'started_at',\n  'Stop Time': 'ended_at',\n  'Start Station Latitude': 'start_lat',\n  'Start Station Longitude': 'start_lng',\n  'End Station Latitude': 'end_lat',\n  'End Station Longitude': 'end_lng'    \n}\n\nand replace  with ’_’, use lower().\n\n\nCode\ndicts_rename = {\n    1: {\n 'tripduration': 'trip_duration',\n 'starttime': 'started_at',\n 'stoptime': 'ended_at',\n 'bikeid': 'bike_id',\n 'usertype': 'user_type',\n 'start station latitude': 'start_lat',\n 'start station longitude': 'start_lng',\n 'end station latitude': 'end_lat',\n 'end station longitude': 'end_lng'\n} ,\n  2:  {\n 'Trip Duration': 'trip_duration',\n  'Start Time': 'started_at',\n  'Stop Time': 'ended_at',\n  'Start Station Latitude': 'start_lat',\n  'Start Station Longitude': 'start_lng',\n  'End Station Latitude': 'end_lat',\n  'End Station Longitude': 'end_lng'    \n}\n}\n\n\nAnother problem.\nstart_station_id, end_station_id is not consistently formatted."
  },
  {
    "objectID": "core/notebooks/xciti_pandas.html#building-a-parquet-replica",
    "href": "core/notebooks/xciti_pandas.html#building-a-parquet-replica",
    "title": "Imports",
    "section": "Building a parquet replica",
    "text": "Building a parquet replica\nTODO: - explain why engine='pyarrow' is useful when using pd.read_csv() - clean up the renaming schemes\nDatetime hand-made parsing for non ISO compliant csv file\n\n\nCode\ndef my_parse(s):\n    \"\"\"datetime parsing for non-ISO enco\n\n    Args:\n        s (str): a datetime encoding string '%m/%d/%Y H:M[:S]'\n\n    Returns:\n        datetime: a datetime object without timezone\n    \"\"\"\n    rem = re.compile(r\"(\\d+)/(\\d+)/(\\d+) (\\d+)?(:\\d+)?(:\\d+)?\")\n\n    matches = rem.search(s).groups()\n    month, day, year, hours, mins, secs = [int(x.replace(':','')) if x else 0 for x in matches]\n\n    zdt = datetime.datetime(year, month, day, hours, mins, secs)\n    return zdt\n\n\nTODO: - parallelize this\n\n\nCode\ndef csv2pq(root, dirs, files):\n    if dirs:\n        return\n\n    for fn in files:\n        if not fn.endswith('.csv'):  \n            continue\n\n        df = pd.read_csv(\n                    os.path.join(root, fn),\n                    engine = 'pyarrow'\n            )\n                    \n        if 1==schemata_numbers[fn]: \n            df = ( \n                df\n                    .rename(columns=dicts_rename[1])\n                    .rename(mapper= lambda s : s.replace(' ', '_'), axis='columns')\n            )                \n        elif 2==schemata_numbers[fn]:\n            df = ( \n                df\n                    .rename(columns=dicts_rename[2])\n                    .rename(mapper= lambda s : s.lower().replace(' ', '_'), axis='columns')\n            )\n\n        if (str(df.dtypes.loc['ended_at'])=='object'):\n            # Format is either '%m/%d/%Y %H:%M:%S'or '%m/%d/%Y %H:%M'\n            try:\n                df['ended_at'] = pd.to_datetime(df.ended_at, format='%m/%d/%Y %H:%M:%S')\n                df['started_at'] = pd.to_datetime(df.started_at, format='%m/%d/%Y %H:%M:%S')\n            except ValueError:\n                df['ended_at'] = pd.to_datetime(df.ended_at, format='%m/%d/%Y %H:%M')\n                df['started_at'] = pd.to_datetime(df.started_at, format='%m/%d/%Y %H:%M')\n            except:\n                df['ended_at'] = df.ended_at.map(my_parse)\n                df['started_at'] = df.started_at.map(my_parse)\n\n\n        # if df.start_station_id.dtype != np.dtype('O'):\n\n        df['start_station_id'] = df.start_station_id.astype(np.dtype('O'))  \n        df['end_station_id'] = df.end_station_id.astype(np.dtype('O')) \n            \n        df['start_year'] = df.started_at.dt.year\n        df['start_month'] = df.started_at.dt.month  \n        \n        table = pa.Table.from_pandas(df)\n\n        logger.info('writing: ' + fn)\n\n        pq.write_to_dataset(\n                table,\n                parquet_dir,\n                partition_cols=[\"start_year\", \"start_month\"],\n        )    \n\n    return root \n\n\n\n\nCode\ntodo = dask.delayed([dask.delayed(csv2pq)(root, dirs ,files)\n    for (root, dirs ,files) in os.walk(extract_dir, topdown=True)\n ])\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[13], line 1\n----&gt; 1 todo = dask.delayed([dask.delayed(csv2pq)(root, dirs ,files)\n      2     for (root, dirs ,files) in os.walk(extract_dir, topdown=True)\n      3  ])\n\nNameError: name 'dask' is not defined\n\n\n\n\n\nCode\nfoo = todo.compute()\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[14], line 1\n----&gt; 1 foo = todo.compute()\n\nNameError: name 'todo' is not defined\n\n\n\n\n\nCode\n[x  for x in foo if x]\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[15], line 1\n----&gt; 1 [x  for x in foo if x]\n\nNameError: name 'foo' is not defined\n\n\n\n\n\nCode\nlist(schemata_numbers.keys())\n\n\n[]\n\n\n\n\nCode\nfor (root, dirs ,files) in tqdm(os.walk(extract_dir, topdown=True)):\n    if dirs:\n        continue\n    for fn in files:\n        if not fn.endswith('.csv'):  \n            continue\n\n        df = pd.read_csv(\n                os.path.join(root, fn),\n                engine = 'pyarrow'\n        )\n                \n        if 1==schemata_numbers[fn]: \n            df = ( \n                df\n                  .rename(columns=dicts_rename[1])\n                  .rename(mapper= lambda s : s.replace(' ', '_'), axis='columns')\n            )                \n        elif 2==schemata_numbers[fn]:\n            df = ( \n                df\n                  .rename(columns=dicts_rename[2])\n                  .rename(mapper= lambda s : s.lower().replace(' ', '_'), axis='columns')\n            )\n        \n        if (str(df.dtypes.loc['ended_at'])=='object'):\n            # Format is either '%m/%d/%Y %H:%M:%S'or '%m/%d/%Y %H:%M'\n            try:\n                df['ended_at'] = pd.to_datetime(df.ended_at, format='%m/%d/%Y %H:%M:%S')\n                df['started_at'] = pd.to_datetime(df.started_at, format='%m/%d/%Y %H:%M:%S')\n            except ValueError:\n                df['ended_at'] = pd.to_datetime(df.ended_at, format='%m/%d/%Y %H:%M')\n                df['started_at'] = pd.to_datetime(df.started_at, format='%m/%d/%Y %H:%M')\n            except:\n                df['ended_at'] = df.ended_at.map(my_parse)\n                df['started_at'] = df.started_at.map(my_parse) \n\n        \n\n        if df.start_station_id.dtype != np.dtype('O'):\n            df['start_station_id'] = df.start_station_id.astype(np.dtype('O'))  \n            df['end_station_id'] = df.end_station_id.astype(np.dtype('O')) \n         \n        df['start_year'] = df.started_at.dt.year\n        df['start_month'] = df.started_at.dt.month  \n      \n        table = pa.Table.from_pandas(df)\n\n        pa.schema(table)\n\n        pq.write_to_dataset(\n             table,\n             parquet_dir,\n             partition_cols=[\"start_year\", \"start_month\"],\n        )\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[17], line 1\n----&gt; 1 for (root, dirs ,files) in tqdm(os.walk(extract_dir, topdown=True)):\n      2     if dirs:\n      3         continue\n\nNameError: name 'tqdm' is not defined\n\n\n\n\n\nCode\ndf.start_station_id.astype(np.dtype('O'))\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[18], line 1\n----&gt; 1 df.start_station_id.astype(np.dtype('O'))\n\nNameError: name 'df' is not defined"
  },
  {
    "objectID": "core/notebooks/xciti_pandas.html#todos",
    "href": "core/notebooks/xciti_pandas.html#todos",
    "title": "Imports",
    "section": "TODOs",
    "text": "TODOs\n\nHandling schema evolution\nSchema changed between 2021 January and 2021 February\n\n\n\n\n\n\n\n\nOld Column\nNew Column\nAction\n\n\n\n\n\nride_id\nPrimary key ?\n\n\n\nride_type\ndocked_bike\n\n\ntripduration\n\nIn seconds, can be recovered from started_at/ended_at\n\n\nstarttime\nstarted_at\nNo need for microseconds before 2021 January\n\n\nstoptime\nended_at\nNo need for microseconds before 2021 January\n\n\nstart station id\nstart_station_id\nOrder mismatch, code mismatch. Before : int. After:\n\n\nstart station name\nstart_station_name\nCheck consistency\n\n\nstart station latitude\nstart_lat\nCheck consistency\n\n\nstart station longitude\nstart_lng\nCheck consistency\n\n\n\n\n\n\n\n\n\n\n\nOld Column\nNew Column\nAction\n\n\n\n\nend station id\nend_station_id\nCheck consistency\n\n\nend station name\nend_station_name\nCheck consistency\n\n\nend station latitude\nend_lat\nCheck consistency\n\n\nend station longitude\nend_lng\nCheck consistency\n\n\nbikeid\n\n\n\n\nusertype\n\nSubscriber/Customer\n\n\nbirth year\n\n\n\n\ngender\n\n0, 1\n\n\n\nmember_casual\ncasual/member\n\n\n\n\nreading side: just read start_time, end_time, start_station_id, end_station_id,\nstart_at and end_at must be translated to start_time, end_time\ntrip_duration, user_type, bike_id, member_casual\nPrepare for a dimension table for stations\n\nid\nname\nlat\nlon\nmore\n\n\n\n\nSelect from the colum names\nCan we read directly as a pyarrow table ? Yes, but Pandas is convenient for datetime manipulations, and possibly for renaming\n\n\nUsage pyarrow.unify_schemas\n\n\nParsing dates\nFor some files, timestamps are not in ISO format.\nFrom 2014-09-01 till 2016-09-.., started_at and ended_at do not abide ISO format, but %m/%d/%Y %H:%M:%S.\nTry to use pd.to_datetime(). If failure, use regular expression to parse the putative date column. Handle the optional field that way.\nBetter ask forgiveness than permission.\n\n\nCode\nroot, dirs, fn = next(os.walk(os.path.join(parquet_dir, 'start_year=2013', 'start_month=9')))\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[19], line 1\n----&gt; 1 root, dirs, fn = next(os.walk(os.path.join(parquet_dir, 'start_year=2013', 'start_month=9')))\n\nNameError: name 'parquet_dir' is not defined\n\n\n\n\n\nCode\nspam = pq.read_metadata(os.path.join(root, fn[0]))\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[20], line 1\n----&gt; 1 spam = pq.read_metadata(os.path.join(root, fn[0]))\n\nNameError: name 'pq' is not defined\n\n\n\n\n\nCode\nfrom  dask import dataframe as dd\n\n\n\n\nCode\nspam = dd.read_parquet(os.path.join(parquet_dir, 'start_year=2023' ))\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[22], line 1\n----&gt; 1 spam = dd.read_parquet(os.path.join(parquet_dir, 'start_year=2023' ))\n\nNameError: name 'parquet_dir' is not defined\n\n\n\n\n\nCode\nspam.dtypes\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[23], line 1\n----&gt; 1 spam.dtypes\n\nNameError: name 'spam' is not defined\n\n\n\n\n\nCode\nfoo_path = os.path.join(parquet_dir, 'start_year=2023', 'start_month=1')\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[24], line 1\n----&gt; 1 foo_path = os.path.join(parquet_dir, 'start_year=2023', 'start_month=1')\n\nNameError: name 'parquet_dir' is not defined\n\n\n\n\n\nCode\nroot, dirs, fn = next(os.walk(foo_path))\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[25], line 1\n----&gt; 1 root, dirs, fn = next(os.walk(foo_path))\n\nNameError: name 'foo_path' is not defined\n\n\n\n\n\nCode\nparquet_file = pq.ParquetFile(os.path.join(root, fn[0]))\nschema = parquet_file.schema\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[26], line 1\n----&gt; 1 parquet_file = pq.ParquetFile(os.path.join(root, fn[0]))\n      2 schema = parquet_file.schema\n\nNameError: name 'pq' is not defined\n\n\n\n\n\nCode\nschema\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[27], line 1\n----&gt; 1 schema\n\nNameError: name 'schema' is not defined"
  },
  {
    "objectID": "core/notebooks/notebook14.html",
    "href": "core/notebooks/notebook14.html",
    "title": "Using with pyspark for data preprocessing",
    "section": "",
    "text": "Code\nimport os\nimport sys\nimport pyarrow as pa\nimport comet as co\nimport pyarrow.parquet as pq\n\n\n\n\nCode\nfrom pyspark.sql import Window\nimport pyspark.sql.functions as func\nfrom pyspark.sql.types import *\nfrom pyspark.sql.functions import col, lit\n\n\n\n\nCode\nfrom functools import reduce\n\n\n\n\nCode\nos.environ['PYSPARK_PYTHON'] = sys.executable\nos.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable\n\n\n\n\nCode\nfrom pyspark.sql import SparkSession\n\nspark = (SparkSession\n    .builder\n    .appName(\"Web data\")         \n    .getOrCreate()\n)\n\n\n25/04/03 15:11:17 WARN Utils: Your hostname, boucheron-Precision-5480 resolves to a loopback address: 127.0.1.1; using 172.23.32.10 instead (on interface eth0)\n25/04/03 15:11:17 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\nSetting default log level to \"WARN\".\nTo adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n25/04/03 15:11:17 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n\n\n\n\nCode\n%timeit\ninput_path = './'\n\ninput_file = os.path.join(input_path, 'webdata.parquet')\n\ndf = spark.read.parquet(input_file)\n\n\n\n\nCode\ndf.printSchema()\n\n\nroot\n |-- xid: string (nullable = true)\n |-- action: string (nullable = true)\n |-- date: timestamp (nullable = true)\n |-- website_id: string (nullable = true)\n |-- url: string (nullable = true)\n |-- category_id: float (nullable = true)\n |-- zipcode: string (nullable = true)\n |-- device: string (nullable = true)\n\n\n\n\n\nCode\ndf.rdd.getNumPartitions()\n\n\n12\n\n\n\n\nCode\n%timeit\ndfa = pq.read_table(input_file)\n\n\n\n\nCode\nprint(dfa.schema)\n\n\nxid: string\naction: string\ndate: timestamp[ns]\nwebsite_id: string\nurl: string\ncategory_id: float\nzipcode: string\ndevice: string\n-- schema metadata --\norg.apache.spark.sql.parquet.row.metadata: '{\"type\":\"struct\",\"fields\":[{\"' + 515\n\n\n\n\nCode\n( \n    df.select('xid')\n      .distinct()\n      .explain()\n)\n\n\n== Physical Plan ==\nAdaptiveSparkPlan isFinalPlan=false\n+- HashAggregate(keys=[xid#0], functions=[])\n   +- Exchange hashpartitioning(xid#0, 200), ENSURE_REQUIREMENTS, [plan_id=18]\n      +- HashAggregate(keys=[xid#0], functions=[])\n         +- FileScan parquet [xid#0] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/home/boucheron/sandbox/IFEBY310/core/notebooks/webdata.parquet], PartitionFilters: [], PushedFilters: [], ReadSchema: struct&lt;xid:string&gt;\n\n\n\n\n\n\nCode\n# \n( \n    df.select('xid')\n      .distinct()\n      .count()\n)\n\n\n473761\n\n\n\n\nCode\nxid_partition = Window.partitionBy('xid')\nn_events = func.count(col('action')).over(xid_partition)\ndf = df.withColumn('n_events', n_events)\ndf.head(n=2)\n\n\n[Row(xid='0006cea7-1679-4264-bdef-0cd089749ede', action='O', date=datetime.datetime(2016, 12, 26, 13, 41, 8), website_id='51', url='https://www.footlegende.fr/mercato-psg-coutinho-10166', category_id=1002.0, zipcode='34290', device='TAB', n_events=1),\n Row(xid='000893c8-a14b-4f33-858f-210440f37def', action='O', date=datetime.datetime(2016, 12, 23, 16, 18, 37), website_id='56', url='http://blague.dumatin.fr/', category_id=1002.0, zipcode=None, device='DSK', n_events=1)]\n\n\n\n\nCode\n( \n  df\n    .groupBy('xid')\n    .agg(func.count('action'))\n    .head(5)\n)\n\n\n[Row(xid='001c4a21-52c6-4890-b6ce-2b9d4ba06a56', count(action)=1),\n Row(xid='0024344b-7ee2-4fcd-a0b4-bec26d8c8b0e', count(action)=4),\n Row(xid='004564e3-87c1-4e16-ad2c-0e96afc3d617', count(action)=1),\n Row(xid='006d807f-91c3-415a-bb5e-6b9f7e6517a1', count(action)=1),\n Row(xid='006e0463-b24c-4996-84ab-d6d0d65a52aa', count(action)=1)]\n\n\n\n\nCode\ndf = df.repartitionByRange(20, 'xid')\n\n\n\n\nCode\ndf.persist()\n\n\nDataFrame[xid: string, action: string, date: timestamp, website_id: string, url: string, category_id: float, zipcode: string, device: string, n_events: bigint]\n\n\n\n\nCode\nxid_partition = Window.partitionBy('xid')\n\nmax_date = (\n  func\n    .max(col('date'))\n    .over(xid_partition)\n)\n\nn_days_since_last_event = func.datediff(func.current_date(), max_date)\n\ndf = df.withColumn('n_days_since_last_event',\n                   n_days_since_last_event)\n\ndf.head(n=2)\n\n\n[Stage 20:===============================================&gt;        (17 + 3) / 20]                                                                                \n\n\n[Row(xid='00008f69-9f2f-4453-80ec-98b4ae8a3085', action='O', date=datetime.datetime(2017, 1, 26, 17, 8, 30), website_id='3', url='http://www.8chances.com/grille/?from=mr', category_id=1002.0, zipcode='16400', device='DSK', n_events=1, n_days_since_last_event=2989),\n Row(xid='000095cc-9a61-49b5-8ad5-83442daa93d6', action='O', date=datetime.datetime(2017, 1, 25, 21, 43, 12), website_id='74', url='http://www.realite-virtuelle.com/guide-comparatif-casque-realite-virtuelle', category_id=1002.0, zipcode='24500', device='DSK', n_events=2, n_days_since_last_event=2990)]\n\n\n\n\nCode\nn_events = func.count(col('action')).over(xid_partition)\n\ndf = df.withColumn('n_events', n_events)\n\ndf.head(n=2)\n\n\n[Row(xid='00008f69-9f2f-4453-80ec-98b4ae8a3085', action='O', date=datetime.datetime(2017, 1, 26, 17, 8, 30), website_id='3', url='http://www.8chances.com/grille/?from=mr', category_id=1002.0, zipcode='16400', device='DSK', n_events=1, n_days_since_last_event=2989),\n Row(xid='000095cc-9a61-49b5-8ad5-83442daa93d6', action='O', date=datetime.datetime(2017, 1, 25, 21, 43, 12), website_id='74', url='http://www.realite-virtuelle.com/guide-comparatif-casque-realite-virtuelle', category_id=1002.0, zipcode='24500', device='DSK', n_events=2, n_days_since_last_event=2990)]\n\n\n\n\nCode\nxid_device_partition = xid_partition.partitionBy('device')\n\nn_events_per_device = (\n    func.count(col('action'))\n        .over(xid_device_partition)\n)\n\ndf = df.withColumn('n_events_per_device', n_events_per_device)\ndf.head(n=2)\n\n\n[Stage 33:&gt;                                                         (0 + 1) / 1]                                                                                \n\n\n[Row(xid='00008f69-9f2f-4453-80ec-98b4ae8a3085', action='O', date=datetime.datetime(2017, 1, 26, 17, 8, 30), website_id='3', url='http://www.8chances.com/grille/?from=mr', category_id=1002.0, zipcode='16400', device='DSK', n_events=1, n_days_since_last_event=2989, n_events_per_device=1021837),\n Row(xid='000095cc-9a61-49b5-8ad5-83442daa93d6', action='O', date=datetime.datetime(2017, 1, 25, 21, 43, 12), website_id='74', url='http://www.realite-virtuelle.com/guide-comparatif-casque-realite-virtuelle', category_id=1002.0, zipcode='24500', device='DSK', n_events=2, n_days_since_last_event=2990, n_events_per_device=1021837)]\n\n\n\n\nCode\nrank_device = (\n  func\n    .dense_rank()\n    .over(xid_partition.orderBy('device'))\n)\n\nn_unique_device = (\n    func\n      .last(rank_device)\n      .over(xid_partition)\n)\n\ndf = df.withColumn('n_device', n_unique_device)\n\ndf.head(n=2)\n\n\n[Stage 40:======================================&gt;                   (2 + 1) / 3]                                                                                \n\n\n[Row(xid='0006cea7-1679-4264-bdef-0cd089749ede', action='O', date=datetime.datetime(2016, 12, 26, 13, 41, 8), website_id='51', url='https://www.footlegende.fr/mercato-psg-coutinho-10166', category_id=1002.0, zipcode='34290', device='TAB', n_events=1, n_days_since_last_event=3020, n_events_per_device=132013, n_device=1),\n Row(xid='000893c8-a14b-4f33-858f-210440f37def', action='O', date=datetime.datetime(2016, 12, 23, 16, 18, 37), website_id='56', url='http://blague.dumatin.fr/', category_id=1002.0, zipcode=None, device='DSK', n_events=1, n_days_since_last_event=3023, n_events_per_device=1021837, n_device=1)]\n\n\n\n\nCode\ndf.printSchema()\n\n\nroot\n |-- xid: string (nullable = true)\n |-- action: string (nullable = true)\n |-- date: timestamp (nullable = true)\n |-- website_id: string (nullable = true)\n |-- url: string (nullable = true)\n |-- category_id: float (nullable = true)\n |-- zipcode: string (nullable = true)\n |-- device: string (nullable = true)\n |-- n_events: long (nullable = false)\n |-- n_days_since_last_event: integer (nullable = true)\n |-- n_events_per_device: long (nullable = false)\n |-- n_device: integer (nullable = true)\n\n\n\n\n\nCode\n(\n  df\n    .where(col('n_device') &gt; 1)\n    .select('xid', \n            'device', \n            'n_events', \n            'n_device', \n            'n_events_per_device')\n    .head(n=8)\n)\n\n\n[Row(xid='4c1dc79d-a140-4da9-ae28-540b4503c3b8', device='DSK', n_events=6, n_device=2, n_events_per_device=1021837),\n Row(xid='4c1dc79d-a140-4da9-ae28-540b4503c3b8', device='DSK', n_events=6, n_device=2, n_events_per_device=1021837),\n Row(xid='4c1dc79d-a140-4da9-ae28-540b4503c3b8', device='DSK', n_events=6, n_device=2, n_events_per_device=1021837),\n Row(xid='4c1dc79d-a140-4da9-ae28-540b4503c3b8', device='DSK', n_events=6, n_device=2, n_events_per_device=1021837),\n Row(xid='4c1dc79d-a140-4da9-ae28-540b4503c3b8', device='DSK', n_events=6, n_device=2, n_events_per_device=1021837),\n Row(xid='4c1dc79d-a140-4da9-ae28-540b4503c3b8', device='MOB', n_events=6, n_device=2, n_events_per_device=1564),\n Row(xid='78156cdf-7229-46eb-bb6b-92d384f9a6fa', device='DSK', n_events=6, n_device=2, n_events_per_device=1021837),\n Row(xid='78156cdf-7229-46eb-bb6b-92d384f9a6fa', device='DSK', n_events=6, n_device=2, n_events_per_device=1021837)]\n\n\n\n\nCode\ndef n_events_transformer(df):\n    xid_partition = Window.partitionBy('xid')\n    n_events = func.count(col('action')).over(xid_partition)\n    \n    df = df.withColumn('n_events', n_events)\n\n    return df\n\n\n\n\nCode\ndef my_count_window_transform(df, input_col, output_col, part_col):\n    w = Window.partitionBy(part_col)\n    out_col = func.count(col(input_col)).over(w)\n\n    return df.withColumn(output_col, out_col)\n\n\n\n\nCode\ndef hour_transformer(df):\n    hour = func.hour(col('date'))\n    df = df.withColumn('hour', hour)\n    return df\n\ndef weekday_transformer(df):\n    weekday = func.date_format(col('date'), 'EEEE')\n    df = df.withColumn('weekday', weekday)\n    return df\n\n\n\n\nCode\ndef n_events_per_hour_transformer(df):\n    xid_hour_partition = Window.partitionBy('xid', 'hour')\n    n_events_per_hour = func.count(col('action')).over(xid_hour_partition)\n    df = df.withColumn('n_events_per_hour', n_events_per_hour)\n    return df\n\n\n\n\nCode\ndef n_events_per_weekday_transformer(df):\n    xid_weekday_partition = Window.partitionBy('xid', 'weekday')\n    n_events_per_weekday = func.count(col('action')).over(xid_weekday_partition)\n    df = df.withColumn('n_events_per_weekday', n_events_per_weekday)\n    return df\n\ndef n_days_since_last_event_transformer(df):\n    xid_partition = Window.partitionBy('xid')\n    max_date = func.max(col('date')).over(xid_partition)\n    n_days_since_last_event = func.datediff(func.current_date(), max_date)\n    df = df.withColumn('n_days_since_last_event',\n                       n_days_since_last_event + lit(0.1))\n    return df\n\ndef n_days_since_last_action_transformer(df):\n    xid_partition_action = Window.partitionBy('xid', 'action')\n    max_date = func.max(col('date')).over(xid_partition_action)\n    n_days_since_last_action = func.datediff(func.current_date(),\n                                                        max_date)\n    df = df.withColumn('n_days_since_last_action',\n                       n_days_since_last_action + lit(0.1))\n    return df\n\ndef n_unique_day_transformer(df):\n    xid_partition = Window.partitionBy('xid')\n    dayofyear = func.dayofyear(col('date'))\n    rank_day = func.dense_rank().over(xid_partition.orderBy(dayofyear))\n    n_unique_day = func.last(rank_day).over(xid_partition)\n    df = df.withColumn('n_unique_day', n_unique_day)\n    return df\n\ndef n_unique_hour_transformer(df):\n    xid_partition = Window.partitionBy('xid')\n    rank_hour = func.dense_rank().over(xid_partition.orderBy('hour'))\n    n_unique_hour = func.last(rank_hour).over(xid_partition)\n    df = df.withColumn('n_unique_hour', n_unique_hour)\n    return df\n\ndef n_events_per_device_transformer(df):\n    xid_device_partition = Window.partitionBy('xid', 'device')\n    n_events_per_device = func.count(func.col('device')) \\\n        .over(xid_device_partition)\n    df = df.withColumn('n_events_per_device', n_events_per_device)\n    return df\n\ndef n_unique_device_transformer(df):\n    xid_partition = Window.partitionBy('xid')\n    rank_device = func.dense_rank().over(xid_partition.orderBy('device'))\n    n_unique_device = func.last(rank_device).over(xid_partition)\n    df = df.withColumn('n_device', n_unique_device)\n    return df\n\ndef n_actions_per_category_id_transformer(df):\n    xid_category_id_partition = Window.partitionBy('xid', 'category_id',\n                                                   'action')\n    n_actions_per_category_id = func.count(func.col('action')) \\\n        .over(xid_category_id_partition)\n    df = df.withColumn('n_actions_per_category_id', n_actions_per_category_id)\n    return df\n\ndef n_unique_category_id_transformer(df):\n    xid_partition = Window.partitionBy('xid')\n    rank_category_id = func.dense_rank().over(xid_partition\\\n                                              .orderBy('category_id'))\n    n_unique_category_id = func.last(rank_category_id).over(xid_partition)\n    df = df.withColumn('n_unique_category_id', n_unique_category_id)\n    return df\n\ndef n_events_per_category_id_transformer(df):\n    xid_category_id_partition = Window.partitionBy('xid', 'category_id')\n    n_events_per_category_id = func.count(func.col('action')) \\\n        .over(xid_category_id_partition)\n    df = df.withColumn('n_events_per_category_id', n_events_per_category_id)\n    return df\n\ndef n_events_per_website_id_transformer(df):\n    xid_website_id_partition = Window.partitionBy('xid', 'website_id')\n    n_events_per_website_id = func.count(col('action'))\\\n        .over(xid_website_id_partition)\n    df = df.withColumn('n_events_per_website_id', n_events_per_website_id)\n    return df"
  },
  {
    "objectID": "core/notebooks/notebook10_graphx.html",
    "href": "core/notebooks/notebook10_graphx.html",
    "title": "",
    "section": "",
    "text": "CodeShow All CodeHide All CodeView Source\n\n\n\n\n\n\nCode\nfrom graphframes import GraphFrame\nfrom pyspark import SparkConf, SparkContext\nfrom pyspark.sql import SparkSession\n\nconf = SparkConf().setAppName(\"Spark graphx Course\")\nsc = SparkContext(conf=conf)  # no need for Spark 3...\n\nspark = (SparkSession\n         .builder\n         .appName(\"Spark graphx Course\")\n         .getOrCreate()\n         )\n\nspark._sc is sc\n\n\n\n---------------------------------------------------------------------------\nModuleNotFoundError                       Traceback (most recent call last)\nCell In[1], line 1\n----&gt; 1 from graphframes import GraphFrame\n      2 from pyspark import SparkConf, SparkContext\n      3 from pyspark.sql import SparkSession\n\nModuleNotFoundError: No module named 'graphframes'\n\n\n\n\n\nCode\nv = spark.createDataFrame([\n    (\"a\", \"Alice\", 34),\n    (\"b\", \"Bob\", 36),\n    (\"c\", \"Charlie\", 30),\n], [\"id\", \"name\", \"age\"])\n# Create an Edge DataFrame with \"src\" and \"dst\" columns\ne = spark.createDataFrame([\n    (\"a\", \"b\", \"friend\"),\n    (\"b\", \"c\", \"follow\"),\n    (\"c\", \"b\", \"follow\"),\n], [\"src\", \"dst\", \"relationship\"])\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[2], line 1\n----&gt; 1 v = spark.createDataFrame([\n      2     (\"a\", \"Alice\", 34),\n      3     (\"b\", \"Bob\", 36),\n      4     (\"c\", \"Charlie\", 30),\n      5 ], [\"id\", \"name\", \"age\"])\n      6 # Create an Edge DataFrame with \"src\" and \"dst\" columns\n      7 e = spark.createDataFrame([\n      8     (\"a\", \"b\", \"friend\"),\n      9     (\"b\", \"c\", \"follow\"),\n     10     (\"c\", \"b\", \"follow\"),\n     11 ], [\"src\", \"dst\", \"relationship\"])\n\nNameError: name 'spark' is not defined\n\n\n\n\n\nCode\ng = GraphFrame(v, e)\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[3], line 1\n----&gt; 1 g = GraphFrame(v, e)\n\nNameError: name 'GraphFrame' is not defined"
  },
  {
    "objectID": "core/notebooks/notebook08_webdata-II.html",
    "href": "core/notebooks/notebook08_webdata-II.html",
    "title": "Using with pyspark for data preprocessing",
    "section": "",
    "text": "The data is a parquet file which contains a dataframe with 8 columns:\n\nxid: unique user id\naction: type of action. ‘C’ is a click, ‘O’ or ‘VSL’ is a web-display\ndate: date of the action\nwebsite_id: unique id of the website\nurl: url of the webpage\ncategory_id: id of the display\nzipcode: postal zipcode of the user\ndevice: type of device used by the user"
  },
  {
    "objectID": "core/notebooks/notebook08_webdata-II.html#data-description",
    "href": "core/notebooks/notebook08_webdata-II.html#data-description",
    "title": "Using with pyspark for data preprocessing",
    "section": "",
    "text": "The data is a parquet file which contains a dataframe with 8 columns:\n\nxid: unique user id\naction: type of action. ‘C’ is a click, ‘O’ or ‘VSL’ is a web-display\ndate: date of the action\nwebsite_id: unique id of the website\nurl: url of the webpage\ncategory_id: id of the display\nzipcode: postal zipcode of the user\ndevice: type of device used by the user"
  },
  {
    "objectID": "core/notebooks/notebook08_webdata-II.html#q1.-some-statistics-computations",
    "href": "core/notebooks/notebook08_webdata-II.html#q1.-some-statistics-computations",
    "title": "Using with pyspark for data preprocessing",
    "section": "Q1. Some statistics / computations",
    "text": "Q1. Some statistics / computations\nUsing pyspark.sql we want to do the following things:\n\nCompute the total number of unique users\nConstruct a column containing the total number of actions per user\nConstruct a column containing the number of days since the last action of the user\nConstruct a column containing the number of actions of each user for each modality of device"
  },
  {
    "objectID": "core/notebooks/notebook08_webdata-II.html#q2.-feature-engineering",
    "href": "core/notebooks/notebook08_webdata-II.html#q2.-feature-engineering",
    "title": "Using with pyspark for data preprocessing",
    "section": "Q2. Feature engineering",
    "text": "Q2. Feature engineering\nThen, we want to construct a classifier to predict the click on the category 1204. Here is an agenda for this:\n\nConstruction of a features matrix for which each line corresponds to the information concerning a user.\nIn this matrix, we need to keep only the users that have been exposed to the display in category 1204"
  },
  {
    "objectID": "core/notebooks/notebook08_webdata-II.html#q3.-classification",
    "href": "core/notebooks/notebook08_webdata-II.html#q3.-classification",
    "title": "Using with pyspark for data preprocessing",
    "section": "Q3. Classification",
    "text": "Q3. Classification\n\nUsing this training dataset, train a binary classifier, and evaluate your classifier using a precision / recall curve computed on test data."
  },
  {
    "objectID": "core/notebooks/notebook08_webdata-II.html#compute-the-total-number-of-unique-users",
    "href": "core/notebooks/notebook08_webdata-II.html#compute-the-total-number-of-unique-users",
    "title": "Using with pyspark for data preprocessing",
    "section": "Compute the total number of unique users",
    "text": "Compute the total number of unique users\n\n\nCode\n( \n    df.select('xid')\n      .distinct()\n      .count()\n)\n\n\n[Stage 1:=============================&gt;                            (6 + 6) / 12]                                                                                \n\n\n473761\n\n\n\n\nCode\ndef foo(x): yield len(set(x))\n\n\n\n\nCode\n( df.rdd\n    .map(lambda x : x.xid)\n    .mapPartitions(foo)\n    .collect()\n)\n\n\n[Stage 7:========================&gt;                                 (5 + 7) / 12][Stage 7:=============================&gt;                            (6 + 6) / 12][Stage 7:===========================================&gt;              (9 + 3) / 12]                                                                                \n\n\n[78120, 78636, 79090, 79754, 79296, 78865, 0, 0, 0, 0, 0, 0]\n\n\nThis might pump up some computational resources\n\n\nCode\n( \n    df.select('xid')\n      .distinct() \n      .explain()\n)\n\n\n== Physical Plan ==\nAdaptiveSparkPlan isFinalPlan=false\n+- HashAggregate(keys=[xid#0], functions=[])\n   +- Exchange hashpartitioning(xid#0, 200), ENSURE_REQUIREMENTS, [plan_id=108]\n      +- HashAggregate(keys=[xid#0], functions=[])\n         +- FileScan parquet [xid#0] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/home/boucheron/sandbox/IFEBY310/core/notebooks/webdata.parquet], PartitionFilters: [], PushedFilters: [], ReadSchema: struct&lt;xid:string&gt;\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe distinct values of xid seem to be evenly spread among the six files making the parquet directory. Note that the last six partitions look empty."
  },
  {
    "objectID": "core/notebooks/notebook08_webdata-II.html#construct-a-column-containing-the-total-number-of-actions-per-user",
    "href": "core/notebooks/notebook08_webdata-II.html#construct-a-column-containing-the-total-number-of-actions-per-user",
    "title": "Using with pyspark for data preprocessing",
    "section": "Construct a column containing the total number of actions per user",
    "text": "Construct a column containing the total number of actions per user\n\n\nCode\nxid_partition = Window.partitionBy('xid')\n\nn_events = func.count(col('action')).over(xid_partition)\n\ndf = df.withColumn('n_events', n_events)\n\ndf.head(n=2)\n\n\n[Row(xid='0006cea7-1679-4264-bdef-0cd089749ede', action='O', date=datetime.datetime(2016, 12, 26, 13, 41, 8), website_id='51', url='https://www.footlegende.fr/mercato-psg-coutinho-10166', category_id=1002.0, zipcode='34290', device='TAB', n_events=1),\n Row(xid='000893c8-a14b-4f33-858f-210440f37def', action='O', date=datetime.datetime(2016, 12, 23, 16, 18, 37), website_id='56', url='http://blague.dumatin.fr/', category_id=1002.0, zipcode=None, device='DSK', n_events=1)]\n\n\n\n\nCode\n( \n  df\n    .groupBy('xid')\n    .agg(func.count('action'))\n    .head(5)\n)\n\n\n[Row(xid='001c4a21-52c6-4890-b6ce-2b9d4ba06a56', count(action)=1),\n Row(xid='0024344b-7ee2-4fcd-a0b4-bec26d8c8b0e', count(action)=4),\n Row(xid='004564e3-87c1-4e16-ad2c-0e96afc3d617', count(action)=1),\n Row(xid='006d807f-91c3-415a-bb5e-6b9f7e6517a1', count(action)=1),\n Row(xid='006e0463-b24c-4996-84ab-d6d0d65a52aa', count(action)=1)]"
  },
  {
    "objectID": "core/notebooks/notebook08_webdata-II.html#construct-a-column-containing-the-number-of-days-since-the-last-action-of-the-user",
    "href": "core/notebooks/notebook08_webdata-II.html#construct-a-column-containing-the-number-of-days-since-the-last-action-of-the-user",
    "title": "Using with pyspark for data preprocessing",
    "section": "Construct a column containing the number of days since the last action of the user",
    "text": "Construct a column containing the number of days since the last action of the user\n\n\nCode\nmax_date = (\n  func\n    .max(col('date'))\n    .over(xid_partition)\n)\n\nn_days_since_last_event = func.datediff(func.current_date(), max_date)\n\ndf = df.withColumn('n_days_since_last_event',\n                   n_days_since_last_event)\n\ndf.head(n=2)\n\n\n[Row(xid='0006cea7-1679-4264-bdef-0cd089749ede', action='O', date=datetime.datetime(2016, 12, 26, 13, 41, 8), website_id='51', url='https://www.footlegende.fr/mercato-psg-coutinho-10166', category_id=1002.0, zipcode='34290', device='TAB', n_events=1, n_days_since_last_event=3020),\n Row(xid='000893c8-a14b-4f33-858f-210440f37def', action='O', date=datetime.datetime(2016, 12, 23, 16, 18, 37), website_id='56', url='http://blague.dumatin.fr/', category_id=1002.0, zipcode=None, device='DSK', n_events=1, n_days_since_last_event=3023)]\n\n\n\n\nCode\ndf.printSchema()\n\n\nroot\n |-- xid: string (nullable = true)\n |-- action: string (nullable = true)\n |-- date: timestamp (nullable = true)\n |-- website_id: string (nullable = true)\n |-- url: string (nullable = true)\n |-- category_id: float (nullable = true)\n |-- zipcode: string (nullable = true)\n |-- device: string (nullable = true)\n |-- n_events: long (nullable = false)\n |-- n_days_since_last_event: integer (nullable = true)"
  },
  {
    "objectID": "core/notebooks/notebook08_webdata-II.html#construct-a-column-containing-the-number-of-actions-of-each-user-for-each-modality-of-device",
    "href": "core/notebooks/notebook08_webdata-II.html#construct-a-column-containing-the-number-of-actions-of-each-user-for-each-modality-of-device",
    "title": "Using with pyspark for data preprocessing",
    "section": "Construct a column containing the number of actions of each user for each modality of device",
    "text": "Construct a column containing the number of actions of each user for each modality of device\nDoes this partitionBy triggers shuffling?\n\n\nCode\nxid_device_partition = xid_partition.partitionBy('device')\n\nn_events_per_device = func.count(col('action')).over(xid_device_partition)\n\ndf = df.withColumn('n_events_per_device', n_events_per_device)\n\ndf.head(n=2)\n\n\n[Stage 19:==&gt;                                                     (1 + 20) / 22]                                                                                \n\n\n[Row(xid='000893c8-a14b-4f33-858f-210440f37def', action='O', date=datetime.datetime(2016, 12, 23, 16, 18, 37), website_id='56', url='http://blague.dumatin.fr/', category_id=1002.0, zipcode=None, device='DSK', n_events=1, n_days_since_last_event=3023, n_events_per_device=1021837),\n Row(xid='0008c5d2-c263-4b55-ae7d-82c4bf566cc4', action='O', date=datetime.datetime(2017, 1, 16, 4, 26, 21), website_id='74', url='http://www.realite-virtuelle.com/meilleure-videos-360-vr', category_id=1002.0, zipcode=None, device='DSK', n_events=1, n_days_since_last_event=2999, n_events_per_device=1021837)]"
  },
  {
    "objectID": "core/notebooks/notebook08_webdata-II.html#number-of-devices-per-user",
    "href": "core/notebooks/notebook08_webdata-II.html#number-of-devices-per-user",
    "title": "Using with pyspark for data preprocessing",
    "section": "Number of devices per user ",
    "text": "Number of devices per user \n\n\nCode\n# xid_partition = Window.partitionBy('xid')\n\nrank_device = (\n  func\n    .dense_rank()\n    .over(xid_partition.orderBy('device'))\n)\n\nn_unique_device = (\n    func\n      .last(rank_device)\n      .over(xid_partition)\n)\n\ndf = df.withColumn('n_device', n_unique_device)\n\ndf.head(n=2)\n\n\n[Stage 28:======================================&gt;                   (2 + 1) / 3]                                                                                \n\n\n[Row(xid='0006cea7-1679-4264-bdef-0cd089749ede', action='O', date=datetime.datetime(2016, 12, 26, 13, 41, 8), website_id='51', url='https://www.footlegende.fr/mercato-psg-coutinho-10166', category_id=1002.0, zipcode='34290', device='TAB', n_events=1, n_days_since_last_event=3020, n_events_per_device=132013, n_device=1),\n Row(xid='000893c8-a14b-4f33-858f-210440f37def', action='O', date=datetime.datetime(2016, 12, 23, 16, 18, 37), website_id='56', url='http://blague.dumatin.fr/', category_id=1002.0, zipcode=None, device='DSK', n_events=1, n_days_since_last_event=3023, n_events_per_device=1021837, n_device=1)]\n\n\n\n\nCode\ndf\\\n    .where(col('n_device') &gt; 1)\\\n    .select('xid', 'device', 'n_events',  'n_device', 'n_events_per_device')\\\n    .head(n=8)\n\n\n[Row(xid='4c1dc79d-a140-4da9-ae28-540b4503c3b8', device='DSK', n_events=6, n_device=2, n_events_per_device=1021837),\n Row(xid='4c1dc79d-a140-4da9-ae28-540b4503c3b8', device='DSK', n_events=6, n_device=2, n_events_per_device=1021837),\n Row(xid='4c1dc79d-a140-4da9-ae28-540b4503c3b8', device='DSK', n_events=6, n_device=2, n_events_per_device=1021837),\n Row(xid='4c1dc79d-a140-4da9-ae28-540b4503c3b8', device='DSK', n_events=6, n_device=2, n_events_per_device=1021837),\n Row(xid='4c1dc79d-a140-4da9-ae28-540b4503c3b8', device='DSK', n_events=6, n_device=2, n_events_per_device=1021837),\n Row(xid='4c1dc79d-a140-4da9-ae28-540b4503c3b8', device='MOB', n_events=6, n_device=2, n_events_per_device=1564),\n Row(xid='78156cdf-7229-46eb-bb6b-92d384f9a6fa', device='DSK', n_events=6, n_device=2, n_events_per_device=1021837),\n Row(xid='78156cdf-7229-46eb-bb6b-92d384f9a6fa', device='DSK', n_events=6, n_device=2, n_events_per_device=1021837)]\n\n\n\n\nCode\ndf\\\n    .where(col('n_device') &gt; 1)\\\n    .select('xid', 'device', 'n_events',  'n_device', 'n_events_per_device')\\\n    .count()\n\n\n3153"
  },
  {
    "objectID": "core/notebooks/notebook08_webdata-II.html#extraction",
    "href": "core/notebooks/notebook08_webdata-II.html#extraction",
    "title": "Using with pyspark for data preprocessing",
    "section": "Extraction",
    "text": "Extraction\nHere extraction is just about reading the data\n\n\nCode\ndf = spark.read.parquet(input_file)\ndf.head(n=3)\n\n\n[Row(xid='001ff9b6-5383-4221-812d-58c2c3f234cc', action='O', date=datetime.datetime(2017, 1, 25, 7, 2, 18), website_id='3', url='http://www.8chances.com/grille', category_id=1002.0, zipcode='11370', device='SMP'),\n Row(xid='0056ab7a-3cba-4ed5-a495-3d4abf79ab66', action='O', date=datetime.datetime(2016, 12, 28, 9, 47, 8), website_id='54', url='http://www.salaire-brut-en-net.fr/differences-brut-net/', category_id=1002.0, zipcode='86000', device='DSK'),\n Row(xid='005ae4ab-363a-41a0-b8f9-faee47d622a4', action='O', date=datetime.datetime(2017, 1, 27, 22, 21, 6), website_id='74', url='http://www.realite-virtuelle.com/top-applications-horreur-vr-halloween', category_id=1002.0, zipcode='49700', device='DSK')]"
  },
  {
    "objectID": "core/notebooks/notebook08_webdata-II.html#transformation-of-the-data",
    "href": "core/notebooks/notebook08_webdata-II.html#transformation-of-the-data",
    "title": "Using with pyspark for data preprocessing",
    "section": "Transformation of the data",
    "text": "Transformation of the data\nAt this step we compute a lot of extra things from the data. The aim is to build features that describe users.\n\n\nCode\ndef n_events_transformer(df):\n    xid_partition = Window.partitionBy('xid')\n    n_events = func.count(col('action')).over(xid_partition)\n    \n    df = df.withColumn('n_events', n_events)\n\n    return df\n\n\n\n\nCode\ndef n_events_per_action_transformer(df):\n    xid_action_partition = Window.partitionBy('xid', 'action')\n    n_events_per_action = func.count(col('action')).over(xid_action_partition)\n\n    df = df.withColumn('n_events_per_action', n_events_per_action)\n    \n    return df\n\n\n\n\nCode\ndef hour_transformer(df):\n    hour = func.hour(col('date'))\n    df = df.withColumn('hour', hour)\n    return df\n\ndef weekday_transformer(df):\n    weekday = func.date_format(col('date'), 'EEEE')\n    df = df.withColumn('weekday', weekday)\n    return df\n\ndef n_events_per_hour_transformer(df):\n    xid_hour_partition = Window.partitionBy('xid', 'hour')\n    n_events_per_hour = func.count(col('action')).over(xid_hour_partition)\n    df = df.withColumn('n_events_per_hour', n_events_per_hour)\n    return df\n\ndef n_events_per_weekday_transformer(df):\n    xid_weekday_partition = Window.partitionBy('xid', 'weekday')\n    n_events_per_weekday = func.count(col('action')).over(xid_weekday_partition)\n    df = df.withColumn('n_events_per_weekday', n_events_per_weekday)\n    return df\n\ndef n_days_since_last_event_transformer(df):\n    xid_partition = Window.partitionBy('xid')\n    max_date = func.max(col('date')).over(xid_partition)\n    n_days_since_last_event = func.datediff(func.current_date(), max_date)\n    df = df.withColumn('n_days_since_last_event',\n                       n_days_since_last_event + lit(0.1))\n    return df\n\ndef n_days_since_last_action_transformer(df):\n    xid_partition_action = Window.partitionBy('xid', 'action')\n    max_date = func.max(col('date')).over(xid_partition_action)\n    n_days_since_last_action = func.datediff(func.current_date(),\n                                                        max_date)\n    df = df.withColumn('n_days_since_last_action',\n                       n_days_since_last_action + lit(0.1))\n    return df\n\ndef n_unique_day_transformer(df):\n    xid_partition = Window.partitionBy('xid')\n    dayofyear = func.dayofyear(col('date'))\n    rank_day = func.dense_rank().over(xid_partition.orderBy(dayofyear))\n    n_unique_day = func.last(rank_day).over(xid_partition)\n    df = df.withColumn('n_unique_day', n_unique_day)\n    return df\n\ndef n_unique_hour_transformer(df):\n    xid_partition = Window.partitionBy('xid')\n    rank_hour = func.dense_rank().over(xid_partition.orderBy('hour'))\n    n_unique_hour = func.last(rank_hour).over(xid_partition)\n    df = df.withColumn('n_unique_hour', n_unique_hour)\n    return df\n\ndef n_events_per_device_transformer(df):\n    xid_device_partition = Window.partitionBy('xid', 'device')\n    n_events_per_device = func.count(func.col('device')) \\\n        .over(xid_device_partition)\n    df = df.withColumn('n_events_per_device', n_events_per_device)\n    return df\n\ndef n_unique_device_transformer(df):\n    xid_partition = Window.partitionBy('xid')\n    rank_device = func.dense_rank().over(xid_partition.orderBy('device'))\n    n_unique_device = func.last(rank_device).over(xid_partition)\n    df = df.withColumn('n_device', n_unique_device)\n    return df\n\ndef n_actions_per_category_id_transformer(df):\n    xid_category_id_partition = Window.partitionBy('xid', 'category_id',\n                                                   'action')\n    n_actions_per_category_id = func.count(func.col('action')) \\\n        .over(xid_category_id_partition)\n    df = df.withColumn('n_actions_per_category_id', n_actions_per_category_id)\n    return df\n\ndef n_unique_category_id_transformer(df):\n    xid_partition = Window.partitionBy('xid')\n    rank_category_id = func.dense_rank().over(xid_partition\\\n                                              .orderBy('category_id'))\n    n_unique_category_id = func.last(rank_category_id).over(xid_partition)\n    df = df.withColumn('n_unique_category_id', n_unique_category_id)\n    return df\n\ndef n_events_per_category_id_transformer(df):\n    xid_category_id_partition = Window.partitionBy('xid', 'category_id')\n    n_events_per_category_id = func.count(func.col('action')) \\\n        .over(xid_category_id_partition)\n    df = df.withColumn('n_events_per_category_id', n_events_per_category_id)\n    return df\n\ndef n_events_per_website_id_transformer(df):\n    xid_website_id_partition = Window.partitionBy('xid', 'website_id')\n    n_events_per_website_id = func.count(col('action'))\\\n        .over(xid_website_id_partition)\n    df = df.withColumn('n_events_per_website_id', n_events_per_website_id)\n    return df\n\n\n\n\nCode\ntransformers = [\n    hour_transformer,\n    weekday_transformer,\n    n_events_per_hour_transformer,\n    n_events_per_weekday_transformer,\n    n_days_since_last_event_transformer,\n    n_days_since_last_action_transformer,\n    n_unique_day_transformer,\n    n_unique_hour_transformer,\n    n_events_per_device_transformer,\n    n_unique_device_transformer,\n    n_actions_per_category_id_transformer,\n    n_events_per_category_id_transformer,\n    n_events_per_website_id_transformer,\n]\n\n\n\n\nCode\nN = 10000\n\n\n\n\nCode\nsample_df = df.sample(withReplacement=False, fraction=.05)\n\n\n\n\nCode\nsample_df.count()\n\n\n58815\n\n\n\n\nCode\nfor transformer in transformers:\n    df = transformer(df)\n\ndf.head(n=1)\n\n\n[Stage 54:===================================================&gt;    (11 + 1) / 12][Stage 56:&gt;                                                         (0 + 1) / 1]                                                                                \n\n\n[Row(xid='0006cea7-1679-4264-bdef-0cd089749ede', action='O', date=datetime.datetime(2016, 12, 26, 13, 41, 8), website_id='51', url='https://www.footlegende.fr/mercato-psg-coutinho-10166', category_id=1002.0, zipcode='34290', device='TAB', hour=13, weekday='Monday', n_events_per_hour=1, n_events_per_weekday=1, n_days_since_last_event=3020.1, n_days_since_last_action=3020.1, n_unique_day=1, n_unique_hour=1, n_events_per_device=1, n_device=1, n_actions_per_category_id=1, n_events_per_category_id=1, n_events_per_website_id=1)]\n\n\n\n\nCode\nfor transformer in transformers:\n    sample_df = transformer(sample_df)\n\nsample_df.head(n=1)\n\n\n[Row(xid='0004d0b7-a7fd-44ca-accc-422b62de4436', action='O', date=datetime.datetime(2017, 1, 19, 8, 12, 31), website_id='3', url='http://www.8chances.com/grille', category_id=1002.0, zipcode='78480', device='DSK', hour=8, weekday='Thursday', n_events_per_hour=2, n_events_per_weekday=2, n_days_since_last_event=2996.1, n_days_since_last_action=2996.1, n_unique_day=1, n_unique_hour=1, n_events_per_device=2, n_device=1, n_actions_per_category_id=2, n_events_per_category_id=2, n_events_per_website_id=2)]\n\n\n\n\nCode\ndf = sample_df\n\n\n\n\nCode\nsorted(df.columns)\n\n\n['action',\n 'category_id',\n 'date',\n 'device',\n 'hour',\n 'n_actions_per_category_id',\n 'n_days_since_last_action',\n 'n_days_since_last_event',\n 'n_device',\n 'n_events_per_category_id',\n 'n_events_per_device',\n 'n_events_per_hour',\n 'n_events_per_website_id',\n 'n_events_per_weekday',\n 'n_unique_day',\n 'n_unique_hour',\n 'url',\n 'website_id',\n 'weekday',\n 'xid',\n 'zipcode']\n\n\n\n\nCode\ndf.explain()\n\n\n== Physical Plan ==\nAdaptiveSparkPlan isFinalPlan=false\n+- Window [count(action#298) windowspecdefinition(xid#297, website_id#300, specifiedwindowframe(RowFrame, unboundedpreceding$(), unboundedfollowing$())) AS n_events_per_website_id#841L], [xid#297, website_id#300]\n   +- Sort [xid#297 ASC NULLS FIRST, website_id#300 ASC NULLS FIRST], false, 0\n      +- Project [xid#297, action#298, date#299, website_id#300, url#301, category_id#302, zipcode#303, device#304, hour#602, weekday#612, n_events_per_hour#624L, n_events_per_weekday#637L, n_days_since_last_event#651, n_days_since_last_action#667, n_unique_day#686, n_unique_hour#719, n_events_per_device#747L, n_device#768, n_actions_per_category_id#798L, n_events_per_category_id#819L]\n         +- Window [count(action#298) windowspecdefinition(xid#297, category_id#302, action#298, specifiedwindowframe(RowFrame, unboundedpreceding$(), unboundedfollowing$())) AS n_actions_per_category_id#798L], [xid#297, knownfloatingpointnormalized(normalizenanandzero(category_id#302)), action#298]\n            +- Sort [xid#297 ASC NULLS FIRST, knownfloatingpointnormalized(normalizenanandzero(category_id#302)) ASC NULLS FIRST, action#298 ASC NULLS FIRST], false, 0\n               +- Window [count(action#298) windowspecdefinition(xid#297, category_id#302, specifiedwindowframe(RowFrame, unboundedpreceding$(), unboundedfollowing$())) AS n_events_per_category_id#819L], [xid#297, knownfloatingpointnormalized(normalizenanandzero(category_id#302))]\n                  +- Sort [xid#297 ASC NULLS FIRST, knownfloatingpointnormalized(normalizenanandzero(category_id#302)) ASC NULLS FIRST], false, 0\n                     +- Project [xid#297, action#298, date#299, website_id#300, url#301, category_id#302, zipcode#303, device#304, hour#602, weekday#612, n_events_per_hour#624L, n_events_per_weekday#637L, n_days_since_last_event#651, n_days_since_last_action#667, n_unique_day#686, n_unique_hour#719, n_events_per_device#747L, n_device#768]\n                        +- Window [count(device#304) windowspecdefinition(xid#297, device#304, specifiedwindowframe(RowFrame, unboundedpreceding$(), unboundedfollowing$())) AS n_events_per_device#747L], [xid#297, device#304]\n                           +- Project [xid#297, action#298, date#299, website_id#300, url#301, category_id#302, zipcode#303, device#304, hour#602, weekday#612, n_events_per_hour#624L, n_events_per_weekday#637L, n_days_since_last_event#651, n_days_since_last_action#667, n_unique_day#686, n_unique_hour#719, n_device#768]\n                              +- Window [last(_w0#775, false) windowspecdefinition(xid#297, specifiedwindowframe(RowFrame, unboundedpreceding$(), unboundedfollowing$())) AS n_device#768], [xid#297]\n                                 +- Window [dense_rank(device#304) windowspecdefinition(xid#297, device#304 ASC NULLS FIRST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS _w0#775], [xid#297], [device#304 ASC NULLS FIRST]\n                                    +- Sort [xid#297 ASC NULLS FIRST, device#304 ASC NULLS FIRST], false, 0\n                                       +- Project [xid#297, action#298, date#299, website_id#300, url#301, category_id#302, zipcode#303, device#304, hour#602, weekday#612, n_events_per_hour#624L, n_events_per_weekday#637L, n_days_since_last_event#651, n_days_since_last_action#667, n_unique_day#686, n_unique_hour#719]\n                                          +- Window [last(_w0#726, false) windowspecdefinition(xid#297, specifiedwindowframe(RowFrame, unboundedpreceding$(), unboundedfollowing$())) AS n_unique_hour#719], [xid#297]\n                                             +- Window [dense_rank(hour#602) windowspecdefinition(xid#297, hour#602 ASC NULLS FIRST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS _w0#726], [xid#297], [hour#602 ASC NULLS FIRST]\n                                                +- Sort [xid#297 ASC NULLS FIRST, hour#602 ASC NULLS FIRST], false, 0\n                                                   +- Project [xid#297, action#298, date#299, website_id#300, url#301, category_id#302, zipcode#303, device#304, hour#602, weekday#612, n_events_per_hour#624L, n_events_per_weekday#637L, n_days_since_last_event#651, n_days_since_last_action#667, n_unique_day#686]\n                                                      +- Window [last(_w0#693, false) windowspecdefinition(xid#297, specifiedwindowframe(RowFrame, unboundedpreceding$(), unboundedfollowing$())) AS n_unique_day#686], [xid#297]\n                                                         +- Project [xid#297, action#298, date#299, website_id#300, url#301, category_id#302, zipcode#303, device#304, hour#602, weekday#612, n_events_per_hour#624L, n_events_per_weekday#637L, n_days_since_last_event#651, n_days_since_last_action#667, _w0#693]\n                                                            +- Window [dense_rank(_w0#694) windowspecdefinition(xid#297, _w0#694 ASC NULLS FIRST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS _w0#693], [xid#297], [_w0#694 ASC NULLS FIRST]\n                                                               +- Sort [xid#297 ASC NULLS FIRST, _w0#694 ASC NULLS FIRST], false, 0\n                                                                  +- Project [xid#297, action#298, date#299, website_id#300, url#301, category_id#302, zipcode#303, device#304, hour#602, weekday#612, n_events_per_hour#624L, n_events_per_weekday#637L, n_days_since_last_event#651, (cast(datediff(2025-04-03, cast(_we0#668 as date)) as double) + 0.1) AS n_days_since_last_action#667, dayofyear(cast(date#299 as date)) AS _w0#694]\n                                                                     +- Window [max(date#299) windowspecdefinition(xid#297, action#298, specifiedwindowframe(RowFrame, unboundedpreceding$(), unboundedfollowing$())) AS _we0#668], [xid#297, action#298]\n                                                                        +- Sort [xid#297 ASC NULLS FIRST, action#298 ASC NULLS FIRST], false, 0\n                                                                           +- Project [xid#297, action#298, date#299, website_id#300, url#301, category_id#302, zipcode#303, device#304, hour#602, weekday#612, n_events_per_hour#624L, n_events_per_weekday#637L, (cast(datediff(2025-04-03, cast(_we0#652 as date)) as double) + 0.1) AS n_days_since_last_event#651]\n                                                                              +- Window [count(action#298) windowspecdefinition(xid#297, weekday#612, specifiedwindowframe(RowFrame, unboundedpreceding$(), unboundedfollowing$())) AS n_events_per_weekday#637L], [xid#297, weekday#612]\n                                                                                 +- Sort [xid#297 ASC NULLS FIRST, weekday#612 ASC NULLS FIRST], false, 0\n                                                                                    +- Project [xid#297, action#298, date#299, website_id#300, url#301, category_id#302, zipcode#303, device#304, hour#602, weekday#612, n_events_per_hour#624L, _we0#652]\n                                                                                       +- Window [count(action#298) windowspecdefinition(xid#297, hour#602, specifiedwindowframe(RowFrame, unboundedpreceding$(), unboundedfollowing$())) AS n_events_per_hour#624L], [xid#297, hour#602]\n                                                                                          +- Sort [xid#297 ASC NULLS FIRST, hour#602 ASC NULLS FIRST], false, 0\n                                                                                             +- Window [max(date#299) windowspecdefinition(xid#297, specifiedwindowframe(RowFrame, unboundedpreceding$(), unboundedfollowing$())) AS _we0#652], [xid#297]\n                                                                                                +- Sort [xid#297 ASC NULLS FIRST], false, 0\n                                                                                                   +- Exchange hashpartitioning(xid#297, 200), ENSURE_REQUIREMENTS, [plan_id=2113]\n                                                                                                      +- Project [xid#297, action#298, date#299, website_id#300, url#301, category_id#302, zipcode#303, device#304, hour(date#299, Some(Europe/Paris)) AS hour#602, date_format(date#299, EEEE, Some(Europe/Paris)) AS weekday#612]\n                                                                                                         +- Sample 0.0, 0.05, false, 7061523513992374299\n                                                                                                            +- FileScan parquet [xid#297,action#298,date#299,website_id#300,url#301,category_id#302,zipcode#303,device#304] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/home/boucheron/sandbox/IFEBY310/core/notebooks/webdata.parquet], PartitionFilters: [], PushedFilters: [], ReadSchema: struct&lt;xid:string,action:string,date:timestamp,website_id:string,url:string,category_id:float,zip...\n\n\n\n\n\n\nCode\nspark._sc.setCheckpointDir(\".\")   \n\ndf.checkpoint()\n\n\nDataFrame[xid: string, action: string, date: timestamp, website_id: string, url: string, category_id: float, zipcode: string, device: string, hour: int, weekday: string, n_events_per_hour: bigint, n_events_per_weekday: bigint, n_days_since_last_event: double, n_days_since_last_action: double, n_unique_day: int, n_unique_hour: int, n_events_per_device: bigint, n_device: int, n_actions_per_category_id: bigint, n_events_per_category_id: bigint, n_events_per_website_id: bigint]\n\n\n\n\nCode\ndf.explain()\n\n\n== Physical Plan ==\nAdaptiveSparkPlan isFinalPlan=true\n+- == Final Plan ==\n   Window [count(action#298) windowspecdefinition(xid#297, website_id#300, specifiedwindowframe(RowFrame, unboundedpreceding$(), unboundedfollowing$())) AS n_events_per_website_id#841L], [xid#297, website_id#300]\n   +- *(13) Sort [xid#297 ASC NULLS FIRST, website_id#300 ASC NULLS FIRST], false, 0\n      +- *(13) Project [xid#297, action#298, date#299, website_id#300, url#301, category_id#302, zipcode#303, device#304, hour#602, weekday#612, n_events_per_hour#624L, n_events_per_weekday#637L, n_days_since_last_event#651, n_days_since_last_action#667, n_unique_day#686, n_unique_hour#719, n_events_per_device#747L, n_device#768, n_actions_per_category_id#798L, n_events_per_category_id#819L]\n         +- Window [count(action#298) windowspecdefinition(xid#297, category_id#302, action#298, specifiedwindowframe(RowFrame, unboundedpreceding$(), unboundedfollowing$())) AS n_actions_per_category_id#798L], [xid#297, knownfloatingpointnormalized(normalizenanandzero(category_id#302)), action#298]\n            +- *(12) Sort [xid#297 ASC NULLS FIRST, knownfloatingpointnormalized(normalizenanandzero(category_id#302)) ASC NULLS FIRST, action#298 ASC NULLS FIRST], false, 0\n               +- Window [count(action#298) windowspecdefinition(xid#297, category_id#302, specifiedwindowframe(RowFrame, unboundedpreceding$(), unboundedfollowing$())) AS n_events_per_category_id#819L], [xid#297, knownfloatingpointnormalized(normalizenanandzero(category_id#302))]\n                  +- *(11) Sort [xid#297 ASC NULLS FIRST, knownfloatingpointnormalized(normalizenanandzero(category_id#302)) ASC NULLS FIRST], false, 0\n                     +- *(11) Project [xid#297, action#298, date#299, website_id#300, url#301, category_id#302, zipcode#303, device#304, hour#602, weekday#612, n_events_per_hour#624L, n_events_per_weekday#637L, n_days_since_last_event#651, n_days_since_last_action#667, n_unique_day#686, n_unique_hour#719, n_events_per_device#747L, n_device#768]\n                        +- Window [count(device#304) windowspecdefinition(xid#297, device#304, specifiedwindowframe(RowFrame, unboundedpreceding$(), unboundedfollowing$())) AS n_events_per_device#747L], [xid#297, device#304]\n                           +- *(10) Project [xid#297, action#298, date#299, website_id#300, url#301, category_id#302, zipcode#303, device#304, hour#602, weekday#612, n_events_per_hour#624L, n_events_per_weekday#637L, n_days_since_last_event#651, n_days_since_last_action#667, n_unique_day#686, n_unique_hour#719, n_device#768]\n                              +- Window [last(_w0#775, false) windowspecdefinition(xid#297, specifiedwindowframe(RowFrame, unboundedpreceding$(), unboundedfollowing$())) AS n_device#768], [xid#297]\n                                 +- Window [dense_rank(device#304) windowspecdefinition(xid#297, device#304 ASC NULLS FIRST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS _w0#775], [xid#297], [device#304 ASC NULLS FIRST]\n                                    +- *(9) Sort [xid#297 ASC NULLS FIRST, device#304 ASC NULLS FIRST], false, 0\n                                       +- *(9) Project [xid#297, action#298, date#299, website_id#300, url#301, category_id#302, zipcode#303, device#304, hour#602, weekday#612, n_events_per_hour#624L, n_events_per_weekday#637L, n_days_since_last_event#651, n_days_since_last_action#667, n_unique_day#686, n_unique_hour#719]\n                                          +- Window [last(_w0#726, false) windowspecdefinition(xid#297, specifiedwindowframe(RowFrame, unboundedpreceding$(), unboundedfollowing$())) AS n_unique_hour#719], [xid#297]\n                                             +- Window [dense_rank(hour#602) windowspecdefinition(xid#297, hour#602 ASC NULLS FIRST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS _w0#726], [xid#297], [hour#602 ASC NULLS FIRST]\n                                                +- *(8) Sort [xid#297 ASC NULLS FIRST, hour#602 ASC NULLS FIRST], false, 0\n                                                   +- *(8) Project [xid#297, action#298, date#299, website_id#300, url#301, category_id#302, zipcode#303, device#304, hour#602, weekday#612, n_events_per_hour#624L, n_events_per_weekday#637L, n_days_since_last_event#651, n_days_since_last_action#667, n_unique_day#686]\n                                                      +- Window [last(_w0#693, false) windowspecdefinition(xid#297, specifiedwindowframe(RowFrame, unboundedpreceding$(), unboundedfollowing$())) AS n_unique_day#686], [xid#297]\n                                                         +- *(7) Project [xid#297, action#298, date#299, website_id#300, url#301, category_id#302, zipcode#303, device#304, hour#602, weekday#612, n_events_per_hour#624L, n_events_per_weekday#637L, n_days_since_last_event#651, n_days_since_last_action#667, _w0#693]\n                                                            +- Window [dense_rank(_w0#694) windowspecdefinition(xid#297, _w0#694 ASC NULLS FIRST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS _w0#693], [xid#297], [_w0#694 ASC NULLS FIRST]\n                                                               +- *(6) Sort [xid#297 ASC NULLS FIRST, _w0#694 ASC NULLS FIRST], false, 0\n                                                                  +- *(6) Project [xid#297, action#298, date#299, website_id#300, url#301, category_id#302, zipcode#303, device#304, hour#602, weekday#612, n_events_per_hour#624L, n_events_per_weekday#637L, n_days_since_last_event#651, (cast(datediff(2025-04-03, cast(_we0#668 as date)) as double) + 0.1) AS n_days_since_last_action#667, dayofyear(cast(date#299 as date)) AS _w0#694]\n                                                                     +- Window [max(date#299) windowspecdefinition(xid#297, action#298, specifiedwindowframe(RowFrame, unboundedpreceding$(), unboundedfollowing$())) AS _we0#668], [xid#297, action#298]\n                                                                        +- *(5) Sort [xid#297 ASC NULLS FIRST, action#298 ASC NULLS FIRST], false, 0\n                                                                           +- *(5) Project [xid#297, action#298, date#299, website_id#300, url#301, category_id#302, zipcode#303, device#304, hour#602, weekday#612, n_events_per_hour#624L, n_events_per_weekday#637L, (cast(datediff(2025-04-03, cast(_we0#652 as date)) as double) + 0.1) AS n_days_since_last_event#651]\n                                                                              +- Window [count(action#298) windowspecdefinition(xid#297, weekday#612, specifiedwindowframe(RowFrame, unboundedpreceding$(), unboundedfollowing$())) AS n_events_per_weekday#637L], [xid#297, weekday#612]\n                                                                                 +- *(4) Sort [xid#297 ASC NULLS FIRST, weekday#612 ASC NULLS FIRST], false, 0\n                                                                                    +- *(4) Project [xid#297, action#298, date#299, website_id#300, url#301, category_id#302, zipcode#303, device#304, hour#602, weekday#612, n_events_per_hour#624L, _we0#652]\n                                                                                       +- Window [count(action#298) windowspecdefinition(xid#297, hour#602, specifiedwindowframe(RowFrame, unboundedpreceding$(), unboundedfollowing$())) AS n_events_per_hour#624L], [xid#297, hour#602]\n                                                                                          +- *(3) Sort [xid#297 ASC NULLS FIRST, hour#602 ASC NULLS FIRST], false, 0\n                                                                                             +- Window [max(date#299) windowspecdefinition(xid#297, specifiedwindowframe(RowFrame, unboundedpreceding$(), unboundedfollowing$())) AS _we0#652], [xid#297]\n                                                                                                +- *(2) Sort [xid#297 ASC NULLS FIRST], false, 0\n                                                                                                   +- AQEShuffleRead coalesced\n                                                                                                      +- ShuffleQueryStage 0\n                                                                                                         +- Exchange hashpartitioning(xid#297, 200), ENSURE_REQUIREMENTS, [plan_id=2171]\n                                                                                                            +- *(1) Project [xid#297, action#298, date#299, website_id#300, url#301, category_id#302, zipcode#303, device#304, hour(date#299, Some(Europe/Paris)) AS hour#602, date_format(date#299, EEEE, Some(Europe/Paris)) AS weekday#612]\n                                                                                                               +- *(1) Sample 0.0, 0.05, false, 7061523513992374299\n                                                                                                                  +- *(1) ColumnarToRow\n                                                                                                                     +- FileScan parquet [xid#297,action#298,date#299,website_id#300,url#301,category_id#302,zipcode#303,device#304] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/home/boucheron/sandbox/IFEBY310/core/notebooks/webdata.parquet], PartitionFilters: [], PushedFilters: [], ReadSchema: struct&lt;xid:string,action:string,date:timestamp,website_id:string,url:string,category_id:float,zip...\n+- == Initial Plan ==\n   Window [count(action#298) windowspecdefinition(xid#297, website_id#300, specifiedwindowframe(RowFrame, unboundedpreceding$(), unboundedfollowing$())) AS n_events_per_website_id#841L], [xid#297, website_id#300]\n   +- Sort [xid#297 ASC NULLS FIRST, website_id#300 ASC NULLS FIRST], false, 0\n      +- Project [xid#297, action#298, date#299, website_id#300, url#301, category_id#302, zipcode#303, device#304, hour#602, weekday#612, n_events_per_hour#624L, n_events_per_weekday#637L, n_days_since_last_event#651, n_days_since_last_action#667, n_unique_day#686, n_unique_hour#719, n_events_per_device#747L, n_device#768, n_actions_per_category_id#798L, n_events_per_category_id#819L]\n         +- Window [count(action#298) windowspecdefinition(xid#297, category_id#302, action#298, specifiedwindowframe(RowFrame, unboundedpreceding$(), unboundedfollowing$())) AS n_actions_per_category_id#798L], [xid#297, knownfloatingpointnormalized(normalizenanandzero(category_id#302)), action#298]\n            +- Sort [xid#297 ASC NULLS FIRST, knownfloatingpointnormalized(normalizenanandzero(category_id#302)) ASC NULLS FIRST, action#298 ASC NULLS FIRST], false, 0\n               +- Window [count(action#298) windowspecdefinition(xid#297, category_id#302, specifiedwindowframe(RowFrame, unboundedpreceding$(), unboundedfollowing$())) AS n_events_per_category_id#819L], [xid#297, knownfloatingpointnormalized(normalizenanandzero(category_id#302))]\n                  +- Sort [xid#297 ASC NULLS FIRST, knownfloatingpointnormalized(normalizenanandzero(category_id#302)) ASC NULLS FIRST], false, 0\n                     +- Project [xid#297, action#298, date#299, website_id#300, url#301, category_id#302, zipcode#303, device#304, hour#602, weekday#612, n_events_per_hour#624L, n_events_per_weekday#637L, n_days_since_last_event#651, n_days_since_last_action#667, n_unique_day#686, n_unique_hour#719, n_events_per_device#747L, n_device#768]\n                        +- Window [count(device#304) windowspecdefinition(xid#297, device#304, specifiedwindowframe(RowFrame, unboundedpreceding$(), unboundedfollowing$())) AS n_events_per_device#747L], [xid#297, device#304]\n                           +- Project [xid#297, action#298, date#299, website_id#300, url#301, category_id#302, zipcode#303, device#304, hour#602, weekday#612, n_events_per_hour#624L, n_events_per_weekday#637L, n_days_since_last_event#651, n_days_since_last_action#667, n_unique_day#686, n_unique_hour#719, n_device#768]\n                              +- Window [last(_w0#775, false) windowspecdefinition(xid#297, specifiedwindowframe(RowFrame, unboundedpreceding$(), unboundedfollowing$())) AS n_device#768], [xid#297]\n                                 +- Window [dense_rank(device#304) windowspecdefinition(xid#297, device#304 ASC NULLS FIRST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS _w0#775], [xid#297], [device#304 ASC NULLS FIRST]\n                                    +- Sort [xid#297 ASC NULLS FIRST, device#304 ASC NULLS FIRST], false, 0\n                                       +- Project [xid#297, action#298, date#299, website_id#300, url#301, category_id#302, zipcode#303, device#304, hour#602, weekday#612, n_events_per_hour#624L, n_events_per_weekday#637L, n_days_since_last_event#651, n_days_since_last_action#667, n_unique_day#686, n_unique_hour#719]\n                                          +- Window [last(_w0#726, false) windowspecdefinition(xid#297, specifiedwindowframe(RowFrame, unboundedpreceding$(), unboundedfollowing$())) AS n_unique_hour#719], [xid#297]\n                                             +- Window [dense_rank(hour#602) windowspecdefinition(xid#297, hour#602 ASC NULLS FIRST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS _w0#726], [xid#297], [hour#602 ASC NULLS FIRST]\n                                                +- Sort [xid#297 ASC NULLS FIRST, hour#602 ASC NULLS FIRST], false, 0\n                                                   +- Project [xid#297, action#298, date#299, website_id#300, url#301, category_id#302, zipcode#303, device#304, hour#602, weekday#612, n_events_per_hour#624L, n_events_per_weekday#637L, n_days_since_last_event#651, n_days_since_last_action#667, n_unique_day#686]\n                                                      +- Window [last(_w0#693, false) windowspecdefinition(xid#297, specifiedwindowframe(RowFrame, unboundedpreceding$(), unboundedfollowing$())) AS n_unique_day#686], [xid#297]\n                                                         +- Project [xid#297, action#298, date#299, website_id#300, url#301, category_id#302, zipcode#303, device#304, hour#602, weekday#612, n_events_per_hour#624L, n_events_per_weekday#637L, n_days_since_last_event#651, n_days_since_last_action#667, _w0#693]\n                                                            +- Window [dense_rank(_w0#694) windowspecdefinition(xid#297, _w0#694 ASC NULLS FIRST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS _w0#693], [xid#297], [_w0#694 ASC NULLS FIRST]\n                                                               +- Sort [xid#297 ASC NULLS FIRST, _w0#694 ASC NULLS FIRST], false, 0\n                                                                  +- Project [xid#297, action#298, date#299, website_id#300, url#301, category_id#302, zipcode#303, device#304, hour#602, weekday#612, n_events_per_hour#624L, n_events_per_weekday#637L, n_days_since_last_event#651, (cast(datediff(2025-04-03, cast(_we0#668 as date)) as double) + 0.1) AS n_days_since_last_action#667, dayofyear(cast(date#299 as date)) AS _w0#694]\n                                                                     +- Window [max(date#299) windowspecdefinition(xid#297, action#298, specifiedwindowframe(RowFrame, unboundedpreceding$(), unboundedfollowing$())) AS _we0#668], [xid#297, action#298]\n                                                                        +- Sort [xid#297 ASC NULLS FIRST, action#298 ASC NULLS FIRST], false, 0\n                                                                           +- Project [xid#297, action#298, date#299, website_id#300, url#301, category_id#302, zipcode#303, device#304, hour#602, weekday#612, n_events_per_hour#624L, n_events_per_weekday#637L, (cast(datediff(2025-04-03, cast(_we0#652 as date)) as double) + 0.1) AS n_days_since_last_event#651]\n                                                                              +- Window [count(action#298) windowspecdefinition(xid#297, weekday#612, specifiedwindowframe(RowFrame, unboundedpreceding$(), unboundedfollowing$())) AS n_events_per_weekday#637L], [xid#297, weekday#612]\n                                                                                 +- Sort [xid#297 ASC NULLS FIRST, weekday#612 ASC NULLS FIRST], false, 0\n                                                                                    +- Project [xid#297, action#298, date#299, website_id#300, url#301, category_id#302, zipcode#303, device#304, hour#602, weekday#612, n_events_per_hour#624L, _we0#652]\n                                                                                       +- Window [count(action#298) windowspecdefinition(xid#297, hour#602, specifiedwindowframe(RowFrame, unboundedpreceding$(), unboundedfollowing$())) AS n_events_per_hour#624L], [xid#297, hour#602]\n                                                                                          +- Sort [xid#297 ASC NULLS FIRST, hour#602 ASC NULLS FIRST], false, 0\n                                                                                             +- Window [max(date#299) windowspecdefinition(xid#297, specifiedwindowframe(RowFrame, unboundedpreceding$(), unboundedfollowing$())) AS _we0#652], [xid#297]\n                                                                                                +- Sort [xid#297 ASC NULLS FIRST], false, 0\n                                                                                                   +- Exchange hashpartitioning(xid#297, 200), ENSURE_REQUIREMENTS, [plan_id=2113]\n                                                                                                      +- Project [xid#297, action#298, date#299, website_id#300, url#301, category_id#302, zipcode#303, device#304, hour(date#299, Some(Europe/Paris)) AS hour#602, date_format(date#299, EEEE, Some(Europe/Paris)) AS weekday#612]\n                                                                                                         +- Sample 0.0, 0.05, false, 7061523513992374299\n                                                                                                            +- FileScan parquet [xid#297,action#298,date#299,website_id#300,url#301,category_id#302,zipcode#303,device#304] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/home/boucheron/sandbox/IFEBY310/core/notebooks/webdata.parquet], PartitionFilters: [], PushedFilters: [], ReadSchema: struct&lt;xid:string,action:string,date:timestamp,website_id:string,url:string,category_id:float,zip..."
  },
  {
    "objectID": "core/notebooks/notebook08_webdata-II.html#load-step",
    "href": "core/notebooks/notebook08_webdata-II.html#load-step",
    "title": "Using with pyspark for data preprocessing",
    "section": "Load step",
    "text": "Load step\nHere, we use all the previous computations (saved in the columns of the dataframe) to compute aggregated informations about each user.\n\n\n\n\n\n\nNote\n\n\n\nThis should be DRYED\n\n\n\n\nCode\ndef n_events_per_hour_loader(df):\n    csr = df\\\n        .select('xid', 'hour', 'n_events_per_hour')\\\n        .withColumnRenamed('n_events_per_hour', 'value')\\\n        .distinct() \n            # action\n    feature_name = func.concat(lit('n_events_per_hour#'), col('hour'))\n\n    csr = csr\\\n        .withColumn('feature_name', feature_name)\\\n        .drop('hour')\n    return csr\n\ndef n_events_per_website_id_loader(df):\n    csr = df.select('xid', 'website_id', 'n_events_per_website_id')\\\n        .withColumnRenamed('n_events_per_hour', 'value')\\\n        .distinct()\n\n    feature_name = func.concat(lit('n_events_per_website_id#'),\n                               col('website_id'))\n    \n    csr = csr\\\n        .withColumn('feature_name', feature_name)\\\n        .drop('website_id')\n    return csr\n\ndef n_events_per_hour_loader(df):\n    csr = df\\\n        .select('xid', 'hour', 'n_events_per_hour')\\\n        .withColumnRenamed('n_events_per_hour', 'value')\\\n        .distinct()\n\n    feature_name = func.concat(lit('n_events_per_hour#'), col('hour'))\n    \n    csr = csr\\\n        .withColumn('feature_name', feature_name)\\\n        .drop('hour')\n    return csr\n\ndef n_events_per_weekday_loader(df):\n    csr = df\\\n        .select('xid', 'weekday', 'n_events_per_weekday')\\\n        .withColumnRenamed('n_events_per_weekday', 'value')\\\n        .distinct()\n\n    feature_name = func.concat(lit('n_events_per_weekday#'), col('weekday'))\n    \n    csr = csr\\\n        .withColumn('feature_name', feature_name)\\\n        .drop('weekday')\n\n    return csr\n\ndef n_days_since_last_event_loader(df):\n    csr = df.select('xid',  'n_days_since_last_event')\\\n        .withColumnRenamed('n_days_since_last_event', 'value')\\\n        .distinct()\n    feature_name = lit('n_days_since_last_event')\n    csr = csr\\\n        .withColumn('feature_name', feature_name)\n    return csr\n\ndef n_days_since_last_action_loader(df):\n    csr = df.select('xid', 'action', 'n_days_since_last_action')\\\n        .withColumnRenamed('n_days_since_last_action', 'value')\\\n        .distinct()\n    feature_name = func.concat(lit('n_days_since_last_action#'), col('action'))\n    csr = csr\\\n        .withColumn('feature_name', feature_name)\\\n        .drop('action')\n    return csr\n\ndef n_unique_day_loader(df):\n    csr = df.select('xid', 'n_unique_day')\\\n        .withColumnRenamed('n_unique_day', 'value')\\\n        .distinct()\n    feature_name = lit('n_unique_day')\n    csr = csr\\\n        .withColumn('feature_name', feature_name)\n    return csr\n\ndef n_unique_hour_loader(df):\n    csr = df.select('xid', 'n_unique_hour')\\\n        .withColumnRenamed('n_unique_hour', 'value')\\\n        .distinct()\n    feature_name = lit('n_unique_hour')\n    csr = csr\\\n        .withColumn('feature_name', feature_name)\n    return csr\n\ndef n_events_per_device_loader(df):\n    csr = df\\\n        .select('xid', 'device', 'n_events_per_device')\\\n        .withColumnRenamed('n_events_per_device', 'value')\\\n        .distinct()\n    feature_name = func.concat(lit('n_events_per_device#'), col('device'))\n    csr = csr\\\n        .withColumn('feature_name', feature_name)\\\n        .drop('device')\n    return csr\n\ndef n_unique_device_loader(df):\n    csr = df.select('xid', 'n_device')\\\n        .withColumnRenamed('n_device', 'value')\\\n        .distinct()\n    feature_name = lit('n_device')\n    csr = csr\\\n        .withColumn('feature_name', feature_name)\n    return csr\n\ndef n_events_per_category_id_loader(df):\n    csr = df.select('xid', 'category_id', 'n_events_per_category_id')\\\n        .withColumnRenamed('n_events_per_category_id', 'value')\\\n        .distinct()\n    feature_name = func.concat(lit('n_events_per_category_id#'),\n                               col('category_id'))\n    csr = csr\\\n        .withColumn('feature_name', feature_name)\\\n        .drop('category_id')\n    return csr\n\ndef n_actions_per_category_id_loader(df):\n    csr = df.select('xid', 'category_id', 'action', 'n_actions_per_category_id')\\\n        .withColumnRenamed('n_actions_per_category_id', 'value')\\\n        .distinct()\n    feature_name = func.concat(lit('n_actions_per_category_id#'),\n                               col('action'), lit('#'), \n                               col('category_id'))\n    csr = csr\\\n        .withColumn('feature_name', feature_name)\\\n        .drop('category_id')\\\n        .drop('action')\n    return csr\n\ndef n_events_per_website_id_loader(df):\n    csr = df.select('xid', 'website_id', 'n_events_per_website_id')\\\n        .withColumnRenamed('n_events_per_website_id', 'value')\\\n        .distinct()\n    feature_name = func.concat(lit('n_events_per_website_id#'),\n                               col('website_id'))\n    csr = csr\\\n        .withColumn('feature_name', feature_name)\\\n        .drop('website_id')\n    return csr\n\n\n\n\nCode\nfrom functools import reduce\n\n\n\n\nCode\nloaders = [\n    n_events_per_hour_loader,\n    n_events_per_website_id_loader,\n    n_events_per_hour_loader,\n    n_events_per_weekday_loader,\n    n_days_since_last_event_loader,\n    n_days_since_last_action_loader,\n    n_unique_day_loader,\n    n_unique_hour_loader,\n    n_events_per_device_loader,\n    n_unique_device_loader,\n    n_events_per_category_id_loader,\n    n_actions_per_category_id_loader,\n    n_events_per_website_id_loader,\n]\n\n\n\n\nCode\ndef union(df, other):\n    return df.union(other)\n\n\n\n\n\n\n\n\nAbout DataFrame.union()\n\n\n\nThis method performs a SQL-style set union of the rows from both DataFrame objects, with no automatic deduplication of elements.\nUse the distinct() method to perform deduplication of rows.\nThe method resolves columns by position (not by name), following the standard behavior in SQL.\n\n\n\n\nCode\nspam = [loader(df) for loader in loaders]\n\n\n\n\nCode\nspam[0].printSchema()\n\n\nroot\n |-- xid: string (nullable = true)\n |-- value: long (nullable = false)\n |-- feature_name: string (nullable = true)\n\n\n\n\n\nCode\nall(spam[0].columns == it.columns for it in spam[1:])\n\n\nTrue\n\n\n\n\nCode\nlen(spam)\n\n\n13\n\n\n\n\nCode\ncsr = reduce(\n    lambda df1, df2: df1.union(df2),\n    spam\n)\n\ncsr.head(n=3)\n\n\n[Stage 72:========&gt;        (6 + 6) / 12][Stage 73:==&gt;             (2 + 10) / 12]                                                                                \n\n\n[Row(xid='0004d0b7-a7fd-44ca-accc-422b62de4436', value=2.0, feature_name='n_events_per_hour#8'),\n Row(xid='00069fa0-492f-4e54-90e2-ec456dd3b3f8', value=1.0, feature_name='n_events_per_hour#22'),\n Row(xid='000717fa-9076-4df0-b261-2637ce5d2325', value=1.0, feature_name='n_events_per_hour#22')]\n\n\n\n\nCode\ncsr.columns\n\n\n['xid', 'value', 'feature_name']\n\n\n\n\nCode\ncsr.show(5)\n\n\n+--------------------+-----+--------------------+\n|                 xid|value|        feature_name|\n+--------------------+-----+--------------------+\n|0004d0b7-a7fd-44c...|  2.0| n_events_per_hour#8|\n|00069fa0-492f-4e5...|  1.0|n_events_per_hour#22|\n|000717fa-9076-4df...|  1.0|n_events_per_hour#22|\n|00077fd6-ea8c-412...|  1.0|n_events_per_hour#17|\n|0008839b-79ea-4bc...|  1.0|n_events_per_hour#19|\n+--------------------+-----+--------------------+\nonly showing top 5 rows\n\n\n\n\n\nCode\ncsr.rdd.getNumPartitions()\n\n\n17\n\n\n\n\nCode\n# Replace features names and xid by a unique number\nfeature_name_partition = Window().orderBy('feature_name')\n\nxid_partition = Window().orderBy('xid')\n\ncol_idx = func.dense_rank().over(feature_name_partition)\nrow_idx = func.dense_rank().over(xid_partition)\n\n\n\n\nCode\ncsr = csr.withColumn('col', col_idx)\\\n    .withColumn('row', row_idx)\n\ncsr = csr.na.drop('any')\n\ncsr.head(n=5)\n\n\n25/04/03 15:09:49 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n25/04/03 15:09:49 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n25/04/03 15:09:49 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n25/04/03 15:09:49 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n25/04/03 15:09:49 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n25/04/03 15:09:50 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n25/04/03 15:09:50 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n25/04/03 15:09:50 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n25/04/03 15:09:50 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n25/04/03 15:09:50 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n25/04/03 15:09:50 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n25/04/03 15:09:50 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n25/04/03 15:09:50 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n25/04/03 15:09:50 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n25/04/03 15:09:50 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n25/04/03 15:09:50 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n25/04/03 15:09:50 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n25/04/03 15:09:50 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n25/04/03 15:09:50 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n25/04/03 15:09:50 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n25/04/03 15:09:50 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n25/04/03 15:09:50 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n25/04/03 15:09:50 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n25/04/03 15:09:50 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n25/04/03 15:09:50 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n25/04/03 15:09:50 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n25/04/03 15:09:50 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n25/04/03 15:09:50 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n25/04/03 15:09:50 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n25/04/03 15:09:50 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n25/04/03 15:09:50 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n25/04/03 15:09:50 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n25/04/03 15:09:50 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n25/04/03 15:09:50 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n25/04/03 15:09:50 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n25/04/03 15:09:50 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n25/04/03 15:09:50 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n25/04/03 15:09:50 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n25/04/03 15:09:50 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n25/04/03 15:09:50 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n25/04/03 15:09:50 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n25/04/03 15:09:50 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n25/04/03 15:09:50 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n25/04/03 15:09:50 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n25/04/03 15:09:50 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n25/04/03 15:09:51 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n25/04/03 15:09:51 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n25/04/03 15:09:51 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n25/04/03 15:09:51 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n\n\n[Row(xid='00009986-95f1-4f7d-a0f8-2941e2ad330c', value=1.0, feature_name='n_actions_per_category_id#O#1002.0', col=4, row=1),\n Row(xid='00009986-95f1-4f7d-a0f8-2941e2ad330c', value=3019.1, feature_name='n_days_since_last_action#O', col=7, row=1),\n Row(xid='00009986-95f1-4f7d-a0f8-2941e2ad330c', value=3019.1, feature_name='n_days_since_last_event', col=8, row=1),\n Row(xid='00009986-95f1-4f7d-a0f8-2941e2ad330c', value=1.0, feature_name='n_device', col=9, row=1),\n Row(xid='00009986-95f1-4f7d-a0f8-2941e2ad330c', value=1.0, feature_name='n_events_per_category_id#1002.0', col=10, row=1)]\n\n\n\n\nCode\n# Let's save the result of our hard work into a new parquet file\noutput_path = './'\noutput_file = os.path.join(output_path, 'csr.parquet')\ncsr.write.parquet(output_file, mode='overwrite')\n\n\n25/04/03 15:09:52 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n25/04/03 15:09:52 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n25/04/03 15:09:52 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n25/04/03 15:09:52 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n25/04/03 15:09:52 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n25/04/03 15:09:52 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n25/04/03 15:09:52 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n25/04/03 15:09:52 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n25/04/03 15:09:52 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n25/04/03 15:09:52 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n25/04/03 15:09:52 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n25/04/03 15:09:52 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n25/04/03 15:09:52 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n25/04/03 15:09:52 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n25/04/03 15:09:52 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n25/04/03 15:09:52 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n25/04/03 15:09:52 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n25/04/03 15:09:52 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n25/04/03 15:09:52 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n25/04/03 15:09:52 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n25/04/03 15:09:52 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n25/04/03 15:09:52 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n25/04/03 15:09:52 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n25/04/03 15:09:52 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n25/04/03 15:09:52 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n25/04/03 15:09:52 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n25/04/03 15:09:52 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n25/04/03 15:09:52 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n25/04/03 15:09:52 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n25/04/03 15:09:52 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n25/04/03 15:09:52 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n25/04/03 15:09:52 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n25/04/03 15:09:52 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n25/04/03 15:09:52 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n25/04/03 15:09:52 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n25/04/03 15:09:52 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n25/04/03 15:09:52 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n25/04/03 15:09:52 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n25/04/03 15:09:52 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n25/04/03 15:09:52 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n25/04/03 15:09:52 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n25/04/03 15:09:53 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n25/04/03 15:09:53 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n25/04/03 15:09:53 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n25/04/03 15:09:53 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n25/04/03 15:09:53 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n25/04/03 15:09:53 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n25/04/03 15:09:53 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n25/04/03 15:09:53 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n25/04/03 15:09:53 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n25/04/03 15:09:53 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n25/04/03 15:09:53 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n25/04/03 15:09:53 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n[Stage 223:&gt;                                                        (0 + 1) / 1]"
  },
  {
    "objectID": "core/notebooks/notebook08_webdata-II.html#finally",
    "href": "core/notebooks/notebook08_webdata-II.html#finally",
    "title": "Using with pyspark for data preprocessing",
    "section": "Finally !!",
    "text": "Finally !!\nWow ! That was a lot of work. Now we have a features matrix \\(X\\) and a vector of labels \\(y\\).\n\n\nCode\nX.indices\n\n\narray([ 1,  3,  4, ..., 74, 78, 79], shape=(120930,), dtype=int32)\n\n\n\n\nCode\nX.indptr\n\n\narray([     0,     12,     22, ..., 120910, 120920, 120930],\n      shape=(11029,), dtype=int32)\n\n\n\n\nCode\nX.shape, X.nnz\n\n\n((11028, 80), 120930)\n\n\n\n\nCode\ny.shape, y.sum()\n\n\n((11028,), np.int64(87))"
  },
  {
    "objectID": "core/notebooks/notebook06_sparksql.html",
    "href": "core/notebooks/notebook06_sparksql.html",
    "title": "DataFrame",
    "section": "",
    "text": "Code\nimport os\nimport sys\n\nos.environ['PYSPARK_PYTHON'] = sys.executable\nos.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable\nCode\nfrom pyspark import SparkConf, SparkContext\nfrom pyspark.sql import SparkSession\n\nconf = SparkConf().setAppName(\"Spark SQL Course\")\nsc = SparkContext(conf=conf)  # no need for Spark 3...\n\nspark = (SparkSession\n    .builder\n    .appName(\"Spark SQL Course\")\n    .getOrCreate()\n)\n\n\n25/04/03 15:08:43 WARN Utils: Your hostname, boucheron-Precision-5480 resolves to a loopback address: 127.0.1.1; using 172.23.32.10 instead (on interface eth0)\n25/04/03 15:08:43 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\nSetting default log level to \"WARN\".\nTo adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n25/04/03 15:08:43 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\nCode\nfrom pyspark.sql import Row\n\nrow1 = Row(name=\"John\", age=21)\nrow2 = Row(name=\"James\", age=32)\nrow3 = Row(name=\"Jane\", age=18)\nrow1['name']\n\n\n'John'\nCode\ndf = spark.createDataFrame([row1, row2, row3])\nCode\ndf.printSchema()\n\n\nroot\n |-- name: string (nullable = true)\n |-- age: long (nullable = true)\nCode\ndf.show()\n\n\n[Stage 0:&gt;                                                          (0 + 1) / 1]                                                                                \n\n\n+-----+---+\n| name|age|\n+-----+---+\n| John| 21|\n|James| 32|\n| Jane| 18|\n+-----+---+\nCode\nprint(df.rdd.toDebugString().decode(\"utf-8\"))\n\n\n(20) MapPartitionsRDD[10] at javaToPython at NativeMethodAccessorImpl.java:0 []\n |   MapPartitionsRDD[9] at javaToPython at NativeMethodAccessorImpl.java:0 []\n |   SQLExecutionRDD[8] at javaToPython at NativeMethodAccessorImpl.java:0 []\n |   MapPartitionsRDD[7] at javaToPython at NativeMethodAccessorImpl.java:0 []\n |   MapPartitionsRDD[4] at applySchemaToPythonRDD at NativeMethodAccessorImpl.java:0 []\n |   MapPartitionsRDD[3] at map at SerDeUtil.scala:69 []\n |   MapPartitionsRDD[2] at mapPartitions at SerDeUtil.scala:117 []\n |   PythonRDD[1] at RDD at PythonRDD.scala:53 []\n |   ParallelCollectionRDD[0] at readRDDFromFile at PythonRDD.scala:289 []\nCode\ndf.rdd.getNumPartitions()\n\n\n20"
  },
  {
    "objectID": "core/notebooks/notebook06_sparksql.html#creating-dataframes",
    "href": "core/notebooks/notebook06_sparksql.html#creating-dataframes",
    "title": "DataFrame",
    "section": "Creating dataframes",
    "text": "Creating dataframes\n\n\nCode\nrows = [\n    Row(name=\"John\", age=21, gender=\"male\"),\n    Row(name=\"James\", age=25, gender=\"female\"),\n    Row(name=\"Albert\", age=46, gender=\"male\")\n]\n\ndf = spark.createDataFrame(rows)\n\n\n\n\nCode\ndf.show()\n\n\n+------+---+------+\n|  name|age|gender|\n+------+---+------+\n|  John| 21|  male|\n| James| 25|female|\n|Albert| 46|  male|\n+------+---+------+\n\n\n\n\n\nCode\nhelp(Row)\n\n\nHelp on class Row in module pyspark.sql.types:\n\nclass Row(builtins.tuple)\n |  Row(*args: Optional[str], **kwargs: Optional[Any]) -&gt; 'Row'\n |\n |  A row in :class:`DataFrame`.\n |  The fields in it can be accessed:\n |\n |  * like attributes (``row.key``)\n |  * like dictionary values (``row[key]``)\n |\n |  ``key in row`` will search through row keys.\n |\n |  Row can be used to create a row object by using named arguments.\n |  It is not allowed to omit a named argument to represent that the value is\n |  None or missing. This should be explicitly set to None in this case.\n |\n |  .. versionchanged:: 3.0.0\n |      Rows created from named arguments no longer have\n |      field names sorted alphabetically and will be ordered in the position as\n |      entered.\n |\n |  Examples\n |  --------\n |  &gt;&gt;&gt; from pyspark.sql import Row\n |  &gt;&gt;&gt; row = Row(name=\"Alice\", age=11)\n |  &gt;&gt;&gt; row\n |  Row(name='Alice', age=11)\n |  &gt;&gt;&gt; row['name'], row['age']\n |  ('Alice', 11)\n |  &gt;&gt;&gt; row.name, row.age\n |  ('Alice', 11)\n |  &gt;&gt;&gt; 'name' in row\n |  True\n |  &gt;&gt;&gt; 'wrong_key' in row\n |  False\n |\n |  Row also can be used to create another Row like class, then it\n |  could be used to create Row objects, such as\n |\n |  &gt;&gt;&gt; Person = Row(\"name\", \"age\")\n |  &gt;&gt;&gt; Person\n |  &lt;Row('name', 'age')&gt;\n |  &gt;&gt;&gt; 'name' in Person\n |  True\n |  &gt;&gt;&gt; 'wrong_key' in Person\n |  False\n |  &gt;&gt;&gt; Person(\"Alice\", 11)\n |  Row(name='Alice', age=11)\n |\n |  This form can also be used to create rows as tuple values, i.e. with unnamed\n |  fields.\n |\n |  &gt;&gt;&gt; row1 = Row(\"Alice\", 11)\n |  &gt;&gt;&gt; row2 = Row(name=\"Alice\", age=11)\n |  &gt;&gt;&gt; row1 == row2\n |  True\n |\n |  Method resolution order:\n |      Row\n |      builtins.tuple\n |      builtins.object\n |\n |  Methods defined here:\n |\n |  __call__(self, *args: Any) -&gt; 'Row'\n |      create new Row object\n |\n |  __contains__(self, item: Any) -&gt; bool\n |      Return bool(key in self).\n |\n |  __getattr__(self, item: str) -&gt; Any\n |\n |  __getitem__(self, item: Any) -&gt; Any\n |      Return self[key].\n |\n |  __reduce__(self) -&gt; Union[str, Tuple[Any, ...]]\n |      Returns a tuple so Python knows how to pickle Row.\n |\n |  __repr__(self) -&gt; str\n |      Printable representation of Row used in Python REPL.\n |\n |  __setattr__(self, key: Any, value: Any) -&gt; None\n |      Implement setattr(self, name, value).\n |\n |  asDict(self, recursive: bool = False) -&gt; Dict[str, Any]\n |      Return as a dict\n |\n |      Parameters\n |      ----------\n |      recursive : bool, optional\n |          turns the nested Rows to dict (default: False).\n |\n |      Notes\n |      -----\n |      If a row contains duplicate field names, e.g., the rows of a join\n |      between two :class:`DataFrame` that both have the fields of same names,\n |      one of the duplicate fields will be selected by ``asDict``. ``__getitem__``\n |      will also return one of the duplicate fields, however returned value might\n |      be different to ``asDict``.\n |\n |      Examples\n |      --------\n |      &gt;&gt;&gt; from pyspark.sql import Row\n |      &gt;&gt;&gt; Row(name=\"Alice\", age=11).asDict() == {'name': 'Alice', 'age': 11}\n |      True\n |      &gt;&gt;&gt; row = Row(key=1, value=Row(name='a', age=2))\n |      &gt;&gt;&gt; row.asDict() == {'key': 1, 'value': Row(name='a', age=2)}\n |      True\n |      &gt;&gt;&gt; row.asDict(True) == {'key': 1, 'value': {'name': 'a', 'age': 2}}\n |      True\n |\n |  ----------------------------------------------------------------------\n |  Static methods defined here:\n |\n |  __new__(cls, *args: Optional[str], **kwargs: Optional[Any]) -&gt; 'Row'\n |      Create and return a new object.  See help(type) for accurate signature.\n |\n |  ----------------------------------------------------------------------\n |  Data descriptors defined here:\n |\n |  __dict__\n |      dictionary for instance variables\n |\n |  ----------------------------------------------------------------------\n |  Methods inherited from builtins.tuple:\n |\n |  __add__(self, value, /)\n |      Return self+value.\n |\n |  __eq__(self, value, /)\n |      Return self==value.\n |\n |  __ge__(self, value, /)\n |      Return self&gt;=value.\n |\n |  __getattribute__(self, name, /)\n |      Return getattr(self, name).\n |\n |  __getnewargs__(self, /)\n |\n |  __gt__(self, value, /)\n |      Return self&gt;value.\n |\n |  __hash__(self, /)\n |      Return hash(self).\n |\n |  __iter__(self, /)\n |      Implement iter(self).\n |\n |  __le__(self, value, /)\n |      Return self&lt;=value.\n |\n |  __len__(self, /)\n |      Return len(self).\n |\n |  __lt__(self, value, /)\n |      Return self&lt;value.\n |\n |  __mul__(self, value, /)\n |      Return self*value.\n |\n |  __ne__(self, value, /)\n |      Return self!=value.\n |\n |  __rmul__(self, value, /)\n |      Return value*self.\n |\n |  count(self, value, /)\n |      Return number of occurrences of value.\n |\n |  index(self, value, start=0, stop=9223372036854775807, /)\n |      Return first index of value.\n |\n |      Raises ValueError if the value is not present.\n |\n |  ----------------------------------------------------------------------\n |  Class methods inherited from builtins.tuple:\n |\n |  __class_getitem__(...)\n |      See PEP 585\n\n\n\n\n\nCode\ncolumn_names = [\"name\", \"age\", \"gender\"]\nrows = [\n    [\"John\", 21, \"male\"],\n    [\"James\", 25, \"female\"],\n    [\"Albert\", 46, \"male\"]\n]\n\ndf = spark.createDataFrame(\n    rows, \n    column_names\n)\n\ndf.show()\n\n\n+------+---+------+\n|  name|age|gender|\n+------+---+------+\n|  John| 21|  male|\n| James| 25|female|\n|Albert| 46|  male|\n+------+---+------+\n\n\n\n\n\nCode\ndf.printSchema()\n\n\nroot\n |-- name: string (nullable = true)\n |-- age: long (nullable = true)\n |-- gender: string (nullable = true)\n\n\n\n\n\nCode\n# sc = SparkContext(conf=conf)  # no need for Spark 3...\n\ncolumn_names = [\"name\", \"age\", \"gender\"]\nrdd = sc.parallelize([\n    (\"John\", 21, \"male\"),\n    (\"James\", 25, \"female\"),\n    (\"Albert\", 46, \"male\")\n])\ndf = spark.createDataFrame(rdd, column_names)\ndf.show()\n\n\n+------+---+------+\n|  name|age|gender|\n+------+---+------+\n|  John| 21|  male|\n| James| 25|female|\n|Albert| 46|  male|\n+------+---+------+"
  },
  {
    "objectID": "core/notebooks/notebook06_sparksql.html#schema",
    "href": "core/notebooks/notebook06_sparksql.html#schema",
    "title": "DataFrame",
    "section": "Schema",
    "text": "Schema\nThere is special type schemata. A object of class StructType is made of a list of objects of type StructField.\n\n\nCode\ndf.schema\n\n\nStructType([StructField('name', StringType(), True), StructField('age', LongType(), True), StructField('gender', StringType(), True)])\n\n\n\n\nCode\ntype(df.schema)\n\n\npyspark.sql.types.StructType\n\n\nA object of type StructField has a name, a PySpark type, an d a boolean parameter.\n\n\nCode\nfrom pyspark.sql.types import *\n\nschema = StructType(\n    [\n        StructField(\"name\", StringType(), True),\n        StructField(\"age\", IntegerType(), True),\n        StructField(\"gender\", StringType(), True)\n    ]\n)\n\nrows = [(\"John\", 21, \"male\")]\ndf = spark.createDataFrame(rows, schema)\ndf.printSchema()\ndf.show()\n\n\nroot\n |-- name: string (nullable = true)\n |-- age: integer (nullable = true)\n |-- gender: string (nullable = true)\n\n+----+---+------+\n|name|age|gender|\n+----+---+------+\n|John| 21|  male|\n+----+---+------+"
  },
  {
    "objectID": "core/notebooks/notebook06_sparksql.html#select-projection-π",
    "href": "core/notebooks/notebook06_sparksql.html#select-projection-π",
    "title": "DataFrame",
    "section": "SELECT (projection \\(π\\))",
    "text": "SELECT (projection \\(π\\))\n\n\nCode\ndf.createOrReplaceTempView(\"table\")    \n\nquery = \"\"\"\n    SELECT \n        name, age \n    FROM \n        table\n\"\"\"\n\nspark.sql(query).show()\n\n\n+----+---+\n|name|age|\n+----+---+\n|John| 21|\n|Jane| 25|\n+----+---+\n\n\n\nUsing the API:\n\n\nCode\n(\n    df\n        .select(\"name\", \"age\")\n        .show()\n)\n\n\n+----+---+\n|name|age|\n+----+---+\n|John| 21|\n|Jane| 25|\n+----+---+\n\n\n\nπ(df, \"name\", \"age\")"
  },
  {
    "objectID": "core/notebooks/notebook06_sparksql.html#where-filter-selection-σ",
    "href": "core/notebooks/notebook06_sparksql.html#where-filter-selection-σ",
    "title": "DataFrame",
    "section": "WHERE (filter, selection, \\(σ\\))",
    "text": "WHERE (filter, selection, \\(σ\\))\n\n\nCode\ndf.createOrReplaceTempView(\"table\")\n\nquery = \"\"\"\n    SELECT \n        * \n    FROM \n        table\n    WHERE \n        age &gt; 21\n\"\"\"\n\nspark.sql(query).show()\n\n\n+----+---+------+\n|name|age|gender|\n+----+---+------+\n|Jane| 25|female|\n+----+---+------+\n\n\n\nUsing the API\n\n\nCode\n( \n    df\n        .where(\"age &gt; 21\")\n        .show()\n)\n\n\n+----+---+------+\n|name|age|gender|\n+----+---+------+\n|Jane| 25|female|\n+----+---+------+\n\n\n\nThis implements σ(df, \"age &gt; 21\")\n\n\nCode\n# Alternatively:\n( \n    df\n      .where(df['age'] &gt; 21)\n      .show()\n)\n\n\n+----+---+------+\n|name|age|gender|\n+----+---+------+\n|Jane| 25|female|\n+----+---+------+\n\n\n\n\n\nCode\n( \n    df\n      .where(df.age &gt; 21)\n      .show()\n)\n\n\n+----+---+------+\n|name|age|gender|\n+----+---+------+\n|Jane| 25|female|\n+----+---+------+\n\n\n\nMethod chaining allows to construct complex queries\n\n\nCode\n( \n    df\n      .where(\"age &gt; 21\")\n      .select([\"name\", \"age\"])\n      .show()\n)\n\n\n+----+---+\n|name|age|\n+----+---+\n|Jane| 25|\n+----+---+\n\n\n\nThis implements\n    σ(df, \"age &gt; 21\") |&gt;\n    π([\"name\", \"age\"])"
  },
  {
    "objectID": "core/notebooks/notebook06_sparksql.html#limit",
    "href": "core/notebooks/notebook06_sparksql.html#limit",
    "title": "DataFrame",
    "section": "LIMIT",
    "text": "LIMIT\n\n\nCode\ndf.createOrReplaceTempView(\"table\")\n\nquery = \"\"\"\n    SELECT \n        * \n    FROM \n        table \n    LIMIT 1\n\"\"\"\n\nspark.sql(query).show()\n\n\n+----+---+------+\n|name|age|gender|\n+----+---+------+\n|John| 21|  male|\n+----+---+------+\n\n\n\n\n\nCode\ndf.limit(1).show()\n\n\n+----+---+------+\n|name|age|gender|\n+----+---+------+\n|John| 21|  male|\n+----+---+------+\n\n\n\n\n\nCode\ndf.select(\"*\").limit(1).show()\n\n\n+----+---+------+\n|name|age|gender|\n+----+---+------+\n|John| 21|  male|\n+----+---+------+"
  },
  {
    "objectID": "core/notebooks/notebook06_sparksql.html#order-by",
    "href": "core/notebooks/notebook06_sparksql.html#order-by",
    "title": "DataFrame",
    "section": "ORDER BY",
    "text": "ORDER BY\n\n\nCode\ndf.createOrReplaceTempView(\"table\")\n\nquery = \"\"\"\n    SELECT \n        * \n    FROM \n        table\n    ORDER BY \n        name ASC\n\"\"\"\n\nspark.sql(query).show()\n\n\n+----+---+------+\n|name|age|gender|\n+----+---+------+\n|Jane| 25|female|\n|John| 21|  male|\n+----+---+------+\n\n\n\n\n\nCode\ndf.orderBy(df.name.asc()).show()\n\n\n+----+---+------+\n|name|age|gender|\n+----+---+------+\n|Jane| 25|female|\n|John| 21|  male|\n+----+---+------+"
  },
  {
    "objectID": "core/notebooks/notebook06_sparksql.html#alias-rename",
    "href": "core/notebooks/notebook06_sparksql.html#alias-rename",
    "title": "DataFrame",
    "section": "ALIAS (rename)",
    "text": "ALIAS (rename)\n\n\nCode\ndf.createOrReplaceTempView(\"table\")\nquery = \"SELECT name, age, gender AS sex FROM table\"\nspark.sql(query).show()\n\n\n+----+---+------+\n|name|age|   sex|\n+----+---+------+\n|John| 21|  male|\n|Jane| 25|female|\n+----+---+------+\n\n\n\n\n\nCode\ntype(df.age)\n\n\npyspark.sql.column.Column\n\n\n\n\nCode\ndf.select(df.name, df.age, df.gender.alias('sex')).show()\n\n\n+----+---+------+\n|name|age|   sex|\n+----+---+------+\n|John| 21|  male|\n|Jane| 25|female|\n+----+---+------+"
  },
  {
    "objectID": "core/notebooks/notebook06_sparksql.html#cast",
    "href": "core/notebooks/notebook06_sparksql.html#cast",
    "title": "DataFrame",
    "section": "CAST",
    "text": "CAST\n\n\nCode\ndf.createOrReplaceTempView(\"table\")\nquery = \"SELECT name, cast(age AS float) AS age_f FROM table\"\nspark.sql(query).show()\n\n\n+----+-----+\n|name|age_f|\n+----+-----+\n|John| 21.0|\n|Jane| 25.0|\n+----+-----+\n\n\n\n\n\nCode\ndf.select(df.name, df.age.cast(\"float\").alias(\"age_f\")).show()\n\n\n+----+-----+\n|name|age_f|\n+----+-----+\n|John| 21.0|\n|Jane| 25.0|\n+----+-----+\n\n\n\n\n\nCode\nnew_age_col = df.age.cast(\"float\").alias(\"age_f\")\ntype(new_age_col), type(df.age)\n\n\n(pyspark.sql.column.Column, pyspark.sql.column.Column)\n\n\n\n\nCode\ndf.select(df.name, new_age_col).show()\n\n\n+----+-----+\n|name|age_f|\n+----+-----+\n|John| 21.0|\n|Jane| 25.0|\n+----+-----+"
  },
  {
    "objectID": "core/notebooks/notebook06_sparksql.html#adding-new-columns",
    "href": "core/notebooks/notebook06_sparksql.html#adding-new-columns",
    "title": "DataFrame",
    "section": "Adding new columns",
    "text": "Adding new columns\n\n\nCode\ndf.createOrReplaceTempView(\"table\")\n\nquery = \"\"\"\n    SELECT \n        *, \n        12*age AS age_months \n    FROM \n        table\n\"\"\"\n\nspark.sql(query).show()\n\n\n+----+---+------+----------+\n|name|age|gender|age_months|\n+----+---+------+----------+\n|John| 21|  male|       252|\n|Jane| 25|female|       300|\n+----+---+------+----------+\n\n\n\n\n\nCode\n( \n    df\n        .withColumn(\"age_months\", df.age * 12)\n        .show()\n)\n\n\n+----+---+------+----------+\n|name|age|gender|age_months|\n+----+---+------+----------+\n|John| 21|  male|       252|\n|Jane| 25|female|       300|\n+----+---+------+----------+\n\n\n\n\n\nCode\n(\n    df\n        .select(\"*\", \n                (df.age * 12).alias(\"age_months\"))\n        .show()\n)\n\n\n+----+---+------+----------+\n|name|age|gender|age_months|\n+----+---+------+----------+\n|John| 21|  male|       252|\n|Jane| 25|female|       300|\n+----+---+------+----------+\n\n\n\n\n\nCode\nimport datetime\n\nhui = datetime.date.today()\n\nhui = hui.replace(year=hui.year-21)\n\nstr(hui)\n\n\n'2004-04-03'\n\n\n\n\nCode\n# df.select(\"*\", hui.replace(year=hui.year - df.age ).alias(\"yob\")).show()"
  },
  {
    "objectID": "core/notebooks/notebook06_sparksql.html#numeric-functions-examples",
    "href": "core/notebooks/notebook06_sparksql.html#numeric-functions-examples",
    "title": "DataFrame",
    "section": "Numeric functions examples",
    "text": "Numeric functions examples\n\n\nCode\nfrom pyspark.sql import functions as fn\n\ncolumns = [\"brand\", \"cost\"]\ndf = spark.createDataFrame([\n    (\"garnier\", 3.49),\n    (\"elseve\", 2.71)\n], columns)\n\nround_cost = fn.round(df.cost, 1)\nfloor_cost = fn.floor(df.cost)\nceil_cost = fn.ceil(df.cost)\n\ndf.withColumn('round', round_cost)\\\n    .withColumn('floor', floor_cost)\\\n    .withColumn('ceil', ceil_cost)\\\n    .show()\n\n\n+-------+----+-----+-----+----+\n|  brand|cost|round|floor|ceil|\n+-------+----+-----+-----+----+\n|garnier|3.49|  3.5|    3|   4|\n| elseve|2.71|  2.7|    2|   3|\n+-------+----+-----+-----+----+"
  },
  {
    "objectID": "core/notebooks/notebook06_sparksql.html#string-functions-examples",
    "href": "core/notebooks/notebook06_sparksql.html#string-functions-examples",
    "title": "DataFrame",
    "section": "String functions examples",
    "text": "String functions examples\n\n\nCode\nfrom pyspark.sql import functions as fn\n\ncolumns = [\"first_name\", \"last_name\"]\n\ndf = spark.createDataFrame([\n    (\"John\", \"Doe\"),\n    (\"Mary\", \"Jane\")\n], columns)\n\nlast_name_initial = fn.substring(df.last_name, 0, 1)\n# last_name_initial_dotted = fn.concat(last_name_initial, \".\")\n\nname = fn.concat_ws(\" \", df.first_name, last_name_initial)\ndf.withColumn(\"name\", name).show()\n\n\n+----------+---------+------+\n|first_name|last_name|  name|\n+----------+---------+------+\n|      John|      Doe|John D|\n|      Mary|     Jane|Mary J|\n+----------+---------+------+\n\n\n\n\n\nCode\n( \n    df.selectExpr(\"*\", \"substring(last_name, 0, 1) as lni\")\n      .selectExpr(\"first_name\", \"last_name\", \"concat(first_name, ' ', lni, '.') as nname\")\n      .show()\n)\n\n\n+----------+---------+-------+\n|first_name|last_name|  nname|\n+----------+---------+-------+\n|      John|      Doe|John D.|\n|      Mary|     Jane|Mary J.|\n+----------+---------+-------+"
  },
  {
    "objectID": "core/notebooks/notebook06_sparksql.html#date-functions-examples",
    "href": "core/notebooks/notebook06_sparksql.html#date-functions-examples",
    "title": "DataFrame",
    "section": "Date functions examples",
    "text": "Date functions examples\n\n\nCode\nfrom datetime import date\nfrom pyspark.sql import functions as fn\n\ndf = spark.createDataFrame([\n    (date(2015, 1, 1), date(2015, 1, 15)),\n    (date(2015, 2, 21), date(2015, 3, 8)),\n], [\"start_date\", \"end_date\"])\n\ndays_between = fn.datediff(df.end_date, df.start_date)\nstart_month = fn.month(df.start_date)\n\ndf.withColumn('days_between', days_between)\\\n    .withColumn('start_month', start_month)\\\n    .show()\n\n\n+----------+----------+------------+-----------+\n|start_date|  end_date|days_between|start_month|\n+----------+----------+------------+-----------+\n|2015-01-01|2015-01-15|          14|          1|\n|2015-02-21|2015-03-08|          15|          2|\n+----------+----------+------------+-----------+\n\n\n\n\n\nCode\nstr(date(2015, 1, 1) - date(2015, 1, 15))\n\n\n'-14 days, 0:00:00'\n\n\n\n\nCode\nfrom datetime import timedelta\n\ndate(2023, 2 , 14) + timedelta(days=3)\n\n\ndatetime.date(2023, 2, 17)"
  },
  {
    "objectID": "core/notebooks/notebook06_sparksql.html#conditional-transformations",
    "href": "core/notebooks/notebook06_sparksql.html#conditional-transformations",
    "title": "DataFrame",
    "section": "Conditional transformations",
    "text": "Conditional transformations\n\n\nCode\ndf = spark.createDataFrame([\n    (\"John\", 21, \"male\"),\n    (\"Jane\", 25, \"female\"),\n    (\"Albert\", 46, \"male\"),\n    (\"Brad\", 49, \"super-hero\")\n], [\"name\", \"age\", \"gender\"])\n\n\n\n\nCode\nsupervisor = ( \n    fn.when(df.gender == 'male', 'Mr. Smith')\n      .when(df.gender == 'female', 'Miss Jones')\n      .otherwise('NA')\n)\n\ntype(supervisor), type(fn.when)\n\n\n(pyspark.sql.column.Column, function)\n\n\n\n\nCode\ndf.withColumn(\"supervisor\", supervisor).show()\n\n\n+------+---+----------+----------+\n|  name|age|    gender|supervisor|\n+------+---+----------+----------+\n|  John| 21|      male| Mr. Smith|\n|  Jane| 25|    female|Miss Jones|\n|Albert| 46|      male| Mr. Smith|\n|  Brad| 49|super-hero|        NA|\n+------+---+----------+----------+"
  },
  {
    "objectID": "core/notebooks/notebook06_sparksql.html#user-defined-functions",
    "href": "core/notebooks/notebook06_sparksql.html#user-defined-functions",
    "title": "DataFrame",
    "section": "User-defined functions",
    "text": "User-defined functions\n\n\nCode\nfrom pyspark.sql import functions as fn\nfrom pyspark.sql.types import StringType\n\ndf = spark.createDataFrame([(1, 3), (4, 2)], [\"first\", \"second\"])\n\ndef my_func(col_1, col_2):\n    if (col_1 &gt; col_2):\n        return \"{} is bigger than {}\".format(col_1, col_2)\n    else:\n        return \"{} is bigger than {}\".format(col_2, col_1)\n\nmy_udf = fn.udf(my_func, StringType())\n\ndf.withColumn(\"udf\", my_udf(df['first'], df['second'])).show()\n\n\n+-----+------+------------------+\n|first|second|               udf|\n+-----+------+------------------+\n|    1|     3|3 is bigger than 1|\n|    4|     2|4 is bigger than 2|\n+-----+------+------------------+"
  },
  {
    "objectID": "core/notebooks/notebook06_sparksql.html#using-the-spark.sql-api",
    "href": "core/notebooks/notebook06_sparksql.html#using-the-spark.sql-api",
    "title": "DataFrame",
    "section": "Using the spark.sql API",
    "text": "Using the spark.sql API\n\n\nCode\nfrom datetime import date\n\nproducts = spark.createDataFrame([\n    ('1', 'mouse', 'microsoft', 39.99),\n    ('2', 'keyboard', 'logitech', 59.99),\n], ['prod_id', 'prod_cat', 'prod_brand', 'prod_value'])\n\npurchases = spark.createDataFrame([\n    (date(2017, 11, 1), 2, '1'),\n    (date(2017, 11, 2), 1, '1'),\n    (date(2017, 11, 5), 1, '2'),\n], ['date', 'quantity', 'prod_id'])\n\n# The default join type is the \"INNER\" join\npurchases.join(products, 'prod_id').show()\n\n\n+-------+----------+--------+--------+----------+----------+\n|prod_id|      date|quantity|prod_cat|prod_brand|prod_value|\n+-------+----------+--------+--------+----------+----------+\n|      1|2017-11-01|       2|   mouse| microsoft|     39.99|\n|      1|2017-11-02|       1|   mouse| microsoft|     39.99|\n|      2|2017-11-05|       1|keyboard|  logitech|     59.99|\n+-------+----------+--------+--------+----------+----------+\n\n\n\n\n\nCode\npurchases.join(products, 'prod_id').explain()\n\n\n== Physical Plan ==\nAdaptiveSparkPlan isFinalPlan=false\n+- Project [prod_id#577, date#575, quantity#576L, prod_cat#568, prod_brand#569, prod_value#570]\n   +- SortMergeJoin [prod_id#577], [prod_id#567], Inner\n      :- Sort [prod_id#577 ASC NULLS FIRST], false, 0\n      :  +- Exchange hashpartitioning(prod_id#577, 200), ENSURE_REQUIREMENTS, [plan_id=575]\n      :     +- Filter isnotnull(prod_id#577)\n      :        +- Scan ExistingRDD[date#575,quantity#576L,prod_id#577]\n      +- Sort [prod_id#567 ASC NULLS FIRST], false, 0\n         +- Exchange hashpartitioning(prod_id#567, 200), ENSURE_REQUIREMENTS, [plan_id=576]\n            +- Filter isnotnull(prod_id#567)\n               +- Scan ExistingRDD[prod_id#567,prod_cat#568,prod_brand#569,prod_value#570]"
  },
  {
    "objectID": "core/notebooks/notebook06_sparksql.html#using-a-sql-query",
    "href": "core/notebooks/notebook06_sparksql.html#using-a-sql-query",
    "title": "DataFrame",
    "section": "Using a SQL query",
    "text": "Using a SQL query\n\n\nCode\nproducts.createOrReplaceTempView(\"products\")\npurchases.createOrReplaceTempView(\"purchases\")\n\nquery = \"\"\"\n    SELECT * \n    FROM purchases AS prc INNER JOIN \n        products AS prd \n    ON prc.prod_id = prd.prod_id\n\"\"\"\nspark.sql(query).show()\n\n\n+----------+--------+-------+-------+--------+----------+----------+\n|      date|quantity|prod_id|prod_id|prod_cat|prod_brand|prod_value|\n+----------+--------+-------+-------+--------+----------+----------+\n|2017-11-01|       2|      1|      1|   mouse| microsoft|     39.99|\n|2017-11-02|       1|      1|      1|   mouse| microsoft|     39.99|\n|2017-11-05|       1|      2|      2|keyboard|  logitech|     59.99|\n+----------+--------+-------+-------+--------+----------+----------+\n\n\n\n\n\nCode\nspark.sql(query).explain()\n\n\n== Physical Plan ==\nAdaptiveSparkPlan isFinalPlan=false\n+- SortMergeJoin [prod_id#577], [prod_id#567], Inner\n   :- Sort [prod_id#577 ASC NULLS FIRST], false, 0\n   :  +- Exchange hashpartitioning(prod_id#577, 200), ENSURE_REQUIREMENTS, [plan_id=718]\n   :     +- Filter isnotnull(prod_id#577)\n   :        +- Scan ExistingRDD[date#575,quantity#576L,prod_id#577]\n   +- Sort [prod_id#567 ASC NULLS FIRST], false, 0\n      +- Exchange hashpartitioning(prod_id#567, 200), ENSURE_REQUIREMENTS, [plan_id=719]\n         +- Filter isnotnull(prod_id#567)\n            +- Scan ExistingRDD[prod_id#567,prod_cat#568,prod_brand#569,prod_value#570]\n\n\n\n\n\n\nCode\nnew_purchases = spark.createDataFrame([\n    (date(2017, 11, 1), 2, '1'),\n    (date(2017, 11, 2), 1, '3'),\n], ['date', 'quantity', 'prod_id_x'])\n\n# The default join type is the \"INNER\" join\njoin_rule = new_purchases.prod_id_x == products.prod_id\n\nprint(type(join_rule))\n\nnew_purchases.join(products, join_rule, 'left').show()\n\n\n&lt;class 'pyspark.sql.column.Column'&gt;\n+----------+--------+---------+-------+--------+----------+----------+\n|      date|quantity|prod_id_x|prod_id|prod_cat|prod_brand|prod_value|\n+----------+--------+---------+-------+--------+----------+----------+\n|2017-11-01|       2|        1|      1|   mouse| microsoft|     39.99|\n|2017-11-02|       1|        3|   NULL|    NULL|      NULL|      NULL|\n+----------+--------+---------+-------+--------+----------+----------+\n\n\n\n\n\nCode\njoin_rule.info\n\n\nColumn&lt;'(prod_id_x = prod_id)[info]'&gt;\n\n\n\n\nCode\nnew_purchases = spark.createDataFrame([\n    (date(2017, 11, 1), 2, '1'),\n    (date(2017, 11, 2), 1, '3'),\n], ['date', 'quantity', 'prod_id_x'])\n\n# The default join type is the \"INNER\" join\njoin_rule = new_purchases.prod_id_x == products.prod_id\n\nnew_purchases.join(products, join_rule, 'left').show()\n\n\n+----------+--------+---------+-------+--------+----------+----------+\n|      date|quantity|prod_id_x|prod_id|prod_cat|prod_brand|prod_value|\n+----------+--------+---------+-------+--------+----------+----------+\n|2017-11-01|       2|        1|      1|   mouse| microsoft|     39.99|\n|2017-11-02|       1|        3|   NULL|    NULL|      NULL|      NULL|\n+----------+--------+---------+-------+--------+----------+----------+"
  },
  {
    "objectID": "core/notebooks/notebook06_sparksql.html#various-types-of-joins",
    "href": "core/notebooks/notebook06_sparksql.html#various-types-of-joins",
    "title": "DataFrame",
    "section": "Various types of joins",
    "text": "Various types of joins\n\n\nCode\nleft = spark.createDataFrame([\n    (1, \"A1\"), (2, \"A2\"), (3, \"A3\"), (4, \"A4\")], \n    [\"id\", \"value\"])\n\nright = spark.createDataFrame([\n    (3, \"A3\"), (4, \"A4\"), (4, \"A4_1\"), (5, \"A5\"), (6, \"A6\")], \n    [\"id\", \"value\"])\n\njoin_types = [\n    \"inner\", \"outer\", \"left\", \"right\",\n    \"leftsemi\", \"leftanti\"\n]\n\n\n\n\nCode\nfor join_type in join_types:\n    print(join_type)\n    left.join(right, on=\"id\", how=join_type)\\\n        .orderBy(\"id\")\\\n        .show()\n\n\ninner\n+---+-----+-----+\n| id|value|value|\n+---+-----+-----+\n|  3|   A3|   A3|\n|  4|   A4|   A4|\n|  4|   A4| A4_1|\n+---+-----+-----+\n\nouter\n+---+-----+-----+\n| id|value|value|\n+---+-----+-----+\n|  1|   A1| NULL|\n|  2|   A2| NULL|\n|  3|   A3|   A3|\n|  4|   A4|   A4|\n|  4|   A4| A4_1|\n|  5| NULL|   A5|\n|  6| NULL|   A6|\n+---+-----+-----+\n\nleft\n+---+-----+-----+\n| id|value|value|\n+---+-----+-----+\n|  1|   A1| NULL|\n|  2|   A2| NULL|\n|  3|   A3|   A3|\n|  4|   A4|   A4|\n|  4|   A4| A4_1|\n+---+-----+-----+\n\nright\n+---+-----+-----+\n| id|value|value|\n+---+-----+-----+\n|  3|   A3|   A3|\n|  4|   A4|   A4|\n|  4|   A4| A4_1|\n|  5| NULL|   A5|\n|  6| NULL|   A6|\n+---+-----+-----+\n\nleftsemi\n+---+-----+\n| id|value|\n+---+-----+\n|  3|   A3|\n|  4|   A4|\n+---+-----+\n\nleftanti\n+---+-----+\n| id|value|\n+---+-----+\n|  1|   A1|\n|  2|   A2|\n+---+-----+"
  },
  {
    "objectID": "core/notebooks/notebook06_sparksql.html#examples-using-the-api",
    "href": "core/notebooks/notebook06_sparksql.html#examples-using-the-api",
    "title": "DataFrame",
    "section": "Examples using the API",
    "text": "Examples using the API\n\n\nCode\nfrom pyspark.sql import functions as fn\n\nproducts = spark.createDataFrame([\n    ('1', 'mouse', 'microsoft', 39.99),\n    ('2', 'mouse', 'microsoft', 59.99),\n    ('3', 'keyboard', 'microsoft', 59.99),\n    ('4', 'keyboard', 'logitech', 59.99),\n    ('5', 'mouse', 'logitech', 29.99),\n], ['prod_id', 'prod_cat', 'prod_brand', 'prod_value'])\n\n( \n    products\n        .groupBy('prod_cat')\n        .avg('prod_value')\n        .show()\n)\n\n\n+--------+-----------------+\n|prod_cat|  avg(prod_value)|\n+--------+-----------------+\n|   mouse|43.32333333333333|\n|keyboard|            59.99|\n+--------+-----------------+\n\n\n\n\n\nCode\n(\n    products\n        .groupBy('prod_cat')\n        .agg(fn.avg('prod_value'))\n        .show()\n)\n\n\n+--------+-----------------+\n|prod_cat|  avg(prod_value)|\n+--------+-----------------+\n|   mouse|43.32333333333333|\n|keyboard|            59.99|\n+--------+-----------------+\n\n\n\n\n\nCode\n(\n    products\n        .groupBy('prod_cat')\n        .agg(\n            fn.mean('prod_value'), \n            fn.stddev('prod_value')\n        )\n        .show()\n)\n\n\n+--------+-----------------+------------------+\n|prod_cat|  avg(prod_value)|stddev(prod_value)|\n+--------+-----------------+------------------+\n|   mouse|43.32333333333333|15.275252316519468|\n|keyboard|            59.99|               0.0|\n+--------+-----------------+------------------+\n\n\n\n\n\nCode\nfrom pyspark.sql import functions as fn\n\n(\n    products\n        .groupBy('prod_brand', 'prod_cat')\\\n        .agg(\n            fn.avg('prod_value')\n        )\n        .show()\n)\n\n\n+----------+--------+---------------+\n|prod_brand|prod_cat|avg(prod_value)|\n+----------+--------+---------------+\n| microsoft|   mouse|          49.99|\n| microsoft|keyboard|          59.99|\n|  logitech|keyboard|          59.99|\n|  logitech|   mouse|          29.99|\n+----------+--------+---------------+\n\n\n\n\n\nCode\nfrom pyspark.sql import functions as fn\n\n(\n    products\n        .groupBy('prod_brand')\n        .agg(\n            fn.round(\n                fn.avg('prod_value'), 1)\n                .alias('average'),\n            fn.ceil(\n                fn.sum('prod_value'))\n                .alias('sum'),\n            fn.min('prod_value')\n                .alias('min')\n        )\n        .show()\n)\n\n\n+----------+-------+---+-----+\n|prod_brand|average|sum|  min|\n+----------+-------+---+-----+\n| microsoft|   53.3|160|39.99|\n|  logitech|   45.0| 90|29.99|\n+----------+-------+---+-----+"
  },
  {
    "objectID": "core/notebooks/notebook06_sparksql.html#example-using-a-query",
    "href": "core/notebooks/notebook06_sparksql.html#example-using-a-query",
    "title": "DataFrame",
    "section": "Example using a query",
    "text": "Example using a query\n\n\nCode\nproducts.createOrReplaceTempView(\"products\")\n\n\n\n\nCode\nquery = \"\"\"\nSELECT\n    prod_brand,\n    round(avg(prod_value), 1) AS average,\n    min(prod_value) AS min\nFROM \n    products\nGROUP BY \n    prod_brand\n\"\"\"\n\nspark.sql(query).show()\n\n\n+----------+-------+-----+\n|prod_brand|average|  min|\n+----------+-------+-----+\n| microsoft|   53.3|39.99|\n|  logitech|   45.0|29.99|\n+----------+-------+-----+"
  },
  {
    "objectID": "core/notebooks/notebook06_sparksql.html#numerical-window-functions",
    "href": "core/notebooks/notebook06_sparksql.html#numerical-window-functions",
    "title": "DataFrame",
    "section": "Numerical window functions",
    "text": "Numerical window functions\n\n\nCode\nfrom pyspark.sql import Window\nfrom pyspark.sql import functions as fn\n\n# First, we create the Window definition\nwindow = Window.partitionBy('prod_brand')\n\nprint(type(window))\n\n\n&lt;class 'pyspark.sql.window.WindowSpec'&gt;\n\n\nThen, we can use over to aggregate on this window\n\n\nCode\navg = fn.avg('prod_value').over(window)\n\n# Finally, we can it as a classical column\n(\n    products\n        .withColumn('avg_brand_value', fn.round(avg, 2))\n        .show()\n)\n\n\n+-------+--------+----------+----------+---------------+\n|prod_id|prod_cat|prod_brand|prod_value|avg_brand_value|\n+-------+--------+----------+----------+---------------+\n|      4|keyboard|  logitech|     59.99|          44.99|\n|      5|   mouse|  logitech|     29.99|          44.99|\n|      1|   mouse| microsoft|     39.99|          53.32|\n|      2|   mouse| microsoft|     59.99|          53.32|\n|      3|keyboard| microsoft|     59.99|          53.32|\n+-------+--------+----------+----------+---------------+\n\n\n\nWith SQL queries, using multiple windows is not a problem\n\n\nCode\nquery = \"\"\"\n    SELECT \n        *, \n        ROUND(AVG(prod_value) OVER w1, 2)  AS avg_brand_value,\n        ROUND(AVG(prod_value) OVER w2, 1)  AS avg_prod_value\n    FROM \n        products\n    WINDOW \n        w1 AS (PARTITION BY prod_brand),\n        w2 AS (PARTITION BY prod_cat)\n\"\"\"\n\nspark.sql(query).show()\n\n\n+-------+--------+----------+----------+---------------+--------------+\n|prod_id|prod_cat|prod_brand|prod_value|avg_brand_value|avg_prod_value|\n+-------+--------+----------+----------+---------------+--------------+\n|      4|keyboard|  logitech|     59.99|          44.99|          60.0|\n|      3|keyboard| microsoft|     59.99|          53.32|          60.0|\n|      5|   mouse|  logitech|     29.99|          44.99|          43.3|\n|      1|   mouse| microsoft|     39.99|          53.32|          43.3|\n|      2|   mouse| microsoft|     59.99|          53.32|          43.3|\n+-------+--------+----------+----------+---------------+--------------+\n\n\n\n\n\nCode\nwindow2 = Window.partitionBy('prod_cat')\n\navg2 = fn.avg('prod_value').over(window2)\n\n# Finally, we can it as a classical column\n( \n    products\n        .withColumn('avg_brand_value', fn.round(avg, 2))\n        .withColumn('avg_prod_value', fn.round(avg2, 1))\n        .show()\n)\n\n\n+-------+--------+----------+----------+---------------+--------------+\n|prod_id|prod_cat|prod_brand|prod_value|avg_brand_value|avg_prod_value|\n+-------+--------+----------+----------+---------------+--------------+\n|      4|keyboard|  logitech|     59.99|          44.99|          60.0|\n|      3|keyboard| microsoft|     59.99|          53.32|          60.0|\n|      5|   mouse|  logitech|     29.99|          44.99|          43.3|\n|      1|   mouse| microsoft|     39.99|          53.32|          43.3|\n|      2|   mouse| microsoft|     59.99|          53.32|          43.3|\n+-------+--------+----------+----------+---------------+--------------+\n\n\n\nNow we can compare the physical plans associated with the two jobs.\n\n\nCode\n( \n    products\n        .withColumn('avg_brand_value', fn.round(avg, 2))\n        .withColumn('avg_prod_value', fn.round(avg2, 1))\n        .explain()\n)\n\n\n== Physical Plan ==\nAdaptiveSparkPlan isFinalPlan=false\n+- Project [prod_id#854, prod_cat#855, prod_brand#856, prod_value#857, avg_brand_value#1195, round(_we0#1203, 1) AS avg_prod_value#1202]\n   +- Window [avg(prod_value#857) windowspecdefinition(prod_cat#855, specifiedwindowframe(RowFrame, unboundedpreceding$(), unboundedfollowing$())) AS _we0#1203], [prod_cat#855]\n      +- Sort [prod_cat#855 ASC NULLS FIRST], false, 0\n         +- Exchange hashpartitioning(prod_cat#855, 200), ENSURE_REQUIREMENTS, [plan_id=2249]\n            +- Project [prod_id#854, prod_cat#855, prod_brand#856, prod_value#857, round(_we0#1196, 2) AS avg_brand_value#1195]\n               +- Window [avg(prod_value#857) windowspecdefinition(prod_brand#856, specifiedwindowframe(RowFrame, unboundedpreceding$(), unboundedfollowing$())) AS _we0#1196], [prod_brand#856]\n                  +- Sort [prod_brand#856 ASC NULLS FIRST], false, 0\n                     +- Exchange hashpartitioning(prod_brand#856, 200), ENSURE_REQUIREMENTS, [plan_id=2244]\n                        +- Scan ExistingRDD[prod_id#854,prod_cat#855,prod_brand#856,prod_value#857]\n\n\n\n\n\n\nCode\nspark.sql(query).explain()\n\n\n== Physical Plan ==\nAdaptiveSparkPlan isFinalPlan=false\n+- Project [prod_id#854, prod_cat#855, prod_brand#856, prod_value#857, round(_we0#1214, 2) AS avg_brand_value#1210, round(_we1#1215, 1) AS avg_prod_value#1211]\n   +- Window [avg(prod_value#857) windowspecdefinition(prod_cat#855, specifiedwindowframe(RowFrame, unboundedpreceding$(), unboundedfollowing$())) AS _we1#1215], [prod_cat#855]\n      +- Sort [prod_cat#855 ASC NULLS FIRST], false, 0\n         +- Exchange hashpartitioning(prod_cat#855, 200), ENSURE_REQUIREMENTS, [plan_id=2273]\n            +- Window [avg(prod_value#857) windowspecdefinition(prod_brand#856, specifiedwindowframe(RowFrame, unboundedpreceding$(), unboundedfollowing$())) AS _we0#1214], [prod_brand#856]\n               +- Sort [prod_brand#856 ASC NULLS FIRST], false, 0\n                  +- Exchange hashpartitioning(prod_brand#856, 200), ENSURE_REQUIREMENTS, [plan_id=2269]\n                     +- Scan ExistingRDD[prod_id#854,prod_cat#855,prod_brand#856,prod_value#857]"
  },
  {
    "objectID": "core/notebooks/notebook06_sparksql.html#lag-and-lead",
    "href": "core/notebooks/notebook06_sparksql.html#lag-and-lead",
    "title": "DataFrame",
    "section": "Lag and Lead",
    "text": "Lag and Lead\n\n\nCode\npurchases = spark.createDataFrame(\n    [\n        (date(2017, 11, 1), 'mouse'),\n        (date(2017, 11, 2), 'mouse'),\n        (date(2017, 11, 4), 'keyboard'),\n        (date(2017, 11, 6), 'keyboard'),\n        (date(2017, 11, 9), 'keyboard'),\n        (date(2017, 11, 12), 'mouse'),\n        (date(2017, 11, 18), 'keyboard')\n    ], \n    ['date', 'prod_cat']\n)\n\npurchases.show()\n\nwindow = Window.partitionBy('prod_cat').orderBy('date')\n\nprev_purch = fn.lag('date', 1).over(window)\nnext_purch = fn.lead('date', 1).over(window)\n\npurchases\\\n    .withColumn('prev', prev_purch)\\\n    .withColumn('next', next_purch)\\\n    .orderBy('prod_cat', 'date')\\\n    .show()\n\n\n+----------+--------+\n|      date|prod_cat|\n+----------+--------+\n|2017-11-01|   mouse|\n|2017-11-02|   mouse|\n|2017-11-04|keyboard|\n|2017-11-06|keyboard|\n|2017-11-09|keyboard|\n|2017-11-12|   mouse|\n|2017-11-18|keyboard|\n+----------+--------+\n\n+----------+--------+----------+----------+\n|      date|prod_cat|      prev|      next|\n+----------+--------+----------+----------+\n|2017-11-04|keyboard|      NULL|2017-11-06|\n|2017-11-06|keyboard|2017-11-04|2017-11-09|\n|2017-11-09|keyboard|2017-11-06|2017-11-18|\n|2017-11-18|keyboard|2017-11-09|      NULL|\n|2017-11-01|   mouse|      NULL|2017-11-02|\n|2017-11-02|   mouse|2017-11-01|2017-11-12|\n|2017-11-12|   mouse|2017-11-02|      NULL|\n+----------+--------+----------+----------+"
  },
  {
    "objectID": "core/notebooks/notebook06_sparksql.html#rank-denserank-and-rownumber",
    "href": "core/notebooks/notebook06_sparksql.html#rank-denserank-and-rownumber",
    "title": "DataFrame",
    "section": "Rank, DenseRank and RowNumber",
    "text": "Rank, DenseRank and RowNumber\n\n\nCode\ncontestants = spark.createDataFrame(\n    [   \n        ('veterans', 'John', 3000),\n        ('veterans', 'Bob', 3200),\n        ('veterans', 'Mary', 4000),\n        ('young', 'Jane', 4000),\n        ('young', 'April', 3100),\n        ('young', 'Alice', 3700),\n        ('young', 'Micheal', 4000),\n    ], \n    ['category', 'name', 'points']\n)\n\ncontestants.show()\n\n\n+--------+-------+------+\n|category|   name|points|\n+--------+-------+------+\n|veterans|   John|  3000|\n|veterans|    Bob|  3200|\n|veterans|   Mary|  4000|\n|   young|   Jane|  4000|\n|   young|  April|  3100|\n|   young|  Alice|  3700|\n|   young|Micheal|  4000|\n+--------+-------+------+\n\n\n\n\n\nCode\nwindow = (\n    Window\n        .partitionBy('category')\n        .orderBy(contestants.points.desc())\n)\n\nrank = fn.rank().over(window)\ndense_rank = fn.dense_rank().over(window)\nrow_number = fn.row_number().over(window)\n\ncontestants\\\n    .withColumn('rank', rank)\\\n    .withColumn('dense_rank', dense_rank)\\\n    .withColumn('row_number', row_number)\\\n    .orderBy('category', fn.col('points').desc())\\\n    .show()\n\n\n+--------+-------+------+----+----------+----------+\n|category|   name|points|rank|dense_rank|row_number|\n+--------+-------+------+----+----------+----------+\n|veterans|   Mary|  4000|   1|         1|         1|\n|veterans|    Bob|  3200|   2|         2|         2|\n|veterans|   John|  3000|   3|         3|         3|\n|   young|   Jane|  4000|   1|         1|         1|\n|   young|Micheal|  4000|   1|         1|         2|\n|   young|  Alice|  3700|   3|         2|         3|\n|   young|  April|  3100|   4|         3|         4|\n+--------+-------+------+----+----------+----------+"
  },
  {
    "objectID": "core/notebooks/notebook04_pandas_spark.html",
    "href": "core/notebooks/notebook04_pandas_spark.html",
    "title": "Data preprocessing and visualisation of a credit scoring dataset",
    "section": "",
    "text": "We’ll work on a dataset gro.csv for credit scoring that was proposed some years ago as a data challenge on some data challenge website. It is a realistic and somewhat messy dataset that contains a lot of missing values, several types of features (dates, categories, continuous features), so that serious data cleaning and formating is required. This dataset contains the following columns:"
  },
  {
    "objectID": "core/notebooks/notebook04_pandas_spark.html#lets-assess-what-we-did",
    "href": "core/notebooks/notebook04_pandas_spark.html#lets-assess-what-we-did",
    "title": "Data preprocessing and visualisation of a credit scoring dataset",
    "section": "Let’s assess what we did",
    "text": "Let’s assess what we did\nIt appears that we have to work a little bit more for a correct import of the data. Here is a list of the problems we face. - The last three columns are empty - Dates are actually str (python’s string type) - There is a lot of missing values - Categorial features are str - The Net_Annual_Income is imported as a string\nBy looking at the column names, the descriptions of the columns and using some basic, we infer the type of features that we have. There are dates features, continuous features, categorical features, and some features that could be either treated as categorical or continuous.\n\nThere are many missing values, that need to be handled.\nThe annual net income is imported as a string, we need to understand why.\nWe really need to treat dates as dates and not strings (because we want to compute the age of a client based on its birth year for instance).\n\nHere is a tentative structure of the features\nContinuous features\n\nYears_At_Residence\nNet_Annual_Income\nYears_At_Business\n\nFeatures to be decided\n\nNumber_Of_Dependant\nNb_Of_Products\n\nCategorical features\n\nCustomer_Type\nP_Client\nEducational_Level\nMarital_Status\nProd_Sub_Category\nSource\nType_Of_Residence\nProd_Category\n\nDate features\n\nBirthDate\nCustomer_Open_Date\nProd_Decision_Date\nProd_Closed_Date"
  },
  {
    "objectID": "core/notebooks/notebook04_pandas_spark.html#the-last-three-columns-are-weird-and-empty",
    "href": "core/notebooks/notebook04_pandas_spark.html#the-last-three-columns-are-weird-and-empty",
    "title": "Data preprocessing and visualisation of a credit scoring dataset",
    "section": "The last three columns are weird and empty",
    "text": "The last three columns are weird and empty\nIt seems to come from the fact that the data always ends with several ';' characters. We can remove them simply using the usecols option from read_csv."
  },
  {
    "objectID": "core/notebooks/notebook04_pandas_spark.html#dates-are-actually-str",
    "href": "core/notebooks/notebook04_pandas_spark.html#dates-are-actually-str",
    "title": "Data preprocessing and visualisation of a credit scoring dataset",
    "section": "Dates are actually str",
    "text": "Dates are actually str\nWe need to specify which columns must be encoded as dates using the parse_dates option from read_csv. Fortunately enough, pandas is clever enough to interpret the date format.\n\n\nCode\ntype(psdf.loc[0, 'BirthDate'])\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[15], line 1\n----&gt; 1 type(psdf.loc[0, 'BirthDate'])\n\nNameError: name 'psdf' is not defined"
  },
  {
    "objectID": "core/notebooks/notebook04_pandas_spark.html#there-is-a-lot-of-missing-values",
    "href": "core/notebooks/notebook04_pandas_spark.html#there-is-a-lot-of-missing-values",
    "title": "Data preprocessing and visualisation of a credit scoring dataset",
    "section": "There is a lot of missing values",
    "text": "There is a lot of missing values\nWe’ll see below that actually a single column mostly contain missing values.\n\n\nCode\npsdf.isnull().sum()\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[16], line 1\n----&gt; 1 psdf.isnull().sum()\n\nNameError: name 'psdf' is not defined\n\n\n\nThe column Prod_Closed_Date contains mostly missing values !\n\n\nCode\npsdf[['Prod_Closed_Date']].head(5)\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[17], line 1\n----&gt; 1 psdf[['Prod_Closed_Date']].head(5)\n\nNameError: name 'psdf' is not defined\n\n\n\nLet’s remove the useless columns and check the remaining missing values\nAgain there are variations. Keyword inplace is not legal in Pandas API on Spark\n\n\nCode\n# df.drop(['Prod_Closed_Date', 'Unnamed: 19', \n#          'Unnamed: 20', 'Unnamed: 21'], axis=\"columns\", inplace=True)\n\npsdf = psdf.drop(['Prod_Closed_Date', \n        'Unnamed: 19', \n        'Unnamed: 20', \n        'Unnamed: 21'], \n        axis=\"columns\")\n        \npsdf.head()         \n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[18], line 4\n      1 # df.drop(['Prod_Closed_Date', 'Unnamed: 19', \n      2 #          'Unnamed: 20', 'Unnamed: 21'], axis=\"columns\", inplace=True)\n----&gt; 4 psdf = psdf.drop(['Prod_Closed_Date', \n      5         'Unnamed: 19', \n      6         'Unnamed: 20', \n      7         'Unnamed: 21'], \n      8         axis=\"columns\")\n     10 psdf.head()         \n\nNameError: name 'psdf' is not defined\n\n\n\nLet’s display the rows with missing values and let’s highlight them\n\n\nCode\n# psdf[psdf.isnull().any(axis=\"columns\")].style.highlight_null()"
  },
  {
    "objectID": "core/notebooks/notebook04_pandas_spark.html#categorial-features-are-str",
    "href": "core/notebooks/notebook04_pandas_spark.html#categorial-features-are-str",
    "title": "Data preprocessing and visualisation of a credit scoring dataset",
    "section": "Categorial features are str",
    "text": "Categorial features are str\nWe need to say the dtype we want to use for some columns using the dtype option of read_csv.\n\n\nCode\ntype(psdf.loc[0, 'Prod_Sub_Category'])\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[20], line 1\n----&gt; 1 type(psdf.loc[0, 'Prod_Sub_Category'])\n\nNameError: name 'psdf' is not defined\n\n\n\n\n\nCode\npsdf['Prod_Sub_Category'].unique()\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[21], line 1\n----&gt; 1 psdf['Prod_Sub_Category'].unique()\n\nNameError: name 'psdf' is not defined"
  },
  {
    "objectID": "core/notebooks/notebook04_pandas_spark.html#the-annual-net-income-is-imported-as-a-string",
    "href": "core/notebooks/notebook04_pandas_spark.html#the-annual-net-income-is-imported-as-a-string",
    "title": "Data preprocessing and visualisation of a credit scoring dataset",
    "section": "The annual net income is imported as a string",
    "text": "The annual net income is imported as a string\nThis problem comes from the fact that the decimal separator is in European notation: it’s a ',' and not a '.', so we need to specify it using the decimal option to read_csv. (Data is French, pardon my French…)\n\n\nCode\ntype(psdf.loc[0, 'Net_Annual_Income'])\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[22], line 1\n----&gt; 1 type(psdf.loc[0, 'Net_Annual_Income'])\n\nNameError: name 'psdf' is not defined\n\n\n\n\n\nCode\npsdf['Net_Annual_Income'].head(n=10)\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[23], line 1\n----&gt; 1 psdf['Net_Annual_Income'].head(n=10)\n\nNameError: name 'psdf' is not defined"
  },
  {
    "objectID": "core/notebooks/notebook04_pandas_spark.html#comment-on-file-formats",
    "href": "core/notebooks/notebook04_pandas_spark.html#comment-on-file-formats",
    "title": "Data preprocessing and visualisation of a credit scoring dataset",
    "section": "Comment on file formats",
    "text": "Comment on file formats\nYou can use other methods starting with .to_XX to save in another format. Here are some main examples\n\nOK to use csv for “small” datasets (several MB)\nUse pickle for more compressed and faster format (limited to 4GB). It’s the standard binary serialization format of Python\nfeather is another fast and lightweight file format for storing data frames. A very popular exchange format.\nparquet is a format for big distributed data (works nicely with Spark)\n\namong several others…\n\n\nCode\n#df.to_pickle(\"gro_cleaned.pkl\")\npssdf.to_parquet(\"gro_cleaned.parquet\")\n# pssdf.reset_index().to_feather(\"gro_cleaned.feather\")\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[37], line 2\n      1 #df.to_pickle(\"gro_cleaned.pkl\")\n----&gt; 2 pssdf.to_parquet(\"gro_cleaned.parquet\")\n      3 # pssdf.reset_index().to_feather(\"gro_cleaned.feather\")\n\nNameError: name 'pssdf' is not defined\n\n\n\n\n\nCode\npssdf.index\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[38], line 1\n----&gt; 1 pssdf.index\n\nNameError: name 'pssdf' is not defined\n\n\n\nAnd you can read again using the corresponding read_XX function\n\n\nCode\npssdf = ps.read_parquet(\"gro_cleaned.parquet\")\npssdf.head()\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[39], line 1\n----&gt; 1 pssdf = ps.read_parquet(\"gro_cleaned.parquet\")\n      2 pssdf.head()\n\nNameError: name 'ps' is not defined\n\n\n\n\n\nCode\n!ls -alh gro_cleaned*\n\n\nls: cannot access 'gro_cleaned*': No such file or directory"
  },
  {
    "objectID": "core/notebooks/notebook04_pandas_spark.html#the-net-income-columns-is-very-weird",
    "href": "core/notebooks/notebook04_pandas_spark.html#the-net-income-columns-is-very-weird",
    "title": "Data preprocessing and visualisation of a credit scoring dataset",
    "section": "The net income columns is very weird",
    "text": "The net income columns is very weird\n\n\nCode\nincome = pssdf['Net_Annual_Income']\nincome.describe()\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[41], line 1\n----&gt; 1 income = pssdf['Net_Annual_Income']\n      2 income.describe()\n\nNameError: name 'pssdf' is not defined\n\n\n\n\n\nCode\n(income &lt;= 100).sum(), (income &gt; 100).sum()\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[42], line 1\n----&gt; 1 (income &lt;= 100).sum(), (income &gt; 100).sum()\n\nNameError: name 'income' is not defined\n\n\n\nMost values are smaller than 100, while some are much much larger…\n\n\nCode\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n%matplotlib inline\n\nsns.set_context(\"notebook\", font_scale=1.2)\n\n\n\n\nCode\nsns.displot(x='Net_Annual_Income', \n            data=pssdf, \n            bins=20,\n            height=4, \n            aspect=1.5)\n\n\n\n\nCode\npssdf[\"Net_Annual_Income\"].plot.hist(bins=40, \nhitsnorm='density', log_x=True)\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[43], line 1\n----&gt; 1 pssdf[\"Net_Annual_Income\"].plot.hist(bins=40, \n      2 hitsnorm='density', log_x=True)\n\nNameError: name 'pssdf' is not defined\n\n\n\n\n\nCode\npssdf[\"Net_Annual_Income\"].plot.kde(bw_method=10, log_x=True)\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[44], line 1\n----&gt; 1 pssdf[\"Net_Annual_Income\"].plot.kde(bw_method=10, log_x=True)\n\nNameError: name 'pssdf' is not defined\n\n\n\nThis is annoying, we don’t really see much…\n\n\nCode\nsns.displot(x='Net_Annual_Income', \n            data=pssdf, \n            bins=20, \n            height=4, \n            aspect=1.5, \n            log_scale=(False, True))\n\n\nDistribution for less than 100K revenue\n\n\nCode\nsns.displot(x='Net_Annual_Income', \n            data=pssdf[pssdf['Net_Annual_Income'] &lt; 100], \n            bins=15, \n            height=4, \n            aspect=1.5)\n\n\nDistribution for less than 400K revenue\n\n\nCode\nsns.displot(x='Net_Annual_Income', data=pssdf[pssdf['Net_Annual_Income'] &lt; 400], \n            bins=15, height=4, aspect=1.5)\n\n\n\n\nCode\n(pssdf['Net_Annual_Income'] == 36.0).sum()\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[45], line 1\n----&gt; 1 (pssdf['Net_Annual_Income'] == 36.0).sum()\n\nNameError: name 'pssdf' is not defined\n\n\n\n\n\nCode\nincome_counts = (\n    ps.DataFrame({\n        \"income_category\": pssdf['Net_Annual_Income'].astype(\"category\"),\n        \"income\": pssdf['Net_Annual_Income']\n    })\n    .groupby(\"income_category\")\n    .count()\n    .reset_index()\n    .rename(columns={\"income\": \"#customers\"})\n    .sort_values(by=\"#customers\", axis=\"index\", ascending=False)\n)\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[46], line 2\n      1 income_counts = (\n----&gt; 2     ps.DataFrame({\n      3         \"income_category\": pssdf['Net_Annual_Income'].astype(\"category\"),\n      4         \"income\": pssdf['Net_Annual_Income']\n      5     })\n      6     .groupby(\"income_category\")\n      7     .count()\n      8     .reset_index()\n      9     .rename(columns={\"income\": \"#customers\"})\n     10     .sort_values(by=\"#customers\", axis=\"index\", ascending=False)\n     11 )\n\nNameError: name 'ps' is not defined\n\n\n\n\n\nCode\nincome_counts[\"%cummulative clients\"] \\\n    = income_counts[\"#customers\"].cumsum() / income_counts[\"#customers\"].sum()\n\nincome_counts.iloc[:20].style.bar(subset=[\"%cummulative clients\"], vmin=0, vmax=1)\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[47], line 2\n      1 income_counts[\"%cummulative clients\"] \\\n----&gt; 2     = income_counts[\"#customers\"].cumsum() / income_counts[\"#customers\"].sum()\n      4 income_counts.iloc[:20].style.bar(subset=[\"%cummulative clients\"], vmin=0, vmax=1)\n\nNameError: name 'income_counts' is not defined\n\n\n\n\nWe have some overrepresented values (many possible explanations for this)\nTo clean the data, we can, for instance, keep only the revenues between [10, 200], or leave it as such\n\n\n\nCode\ndf = df[(df['Net_Annual_Income'] &gt;= 10) & (df['Net_Annual_Income'] &lt;= 200)]\n\nsns.displot(x='Net_Annual_Income', data=df, bins=15, height=4, aspect=1.5)"
  },
  {
    "objectID": "core/notebooks/notebook04_pandas_spark.html#creation-of-the-features-matrix",
    "href": "core/notebooks/notebook04_pandas_spark.html#creation-of-the-features-matrix",
    "title": "Data preprocessing and visualisation of a credit scoring dataset",
    "section": "Creation of the features matrix",
    "text": "Creation of the features matrix\n\n\nCode\ndf[cnt_featnames].head()\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[54], line 1\n----&gt; 1 df[cnt_featnames].head()\n\nNameError: name 'df' is not defined\n\n\n\n\n\nCode\nbin_features = pd.get_dummies(df[cat_featnames],\n                              prefix_sep='#', drop_first=True)\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[55], line 1\n----&gt; 1 bin_features = pd.get_dummies(df[cat_featnames],\n      2                               prefix_sep='#', drop_first=True)\n\nNameError: name 'df' is not defined\n\n\n\n\n\nCode\nbin_features.head()\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[56], line 1\n----&gt; 1 bin_features.head()\n\nNameError: name 'bin_features' is not defined\n\n\n\n\n\nCode\ncnt_features = df[cnt_featnames]\ncnt_features.head()\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[57], line 1\n----&gt; 1 cnt_features = df[cnt_featnames]\n      2 cnt_features.head()\n\nNameError: name 'df' is not defined\n\n\n\n\n\nCode\nfrom pandas import Timestamp\n\ndef age(x):\n    today = Timestamp.today()\n    return (today - x).dt.days\n\ndate_features = df[date_featnames].apply(age, axis=\"index\")\ndate_features.head()\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[58], line 7\n      4     today = Timestamp.today()\n      5     return (today - x).dt.days\n----&gt; 7 date_features = df[date_featnames].apply(age, axis=\"index\")\n      8 date_features.head()\n\nNameError: name 'df' is not defined\n\n\n\n\n\nCode\ntoday = Timestamp.today()\ntoday\n\n\nTimestamp('2025-04-03 15:08:18.726353')\n\n\n\n\nCode\ntt = (today - df[\"BirthDate\"]).loc[0]\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[60], line 1\n----&gt; 1 tt = (today - df[\"BirthDate\"]).loc[0]\n\nNameError: name 'df' is not defined\n\n\n\n\n\nCode\n(today - df[\"BirthDate\"]).dt.days\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[61], line 1\n----&gt; 1 (today - df[\"BirthDate\"]).dt.days\n\nNameError: name 'df' is not defined\n\n\n\n\n\nCode\ntt\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[62], line 1\n----&gt; 1 tt\n\nNameError: name 'tt' is not defined"
  },
  {
    "objectID": "core/notebooks/notebook04_pandas_spark.html#final-features-matrix",
    "href": "core/notebooks/notebook04_pandas_spark.html#final-features-matrix",
    "title": "Data preprocessing and visualisation of a credit scoring dataset",
    "section": "Final features matrix",
    "text": "Final features matrix\n\n\nCode\nall_features = pd.concat([bin_features, cnt_features, date_features], axis=1)\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[63], line 1\n----&gt; 1 all_features = pd.concat([bin_features, cnt_features, date_features], axis=1)\n\nNameError: name 'bin_features' is not defined\n\n\n\n\n\nCode\nall_features.columns\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[64], line 1\n----&gt; 1 all_features.columns\n\nNameError: name 'all_features' is not defined\n\n\n\n\n\nCode\nall_features.head()\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[65], line 1\n----&gt; 1 all_features.head()\n\nNameError: name 'all_features' is not defined\n\n\n\n\n\nCode\ndf_debile = pd.DataFrame({\"nom etudiant\": [\"yiyang\", \"jaouad\", \"mokhtar\", \"massil\", \"simon\"], \n              \"portable\": [True, True, None, True, False]})\n\n\n\n\nCode\ndf_debile\n\n\n\n\n\n\n\n\n\nnom etudiant\nportable\n\n\n\n\n0\nyiyang\nTrue\n\n\n1\njaouad\nTrue\n\n\n2\nmokhtar\nNone\n\n\n3\nmassil\nTrue\n\n\n4\nsimon\nFalse\n\n\n\n\n\n\n\n\n\nCode\ndf_debile.index\n\n\nRangeIndex(start=0, stop=5, step=1)\n\n\n\n\nCode\ndf_debile.dropna().index\n\n\nIndex([0, 1, 3, 4], dtype='int64')\n\n\n\n\nCode\ndf_debile.info()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 5 entries, 0 to 4\nData columns (total 2 columns):\n #   Column        Non-Null Count  Dtype \n---  ------        --------------  ----- \n 0   nom etudiant  5 non-null      object\n 1   portable      4 non-null      object\ndtypes: object(2)\nmemory usage: 212.0+ bytes\n\n\nVERY IMPORTANT: we removed lines of data that contained missing values. The index of the dataframe is therefore not contiguous anymore\n\n\nCode\nall_features.index.max()\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[71], line 1\n----&gt; 1 all_features.index.max()\n\nNameError: name 'all_features' is not defined\n\n\n\nThis could be a problem for later. So let’s reset the index to get a contiguous one\n\n\nCode\nall_features.shape\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[72], line 1\n----&gt; 1 all_features.shape\n\nNameError: name 'all_features' is not defined\n\n\n\n\n\nCode\nall_features.reset_index(inplace=True, drop=True)\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[73], line 1\n----&gt; 1 all_features.reset_index(inplace=True, drop=True)\n\nNameError: name 'all_features' is not defined\n\n\n\n\n\nCode\nall_features.head()\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[74], line 1\n----&gt; 1 all_features.head()\n\nNameError: name 'all_features' is not defined"
  },
  {
    "objectID": "core/notebooks/notebook04_pandas_spark.html#lets-save-the-data-using-pickle",
    "href": "core/notebooks/notebook04_pandas_spark.html#lets-save-the-data-using-pickle",
    "title": "Data preprocessing and visualisation of a credit scoring dataset",
    "section": "Let’s save the data using pickle",
    "text": "Let’s save the data using pickle\n\n\nCode\nimport pickle as pkl\n\nX = all_features\ny = df['Y']\n\n# Let's put eveything in a dictionary\ndf_pkl = {}\n# The features and the labels\ndf_pkl['features'] = X\ndf_pkl['labels'] = y\n# And also the list of columns we built above\ndf_pkl['cnt_featnames'] = cnt_featnames\ndf_pkl['cat_featnames'] = cat_featnames\ndf_pkl['date_featnames'] = date_featnames\n\nwith open(\"gro_training.pkl\", 'wb') as f:\n    pkl.dump(df_pkl, f)\n\n\n\n\nCode\nls -al gro*\n\n\n-rw-rw-r-- 1 boucheron boucheron 9115 avril  3 15:08 gro.csv.gz\n\n\nThe preprocessed data is saved in a pickle file called gro_training.pkdfl.\nDatabricks blog about Koalas, SPIP, Zen\n\npandas users will be able scale their workloads with one simple line change in the upcoming Spark 3.2 release:\n\n&lt;s&gt;from pandas import read_csv&lt;/s&gt;\nfrom pyspark.pandas import read_csv\npdf = read_csv(\"data.csv\")"
  },
  {
    "objectID": "core/notebooks/notebook02_numpy.html",
    "href": "core/notebooks/notebook02_numpy.html",
    "title": "Introduction to numpy",
    "section": "",
    "text": "NumPy is the fundamental package for scientific computing with Python. It contains among other things:\nBesides its obvious scientific uses, NumPy can also be used as an efficient multi-dimensional container for general data. Arbitrary data-types can be defined. This allows NumPy to seamlessly and speedily integrate with a wide variety of databases.\nLibrary documentation: http://numpy.org/"
  },
  {
    "objectID": "core/notebooks/notebook02_numpy.html#the-base-numpy.array-object",
    "href": "core/notebooks/notebook02_numpy.html#the-base-numpy.array-object",
    "title": "Introduction to numpy",
    "section": "The base numpy.array object",
    "text": "The base numpy.array object\n\n\nCode\nimport numpy as np\n\n# declare a vector using a list as the argument\nv = np.array([1, 2.0, 3, 4])\nv\n\n\narray([1., 2., 3., 4.])\n\n\n\n\nCode\nlist([1, 2.0, 3, 4])\n\n\n[1, 2.0, 3, 4]\n\n\n\n\nCode\ntype(v)\n\n\nnumpy.ndarray\n\n\n\n\nCode\nv.shape\n\n\n(4,)\n\n\n\n\nCode\nv.ndim\n\n\n1\n\n\n\n\nCode\nv.dtype is float\n\n\nFalse\n\n\n\n\nCode\nv.dtype \n\n\ndtype('float64')\n\n\n\n\nCode\nnp.uint8 is int\n\n\nFalse\n\n\n\n\n\n\n\n\nTip\n\n\n\n\n\nUse copilot explain to understand the chunks:\n\nThe np.uint8 is a data type in NumPy, representing an unsigned 8-bit integer, which can store values from 0 to 255. The int type is the built-in integer type in Python, which can represent any integer value without a fixed size limit.\n\n\n\n\n\n\nCode\nnp.array([2**120, 2**40], dtype=np.int64)\n\n\n\n---------------------------------------------------------------------------\nOverflowError                             Traceback (most recent call last)\nCell In[9], line 1\n----&gt; 1 np.array([2**120, 2**40], dtype=np.int64)\n\nOverflowError: Python int too large to convert to C long\n\n\n\n\n\nCode\nnp.uint16 is int \n\n\nFalse\n\n\n\n\nCode\nnp.uint32  is int\n\n\nFalse\n\n\n\n\nCode\nw = np.array([1.3, 2, 3, 4], dtype=np.int64)\nw\n\n\narray([1, 2, 3, 4])\n\n\n\n\nCode\nw.dtype\n\n\ndtype('int64')\n\n\n\n\nCode\na = np.arange(100)\n\n\n\n\nCode\ntype(a)\n\n\nnumpy.ndarray\n\n\n\n\nCode\nnp.array(range(100))\n\n\narray([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n       34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50,\n       51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67,\n       68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84,\n       85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99])\n\n\n\n\nCode\na\n\n\narray([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n       34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50,\n       51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67,\n       68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84,\n       85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99])\n\n\n\n\nCode\na.dtype\n\n\ndtype('int64')\n\n\n\n\nCode\n-3 * a ** 2\n\n\narray([     0,     -3,    -12,    -27,    -48,    -75,   -108,   -147,\n         -192,   -243,   -300,   -363,   -432,   -507,   -588,   -675,\n         -768,   -867,   -972,  -1083,  -1200,  -1323,  -1452,  -1587,\n        -1728,  -1875,  -2028,  -2187,  -2352,  -2523,  -2700,  -2883,\n        -3072,  -3267,  -3468,  -3675,  -3888,  -4107,  -4332,  -4563,\n        -4800,  -5043,  -5292,  -5547,  -5808,  -6075,  -6348,  -6627,\n        -6912,  -7203,  -7500,  -7803,  -8112,  -8427,  -8748,  -9075,\n        -9408,  -9747, -10092, -10443, -10800, -11163, -11532, -11907,\n       -12288, -12675, -13068, -13467, -13872, -14283, -14700, -15123,\n       -15552, -15987, -16428, -16875, -17328, -17787, -18252, -18723,\n       -19200, -19683, -20172, -20667, -21168, -21675, -22188, -22707,\n       -23232, -23763, -24300, -24843, -25392, -25947, -26508, -27075,\n       -27648, -28227, -28812, -29403])\n\n\n\n\nCode\na[42] = 13\n\n\n\n\nCode\na[42] = 1025\n\n\n\n\nCode\nnp.info(np.int16)\n\n\n int16()\n\nSigned integer type, compatible with C ``short``.\n\n:Character code: ``'h'``\n:Canonical name: `numpy.short`\n:Alias on this platform (Linux x86_64): `numpy.int16`: 16-bit signed integer (``-32_768`` to ``32_767``).\n\n\nMethods:\n\n  all  --  Scalar method identical to the corresponding array attribute.\n  any  --  Scalar method identical to the corresponding array attribute.\n  argmax  --  Scalar method identical to the corresponding array attribute.\n  argmin  --  Scalar method identical to the corresponding array attribute.\n  argsort  --  Scalar method identical to the corresponding array attribute.\n  astype  --  Scalar method identical to the corresponding array attribute.\n  bit_count  --  int16.bit_count() -&gt; int\n  byteswap  --  Scalar method identical to the corresponding array attribute.\n  choose  --  Scalar method identical to the corresponding array attribute.\n  clip  --  Scalar method identical to the corresponding array attribute.\n  compress  --  Scalar method identical to the corresponding array attribute.\n  conj  --  None\n  conjugate  --  Scalar method identical to the corresponding array attribute.\n  copy  --  Scalar method identical to the corresponding array attribute.\n  cumprod  --  Scalar method identical to the corresponding array attribute.\n  cumsum  --  Scalar method identical to the corresponding array attribute.\n  diagonal  --  Scalar method identical to the corresponding array attribute.\n  dump  --  Scalar method identical to the corresponding array attribute.\n  dumps  --  Scalar method identical to the corresponding array attribute.\n  fill  --  Scalar method identical to the corresponding array attribute.\n  flatten  --  Scalar method identical to the corresponding array attribute.\n  getfield  --  Scalar method identical to the corresponding array attribute.\n  is_integer  --  integer.is_integer() -&gt; bool\n  item  --  Scalar method identical to the corresponding array attribute.\n  max  --  Scalar method identical to the corresponding array attribute.\n  mean  --  Scalar method identical to the corresponding array attribute.\n  min  --  Scalar method identical to the corresponding array attribute.\n  nonzero  --  Scalar method identical to the corresponding array attribute.\n  prod  --  Scalar method identical to the corresponding array attribute.\n  put  --  Scalar method identical to the corresponding array attribute.\n  ravel  --  Scalar method identical to the corresponding array attribute.\n  repeat  --  Scalar method identical to the corresponding array attribute.\n  reshape  --  Scalar method identical to the corresponding array attribute.\n  resize  --  Scalar method identical to the corresponding array attribute.\n  round  --  Scalar method identical to the corresponding array attribute.\n  searchsorted  --  Scalar method identical to the corresponding array attribute.\n  setfield  --  Scalar method identical to the corresponding array attribute.\n  setflags  --  Scalar method identical to the corresponding array attribute.\n  sort  --  Scalar method identical to the corresponding array attribute.\n  squeeze  --  Scalar method identical to the corresponding array attribute.\n  std  --  Scalar method identical to the corresponding array attribute.\n  sum  --  Scalar method identical to the corresponding array attribute.\n  swapaxes  --  Scalar method identical to the corresponding array attribute.\n  take  --  Scalar method identical to the corresponding array attribute.\n  to_device  --  None\n  tobytes  --  None\n  tofile  --  Scalar method identical to the corresponding array attribute.\n  tolist  --  Scalar method identical to the corresponding array attribute.\n  tostring  --  Scalar method identical to the corresponding array attribute.\n  trace  --  Scalar method identical to the corresponding array attribute.\n  transpose  --  Scalar method identical to the corresponding array attribute.\n  var  --  Scalar method identical to the corresponding array attribute.\n  view  --  Scalar method identical to the corresponding array attribute.\n\n\n\n\nCode\nnp.int16\n\n\nnumpy.int16\n\n\n\n\nCode\ndict(enumerate(a))\n\n\n{0: np.int64(0),\n 1: np.int64(1),\n 2: np.int64(2),\n 3: np.int64(3),\n 4: np.int64(4),\n 5: np.int64(5),\n 6: np.int64(6),\n 7: np.int64(7),\n 8: np.int64(8),\n 9: np.int64(9),\n 10: np.int64(10),\n 11: np.int64(11),\n 12: np.int64(12),\n 13: np.int64(13),\n 14: np.int64(14),\n 15: np.int64(15),\n 16: np.int64(16),\n 17: np.int64(17),\n 18: np.int64(18),\n 19: np.int64(19),\n 20: np.int64(20),\n 21: np.int64(21),\n 22: np.int64(22),\n 23: np.int64(23),\n 24: np.int64(24),\n 25: np.int64(25),\n 26: np.int64(26),\n 27: np.int64(27),\n 28: np.int64(28),\n 29: np.int64(29),\n 30: np.int64(30),\n 31: np.int64(31),\n 32: np.int64(32),\n 33: np.int64(33),\n 34: np.int64(34),\n 35: np.int64(35),\n 36: np.int64(36),\n 37: np.int64(37),\n 38: np.int64(38),\n 39: np.int64(39),\n 40: np.int64(40),\n 41: np.int64(41),\n 42: np.int64(1025),\n 43: np.int64(43),\n 44: np.int64(44),\n 45: np.int64(45),\n 46: np.int64(46),\n 47: np.int64(47),\n 48: np.int64(48),\n 49: np.int64(49),\n 50: np.int64(50),\n 51: np.int64(51),\n 52: np.int64(52),\n 53: np.int64(53),\n 54: np.int64(54),\n 55: np.int64(55),\n 56: np.int64(56),\n 57: np.int64(57),\n 58: np.int64(58),\n 59: np.int64(59),\n 60: np.int64(60),\n 61: np.int64(61),\n 62: np.int64(62),\n 63: np.int64(63),\n 64: np.int64(64),\n 65: np.int64(65),\n 66: np.int64(66),\n 67: np.int64(67),\n 68: np.int64(68),\n 69: np.int64(69),\n 70: np.int64(70),\n 71: np.int64(71),\n 72: np.int64(72),\n 73: np.int64(73),\n 74: np.int64(74),\n 75: np.int64(75),\n 76: np.int64(76),\n 77: np.int64(77),\n 78: np.int64(78),\n 79: np.int64(79),\n 80: np.int64(80),\n 81: np.int64(81),\n 82: np.int64(82),\n 83: np.int64(83),\n 84: np.int64(84),\n 85: np.int64(85),\n 86: np.int64(86),\n 87: np.int64(87),\n 88: np.int64(88),\n 89: np.int64(89),\n 90: np.int64(90),\n 91: np.int64(91),\n 92: np.int64(92),\n 93: np.int64(93),\n 94: np.int64(94),\n 95: np.int64(95),\n 96: np.int64(96),\n 97: np.int64(97),\n 98: np.int64(98),\n 99: np.int64(99)}\n\n\n\n\nCode\na + 1\n\n\narray([   1,    2,    3,    4,    5,    6,    7,    8,    9,   10,   11,\n         12,   13,   14,   15,   16,   17,   18,   19,   20,   21,   22,\n         23,   24,   25,   26,   27,   28,   29,   30,   31,   32,   33,\n         34,   35,   36,   37,   38,   39,   40,   41,   42, 1026,   44,\n         45,   46,   47,   48,   49,   50,   51,   52,   53,   54,   55,\n         56,   57,   58,   59,   60,   61,   62,   63,   64,   65,   66,\n         67,   68,   69,   70,   71,   72,   73,   74,   75,   76,   77,\n         78,   79,   80,   81,   82,   83,   84,   85,   86,   87,   88,\n         89,   90,   91,   92,   93,   94,   95,   96,   97,   98,   99,\n        100])\n\n\n\n\nCode\nb = a + 1\nb\n\n\narray([   1,    2,    3,    4,    5,    6,    7,    8,    9,   10,   11,\n         12,   13,   14,   15,   16,   17,   18,   19,   20,   21,   22,\n         23,   24,   25,   26,   27,   28,   29,   30,   31,   32,   33,\n         34,   35,   36,   37,   38,   39,   40,   41,   42, 1026,   44,\n         45,   46,   47,   48,   49,   50,   51,   52,   53,   54,   55,\n         56,   57,   58,   59,   60,   61,   62,   63,   64,   65,   66,\n         67,   68,   69,   70,   71,   72,   73,   74,   75,   76,   77,\n         78,   79,   80,   81,   82,   83,   84,   85,   86,   87,   88,\n         89,   90,   91,   92,   93,   94,   95,   96,   97,   98,   99,\n        100])\n\n\n\n\nCode\na is b\n\n\nFalse\n\n\n\n\nCode\nf = id(a)\na += 1\nf, id(a)\n\n\n(134005493570928, 134005493570928)\n\n\n\n\nCode\na\n\n\narray([   1,    2,    3,    4,    5,    6,    7,    8,    9,   10,   11,\n         12,   13,   14,   15,   16,   17,   18,   19,   20,   21,   22,\n         23,   24,   25,   26,   27,   28,   29,   30,   31,   32,   33,\n         34,   35,   36,   37,   38,   39,   40,   41,   42, 1026,   44,\n         45,   46,   47,   48,   49,   50,   51,   52,   53,   54,   55,\n         56,   57,   58,   59,   60,   61,   62,   63,   64,   65,   66,\n         67,   68,   69,   70,   71,   72,   73,   74,   75,   76,   77,\n         78,   79,   80,   81,   82,   83,   84,   85,   86,   87,   88,\n         89,   90,   91,   92,   93,   94,   95,   96,   97,   98,   99,\n        100])\n\n\n\n\nCode\nb\n\n\narray([   1,    2,    3,    4,    5,    6,    7,    8,    9,   10,   11,\n         12,   13,   14,   15,   16,   17,   18,   19,   20,   21,   22,\n         23,   24,   25,   26,   27,   28,   29,   30,   31,   32,   33,\n         34,   35,   36,   37,   38,   39,   40,   41,   42, 1026,   44,\n         45,   46,   47,   48,   49,   50,   51,   52,   53,   54,   55,\n         56,   57,   58,   59,   60,   61,   62,   63,   64,   65,   66,\n         67,   68,   69,   70,   71,   72,   73,   74,   75,   76,   77,\n         78,   79,   80,   81,   82,   83,   84,   85,   86,   87,   88,\n         89,   90,   91,   92,   93,   94,   95,   96,   97,   98,   99,\n        100])\n\n\n\n\n\n\n\n\nWarning\n\n\n\nBeware of the dimensions: a 1D array is not the same as a 2D array with 1 column\n\n\n\n\nCode\na1 = np.array([1, 2, 3])\nprint(a1, a1.shape, a1.ndim)\n\n\n[1 2 3] (3,) 1\n\n\n\n\nCode\na2 = np.array([1, 2, 3])\nprint(a2, a2.shape, a2.ndim)\n\n\n[1 2 3] (3,) 1\n\n\nMore on NumPy quickstart\n\n\n\n\n\n\nNote\n\n\n\nList the attributes and methods of class numpy.ndarray. You may use function dir() and filter the result using methods for objects of class string."
  },
  {
    "objectID": "core/notebooks/notebook02_numpy.html#matrix-multiplication",
    "href": "core/notebooks/notebook02_numpy.html#matrix-multiplication",
    "title": "Introduction to numpy",
    "section": "Matrix multiplication",
    "text": "Matrix multiplication\n\n\nCode\na2.dot(a1) # inner product \n\n\nnp.int64(14)\n\n\n\n\nCode\n( \n    np.array([a2])\n        .transpose() # column vector\n        .dot(np.array([a1]))\n) # column vector multiplied by row vector\n\n\narray([[1, 2, 3],\n       [2, 4, 6],\n       [3, 6, 9]])\n\n\n\n\nCode\n(\n    np.array([a2])\n    .transpose()#.shape\n)\n\n\narray([[1],\n       [2],\n       [3]])\n\n\n\n\nCode\n(\n    a2.reshape(3,1)  # all explicit\n      .dot(a1.reshape(1, 3))\n)\n\n\narray([[1, 2, 3],\n       [2, 4, 6],\n       [3, 6, 9]])\n\n\n\n\nCode\n# Declare a 2D array using a nested list as the constructor argument\nM = np.array([[1,2], \n              [3,4], \n              [3.14, -9.17]])\nM\n\n\narray([[ 1.  ,  2.  ],\n       [ 3.  ,  4.  ],\n       [ 3.14, -9.17]])\n\n\n\n\nCode\nM.shape, M.size\n\n\n((3, 2), 6)\n\n\n\n\nCode\nM.ravel(), M.ndim, M.ravel().shape\n\n\n(array([ 1.  ,  2.  ,  3.  ,  4.  ,  3.14, -9.17]), 2, (6,))\n\n\n\n\nCode\n# arguments: start, stop, step\nx = (\n     np.arange(12)\n       .reshape(4, 3)\n)\nx\n\n\narray([[ 0,  1,  2],\n       [ 3,  4,  5],\n       [ 6,  7,  8],\n       [ 9, 10, 11]])\n\n\n\n\nCode\ny = np.arange(3).reshape(3,1)\n\ny\n\n\narray([[0],\n       [1],\n       [2]])\n\n\n\n\nCode\nx @ y, x.dot(y)\n\n\n(array([[ 5],\n        [14],\n        [23],\n        [32]]),\n array([[ 5],\n        [14],\n        [23],\n        [32]]))\n\n\n\n\n\n\n\n\nNote"
  },
  {
    "objectID": "core/notebooks/notebook02_numpy.html#generating-arrays",
    "href": "core/notebooks/notebook02_numpy.html#generating-arrays",
    "title": "Introduction to numpy",
    "section": "Generating arrays",
    "text": "Generating arrays\n\n\nCode\nnp.linspace(0, 10, 51)  # meaning of the 3 positional parameters ? \n\n\narray([ 0. ,  0.2,  0.4,  0.6,  0.8,  1. ,  1.2,  1.4,  1.6,  1.8,  2. ,\n        2.2,  2.4,  2.6,  2.8,  3. ,  3.2,  3.4,  3.6,  3.8,  4. ,  4.2,\n        4.4,  4.6,  4.8,  5. ,  5.2,  5.4,  5.6,  5.8,  6. ,  6.2,  6.4,\n        6.6,  6.8,  7. ,  7.2,  7.4,  7.6,  7.8,  8. ,  8.2,  8.4,  8.6,\n        8.8,  9. ,  9.2,  9.4,  9.6,  9.8, 10. ])\n\n\n\n\nCode\nnp.logspace(0, 10, 11, base=np.e), np.e**(np.arange(11))\n\n\n(array([1.00000000e+00, 2.71828183e+00, 7.38905610e+00, 2.00855369e+01,\n        5.45981500e+01, 1.48413159e+02, 4.03428793e+02, 1.09663316e+03,\n        2.98095799e+03, 8.10308393e+03, 2.20264658e+04]),\n array([1.00000000e+00, 2.71828183e+00, 7.38905610e+00, 2.00855369e+01,\n        5.45981500e+01, 1.48413159e+02, 4.03428793e+02, 1.09663316e+03,\n        2.98095799e+03, 8.10308393e+03, 2.20264658e+04]))\n\n\n\n\nCode\nimport matplotlib.pyplot as plt\n\n# Random standard Gaussian numbers\nfig = plt.figure(figsize=(8, 4))\nwn = np.random.randn(1000)\nbm = wn.cumsum()\n\nplt.plot(bm, lw=3)\n\n\n\n\n\n\n\n\n\n\n\nCode\nnp.diag(np.arange(10))\n\n\narray([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 2, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 3, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 4, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 5, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 6, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 7, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 8, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 0, 9]])\n\n\n\n\nCode\nzozo = np.zeros((10, 10), dtype=np.float32)\nzozo\n\n\narray([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]], dtype=float32)\n\n\n\n\nCode\nzozo.shape\n\n\n(10, 10)\n\n\n\n\nCode\nprint(M)\n\n\n[[ 1.    2.  ]\n [ 3.    4.  ]\n [ 3.14 -9.17]]\n\n\n\n\nCode\nM[1, 1]\n\n\nnp.float64(4.0)\n\n\n\n\nCode\n# assign new value\nM[0, 0] = 7\nM[:, 0] = 42\nM\n\n\narray([[42.  ,  2.  ],\n       [42.  ,  4.  ],\n       [42.  , -9.17]])\n\n\n\n\nCode\nM\n\n\narray([[42.  ,  2.  ],\n       [42.  ,  4.  ],\n       [42.  , -9.17]])\n\n\n\n\nCode\n# Warning: the next m is a **view** on M. \n# One again, no copies unless you ask for one!\nm = M[0, :]\nm\n\n\narray([42.,  2.])\n\n\n\n\nCode\nm[:] = 3.14\nM\n\n\narray([[ 3.14,  3.14],\n       [42.  ,  4.  ],\n       [42.  , -9.17]])\n\n\n\n\nCode\nm[:] = 7\nM\n\n\narray([[ 7.  ,  7.  ],\n       [42.  ,  4.  ],\n       [42.  , -9.17]])"
  },
  {
    "objectID": "core/notebooks/notebook02_numpy.html#copies",
    "href": "core/notebooks/notebook02_numpy.html#copies",
    "title": "Introduction to numpy",
    "section": "Copies",
    "text": "Copies\nDon’t forget that python does not make copies unless told to do so (same as with any mutable type)\nIf you are not careful enough, this typically leads to a lot of errors and to being fired !!\n\n\nCode\ny = x = np.arange(6)\nx[2] = 123\ny\n\n\narray([  0,   1, 123,   3,   4,   5])\n\n\n\n\nCode\nx is y\n\n\nTrue\n\n\n\n\nCode\n# A real copy\ny = x.copy()\nx is y \n\n\nFalse\n\n\n\n\nCode\n# Or equivalently (but the one above is better...)\ny = np.copy(x)\n\n\n\n\nCode\nx[0] = -12\nprint(x, y, x is y)\n\n\n[-12   1 123   3   4   5] [  0   1 123   3   4   5] False\n\n\nTo put values of x in y (copy values into an existing array) use\n\n\nCode\nx = np.random.randn(10)\nx, id(x)\n\n\n(array([ 0.38046554,  0.74957713, -1.24302298,  0.63151793, -2.31400049,\n        -0.16159741,  0.83592963,  2.16443975, -0.05600288, -0.70457148]),\n 134005491205392)\n\n\n\n\nCode\nx.fill(2.78)   # in place. \nx, id(x)\n\n\n(array([2.78, 2.78, 2.78, 2.78, 2.78, 2.78, 2.78, 2.78, 2.78, 2.78]),\n 134005491205392)\n\n\n\n\nCode\nx[:] = 3.14  # x.fill(3.14)  can. be chained ...\nx, id(x)\n\n\n(array([3.14, 3.14, 3.14, 3.14, 3.14, 3.14, 3.14, 3.14, 3.14, 3.14]),\n 134005491205392)\n\n\n\n\nCode\nx[:] = np.random.randn(x.shape[0])\nx, id(x)\n\n\n(array([ 0.62933532, -1.23653201,  0.13433891,  0.40760153,  1.05453221,\n        -0.24900888,  1.15975529,  0.0703507 ,  0.64917964,  0.71881338]),\n 134005491205392)\n\n\n\n\nCode\ny = np.empty(x.shape)  # how does empty() work ?\ny, id(y)\n\n\n(array([0.62933532, 1.23653201, 0.13433891, 0.40760153, 1.05453221,\n        0.24900888, 1.15975529, 0.0703507 , 0.64917964, 0.71881338]),\n 134005491205200)\n\n\n\n\nCode\ny = x\ny, id(y), id(x), y is x\n\n\n(array([ 0.62933532, -1.23653201,  0.13433891,  0.40760153,  1.05453221,\n        -0.24900888,  1.15975529,  0.0703507 ,  0.64917964,  0.71881338]),\n 134005491205392,\n 134005491205392,\n True)\n\n\n\n\n\n\n\n\nFinal warning\n\n\n\n\n\n\nIn the next line you copy the values of x into an existing array y (of same size…)\n\n\nCode\ny = np.zeros(x.shape)\ny[:] = x\ny, y is x, np.all(y==x)\n\n\n(array([ 0.62933532, -1.23653201,  0.13433891,  0.40760153,  1.05453221,\n        -0.24900888,  1.15975529,  0.0703507 ,  0.64917964,  0.71881338]),\n False,\n np.True_)\n\n\nWhile in the next line, you are aliasing, you are giving a new name y to the object named x (you should never, ever write something like this)\n\n\nCode\ny = x\ny is x\n\n\nTrue"
  },
  {
    "objectID": "core/notebooks/notebook02_numpy.html#miscellanea",
    "href": "core/notebooks/notebook02_numpy.html#miscellanea",
    "title": "Introduction to numpy",
    "section": "Miscellanea",
    "text": "Miscellanea\n\nNon-numerical values\nA numpy array can contain other things than numeric types\n\n\nCode\narr = np.array(['Labore', 'neque', 'ipsum', 'ut', 'non', 'quiquia', 'dolore.'])\narr, arr.shape, arr.dtype\n\n\n(array(['Labore', 'neque', 'ipsum', 'ut', 'non', 'quiquia', 'dolore.'],\n       dtype='&lt;U7'),\n (7,),\n dtype('&lt;U7'))\n\n\n\n\nCode\n# arr.sum()\n\n\n\n\nCode\n\"_\".join(arr)\n\n\n'Labore_neque_ipsum_ut_non_quiquia_dolore.'\n\n\n\n\nCode\narr.dtype\n\n\ndtype('&lt;U7')"
  },
  {
    "objectID": "core/notebooks/notebook02_numpy.html#a-matrix-is-no-2d-array-in-numpy",
    "href": "core/notebooks/notebook02_numpy.html#a-matrix-is-no-2d-array-in-numpy",
    "title": "Introduction to numpy",
    "section": "A matrix is no 2D array in numpy",
    "text": "A matrix is no 2D array in numpy\nSo far, we have only used array or ndarray objects\nThe is another type: the matrix type\nIn words: don’t use it (IMhO) and stick with arrays\n\n\nCode\n# Matrix VS array objects in numpy\nm1 = np.matrix(np.arange(3))\nm2 = np.matrix(np.arange(3))\nm1, m2\n\n\n(matrix([[0, 1, 2]]), matrix([[0, 1, 2]]))\n\n\n\n\nCode\nm1.transpose() @ m2, m1.shape, m1.transpose() * m2\n\n\n(matrix([[0, 0, 0],\n         [0, 1, 2],\n         [0, 2, 4]]),\n (1, 3),\n matrix([[0, 0, 0],\n         [0, 1, 2],\n         [0, 2, 4]]))\n\n\n\n\nCode\na1 = np.arange(3)\na2 = np.arange(3)\na1, a2\n\n\n(array([0, 1, 2]), array([0, 1, 2]))\n\n\n\n\nCode\nm1 * m2.T, m1.dot(m2.T)\n\n\n(matrix([[5]]), matrix([[5]]))\n\n\n\n\nCode\na1 * a2\n\n\narray([0, 1, 4])\n\n\n\n\nCode\na1.dot(a2)\n\n\nnp.int64(5)\n\n\n\n\nCode\nnp.outer(a1, a2)\n\n\narray([[0, 0, 0],\n       [0, 1, 2],\n       [0, 2, 4]])\n\n\n\n\n\n\n\n\nNote\n\n\n\nVisit https://numpy.org/doc/stable/reference/arrays.ndarray.html#arrays-ndarray"
  },
  {
    "objectID": "core/notebooks/notebook02_numpy.html#sparse-matrices",
    "href": "core/notebooks/notebook02_numpy.html#sparse-matrices",
    "title": "Introduction to numpy",
    "section": "Sparse matrices",
    "text": "Sparse matrices\n\n\nCode\nfrom scipy.sparse import csc_matrix, csr_matrix, coo_matrix\n\n\n\n\nCode\nprobs = np.full(fill_value=1/4, shape=(4,))\nprobs\n\n\narray([0.25, 0.25, 0.25, 0.25])\n\n\n\n\nCode\nX = np.random.multinomial(n=2, pvals=probs, size=4)   # check you understand what is going on \nX\n\n\narray([[0, 0, 0, 2],\n       [0, 0, 1, 1],\n       [1, 1, 0, 0],\n       [1, 0, 0, 1]])\n\n\n\n\nCode\nprobs\n\n\narray([0.25, 0.25, 0.25, 0.25])\n\n\n\n\nCode\nX_coo = coo_matrix(X)  ## coordinate format\n\n\n\n\nCode\nprint(X_coo)\nX_coo\n\n\n&lt;COOrdinate sparse matrix of dtype 'int64'\n    with 7 stored elements and shape (4, 4)&gt;\n  Coords    Values\n  (0, 3)    2\n  (1, 2)    1\n  (1, 3)    1\n  (2, 0)    1\n  (2, 1)    1\n  (3, 0)    1\n  (3, 3)    1\n\n\n&lt;COOrdinate sparse matrix of dtype 'int64'\n    with 7 stored elements and shape (4, 4)&gt;\n\n\n\n\nCode\nX_coo.nnz    # number pf non-zero coordinates \n\n\n7\n\n\n\n\nCode\nprint(X, end='\\n----\\n')\nprint(X_coo.data, end='\\n----\\n')\nprint(X_coo.row, end='\\n----\\n')\nprint(X_coo.col, end='\\n----\\n')\n\n\n[[0 0 0 2]\n [0 0 1 1]\n [1 1 0 0]\n [1 0 0 1]]\n----\n[2 1 1 1 1 1 1]\n----\n[0 1 1 2 2 3 3]\n----\n[3 2 3 0 1 0 3]\n----\n\n\nThere is also\n\ncsr_matrix: sparse rows format\ncsc_matrix: sparse columns format\n\nSparse rows is often used for machine learning: sparse features vectors\nBut sparse column format useful as well (e.g. coordinate gradient descent)"
  },
  {
    "objectID": "core/notebooks/notebook02_numpy.html#bored-with-decimals",
    "href": "core/notebooks/notebook02_numpy.html#bored-with-decimals",
    "title": "Introduction to numpy",
    "section": "Bored with decimals?",
    "text": "Bored with decimals?\n\n\nCode\nX = np.random.randn(5, 5)\nX\n\n\narray([[ 0.83342913,  0.08040693, -1.07220735,  0.87379811,  0.59755824],\n       [ 0.32755901,  0.10432441,  0.17615545,  0.07566302, -0.02653956],\n       [ 0.19095091, -0.71199912,  0.20379812, -1.08511624, -0.50568562],\n       [ 0.21171716, -0.74242228, -0.37705104,  0.91580081, -1.48349099],\n       [-0.66164491,  1.00021721,  0.93630598,  0.22694503,  1.17406202]])\n\n\n\n\nCode\n# All number displayed by numpy (in the current kernel) are with 3 decimals max\nnp.set_printoptions(precision=3)\nprint(X)\nnp.set_printoptions(precision=8)\n\n\n[[ 0.833  0.08  -1.072  0.874  0.598]\n [ 0.328  0.104  0.176  0.076 -0.027]\n [ 0.191 -0.712  0.204 -1.085 -0.506]\n [ 0.212 -0.742 -0.377  0.916 -1.483]\n [-0.662  1.     0.936  0.227  1.174]]"
  },
  {
    "objectID": "core/notebooks/notebook02_numpy.html#not-limited-to-2d",
    "href": "core/notebooks/notebook02_numpy.html#not-limited-to-2d",
    "title": "Introduction to numpy",
    "section": "Not limited to 2D!",
    "text": "Not limited to 2D!\nnumpy arrays can have any number of dimension (hence the name ndarray)\n\n\nCode\nX = np.arange(18).reshape(3, 2, 3)\nX\n\n\narray([[[ 0,  1,  2],\n        [ 3,  4,  5]],\n\n       [[ 6,  7,  8],\n        [ 9, 10, 11]],\n\n       [[12, 13, 14],\n        [15, 16, 17]]])\n\n\n\n\nCode\nX.shape\n\n\n(3, 2, 3)\n\n\n\n\nCode\nX.ndim\n\n\n3\n\n\nVisit https://numpy.org/doc/stable/reference/arrays.ndarray.html#arrays-ndarray"
  },
  {
    "objectID": "core/notebooks/notebook02_numpy.html#inner-products",
    "href": "core/notebooks/notebook02_numpy.html#inner-products",
    "title": "Introduction to numpy",
    "section": "Inner products",
    "text": "Inner products\n\n\nCode\n# Inner product between vectors\nprint(v1.dot(v2))\n\n# You can use also (but first solution is better)\nprint(np.dot(v1, v2))\n\n\n80\n80\n\n\n\n\nCode\nA, v1\n\n\n(array([[ 0,  1,  2,  3,  4],\n        [ 5,  6,  7,  8,  9],\n        [10, 11, 12, 13, 14],\n        [15, 16, 17, 18, 19],\n        [20, 21, 22, 23, 24],\n        [25, 26, 27, 28, 29]]),\n array([0, 1, 2, 3, 4]))\n\n\n\n\nCode\nA.shape, v1.shape\n\n\n((6, 5), (5,))\n\n\n\n\nCode\n# Matrix-vector inner product\nA.dot(v1)\n\n\narray([ 30,  80, 130, 180, 230, 280])\n\n\n\n\nCode\n# Transpose\nA.T\n\n\narray([[ 0,  5, 10, 15, 20, 25],\n       [ 1,  6, 11, 16, 21, 26],\n       [ 2,  7, 12, 17, 22, 27],\n       [ 3,  8, 13, 18, 23, 28],\n       [ 4,  9, 14, 19, 24, 29]])\n\n\n\n\nCode\nprint(v1)\n# Inline operations (same for *=, /=, -=)\nv1 += 2\n\n\n[0 1 2 3 4]"
  },
  {
    "objectID": "core/notebooks/notebook02_numpy.html#linear-systems",
    "href": "core/notebooks/notebook02_numpy.html#linear-systems",
    "title": "Introduction to numpy",
    "section": "Linear systems",
    "text": "Linear systems\n\n\nCode\nA = np.array([[42,2,3], [4,5,6], [7,8,9]])\nb = np.array([1,2,3])\nprint(A, b, sep=2 * '\\n')\n\n\n[[42  2  3]\n [ 4  5  6]\n [ 7  8  9]]\n\n[1 2 3]\n\n\n\n\nCode\n# solve a system of linear equations\nx = np.linalg.solve(A, b)\nx\n\n\narray([2.18366847e-18, 2.31698718e-16, 3.33333333e-01])\n\n\n\n\nCode\nA.dot(x)\n\n\narray([1., 2., 3.])"
  },
  {
    "objectID": "core/notebooks/notebook02_numpy.html#eigenvalues-and-eigenvectors",
    "href": "core/notebooks/notebook02_numpy.html#eigenvalues-and-eigenvectors",
    "title": "Introduction to numpy",
    "section": "Eigenvalues and eigenvectors",
    "text": "Eigenvalues and eigenvectors\n\n\nCode\nA = np.random.rand(3,3)\nB = np.random.rand(3,3)\n\nevals, evecs = np.linalg.eig(A)\nevals\n\n\narray([ 1.30118494,  0.68860835, -0.46662428])\n\n\n\n\nCode\nevecs\n\n\narray([[-0.81651435, -0.96384507, -0.42136945],\n       [-0.40428605,  0.13465887, -0.55860129],\n       [-0.41213723,  0.22993406,  0.71443152]])"
  },
  {
    "objectID": "core/notebooks/notebook02_numpy.html#singular-value-decomposition-svd",
    "href": "core/notebooks/notebook02_numpy.html#singular-value-decomposition-svd",
    "title": "Introduction to numpy",
    "section": "Singular value decomposition (SVD)",
    "text": "Singular value decomposition (SVD)\nDecomposes any matrix \\(A \\in \\mathbb R^{m \\times n}\\) as follows: \\[\nA = U \\times S \\times V^\\top\n\\] where - \\(U\\) and \\(V\\) are orthonormal matrices (meaning that \\(U^\\top \\times U = I\\) and \\(V^\\top \\times V = I\\)) - \\(S\\) is a diagonal matrix that contains the singular values in non-increasing order\n\n\nCode\nprint(A)\nU, S, V = np.linalg.svd(A)\n\n\n[[0.88413509 0.01677612 0.80979055]\n [0.11948734 0.34283133 0.70337219]\n [0.03862284 0.94649455 0.29620259]]\n\n\n\n\nCode\nU.dot(np.diag(S)).dot(V)\n\n\narray([[0.88413509, 0.01677612, 0.80979055],\n       [0.11948734, 0.34283133, 0.70337219],\n       [0.03862284, 0.94649455, 0.29620259]])\n\n\n\n\nCode\nA - U @ np.diag(S) @ V\n\n\narray([[ 4.44089210e-16,  1.90819582e-16, -6.66133815e-16],\n       [-2.77555756e-17,  2.77555756e-16,  1.11022302e-16],\n       [-5.48172618e-16,  1.11022302e-16,  2.22044605e-16]])\n\n\n\n\nCode\n# U and V are indeed orthonormal\nnp.set_printoptions(precision=2)\nprint(U.T.dot(U), V.T.dot(V), sep=2 * '\\n')\nnp.set_printoptions(precision=8)\n\n\n[[ 1.00e+00 -1.27e-17  5.56e-17]\n [-1.27e-17  1.00e+00 -1.55e-17]\n [ 5.56e-17 -1.55e-17  1.00e+00]]\n\n[[ 1.00e+00  5.45e-17  1.05e-16]\n [ 5.45e-17  1.00e+00 -6.47e-17]\n [ 1.05e-16 -6.47e-17  1.00e+00]]"
  },
  {
    "objectID": "core/notebooks/notebook02_numpy.html#exercice-the-racoon-svd",
    "href": "core/notebooks/notebook02_numpy.html#exercice-the-racoon-svd",
    "title": "Introduction to numpy",
    "section": "Exercice: the racoon SVD",
    "text": "Exercice: the racoon SVD\n\nLoad the racoon face picture using scipy.misc.face()\nVisualize the picture\nWrite a function which reshapes the picture into a 2D array, and computes the best rank-r approximation of it (the prototype of the function is compute_approx(X, r)\nDisplay the different approximations for r between 5 and 100\n\n\n\nCode\n!pip3 install pooch\n\n\nRequirement already satisfied: pooch in /home/boucheron/sandbox/IFEBY310/.venv/lib/python3.12/site-packages (1.8.2)\nRequirement already satisfied: platformdirs&gt;=2.5.0 in /home/boucheron/sandbox/IFEBY310/.venv/lib/python3.12/site-packages (from pooch) (4.3.6)\nRequirement already satisfied: packaging&gt;=20.0 in /home/boucheron/sandbox/IFEBY310/.venv/lib/python3.12/site-packages (from pooch) (24.2)\nRequirement already satisfied: requests&gt;=2.19.0 in /home/boucheron/sandbox/IFEBY310/.venv/lib/python3.12/site-packages (from pooch) (2.32.3)\nRequirement already satisfied: charset-normalizer&lt;4,&gt;=2 in /home/boucheron/sandbox/IFEBY310/.venv/lib/python3.12/site-packages (from requests&gt;=2.19.0-&gt;pooch) (3.4.1)\nRequirement already satisfied: idna&lt;4,&gt;=2.5 in /home/boucheron/sandbox/IFEBY310/.venv/lib/python3.12/site-packages (from requests&gt;=2.19.0-&gt;pooch) (3.10)\nRequirement already satisfied: urllib3&lt;3,&gt;=1.21.1 in /home/boucheron/sandbox/IFEBY310/.venv/lib/python3.12/site-packages (from requests&gt;=2.19.0-&gt;pooch) (2.3.0)\nRequirement already satisfied: certifi&gt;=2017.4.17 in /home/boucheron/sandbox/IFEBY310/.venv/lib/python3.12/site-packages (from requests&gt;=2.19.0-&gt;pooch) (2024.12.14)\n\n\n\n\nCode\nimport numpy as np\nfrom scipy.datasets import face\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nX = face()\n\n\n\n\nCode\ntype(X)\n\n\nnumpy.ndarray\n\n\n\n\nCode\nplt.imshow(X)\n_ = plt.axis('off')\n\n\n\n\n\n\n\n\n\n\n\nCode\nn_rows, n_cols, n_channels = X.shape\nX_reshaped = X.reshape(n_rows, n_cols * n_channels)\nU, S, V = np.linalg.svd(X_reshaped, full_matrices=False)\n\n\n\n\nCode\nX_reshaped.shape\n\n\n(768, 3072)\n\n\n\n\nCode\nX.shape\n\n\n(768, 1024, 3)\n\n\n\n\nCode\nplt.plot(S**2)  ## a kind of screeplot\nplt.yscale(\"log\")\n\n\n\n\n\n\n\n\n\n\n\nCode\ndef compute_approx(X: np.ndarray, r: int):\n    \"\"\"Computes the best rank-r approximation of X using SVD.\n    We expect X to the 3D array corresponding to a color image, that we \n    reduce to a 2D one to apply SVD (no broadcasting).\n    \n    Parameters\n    ----------\n    X : `np.ndarray`, shape=(n_rows, n_cols, 3)\n        The input 3D ndarray\n    \n    r : `int`\n        The desired rank\n        \n    Return\n    ------\n    output : `np.ndarray`, shape=(n_rows, n_cols, 3)\n        The best rank-r approximation of X\n    \"\"\"\n    n_rows, n_cols, n_channels = X.shape\n    # Reshape X to a 2D array\n    X_reshape = X.reshape(n_rows, n_cols * n_channels)\n    # Compute SVD\n    U, S, V = np.linalg.svd(X_reshape, full_matrices=False)\n    # Keep only the top r first singular values\n    S[r:] = 0\n    # Compute the approximation\n    X_reshape_r = U.dot(np.diag(S)).dot(V)\n    # Put it between 0 and 255 again and cast to integer type\n    return X_reshape_r.clip(min=0, max=255).astype('int')\\\n        .reshape(n_rows, n_cols, n_channels)\n\n\n\n\nCode\nranks = [100, 70, 50, 30, 10, 5]\nn_ranks = len(ranks)\nfor i, r in enumerate(ranks):\n    X_r = compute_approx(X, r)\n    # plt.subplot(n_ranks, 1, i + 1)\n    plt.figure(figsize=(5, 5))\n    plt.imshow(X_r)\n    _ = plt.axis('off')\n    # plt.title(f'Rank {r} approximation of the racoon' % r, fontsize=16)\n    plt.tight_layout()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVariations\n\n\n\nIn the code above, we recompute the SVD of X for every element in list rank.\nIn the next chunk, we compute the SVD once, and define a generator to generate the low rank approximations of matrix X. We take advantage of the fact that the SVD defines an orthonormal basis for the space of matrices. In this adapted orthonormal basis the optimal low rank approximations of \\(X\\) have a sparse expansion.\n\n\n\n\nCode\ndef gen_rank_k_approx(X):\n    \"\"\"Generator for low rank \n    approximation of a matrix X using truncated SVD.\n\n    Args:\n        X (numpy.ndarray): a numerical matrix\n\n    Yields:\n        (int,numpy.ndarray): rank k and best rank-k approximation of X using truncated SVD(according to Eckart-Young theorem).\n    \"\"\"  \n    U, S, V = np.linalg.svd(X, full_matrices=False)\n    r = 0\n    Y = np.zeros_like(X, dtype='float64')\n    while (r&lt;len(S)):\n      Y = Y + S[r] * (U[:,r,np.newaxis] @ V[r,:, np.newaxis].T)\n      r += 1\n      yield r, Y\n\n\n\n\nCode\ng = gen_rank_k_approx(X_reshaped) \n\n\n\n\nCode\nfor i in range(100):\n    _, Xr = next(g)\n    if i % 10 ==0:  \n      plt.figure(figsize=(5, 5))\n      plt.imshow(\n          Xr\n          .clip(min=0, max=255)\n          .astype('int')\n          .reshape(n_rows, n_cols, n_channels)\n      )\n      _ = plt.axis('off')\n      plt.tight_layout()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVisit https://numpy.org/numpy-tutorials/content/tutorial-svd.html"
  },
  {
    "objectID": "core/notebooks/notebook-0.html",
    "href": "core/notebooks/notebook-0.html",
    "title": "Notebook 0",
    "section": "",
    "text": "Iterators\n\n\nGenerators\n\n\nCoroutines"
  },
  {
    "objectID": "computing-pipelines.html",
    "href": "computing-pipelines.html",
    "title": "Pipelines",
    "section": "",
    "text": "Interroger une base de données avec R (via ODBC)\n\n\nInterroger une base de données avec Python (via ODBC)"
  },
  {
    "objectID": "computing-access.html",
    "href": "computing-access.html",
    "title": "Moyens de calcul",
    "section": "",
    "text": "Serveur\nLe cours et les TP\n\nServeur PostGreSQL\nMachine dédiée : etu-pgsql.math.univ-paris-diderot.fr\n\n\n\nClients\nEn salle TP, vous pourrez choisir entre trois clients\n\npsql\npgcli\ndbeaver\n\npsql et pgcli sont très proches. Ce sont des applications qui fonctionnent en mode ligne de commande. pgcli est un peu plus conviviale que psql avec un système de complétion plus performant. L’ensemble des commandes spéciales proposées par pgcli est un peu moins vaste que celui proposé par psql\ndbeaver est un client graphique qui ne tombe pas dans le cliquodrome. dbeaver permet d’attaquer une grande famille de SGBDs.\nTous ces clients doivent utiliser des connexions sécurisées ssh.\n\n\nConnexions ssh (Linux/MacOS)\nPour accéder au serveur Postgres (SGBD), il faut d’abord se connecter à la machine qui héberge ce serveur etu-pgsql.math.univ-paris-diderot.fr.\nPour se connecter à etu-pgsql.math.univ-paris-diderot.fr, on utilise le protocole ssh, avec son identifiant et son mot de passe ENT.\n$ ssh username@etu-pgsql.math.univ-paris-diderot.fr\n(username@etu-pgsql.math.univ-paris-diderot.fr) Password for username@etu-pgsql.math.univ-paris-diderot.fr:\nLast login: ..................\n....\n....\nThis fortune brought to you by:\n$FreeBSD$\n[username@etu-pgsql ~]$ ...\nAttention : pas d’écho lorsque vous saisissez votre mot de passe.\nOn peut maintenant se connecter au serveur Postgres, toujours avec son identifiant et son mot de passe ENT\nIci, nous proposons d’utiliser le client pgcli exécuté sur etu-pgsql.math.univ-paris-diderot.fr :\n[username@etu-pgsql ~]$ pgcli -d bd_2023-24 \nPassword for username: \nServer: PostgreSQL 13.8\nVersion: 3.5.0\nHome: http://pgcli.com\nuserame@/tmp:bd_2023-24&gt; \n\n\n\n\n\n\nPourquoi préciser -d bd_2023-24 ?\n\n\n\nbd_2023-24 est un des catalogues hébergés par le serveur PostGres. bd_2023-24 contient les schemas sur lesquels nous travaillerons.\n\n\nNous sommes maintenant dans une session du serveur\nusername@/tmp:bd_2023-24&gt; \\conninfo\nYou are connected to database \"bd_2023-24\" as user \"username\" on socket \"/tmp\" at port \"None\".\nTime: 0.000s\nPour obtenir de l’aide, utiliser la commande \\d? et laissez vous guider.\nNous nous intéresserons en général à un ensemble de tables formant un schéma. Ici, nous choisissons comme schéma par défaut world et nous affichons les tables du schéma.\nusername@/tmp:bd_2023-24&gt; set search_path to world ;\nSET\nTime: 0.001s\nusername@/tmp:bd_2023-24&gt; \\dt\n+--------+-----------------+-------+-----------+\n| Schema | Name            | Type  | Owner     |\n|--------+-----------------+-------+-----------|\n| world  | city            | table | boucheron |\n| world  | country         | table | boucheron |\n| world  | countrylanguage | table | boucheron |\n+--------+-----------------+-------+-----------+\nSELECT 3\nTime: 0.011s\nusername@/tmp:bd_2023-24&gt; ...\n\n\n\n\n\n\nPlus pratique\n\n\n\nPour pgcli et psql, il est plus pratique d’exécuter psql et/ou pgcli sur votre machine et de communiquer avec le serveur Postgres via un tunnel ssh. Voir détails pour pgcli et détails pour psql.\n\n\n\n\nConnexions ssh sous windows\nPour accéder au serveur Postgres (SGBD), il faut d’abord se connecter à la machine qui héberge ce serveur etu-pgsql.math.univ-paris-diderot.fr.\nPour se connecter à etu-pgsql.math.univ-paris-diderot.fr, on utilise le protocole ssh, avec son identifiant et son mot de passe ENT.\nOn lance d’abord une fenêtre Powershell.\n$ ssh username@etu-pgsql.math.univ-paris-diderot.fr\n(username@etu-pgsql.math.univ-paris-diderot.fr) Password for username@etu-pgsql.math.univ-paris-diderot.fr:\nLast login: ..................\n....\n....\nThis fortune brought to you by:\n$FreeBSD$\n[username@etu-pgsql ~]$ ...\n Attention : pas d’écho lorsque vous saisissez votre mot de passe.\nOn peut maintenant se connecter au serveur Postgres, toujours avec son identifiant et son mot de passe ENT\nIci, nous proposons d’utiliser le client pgcli :\n[username@etu-pgsql ~]$ pgcli -d bd_2023-24 \nPassword for username: \nServer: PostgreSQL 13.8\nVersion: 3.5.0\nHome: http://pgcli.com\nuserame@/tmp:bd_2023-24&gt; \n\n\n\n\n\n\nPourquoi préciser -d bd_2023-24 ?\n\n\n\nbd_2023-24 est un des catalogues hébergés par le serveur PostGres. bd_2023-24 contient les schemas sur lesquels nous travaillerons.\n\n\nNous sommes maintenant dans une session du serveur\nusername@/tmp:bd_2023-24&gt; \\conninfo\nYou are connected to database \"bd_2023-24\" as user \"username\" on socket \"/tmp\" at port \"None\".\nTime: 0.000s\nPour obtenir de l’aide, utiliser la commande \\d? et laissez vous guider.\nNous nous intéresserons en général à un ensemble de tables formant un schéma. Ici, nous choisissons comme schéma par défaut world et nous affichons les tables du schéma.\nusername@/tmp:bd_2023-24&gt; set search_path to world ;\nSET\nTime: 0.001s\nusername@/tmp:bd_2023-24&gt; \\dt\n+--------+-----------------+-------+-----------+\n| Schema | Name            | Type  | Owner     |\n|--------+-----------------+-------+-----------|\n| world  | city            | table | boucheron |\n| world  | country         | table | boucheron |\n| world  | countrylanguage | table | boucheron |\n+--------+-----------------+-------+-----------+\nSELECT 3\nTime: 0.011s\nusername@/tmp:bd_2023-24&gt; ...\n\n\nConnexions ssh avec tunnel\nVous avez pu constater que les connexions ssh sous MacOS, Linux et Windows sont presque identiques.\nMais utiliser une connexion ssh et un client base de données qui s’exécute sur etu-pgsql.math.univ-paris-diderot.fr n’est pas la manière la plus confortable de travailler.\nIl est plus agréable d’utiliser un client base de données qui s’exécute sur sa propre machine (en local) et qui interagit avec le serveur PostGres au travers d’un tunnel ssh.\nLa commande suivante établit un tunnel en tâche de fond (background job) grâce à l’option -f\n$ ssh -f username@etu-pgsql.math.univ-paris-diderot.fr -L 5436:localhost:5432 -N\n(username@etu-pgsql.math.univ-paris-diderot.fr) Password for username@etu-pgsql.math.univ-paris-diderot.fr:\n$ \nUne fois le tunnel établi, on peut continuer d’utiliser la fenêtre terminal, par exemple pour lancer pgcli ou psql.\nLa commande suivante établit aussi un tunnel mais en tâche de premier plan.\n$ ssh username@etu-pgsql.math.univ-paris-diderot.fr -L 5436:localhost:5432 -N\n(username@etu-pgsql.math.univ-paris-diderot.fr) Password for username@etu-pgsql.math.univ-paris-diderot.fr:\n$ \nUne fois le tunnel établi, la fenêtre terminal est bloquée. Si on veut lancer pgcli ou psql, il faut disposer d’une autre fenêtre.\n\n\n\n\n\n\nTunnel en tâche de fond ou pas ?\n\n\n\nLe lancement du tunnel en tâche de premier plan peut paraître moins pratique que le lancement du tunnel en tâche de fond.\nIl présente un avantage : lorsque le tunnel cesse de fonctionner (en général parce qu’on ne s’en est pas servi depuis quelques minutes), il faut termniner (tuer) le processus qui contrôle le tunnel, pour pouvoir récupérer l’usage du port local ; si le tunnel est contrôlé par une tache de premier plan, c’est trivial (^C sous Unix), si le tunnel est contrôlé par une tâche de fond, il faut déterminer le processus contrôleur, puis le terminer explicitement ($ kill -9 pid).\n\n\n\n\n\n\n\n\nRenvoi de port -L 5436:localhost:5432\n\n\n\nUn serveur PostGres écoute (attend) d’éventuels clients sur le port officiel 5432. Le serveur que nous utiliserons attend effectivement ses clients sur le port 5432 de la machine qui l’héberge. Notre client local ne va pas s’adresser directement au port 5432 de etu-pgsql.math.univ-paris-diderot.fr (c’est interdit). Notre client local s’adressera au port 5436 de la machine qui héberge le client et qui est lui-même renvoyé via le tunnel ssh vers le port 5432 de la machine qui héberge le serveur.\n\n\nOn peut maintenant lancer un client sur sa propre machine (localhost) en précisant qu’on s’adresse au port local 5436 (ou le port que vous choisissez), la requête de conexion au serveur PostGres distant sera transmise par le tunnel : elle sera envoyée sur le port officiel 5432 de la machine distante. Une fois la session établie, tout se passsera comme précédemment (ou presque).\n$ pgcli -d bd_2023-24 -h localhost -p 5436 -u username -W\nServer: PostgreSQL 13.8\nVersion: 3.5.0\nHome: http://pgcli.com\nbd_2023-24&gt; \\dn\n+----------------+--------------+\n| Name           | Owner        |\n|----------------+--------------|\n...\n...\n\n\nClient dbeaver\nLe mécanisme du tunnel ssh peut être utilisé pour connecter un client plus ambitieux au serveur. Le client dbeaver est particulièrement facile à utiliser.\n\n\nClient VS Code + extensions SQLTools\nSi vous êtes déjà habitué à l’éditeur Visual Studio Code (VS Code), vous pouvez utiliser l’extension SQLToos et son pilote ‘PostgreSQL/Cockroach’.\nVotre configuration de connexion devrait ressembler à :\n{\n  \"label\": \"etu-pgsql\",\n  \"host\": \"localhost\",\n  \"user\": \"&lt;identifiant ENT&gt;\",\n  \"port\": 5436,\n  \"ssl\": false,\n  \"database\": \"bd_2023-24\",\n  \"schema\": \"world\",\n  \"password\": \"Ask on connect\"\n}\nIl faut par ailleurs ouvrir un tunnel SSH dans un terminal\n$ ssh  username@etu-pgsql.math.univ-paris-diderot.fr -L 5436:localhost:5432 -N\n(username@etu-pgsql.math.univ-paris-diderot.fr) Password for username@etu-pgsql.math.univ-paris-diderot.fr:\n$ \nen remplaçant username par votre identifiant ENT.",
    "crumbs": [
      "Support",
      "Computing resources"
    ]
  },
  {
    "objectID": "computing-docker.html",
    "href": "computing-docker.html",
    "title": ": Docker",
    "section": "",
    "text": "Download, install and launch docker on your laptop\n\nFollow instructions at https://www.docker.com/get-started\n\n\n\n\n\n\nNote\n\n\n\nIt is enough to install the CLI tool.\n\n\n\nCreate an account on docker hub (if you don’t have one already) at https://hub.docker.com\n\n\nDocker Hub is a service provided by Docker for finding and sharing container images with your team. Learn more and find images at https://hub.docker.com\n\n\nOpen a terminal (powershell on windows) and type\n\ndocker login\nand provide the username and password you use at https://hub.docker.com",
    "crumbs": [
      "Support",
      "Docker"
    ]
  },
  {
    "objectID": "computing-docker.html#get-a-docker-account-and-connect",
    "href": "computing-docker.html#get-a-docker-account-and-connect",
    "title": ": Docker",
    "section": "",
    "text": "Download, install and launch docker on your laptop\n\nFollow instructions at https://www.docker.com/get-started\n\n\n\n\n\n\nNote\n\n\n\nIt is enough to install the CLI tool.\n\n\n\nCreate an account on docker hub (if you don’t have one already) at https://hub.docker.com\n\n\nDocker Hub is a service provided by Docker for finding and sharing container images with your team. Learn more and find images at https://hub.docker.com\n\n\nOpen a terminal (powershell on windows) and type\n\ndocker login\nand provide the username and password you use at https://hub.docker.com",
    "crumbs": [
      "Support",
      "Docker"
    ]
  },
  {
    "objectID": "computing-docker.html#docker-desktop-optional",
    "href": "computing-docker.html#docker-desktop-optional",
    "title": ": Docker",
    "section": "Docker desktop (optional)",
    "text": "Docker desktop (optional)",
    "crumbs": [
      "Support",
      "Docker"
    ]
  },
  {
    "objectID": "computing-docker.html#pull-docker-iamges",
    "href": "computing-docker.html#pull-docker-iamges",
    "title": ": Docker",
    "section": "Pull docker iamges",
    "text": "Pull docker iamges\ndocker pull svbo/ifeby310\ndocker image ls",
    "crumbs": [
      "Support",
      "Docker"
    ]
  },
  {
    "objectID": "computing-docker.html#run-containers",
    "href": "computing-docker.html#run-containers",
    "title": ": Docker",
    "section": "Run containers",
    "text": "Run containers\n\nA container is a runtime instance of a docker image. A container will always run the same, regardless of the infrastructure. Containers isolate software from its environment and ensure that it works uniformly despite differences for instance between development and staging.\n\n\nConfigure docker-compose.yml\ndocker-compose.yml\nversion: \"3.7\"\nservices:\n  big_data_course:\n    container_name: ifeby310  \n    image: svbo/ifeby310\n    ports:\n      - \"8192:8192\"\n      - \"8888:8888\"\n      - \"4040:4040\"\n    restart: always\n    volumes:\n      - \"PATH_GROSSES_DATA:/opt/polynote/notebooks/\"\n    restart: always\n    environment:\n      - PYSPARK_ALLOW_INSECURE_GATEWAY=1\n\n\n\n\n\n\nImportant\n\n\n\nPATH_GROSSES_DATA denotes the path on your hard drive where you will work during this course. It denotes a local volume that is mapped on container path /opt/polynote/notebooks\n\n\n\n\nCompose the container\ndocker-compose up\ndocker container ls",
    "crumbs": [
      "Support",
      "Docker"
    ]
  },
  {
    "objectID": "computing-docker.html#what-happens",
    "href": "computing-docker.html#what-happens",
    "title": ": Docker",
    "section": "What happens?",
    "text": "What happens?",
    "crumbs": [
      "Support",
      "Docker"
    ]
  },
  {
    "objectID": "computing-docker.html#use-jupyter-notebooks",
    "href": "computing-docker.html#use-jupyter-notebooks",
    "title": ": Docker",
    "section": "Use jupyter notebooks",
    "text": "Use jupyter notebooks",
    "crumbs": [
      "Support",
      "Docker"
    ]
  },
  {
    "objectID": "computing-docker.html#use-pyspark-and-spark-submit",
    "href": "computing-docker.html#use-pyspark-and-spark-submit",
    "title": ": Docker",
    "section": "Use pyspark and spark-submit",
    "text": "Use pyspark and spark-submit",
    "crumbs": [
      "Support",
      "Docker"
    ]
  },
  {
    "objectID": "computing-docker.html#use-polynote",
    "href": "computing-docker.html#use-polynote",
    "title": ": Docker",
    "section": "Use polynote",
    "text": "Use polynote",
    "crumbs": [
      "Support",
      "Docker"
    ]
  },
  {
    "objectID": "computing-docker.html#docker-cheatsheet",
    "href": "computing-docker.html#docker-cheatsheet",
    "title": ": Docker",
    "section": "Docker cheatsheet",
    "text": "Docker cheatsheet\nFrom docker.com",
    "crumbs": [
      "Support",
      "Docker"
    ]
  },
  {
    "objectID": "computing-psql.html",
    "href": "computing-psql.html",
    "title": "Client psql",
    "section": "",
    "text": "Note\n\n\n\nQuelques possibilités si vous disposez d’une machine sur laquelle on peut installer psql et sur laquelle on peut établir des tunnels ssh\n\n\n\nInstaller\n\nGénéra\nWindows\nMacOS\nUbuntu\n\nDocumentation\n\n\nUtiliser\n\n\n\n\n\n\nÉtablissement d’un tunel SSH sur votre machine (ici sous Linux)\n\n\n\nRemplacer id_ent par votre identifiant ENT dans la suite.\nSaisissez votre mot de passe (attention : pas d’écho)\n$ ssh id_ent@etu-pgsql.math.univ-paris-diderot.fr -L 5436:localhost:5432 -N\n(id_ent@etu-pgsql.math.univ-paris-diderot.fr) Password for id_ent@etu-pgsql.math.univ-paris-diderot.fr:\n$\n\n\n\n\n\n\n\n\nConnexion au serveur PostGres, demander la liste des commandes disponibles\n\n\n\nUtilisez votre tunnel SSH pour accéder au serveur PostGres. Dans une autre fenêtre terminal, lancer psql, saisissez à nouveau votre mot de passe.\n$ psql -p 5436 -U id_ent -W -h localhost -d bd_2023-24\nPassword for id_ent: \n\nbd_2023-24=# \\?  \nVous êtes maintenant dans une session sur le serveur PostGres. Vous êtes connecté au catalogue bd_2023-24\nVous pouvez utiliser une grande partie des commandes magiques de psql\n\n\n\n\n\n\n\n\nChoisir un schéma par défaut (ici world)\n\n\n\nbd_2023-24=# SET search_path TO world ;\nSET\n\n\n\n\n\n\n\n\nLister les tables du schéma par défaut\n\n\n\nbd_2023-24=# \\d\n+--------+-----------------+-------+-----------+\n| Schema | Name            | Type  | Owner     |\n|--------+-----------------+-------+-----------|\n| world  | city            | table | boucheron |\n| world  | country         | table | boucheron |\n| world  | countrylanguage | table | boucheron |\n+--------+-----------------+-------+-----------+\n(3 rows)\n\n\n\n\n\n\n\n\nSchéma d’une table\n\n\n\nbd_2023-24=# \\d city\n \n+-------------+--------------+-----------+\n| Column      | Type         | Modifiers |\n|-------------+--------------+-----------|\n| id          | integer      |  not null |\n| name        | text         |  not null |\n| countrycode | character(3) |  not null |\n| district    | text         |  not null |\n| population  | integer      |  not null |\n+-------------+--------------+-----------+\nIndexes:\n    \"city_pkey\" PRIMARY KEY, btree (id)\nForeign-key constraints:\n    \"city_country_fk\" FOREIGN KEY (countrycode) REFERENCES country(countrycode) ON UPDATE CASCADE ON DELETE SET NULL DEFE&gt;\nReferenced by:\n    TABLE \"country\" CONSTRAINT \"country_capital_fkey\" FOREIGN KEY (capital) REFERENCES city(id)\n\n\n\n\n\n\n\n\nInformations de connexion\n\n\n\nbd_2023-24=# \\conninfo\nYou are connected to database \"bd_2023-24\" as user \"id_end\" on host \"localhost\"  (address \"127.0.0.1\") at port \"5436\".\n\n\n\n\n\n\n\n\nEditer, sauvegarder et exécuter des requêtes\n\n\n\nbd_2023-24=# \\e \n\nSelect an editor.  To change later, run 'select-editor'.\n  1. /bin/nano        &lt;---- easiest\n  2. /usr/bin/vim.basic\n  3. /usr/bin/nvim\n  4. /usr/bin/vim.tiny\n  5. /usr/bin/emacs\n  6. /usr/bin/code\n  7. /bin/ed\nChoose 1-7 [1]: 6\nSous mon éditeur préféré (vs code ici), j’edite une requête\nSELECT ci.name, co.name_country\nFROM \n  world.city ci JOIN \n  world.country co ON (\n    ci.countrycode=co.countrycode AND \n    ci.id = co.capital\n  ) \nORDER BY co.name_country;\nsauvegardée dans un fichier de chemin d’accès /tmp/psql.edit.23866.sql (construit automatiquement)\nDans ma session sur bd_2023-24, je peux maintenant inclure et exécuter cette requête.\nbd_2023-24=# \\i /tmp/psql.edit.23866.sql\n               name                |             name_country              \n-----------------------------------+---------------------------------------\n Kabul                             | Afghanistan\n Tirana                            | Albania\n Alger                             | Algeria\n Fagatogo                          | American Samoa\n Andorra la Vella                  | Andorra\n Luanda                            | Angola\n:\n...\nEntrez q pour sortir du pager\n\n\n\n\n\n\n\n\nUn fichier par TP ?\n\n\n\nIl est commode d’archiver le travail d’une séance de TP dans un fichier *.sql. On peut créer les fichiers avant la session ou en cours de session (ici dans un dialecte d’Unix)\nbd_2023-24=# \\! touch tp-x.sql\nbd_2023-24=# -- editer tp-x.sql\nbd_2023-24=# \\e tp-x.sql \nbd_2023-24=# -- charger/exécuter tp-x.sql\nbd_2023-24=# \\i tp-x.sql\n\n\n\n\nRenseignements utiles\nDocumentation psql)"
  },
  {
    "objectID": "core/notebooks/checking_parquet_citibike.html",
    "href": "core/notebooks/checking_parquet_citibike.html",
    "title": "Check consistency of parquet files",
    "section": "",
    "text": "Code\nimport glob\n\nimport os\nimport sys\nimport re \nimport shutils\n\nimport pandas as pd\nimport numpy as np\n\nimport datetime\n\nimport itertools\n\nimport zipfile\nfrom zipfile import ZipFile\nfrom tqdm import tqdm\n\nimport pyarrow as pa\nimport comet    as co\nimport pyarrow.parquet as pq\nimport pyarrow.dataset as ds\n\nos.environ['PYSPARK_PYTHON'] = sys.executable\nos.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable\n\n\n\n---------------------------------------------------------------------------\nModuleNotFoundError                       Traceback (most recent call last)\nCell In[1], line 6\n      4 import sys\n      5 import re \n----&gt; 6 import shutils\n      8 import pandas as pd\n      9 import numpy as np\n\nModuleNotFoundError: No module named 'shutils'"
  },
  {
    "objectID": "core/notebooks/checking_parquet_citibike.html#paths",
    "href": "core/notebooks/checking_parquet_citibike.html#paths",
    "title": "Check consistency of parquet files",
    "section": "Paths",
    "text": "Paths\n\n\nCode\ndata_dir = \"../data\"\nos.path.exists(data_dir)\n\nextract_dir = os.path.join(data_dir, \"xcitibike\")\nif not os.path.exists(extract_dir):\n    os.mkdir(extract_dir)\n\nparquet_dir = os.path.join(data_dir, \"pq_citibike\")\nif not os.path.exists(parquet_dir):\n    os.mkdir(parquet_dir)\n\n\n\n---------------------------------------------------------------------------\nFileNotFoundError                         Traceback (most recent call last)\nCell In[2], line 6\n      4 extract_dir = os.path.join(data_dir, \"xcitibike\")\n      5 if not os.path.exists(extract_dir):\n----&gt; 6     os.mkdir(extract_dir)\n      8 parquet_dir = os.path.join(data_dir, \"pq_citibike\")\n      9 if not os.path.exists(parquet_dir):\n\nFileNotFoundError: [Errno 2] No such file or directory: '../data/xcitibike'"
  },
  {
    "objectID": "core/notebooks/checking_parquet_citibike.html#spark-session",
    "href": "core/notebooks/checking_parquet_citibike.html#spark-session",
    "title": "Check consistency of parquet files",
    "section": "Spark session",
    "text": "Spark session\n\n\nCode\nfrom pyspark import SparkConf, SparkContext\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql import functions as fn\nfrom pyspark.sql.types import *\n\n\n\n\nCode\nspark = (SparkSession\n    .builder\n    .appName(\"Spark checking citibike parquet file\")\n    .getOrCreate()\n)\n\n\n25/04/03 15:07:28 WARN Utils: Your hostname, boucheron-Precision-5480 resolves to a loopback address: 127.0.1.1; using 172.23.32.10 instead (on interface eth0)\n25/04/03 15:07:28 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\nSetting default log level to \"WARN\".\nTo adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n25/04/03 15:07:28 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable"
  },
  {
    "objectID": "core/notebooks/checking_parquet_citibike.html#try-to-load-parquet-file",
    "href": "core/notebooks/checking_parquet_citibike.html#try-to-load-parquet-file",
    "title": "Check consistency of parquet files",
    "section": "Try to load parquet file",
    "text": "Try to load parquet file\n\n\nCode\nsch_1 = StructType([\n    StructField('trip_duration', StringType(), True), \n    StructField('started_at', TimestampType(), True), \n    StructField('ended_at', TimestampType(), True), \n    StructField('start_station_id', StringType(), True), \n    StructField('start_station_name', StringType(), True), \n    StructField('start_lat', StringType(), True), \n    StructField('start_lng', StringType(), True), \n    StructField('end_station_id', StringType(), True), \n    StructField('end_station_name', StringType(), True), \n    StructField('end_lat', StringType(), True), \n    StructField('end_lng', StringType(), True), \n    StructField('bike_id', StringType(), True), \n    StructField('user_type', StringType(), True), \n    StructField('birth_year', StringType(), True), \n    StructField('gender', StringType(), True), \n    StructField('start_year', IntegerType(), True), \n    StructField('start_month', IntegerType(), True)\n    ]\n)\n\n\n\n\nCode\ninput_file = os.path.join(parquet_dir, 'start_year=2022')\n\ndf = ( \n    spark.read\n        .option(\"mergeSchema\", \"true\")\n        .parquet(parquet_dir)\n)\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[6], line 1\n----&gt; 1 input_file = os.path.join(parquet_dir, 'start_year=2022')\n      3 df = ( \n      4     spark.read\n      5         .option(\"mergeSchema\", \"true\")\n      6         .parquet(parquet_dir)\n      7 )\n\nNameError: name 'parquet_dir' is not defined\n\n\n\n\n\nCode\ndf.printSchema()\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[7], line 1\n----&gt; 1 df.printSchema()\n\nNameError: name 'df' is not defined\n\n\n\nroot\n |-- trip_duration: string (nullable = true)\n |-- started_at: timestamp (nullable = true)\n |-- ended_at: timestamp (nullable = true)\n |-- start_station_id: string (nullable = true)\n |-- start_station_name: string (nullable = true)\n |-- start_lat: string (nullable = true)\n |-- start_lng: string (nullable = true)\n |-- end_station_id: string (nullable = true)\n |-- end_station_name: string (nullable = true)\n |-- end_lat: string (nullable = true)\n |-- end_lng: string (nullable = true)\n |-- bike_id: string (nullable = true)\n |-- user_type: string (nullable = true)\n |-- birth_year: string (nullable = true)\n |-- gender: string (nullable = true)\n |-- ride_id: string (nullable = true)\n |-- rideable_type: string (nullable = true)\n |-- member_casual: string (nullable = true)\n |-- start_year: integer (nullable = true)\n |-- start_month: integer (nullable = true)\n\n\nCode\ndf.select([\"started_at\", \"ended_at\"]).show(5)\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[8], line 1\n----&gt; 1 df.select([\"started_at\", \"ended_at\"]).show(5)\n\nNameError: name 'df' is not defined\n\n\n\n\n\nCode\nprint(df.rdd.toDebugString().decode(\"utf-8\"))\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[9], line 1\n----&gt; 1 print(df.rdd.toDebugString().decode(\"utf-8\"))\n\nNameError: name 'df' is not defined\n\n\n\n\n\nCode\ndf.rdd.getNumPartitions()\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[10], line 1\n----&gt; 1 df.rdd.getNumPartitions()\n\nNameError: name 'df' is not defined\n\n\n\n\n\nCode\ndf_pd = df.groupBy(['start_year', 'start_month']).count().toPandas()\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[11], line 1\n----&gt; 1 df_pd = df.groupBy(['start_year', 'start_month']).count().toPandas()\n\nNameError: name 'df' is not defined\n\n\n\n\n\nCode\ndf_pd.sort_values(by=['start_year', 'start_month'])\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[12], line 1\n----&gt; 1 df_pd.sort_values(by=['start_year', 'start_month'])\n\nNameError: name 'df_pd' is not defined\n\n\n\n\n\nCode\n(\n    df\n        .groupBy('rideable_type')\n        .agg(fn.count('started_at'))\n        .show()\n)\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[13], line 2\n      1 (\n----&gt; 2     df\n      3         .groupBy('rideable_type')\n      4         .agg(fn.count('started_at'))\n      5         .show()\n      6 )\n\nNameError: name 'df' is not defined\n\n\n\n\n\nCode\nspark.stop()\n\n\n\n\nCode\nfrom pyspark.sql.types import *\n\n\n\n\nCode\nprint(sch_1)\n\n\nStructType([StructField('trip_duration', StringType(), True), StructField('started_at', TimestampType(), True), StructField('ended_at', TimestampType(), True), StructField('start_station_id', StringType(), True), StructField('start_station_name', StringType(), True), StructField('start_lat', StringType(), True), StructField('start_lng', StringType(), True), StructField('end_station_id', StringType(), True), StructField('end_station_name', StringType(), True), StructField('end_lat', StringType(), True), StructField('end_lng', StringType(), True), StructField('bike_id', StringType(), True), StructField('user_type', StringType(), True), StructField('birth_year', StringType(), True), StructField('gender', StringType(), True), StructField('start_year', IntegerType(), True), StructField('start_month', IntegerType(), True)])"
  },
  {
    "objectID": "core/notebooks/notebook01_python.html",
    "href": "core/notebooks/notebook01_python.html",
    "title": "Introduction to Python",
    "section": "",
    "text": "We introduce here the python language. Only the bare minimum necessary for getting started with the data-science stack (a bunch of libraries for data science). Python is a programming language, as are C++, java, fortran, javascript, etc."
  },
  {
    "objectID": "core/notebooks/notebook01_python.html#specific-features-of-python",
    "href": "core/notebooks/notebook01_python.html#specific-features-of-python",
    "title": "Introduction to Python",
    "section": "Specific features of Python",
    "text": "Specific features of Python\n\nan interpreted (as opposed to compiled) language. Contrary to e.g. C++ or fortran, one does not compile Python code before executing it.\nUsed as a scripting language, by python python script.py in a terminal\nBut can be used also interactively: the jupyter notebook, iPython, etc.\nA free software released under an open-source license: Python can be used and distributed free of charge, even for building commercial software.\nmulti-platform: Python is available for all major operating systems, Windows, Linux/Unix, MacOS X, most likely your mobile phone OS, etc.\nA very readable language with clear non-verbose syntax\nA language for which a large amount of high-quality packages are available for various applications, including web-frameworks and scientific computing\nIt has been one of the top languages for data science and machine learning for several years, because it is expressive and and easy to deploy\nAn object-oriented language\n\nSee https://www.python.org/about/ for more information about distinguishing features of Python.\n\n\n\n\n\n\nPython 2 or Python 3?\n\n\n\n\nSimple answer: don’t use Python 2, use Python 3\nPython 2 is mostly deprecated and has not been maintained for years\nYou’ll end up hanged if you use Python 2\nIf Python 2 is mandatory at your workplace, find another work\n\n\n\n\n\n\n\n\n\nJupyter or Quarto notebooks?\n\n\n\n\nquarto is more git friendly than jupyter\nEnjoy authentic editors\nGo for quarto"
  },
  {
    "objectID": "core/notebooks/notebook01_python.html#integers",
    "href": "core/notebooks/notebook01_python.html#integers",
    "title": "Introduction to Python",
    "section": "Integers",
    "text": "Integers\n\n\nCode\n1 + 42\n\n\n43\n\n\n\n\nCode\ntype(1+1)\n\n\nint\n\n\nWe can assign values to variables with =\n\n\nCode\na = (3 + 5 ** 2) % 4\na\n\n\n0"
  },
  {
    "objectID": "core/notebooks/notebook01_python.html#remark",
    "href": "core/notebooks/notebook01_python.html#remark",
    "title": "Introduction to Python",
    "section": "Remark",
    "text": "Remark\nWe don’t declare the type of a variable before assigning its value. In C, conversely, one should write\nint a = 4;"
  },
  {
    "objectID": "core/notebooks/notebook01_python.html#something-cool",
    "href": "core/notebooks/notebook01_python.html#something-cool",
    "title": "Introduction to Python",
    "section": "Something cool",
    "text": "Something cool\n\nArbitrary large integer arithmetics\n\n\n\nCode\n17 ** 542\n\n\n8004153099680695240677662228684856314409365427758266999205063931175132640587226837141154215226851187899067565063096026317140186260836873939218139105634817684999348008544433671366043519135008200013865245747791955240844192282274023825424476387832943666754140847806277355805648624376507618604963106833797989037967001806494232055319953368448928268857747779203073913941756270620192860844700087001827697624308861431399538404552468712313829522630577767817531374612262253499813723569981496051353450351968993644643291035336065584116155321928452618573467361004489993801594806505273806498684433633838323916674207622468268867047187858269410016150838175127772100983052010703525089"
  },
  {
    "objectID": "core/notebooks/notebook01_python.html#floats",
    "href": "core/notebooks/notebook01_python.html#floats",
    "title": "Introduction to Python",
    "section": "Floats",
    "text": "Floats\nThere exists a floating point type that is created when the variable has decimal values\n\n\nCode\nc = 2.\n\n\n\n\nCode\ntype(c)\n\n\nfloat\n\n\n\n\nCode\nc = 2\ntype(c)\n\n\nint\n\n\n\n\nCode\ntruc = 1 / 2\ntruc\n\n\n0.5\n\n\n\n\nCode\n1 // 2 + 1 % 2\n\n\n1\n\n\n\n\nCode\ntype(truc)\n\n\nfloat"
  },
  {
    "objectID": "core/notebooks/notebook01_python.html#boolean",
    "href": "core/notebooks/notebook01_python.html#boolean",
    "title": "Introduction to Python",
    "section": "Boolean",
    "text": "Boolean\nSimilarly, boolean types are created from a comparison\n\n\nCode\ntest = 3 &gt; 4\ntest\n\n\nFalse\n\n\n\n\nCode\ntype(test)\n\n\nbool\n\n\n\n\nCode\nFalse == (not True)\n\n\nTrue\n\n\n\n\nCode\n1.41 &lt; 2.71 and 2.71 &lt; 3.14\n\n\nTrue\n\n\n\n\nCode\n# It's equivalent to\n1.41 &lt; 2.71 &lt; 3.14\n\n\nTrue"
  },
  {
    "objectID": "core/notebooks/notebook01_python.html#type-conversion-casting",
    "href": "core/notebooks/notebook01_python.html#type-conversion-casting",
    "title": "Introduction to Python",
    "section": "Type conversion (casting)",
    "text": "Type conversion (casting)\n\n\nCode\na = 1\ntype(a)\n\n\nint\n\n\n\n\nCode\nb = float(a)\ntype(b)\n\n\nfloat\n\n\n\n\nCode\nstr(b)\n\n\n'1.0'\n\n\n\n\nCode\nbool(b)\n# All non-zero, non empty objects are casted to boolean as True (more later)\n\n\nTrue\n\n\n\n\nCode\nbool(1-1)\n\n\nFalse"
  },
  {
    "objectID": "core/notebooks/notebook01_python.html#tuples",
    "href": "core/notebooks/notebook01_python.html#tuples",
    "title": "Introduction to Python",
    "section": "Tuples",
    "text": "Tuples\n\n\nCode\ntt = 'truc', 3.14, \"truc\"\ntt\n\n\n('truc', 3.14, 'truc')\n\n\n\n\nCode\ntt[0]\n\n\n'truc'\n\n\nYou can’t change a tuple, we say that it’s immutable\n\n\nCode\ntry:\n    tt[0] = 1\nexcept TypeError:\n    print(f\"TypeError: 'tuple' object does not support item assignment\")\n\n\nTypeError: 'tuple' object does not support item assignment\n\n\nThree ways of doing the same thing\n\n\nCode\n# Method 1\ntuple([1, 2, 3])\n\n\n(1, 2, 3)\n\n\n\n\nCode\n# Method 2\n1, 2, 3\n\n\n(1, 2, 3)\n\n\n\n\nCode\n# Method 3\n(1, 2, 3)\n\n\n(1, 2, 3)\n\n\nSimpler is better in Python, so usually you want to use Method 2.\n\n\nCode\ntoto = 1, 2, 3\ntoto\n\n\n(1, 2, 3)\n\n\n\nThis is serious !"
  },
  {
    "objectID": "core/notebooks/notebook01_python.html#the-zen-of-python-easters-egg",
    "href": "core/notebooks/notebook01_python.html#the-zen-of-python-easters-egg",
    "title": "Introduction to Python",
    "section": "The Zen of Python easter’s egg",
    "text": "The Zen of Python easter’s egg\n\n\nCode\nimport this\n\n\nThe Zen of Python, by Tim Peters\n\nBeautiful is better than ugly.\nExplicit is better than implicit.\nSimple is better than complex.\nComplex is better than complicated.\nFlat is better than nested.\nSparse is better than dense.\nReadability counts.\nSpecial cases aren't special enough to break the rules.\nAlthough practicality beats purity.\nErrors should never pass silently.\nUnless explicitly silenced.\nIn the face of ambiguity, refuse the temptation to guess.\nThere should be one-- and preferably only one --obvious way to do it.\nAlthough that way may not be obvious at first unless you're Dutch.\nNow is better than never.\nAlthough never is often better than *right* now.\nIf the implementation is hard to explain, it's a bad idea.\nIf the implementation is easy to explain, it may be a good idea.\nNamespaces are one honking great idea -- let's do more of those!"
  },
  {
    "objectID": "core/notebooks/notebook01_python.html#lists",
    "href": "core/notebooks/notebook01_python.html#lists",
    "title": "Introduction to Python",
    "section": "Lists",
    "text": "Lists\nA list is an ordered collection of objects. These objects may have different types. For example:\n\n\nCode\ncolors = ['red', 'blue', 'green', 'black', 'white']\n\n\n\n\nCode\ncolors[0]\n\n\n'red'\n\n\n\n\nCode\ntype(colors)\n\n\nlist\n\n\nIndexing: accessing individual objects contained in the list by their position\n\n\nCode\ncolors[2]\n\n\n'green'\n\n\n\n\nCode\ncolors[2] = 3.14\ncolors\n\n\n['red', 'blue', 3.14, 'black', 'white']\n\n\n\n\n\n\n\n\nWarning\n\n\n\nFor any iterable object in Python, indexing starts at 0 (as in C), not at 1 (as in Fortran, R, or Matlab).\n\n\nCounting from the end with negative indices:\n\n\nCode\ncolors[-1]\n\n\n'white'\n\n\nIndex must remain in the range of the list\n\n\nCode\ntry:\n    colors[10]\nexcept IndexError:\n    print(f\"IndexError: 10 &gt;= {len(colors)} ==len(colors), index out of range \")\n\n\n\n\nCode\ncolors\n\n\n['red', 'blue', 3.14, 'black', 'white']\n\n\n\n\nCode\ntt\n\n\n('truc', 3.14, 'truc')\n\n\n\n\nCode\ncolors.append(tt)\ncolors\n\n\n['red', 'blue', 3.14, 'black', 'white', ('truc', 3.14, 'truc')]\n\n\n\n\nCode\nlen(colors)\n\n\n6\n\n\n\n\nCode\nlen(tt)\n\n\n3"
  },
  {
    "objectID": "core/notebooks/notebook01_python.html#slicing-obtaining-sublists-of-regularly-spaced-elements",
    "href": "core/notebooks/notebook01_python.html#slicing-obtaining-sublists-of-regularly-spaced-elements",
    "title": "Introduction to Python",
    "section": "Slicing: obtaining sublists of regularly-spaced elements",
    "text": "Slicing: obtaining sublists of regularly-spaced elements\nThis work with anything iterable whenever it makes sense (list, str, tuple, etc.)\n\n\nCode\ncolors\n\n\n['red', 'blue', 3.14, 'black', 'white', ('truc', 3.14, 'truc')]\n\n\n\n\nCode\nlist(reversed(colors))\n\n\n[('truc', 3.14, 'truc'), 'white', 'black', 3.14, 'blue', 'red']\n\n\n\n\nCode\ncolors[::-1]\n\n\n[('truc', 3.14, 'truc'), 'white', 'black', 3.14, 'blue', 'red']\n\n\n\n\n\n\n\n\nSlicing syntax:\n\n\n\ncolors[start:stop:stride]\nstart, stop, stride are optional, with default values 0, len(sequence), 1\n\n\nl\n\n\nCode\nprint(slice(4))\nprint(slice(1,5))\nprint(slice(None,13,3))\n\n\nslice(None, 4, None)\nslice(1, 5, None)\nslice(None, 13, 3)\n\n\n\n\nCode\nsl = slice(1,5,2)\ncolors[sl]\n\n\n['blue', 'black']\n\n\n\n\nCode\ncolors\n\n\n['red', 'blue', 3.14, 'black', 'white', ('truc', 3.14, 'truc')]\n\n\n\n\nCode\ncolors[3:]\n\n\n['black', 'white', ('truc', 3.14, 'truc')]\n\n\n\n\nCode\ncolors[:3]\n\n\n['red', 'blue', 3.14]\n\n\n\n\nCode\ncolors[1::2]\n\n\n['blue', 'black', ('truc', 3.14, 'truc')]\n\n\n\n\nCode\ncolors[::-1]\n\n\n[('truc', 3.14, 'truc'), 'white', 'black', 3.14, 'blue', 'red']"
  },
  {
    "objectID": "core/notebooks/notebook01_python.html#strings",
    "href": "core/notebooks/notebook01_python.html#strings",
    "title": "Introduction to Python",
    "section": "Strings",
    "text": "Strings\nDifferent string syntaxes (simple, double or triple quotes):\n\n\nCode\ns = 'tintin'\ntype(s)\n\n\nstr\n\n\n\n\nCode\ns\n\n\n'tintin'\n\n\n\n\nCode\ns = \"\"\"         Bonjour,\nJe m'appelle Stephane.\nJe vous souhaite une bonne journée.\nSalut.       \n\"\"\"\ns\n\n\n\"         Bonjour,\\nJe m'appelle Stephane.\\nJe vous souhaite une bonne journée.\\nSalut.       \\n\"\n\n\n\n\nCode\ns.strip()\n\n\n\"Bonjour,\\nJe m'appelle Stephane.\\nJe vous souhaite une bonne journée.\\nSalut.\"\n\n\n\n\nCode\nprint(s.strip())\n\n\nBonjour,\nJe m'appelle Stephane.\nJe vous souhaite une bonne journée.\nSalut.\n\n\n\n\nCode\nlen(s)\n\n\n91\n\n\n\n\nCode\n# Casting to a list\nlist(s.strip()[:15])\n\n\n['B', 'o', 'n', 'j', 'o', 'u', 'r', ',', '\\n', 'J', 'e', ' ', 'm', \"'\", 'a']\n\n\n\n\nCode\n# Arithmetics\nprint('Bonjour' * 2)\nprint('Hello' + ' all')\n\n\nBonjourBonjour\nHello all\n\n\n\n\nCode\nsss = 'A'\nsss += 'bc'\nsss += 'dE'\nsss.lower()\n\n\n'abcde'\n\n\n\n\nCode\nss = s.strip()\nprint(ss[:10] + ss[24:28])\n\n\nBonjour,\nJepha\n\n\n\n\nCode\ns.strip()\n\n\n\"Bonjour,\\nJe m'appelle Stephane.\\nJe vous souhaite une bonne journée.\\nSalut.\"\n\n\n\n\nCode\ns.strip().split('\\n')\n\n\n['Bonjour,',\n \"Je m'appelle Stephane.\",\n 'Je vous souhaite une bonne journée.',\n 'Salut.']\n\n\n\n\nCode\ns[::3]\n\n\n'   BjrJmpl ea.eo ui eoeon.at  \\n'\n\n\n\n\nCode\ns[3:10]\n\n\n'      B'\n\n\n\n\nCode\n\" \".join(['Il', 'fait', 'super', 'beau', \"aujourd'hui\"])\n\n\n\"Il fait super beau aujourd'hui\"\n\n\nChaining method calls is the basic of pipeline building.\n\n\nCode\n( \n    \" \".join(['Il', 'fait', 'super', 'beau', \"aujourd'hui\"])\n       .title()\n       .replace(' ', '')\n       .replace(\"'\",\"\")\n)\n\n\n'IlFaitSuperBeauAujourdHui'\n\n\n\nImportant\nA string is immutable !!\n\n\nCode\ns = 'I am an immutable guy'\n\n\n\n\nCode\ntry:  \n    s[2] = 's'\nexcept TypeError:\n    print(f\"Strings are immutable! s is still '{s}'\")\n\n\nStrings are immutable! s is still 'I am an immutable guy'\n\n\n\n\nCode\nid(s)\n\n\n134288353205360\n\n\n\n\nCode\nprint(s + ', for sure')\nid(s), id(s + ' for sure')\n\n\nI am an immutable guy, for sure\n\n\n(134288353205360, 134288353731856)\n\n\n\n\nExtra stuff with strings\n\n\nCode\n'square of 2 is ' + str(2 ** 2)\n\n\n'square of 2 is 4'\n\n\n\n\nCode\n'square of 2 is %d' % 2 ** 2\n\n\n'square of 2 is 4'\n\n\n\n\nCode\n'square of 2 is {}'.format(2 ** 2)\n\n\n'square of 2 is 4'\n\n\n\n\nCode\n'square of 2 is {square}'.format(square=2 ** 2)\n\n\n'square of 2 is 4'\n\n\n\n\nCode\n# And since Python 3.6 you can use an `f-string`\nnumber = 2\nsquare = number ** 2\n\nf'square of {number} is {square}'\n\n\n'square of 2 is 4'\n\n\n\n\nThe in keyword\nYou can use the in keyword with any container, whenever it makes sense\n\n\nCode\nprint(s)\nprint('Salut' in s)\n\n\nI am an immutable guy\nFalse\n\n\n\n\nCode\nprint(tt)\nprint('truc' in tt)\n\n\n('truc', 3.14, 'truc')\nTrue\n\n\n\n\nCode\nprint(colors)\nprint('truc' in colors)\n\n\n['red', 'blue', 3.14, 'black', 'white', ('truc', 3.14, 'truc')]\nFalse\n\n\n\n\nCode\n('truc', 3.14, 'truc') in colors\n\n\nTrue\n\n\n\n\n\n\n\n\nWarning\n\n\n\nStrings are not bytes. Have a look at chapter 4 Unicode Text versus Bytes in Fluent Python\n\n\n\n\nBrain-teasing\nExplain this weird behaviour:\n\n\nCode\n5 in [1, 2, 3, 4] == False\n\n\nFalse\n\n\n\n\nCode\n[1, 2, 3, 4] == False\n\n\nFalse\n\n\n\n\nCode\n5 not in [1, 2, 3, 4]\n\n\nTrue\n\n\n\n\nCode\n(5 in [1, 2, 3, 4]) == False\n\n\nTrue\n\n\n\n\nCode\n# ANSWER.\n# This is a chained comparison. We have seen that \n1 &lt; 2 &lt; 3\n# is equivalent to\n(1 &lt; 2) and (2 &lt; 3)\n# so that\n5 in [1, 2, 3, 4] == False\n# is equivalent to\n(5 in [1, 2, 3, 4]) and ([1, 2, 3, 4] == False)\n\n\nFalse\n\n\n\n\nCode\n(5 in [1, 2, 3, 4])\n\n\nFalse\n\n\n\n\nCode\n([1, 2, 3, 4] == False)\n\n\nFalse"
  },
  {
    "objectID": "core/notebooks/notebook01_python.html#dictionaries",
    "href": "core/notebooks/notebook01_python.html#dictionaries",
    "title": "Introduction to Python",
    "section": "Dictionaries",
    "text": "Dictionaries\n\nA dictionary is basically an efficient table that maps keys to values.\nThe MOST important container in Python.\nMany things are actually a dict under the hood in Python\n\n\n\nCode\ntel = {'emmanuelle': 5752, 'sebastian': 5578}\nprint(tel)\nprint(type(tel))\n\n\n{'emmanuelle': 5752, 'sebastian': 5578}\n&lt;class 'dict'&gt;\n\n\n\n\nCode\ntel['emmanuelle'], tel['sebastian']\n\n\n(5752, 5578)\n\n\n\n\nCode\ntel['francis'] = '5919'\ntel\n\n\n{'emmanuelle': 5752, 'sebastian': 5578, 'francis': '5919'}\n\n\n\n\nCode\nlen(tel)\n\n\n3\n\n\n\nImportant remarks\n\nKeys can be of different types\nA key must be of immutable type\n\n\n\nCode\ntel[7162453] = [1, 3, 2]\ntel[3.14] = 'bidule'\ntel[('jaouad', 2)] = 1234\ntel\n\n\n{'emmanuelle': 5752,\n 'sebastian': 5578,\n 'francis': '5919',\n 7162453: [1, 3, 2],\n 3.14: 'bidule',\n ('jaouad', 2): 1234}\n\n\n\n\nCode\ntry:\n    sorted(tel)\nexcept TypeError:\n    print(\"TypeError: '&lt;' not supported between instances of 'int' and 'str'\")    \n\n\nTypeError: '&lt;' not supported between instances of 'int' and 'str'\n\n\n\n\nCode\n# A list is mutable and not hashable\ntry:\n    tel[['jaouad']] = '5678'\nexcept TypeError:\n    print(\"TypeError: unhashable type: 'list'\")\n\n\nTypeError: unhashable type: 'list'\n\n\n\n\nCode\ntry:\n    tel[2]\nexcept KeyError:\n    print(\"KeyError: 2\")\n\n\nKeyError: 2\n\n\n\n\nCode\ntel = {'emmanuelle': 5752, 'sebastian' : 5578, 'jaouad' : 1234}\nprint(tel.keys())\nprint(tel.values())\nprint(tel.items())\n\n\ndict_keys(['emmanuelle', 'sebastian', 'jaouad'])\ndict_values([5752, 5578, 1234])\ndict_items([('emmanuelle', 5752), ('sebastian', 5578), ('jaouad', 1234)])\n\n\n\n\nCode\nlist(tel.keys())[2]\n\n\n'jaouad'\n\n\n\n\nCode\ntel.values().mapping\n\n\nmappingproxy({'emmanuelle': 5752, 'sebastian': 5578, 'jaouad': 1234})\n\n\n\n\nCode\ntype(tel.keys())\n\n\ndict_keys\n\n\n\n\nCode\n'rémi' in tel\n\n\nFalse\n\n\n\n\nCode\nlist(tel)\n\n\n['emmanuelle', 'sebastian', 'jaouad']\n\n\n\n\nCode\n'rémi' in tel.keys()\n\n\nFalse\n\n\nYou can swap values like this\n\n\nCode\nprint(tel)\ntel['emmanuelle'], tel['sebastian'] = tel['sebastian'], tel['emmanuelle']\nprint(tel)\n\n\n{'emmanuelle': 5752, 'sebastian': 5578, 'jaouad': 1234}\n{'emmanuelle': 5578, 'sebastian': 5752, 'jaouad': 1234}\n\n\n\n\nCode\n# It works, since\na, b = 2.71, 3.14\na, b = b, a\na, b\n\n\n(3.14, 2.71)\n\n\n\n\nExercise 1\nGet keys of tel sorted by decreasing order\n\n\nCode\ntel = {'emmanuelle': 5752, 'sebastian' : 5578, 'jaouad' : 1234}\n\n\n\n\nExercise 2\nGet keys of tel sorted by increasing values\n\n\nCode\ntel = {'emmanuelle': 5752, 'sebastian' : 5578, 'jaouad' : 1234}\n\n\n\n\nExercise 3\nObtain a sorted-by-key version of tel\n\n\nCode\ntel = {'emmanuelle': 5752, 'sebastian' : 5578, 'jaouad' : 1234}"
  },
  {
    "objectID": "core/notebooks/notebook01_python.html#sets",
    "href": "core/notebooks/notebook01_python.html#sets",
    "title": "Introduction to Python",
    "section": "Sets",
    "text": "Sets\nA set is an unordered container, containing unique elements\n\n\nCode\nss = {1, 2, 2, 2, 3, 3, 'tintin', 'tintin', 'toto'}\nss\n\n\n{1, 2, 3, 'tintin', 'toto'}\n\n\n\n\nCode\ns = 'truc truc bidule truc'\nset(s)\n\n\n{' ', 'b', 'c', 'd', 'e', 'i', 'l', 'r', 't', 'u'}\n\n\n\n\nCode\nset(list(s))\n\n\n{' ', 'b', 'c', 'd', 'e', 'i', 'l', 'r', 't', 'u'}\n\n\n\n\nCode\n{1, 5, 2, 1, 1}.union({1, 2, 3})\n\n\n{1, 2, 3, 5}\n\n\n\n\nCode\nset((1, 5, 3, 2))\n\n\n{1, 2, 3, 5}\n\n\n\n\nCode\nset([1, 5, 2, 1, 1]).intersection(set([1, 2, 3]))\n\n\n{1, 2}\n\n\n\n\nCode\nss.add('tintin')\nss\n\n\n{1, 2, 3, 'tintin', 'toto'}\n\n\n\n\nCode\nss.difference(range(6))\n\n\n{'tintin', 'toto'}\n\n\nYou can combine all containers together\n\n\nCode\ndd = {\n    'truc': [1, 2, 3], \n    5: (1, 4, 2),\n    (1, 3): {'hello', 'world'}\n}\ndd\n\n\n{'truc': [1, 2, 3], 5: (1, 4, 2), (1, 3): {'hello', 'world'}}"
  },
  {
    "objectID": "core/notebooks/notebook01_python.html#everything-is-either-mutable-or-immutable",
    "href": "core/notebooks/notebook01_python.html#everything-is-either-mutable-or-immutable",
    "title": "Introduction to Python",
    "section": "Everything is either mutable or immutable",
    "text": "Everything is either mutable or immutable\n\n\nCode\nss = {1, 2, 3}\nsss = ss\nsss, ss\n\n\n({1, 2, 3}, {1, 2, 3})\n\n\n\n\nCode\nid(ss), id(sss)\n\n\n(134286904486688, 134286904486688)\n\n\n\n\nCode\nsss.add(\"Truc\")\n\n\nQuestion. What is in ss ?\n\n\nCode\nss, sss\n\n\n({1, 2, 3, 'Truc'}, {1, 2, 3, 'Truc'})\n\n\nss and sss are names for the same object\n\n\nCode\nid(ss), id(sss)\n\n\n(134286904486688, 134286904486688)\n\n\n\n\nCode\nss is sss\n\n\nTrue\n\n\n\n\nCode\nhelp('is')\n\n\nComparisons\n***********\n\nUnlike C, all comparison operations in Python have the same priority,\nwhich is lower than that of any arithmetic, shifting or bitwise\noperation.  Also unlike C, expressions like \"a &lt; b &lt; c\" have the\ninterpretation that is conventional in mathematics:\n\n   comparison    ::= or_expr (comp_operator or_expr)*\n   comp_operator ::= \"&lt;\" | \"&gt;\" | \"==\" | \"&gt;=\" | \"&lt;=\" | \"!=\"\n                     | \"is\" [\"not\"] | [\"not\"] \"in\"\n\nComparisons yield boolean values: \"True\" or \"False\". Custom *rich\ncomparison methods* may return non-boolean values. In this case Python\nwill call \"bool()\" on such value in boolean contexts.\n\nComparisons can be chained arbitrarily, e.g., \"x &lt; y &lt;= z\" is\nequivalent to \"x &lt; y and y &lt;= z\", except that \"y\" is evaluated only\nonce (but in both cases \"z\" is not evaluated at all when \"x &lt; y\" is\nfound to be false).\n\nFormally, if *a*, *b*, *c*, …, *y*, *z* are expressions and *op1*,\n*op2*, …, *opN* are comparison operators, then \"a op1 b op2 c ... y\nopN z\" is equivalent to \"a op1 b and b op2 c and ... y opN z\", except\nthat each expression is evaluated at most once.\n\nNote that \"a op1 b op2 c\" doesn’t imply any kind of comparison between\n*a* and *c*, so that, e.g., \"x &lt; y &gt; z\" is perfectly legal (though\nperhaps not pretty).\n\n\nValue comparisons\n=================\n\nThe operators \"&lt;\", \"&gt;\", \"==\", \"&gt;=\", \"&lt;=\", and \"!=\" compare the values\nof two objects.  The objects do not need to have the same type.\n\nChapter Objects, values and types states that objects have a value (in\naddition to type and identity).  The value of an object is a rather\nabstract notion in Python: For example, there is no canonical access\nmethod for an object’s value.  Also, there is no requirement that the\nvalue of an object should be constructed in a particular way, e.g.\ncomprised of all its data attributes. Comparison operators implement a\nparticular notion of what the value of an object is.  One can think of\nthem as defining the value of an object indirectly, by means of their\ncomparison implementation.\n\nBecause all types are (direct or indirect) subtypes of \"object\", they\ninherit the default comparison behavior from \"object\".  Types can\ncustomize their comparison behavior by implementing *rich comparison\nmethods* like \"__lt__()\", described in Basic customization.\n\nThe default behavior for equality comparison (\"==\" and \"!=\") is based\non the identity of the objects.  Hence, equality comparison of\ninstances with the same identity results in equality, and equality\ncomparison of instances with different identities results in\ninequality.  A motivation for this default behavior is the desire that\nall objects should be reflexive (i.e. \"x is y\" implies \"x == y\").\n\nA default order comparison (\"&lt;\", \"&gt;\", \"&lt;=\", and \"&gt;=\") is not provided;\nan attempt raises \"TypeError\".  A motivation for this default behavior\nis the lack of a similar invariant as for equality.\n\nThe behavior of the default equality comparison, that instances with\ndifferent identities are always unequal, may be in contrast to what\ntypes will need that have a sensible definition of object value and\nvalue-based equality.  Such types will need to customize their\ncomparison behavior, and in fact, a number of built-in types have done\nthat.\n\nThe following list describes the comparison behavior of the most\nimportant built-in types.\n\n* Numbers of built-in numeric types (Numeric Types — int, float,\n  complex) and of the standard library types \"fractions.Fraction\" and\n  \"decimal.Decimal\" can be compared within and across their types,\n  with the restriction that complex numbers do not support order\n  comparison.  Within the limits of the types involved, they compare\n  mathematically (algorithmically) correct without loss of precision.\n\n  The not-a-number values \"float('NaN')\" and \"decimal.Decimal('NaN')\"\n  are special.  Any ordered comparison of a number to a not-a-number\n  value is false. A counter-intuitive implication is that not-a-number\n  values are not equal to themselves.  For example, if \"x =\n  float('NaN')\", \"3 &lt; x\", \"x &lt; 3\" and \"x == x\" are all false, while \"x\n  != x\" is true.  This behavior is compliant with IEEE 754.\n\n* \"None\" and \"NotImplemented\" are singletons.  **PEP 8** advises that\n  comparisons for singletons should always be done with \"is\" or \"is\n  not\", never the equality operators.\n\n* Binary sequences (instances of \"bytes\" or \"bytearray\") can be\n  compared within and across their types.  They compare\n  lexicographically using the numeric values of their elements.\n\n* Strings (instances of \"str\") compare lexicographically using the\n  numerical Unicode code points (the result of the built-in function\n  \"ord()\") of their characters. [3]\n\n  Strings and binary sequences cannot be directly compared.\n\n* Sequences (instances of \"tuple\", \"list\", or \"range\") can be compared\n  only within each of their types, with the restriction that ranges do\n  not support order comparison.  Equality comparison across these\n  types results in inequality, and ordering comparison across these\n  types raises \"TypeError\".\n\n  Sequences compare lexicographically using comparison of\n  corresponding elements.  The built-in containers typically assume\n  identical objects are equal to themselves.  That lets them bypass\n  equality tests for identical objects to improve performance and to\n  maintain their internal invariants.\n\n  Lexicographical comparison between built-in collections works as\n  follows:\n\n  * For two collections to compare equal, they must be of the same\n    type, have the same length, and each pair of corresponding\n    elements must compare equal (for example, \"[1,2] == (1,2)\" is\n    false because the type is not the same).\n\n  * Collections that support order comparison are ordered the same as\n    their first unequal elements (for example, \"[1,2,x] &lt;= [1,2,y]\"\n    has the same value as \"x &lt;= y\").  If a corresponding element does\n    not exist, the shorter collection is ordered first (for example,\n    \"[1,2] &lt; [1,2,3]\" is true).\n\n* Mappings (instances of \"dict\") compare equal if and only if they\n  have equal \"(key, value)\" pairs. Equality comparison of the keys and\n  values enforces reflexivity.\n\n  Order comparisons (\"&lt;\", \"&gt;\", \"&lt;=\", and \"&gt;=\") raise \"TypeError\".\n\n* Sets (instances of \"set\" or \"frozenset\") can be compared within and\n  across their types.\n\n  They define order comparison operators to mean subset and superset\n  tests.  Those relations do not define total orderings (for example,\n  the two sets \"{1,2}\" and \"{2,3}\" are not equal, nor subsets of one\n  another, nor supersets of one another).  Accordingly, sets are not\n  appropriate arguments for functions which depend on total ordering\n  (for example, \"min()\", \"max()\", and \"sorted()\" produce undefined\n  results given a list of sets as inputs).\n\n  Comparison of sets enforces reflexivity of its elements.\n\n* Most other built-in types have no comparison methods implemented, so\n  they inherit the default comparison behavior.\n\nUser-defined classes that customize their comparison behavior should\nfollow some consistency rules, if possible:\n\n* Equality comparison should be reflexive. In other words, identical\n  objects should compare equal:\n\n     \"x is y\" implies \"x == y\"\n\n* Comparison should be symmetric. In other words, the following\n  expressions should have the same result:\n\n     \"x == y\" and \"y == x\"\n\n     \"x != y\" and \"y != x\"\n\n     \"x &lt; y\" and \"y &gt; x\"\n\n     \"x &lt;= y\" and \"y &gt;= x\"\n\n* Comparison should be transitive. The following (non-exhaustive)\n  examples illustrate that:\n\n     \"x &gt; y and y &gt; z\" implies \"x &gt; z\"\n\n     \"x &lt; y and y &lt;= z\" implies \"x &lt; z\"\n\n* Inverse comparison should result in the boolean negation. In other\n  words, the following expressions should have the same result:\n\n     \"x == y\" and \"not x != y\"\n\n     \"x &lt; y\" and \"not x &gt;= y\" (for total ordering)\n\n     \"x &gt; y\" and \"not x &lt;= y\" (for total ordering)\n\n  The last two expressions apply to totally ordered collections (e.g.\n  to sequences, but not to sets or mappings). See also the\n  \"total_ordering()\" decorator.\n\n* The \"hash()\" result should be consistent with equality. Objects that\n  are equal should either have the same hash value, or be marked as\n  unhashable.\n\nPython does not enforce these consistency rules. In fact, the\nnot-a-number values are an example for not following these rules.\n\n\nMembership test operations\n==========================\n\nThe operators \"in\" and \"not in\" test for membership.  \"x in s\"\nevaluates to \"True\" if *x* is a member of *s*, and \"False\" otherwise.\n\"x not in s\" returns the negation of \"x in s\".  All built-in sequences\nand set types support this as well as dictionary, for which \"in\" tests\nwhether the dictionary has a given key. For container types such as\nlist, tuple, set, frozenset, dict, or collections.deque, the\nexpression \"x in y\" is equivalent to \"any(x is e or x == e for e in\ny)\".\n\nFor the string and bytes types, \"x in y\" is \"True\" if and only if *x*\nis a substring of *y*.  An equivalent test is \"y.find(x) != -1\".\nEmpty strings are always considered to be a substring of any other\nstring, so \"\"\" in \"abc\"\" will return \"True\".\n\nFor user-defined classes which define the \"__contains__()\" method, \"x\nin y\" returns \"True\" if \"y.__contains__(x)\" returns a true value, and\n\"False\" otherwise.\n\nFor user-defined classes which do not define \"__contains__()\" but do\ndefine \"__iter__()\", \"x in y\" is \"True\" if some value \"z\", for which\nthe expression \"x is z or x == z\" is true, is produced while iterating\nover \"y\". If an exception is raised during the iteration, it is as if\n\"in\" raised that exception.\n\nLastly, the old-style iteration protocol is tried: if a class defines\n\"__getitem__()\", \"x in y\" is \"True\" if and only if there is a non-\nnegative integer index *i* such that \"x is y[i] or x == y[i]\", and no\nlower integer index raises the \"IndexError\" exception.  (If any other\nexception is raised, it is as if \"in\" raised that exception).\n\nThe operator \"not in\" is defined to have the inverse truth value of\n\"in\".\n\n\nIdentity comparisons\n====================\n\nThe operators \"is\" and \"is not\" test for an object’s identity: \"x is\ny\" is true if and only if *x* and *y* are the same object.  An\nObject’s identity is determined using the \"id()\" function.  \"x is not\ny\" yields the inverse truth value. [4]\n\nRelated help topics: EXPRESSIONS, BASICMETHODS"
  },
  {
    "objectID": "core/notebooks/notebook01_python.html#about-assigments",
    "href": "core/notebooks/notebook01_python.html#about-assigments",
    "title": "Introduction to Python",
    "section": "About assigments",
    "text": "About assigments\n\nPython never copies an object\nUnless you ask him to\n\nWhen you code\nx = [1, 2, 3]\ny = x\nyou just - bind the variable name x to a list [1, 2, 3] - give another name y to the same object\nImportant remarks\n\nEverything is an object in Python\nEither immutable or mutable\n\n\n\nCode\nid(1), id(1+1), id(2)\n\n\n(11753896, 11753928, 11753928)\n\n\nA list is mutable\n\n\nCode\nx = [1, 2, 3]\nprint(id(x), x)\nx[0] += 42; x.append(3.14)\nprint(id(x), x)\n\n\n134288353648384 [1, 2, 3]\n134288353648384 [43, 2, 3, 3.14]\n\n\nA str is immutable\nIn order to “change” an immutable object, Python creates a new one\n\n\nCode\ns = 'to'\nprint(id(s), s)\ns += 'to'\nprint(id(s), s)\n\n\n134288676817440 to\n134288353246496 toto\n\n\nOnce again, a list is mutable\n\n\nCode\nsuper_list = [3.14, (1, 2, 3), 'tintin']\nother_list = super_list\nid(other_list), id(super_list)\n\n\n(134288353864832, 134288353864832)\n\n\n\nother_list and super_list are the same list\nIf you change one, you change the other.\nid returns the identity of an object. Two objects with the same idendity are the same (not only the same type, but the same instance)\n\n\n\nCode\nother_list[1] = 'youps'\nother_list, super_list\n\n\n([3.14, 'youps', 'tintin'], [3.14, 'youps', 'tintin'])\n\n\n\n\nCode\nid(super_list), id(other_list)\n\n\n(134288353864832, 134288353864832)"
  },
  {
    "objectID": "core/notebooks/notebook01_python.html#if-you-want-a-copy-to-need-to-ask-for-one",
    "href": "core/notebooks/notebook01_python.html#if-you-want-a-copy-to-need-to-ask-for-one",
    "title": "Introduction to Python",
    "section": "If you want a copy, to need to ask for one",
    "text": "If you want a copy, to need to ask for one\n\n\nCode\nother_list = super_list.copy()\nid(other_list), id(super_list)\n\n\n(134288353471744, 134288353864832)\n\n\n\n\nCode\nother_list[1] = 'copy'\nother_list, super_list\n\n\n([3.14, 'copy', 'tintin'], [3.14, 'youps', 'tintin'])\n\n\nOnly other_list is modified.\nBut… what if you have a list of list ? (or a mutable object containing mutable objects)\n\n\nCode\nl1, l2 = [1, 2, 3], [4, 5, 6]\nlist_list = [l1, l2]\nlist_list\n\n\n[[1, 2, 3], [4, 5, 6]]\n\n\n\n\nCode\nid(list_list), id(list_list[0]), id(l1), list_list[0] is l1\n\n\n(134288353460288, 134288353211264, 134288353211264, True)\n\n\nLet’s make a copy of list_list\n\n\nCode\ncopy_list = list_list.copy()\ncopy_list.append('super')\nlist_list, copy_list\n\n\n([[1, 2, 3], [4, 5, 6]], [[1, 2, 3], [4, 5, 6], 'super'])\n\n\n\n\nCode\nid(list_list[0]), id(copy_list[0])\n\n\n(134288353211264, 134288353211264)\n\n\nOK, only copy_list is modified, as expected\nBut now…\n\n\nCode\ncopy_list[0][1] = 'oups'\ncopy_list, list_list\n\n\n([[1, 'oups', 3], [4, 5, 6], 'super'], [[1, 'oups', 3], [4, 5, 6]])\n\n\nQuestion. What happened ?!?\n\nThe list_list object is copied\nBut NOT what it’s containing !\nBy default copy does a shallow copy, not a deep copy\nIt does not build copies of what is contained\nIf you want to copy an object and all that is contained in it, you need to use deepcopy.\n\n\n\nCode\nfrom copy import deepcopy\n\ncopy_list = deepcopy(list_list)\ncopy_list[0][1] = 'incredible !'\nlist_list, copy_list\n\n\n([[1, 'oups', 3], [4, 5, 6]], [[1, 'incredible !', 3], [4, 5, 6]])"
  },
  {
    "objectID": "core/notebooks/notebook01_python.html#final-remarks",
    "href": "core/notebooks/notebook01_python.html#final-remarks",
    "title": "Introduction to Python",
    "section": "Final remarks",
    "text": "Final remarks\n\n\nCode\ntt = ([1, 2, 3], [4, 5, 6])\nprint(id(tt), tt)\nprint(list(map(id, tt)))\n\n\n134286902638208 ([1, 2, 3], [4, 5, 6])\n[134288353646976, 134288353647872]\n\n\n\n\nCode\ntt[0][1] = '42'\nprint(id(tt), tt)\nprint(list(map(id, tt)))\n\n\n134286902638208 ([1, '42', 3], [4, 5, 6])\n[134288353646976, 134288353647872]\n\n\n\n\nCode\ns = [1, 2, 3]\n\n\n\n\nCode\ns2 = s\n\n\n\n\nCode\ns2 is s\n\n\nTrue\n\n\n\n\nCode\nid(s2), id(s)\n\n\n(134286903736640, 134286903736640)"
  },
  {
    "objectID": "core/notebooks/notebook01_python.html#blocks-are-delimited-by-indentation",
    "href": "core/notebooks/notebook01_python.html#blocks-are-delimited-by-indentation",
    "title": "Introduction to Python",
    "section": "Blocks are delimited by indentation!",
    "text": "Blocks are delimited by indentation!\n\n\nCode\na = 3\nif a &gt; 0:\n    if a == 1:\n        print(1)\n    elif a == 2:\n        print(2)\nelif a == 2:\n    print(2)\nelif a == 3:\n    print(3)\nelse:\n    print(a)"
  },
  {
    "objectID": "core/notebooks/notebook01_python.html#anything-can-be-understood-as-a-boolean",
    "href": "core/notebooks/notebook01_python.html#anything-can-be-understood-as-a-boolean",
    "title": "Introduction to Python",
    "section": "Anything can be understood as a boolean",
    "text": "Anything can be understood as a boolean\nFor example, don’t do this to test if a list is empty\n\n\nCode\nl2 = ['hello', 'everybody']\n\nif len(l2) &gt; 0:\n    print(l2[0])\n\n\nhello\n\n\nbut this\n\n\nCode\nif l2:\n    print(l2[0])\n\n\nhello\n\n\nSome poetry\n\nAn empty dict is False\nAn empty string is False\nAn empty list is False\nAn empty tuple is False\nAn empty set is False\n0 is False\n.0 is False\netc…\neverything else is True"
  },
  {
    "objectID": "core/notebooks/notebook01_python.html#while-loops",
    "href": "core/notebooks/notebook01_python.html#while-loops",
    "title": "Introduction to Python",
    "section": "While loops",
    "text": "While loops\n\n\nCode\na = 10\nb = 1\nwhile b &lt; a:\n    b = b + 1\n    print(b)\n\n\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\nCompute the decimals of Pi using the Wallis formula\n\\[\n\\pi = 2 \\prod_{i=1}^{100} \\frac{4i^2}{4i^2 - 1}\n\\]\n\n\nCode\npi = 2\neps = 1e-10\ndif = 2 * eps\ni = 1\nwhile dif &gt; eps:\n    pi, i, old_pi = pi * 4 * i ** 2 / (4 * i ** 2 - 1), i + 1, pi\n    dif = pi - old_pi\n\n\n\n\nCode\npi\n\n\n3.1415837914138556\n\n\n\n\nCode\nfrom math import pi\n\npi\n\n\n3.141592653589793"
  },
  {
    "objectID": "core/notebooks/notebook01_python.html#for-loop-with-range",
    "href": "core/notebooks/notebook01_python.html#for-loop-with-range",
    "title": "Introduction to Python",
    "section": "for loop with range",
    "text": "for loop with range\n\nIteration with an index, with a list, with many things !\nrange has the same parameters as with slicing start:end:stride, all parameters being optional\n\n\n\nCode\nfor i in range(10):\n    print(i)\n\n\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n\n\nCode\nfor i in range(4):\n    print(i + 1)\nprint('-')\n\nfor i in range(1, 5):\n    print(i)\nprint('-')\n\nfor i in range(1, 10, 3):\n    print(i)\n\n\n1\n2\n3\n4\n-\n1\n2\n3\n4\n-\n1\n4\n7\n\n\nSomething for nerds. You can use else in a for loop\n\n\nCode\nnames = ['stephane', 'mokhtar', 'jaouad', 'simon', 'yiyang']\n\nfor name in names:\n    if name.startswith('u'):\n        print(name)\n        break\nelse:\n    print('Not found.')\n\n\nNot found.\n\n\n\n\nCode\nnames = ['stephane', 'mokhtar', 'jaouad', 'ulysse', 'simon', 'yiyang']\n\nfor name in names:\n    if name.startswith('u'):\n        print(name)\n        break\nelse:\n    print('Not found.')\n\n\nulysse"
  },
  {
    "objectID": "core/notebooks/notebook01_python.html#for-loops-over-iterable-objects",
    "href": "core/notebooks/notebook01_python.html#for-loops-over-iterable-objects",
    "title": "Introduction to Python",
    "section": "For loops over iterable objects",
    "text": "For loops over iterable objects\nYou can iterate using for over any container: list, tuple, dict, str, set among others…\n\n\nCode\ncolors = ['red', 'blue', 'black', 'white']\npeoples = ['stephane', 'jaouad', 'mokhtar', 'yiyang', 'rémi']\n\n\n\n\nCode\n# This is stupid\nfor i in range(len(colors)):\n    print(colors[i])\n    \n# This is better\nfor color in colors:\n    print(color)\n\n\nred\nblue\nblack\nwhite\nred\nblue\nblack\nwhite\n\n\nTo iterate over several sequences at the same time, use zip\n\n\nCode\nfor color, people in zip(colors, peoples):\n    print(color, people)\n\n\nred stephane\nblue jaouad\nblack mokhtar\nwhite yiyang\n\n\n\n\nCode\nl = [\"Bonjour\", {'francis': 5214, 'stephane': 5123}, ('truc', 3)]\nfor e in l:\n    print(e, len(e))\n\n\nBonjour 7\n{'francis': 5214, 'stephane': 5123} 2\n('truc', 3) 2\n\n\nLoop over a str\n\n\nCode\ns = 'Bonjour'\nfor c in s:\n    print(c)\n\n\nB\no\nn\nj\no\nu\nr\n\n\nLoop over a dict\n\n\nCode\ndd = {(1, 3): {'hello', 'world'}, 'truc': [1, 2, 3], 5: (1, 4, 2)}\n\n# Default is to loop over keys\nfor key in dd:\n    print(key)\n\n\n(1, 3)\ntruc\n5\n\n\n\n\nCode\n# Loop over values\nfor e in dd.values():\n    print(e)\n\n\n{'hello', 'world'}\n[1, 2, 3]\n(1, 4, 2)\n\n\n\n\nCode\n# Loop over items (key, value) pairs\nfor key, val in dd.items():\n    print(key, val)\n\n\n(1, 3) {'hello', 'world'}\ntruc [1, 2, 3]\n5 (1, 4, 2)\n\n\n\n\nCode\nfor t in dd.items():\n    print(t)\n\n\n((1, 3), {'hello', 'world'})\n('truc', [1, 2, 3])\n(5, (1, 4, 2))"
  },
  {
    "objectID": "core/notebooks/notebook01_python.html#comprehensions",
    "href": "core/notebooks/notebook01_python.html#comprehensions",
    "title": "Introduction to Python",
    "section": "Comprehensions",
    "text": "Comprehensions\nYou can construct a list, dict, set and others using the comprehension syntax\nlist comprehension\n\n\nCode\nprint(colors)\nprint(peoples)\n\n\n['red', 'blue', 'black', 'white']\n['stephane', 'jaouad', 'mokhtar', 'yiyang', 'rémi']\n\n\n\n\nCode\nl = []\nfor p, c in zip(peoples, colors):\n    if len(c)&lt;=4 :\n        l.append(p)\nprint(l)\n\n\n['stephane', 'jaouad']\n\n\n\n\nCode\n# The list of people with favorite color that has no more than 4 characters\n\n[people for color, people in zip(colors, peoples) if len(color) &lt;= 4]\n\n\n['stephane', 'jaouad']\n\n\ndict comprehension\n\n\nCode\n{people: color for color, people in zip(colors, peoples) if len(color) &lt;= 4}\n\n\n{'stephane': 'red', 'jaouad': 'blue'}\n\n\n\n\nCode\n# Allows to build a dict from two lists (for keys and values)\n{key: value for (key, value) in zip(peoples, colors)}\n\n\n{'stephane': 'red', 'jaouad': 'blue', 'mokhtar': 'black', 'yiyang': 'white'}\n\n\n\n\nCode\n# But it's simpler (so better) to use\ndict(zip(peoples, colors))\n\n\n{'stephane': 'red', 'jaouad': 'blue', 'mokhtar': 'black', 'yiyang': 'white'}\n\n\nSomething very convenient is enumerate\n\n\nCode\nfor i, color in enumerate(colors):\n    print(i, color)\n\n\n0 red\n1 blue\n2 black\n3 white\n\n\n\n\nCode\nlist(enumerate(colors))\n\n\n[(0, 'red'), (1, 'blue'), (2, 'black'), (3, 'white')]\n\n\n\n\nCode\ndict(enumerate(s))\n\n\n{0: 'B', 1: 'o', 2: 'n', 3: 'j', 4: 'o', 5: 'u', 6: 'r'}\n\n\n\n\nCode\nprint(dict(enumerate(s)))\n\n\n{0: 'B', 1: 'o', 2: 'n', 3: 'j', 4: 'o', 5: 'u', 6: 'r'}\n\n\n\n\nCode\ns = 'Hey everyone'\n{c: i for i, c in enumerate(s)}\n\n\n{'H': 0, 'e': 11, 'y': 8, ' ': 3, 'v': 5, 'r': 7, 'o': 9, 'n': 10}"
  },
  {
    "objectID": "core/notebooks/notebook01_python.html#about-functional-programming",
    "href": "core/notebooks/notebook01_python.html#about-functional-programming",
    "title": "Introduction to Python",
    "section": "About functional programming",
    "text": "About functional programming\nWe can use lambda to define anonymous functions, and use them in the map and reduce functions\n\n\nCode\nsquare = lambda x: x ** 2\nsquare(2)\n\n\n4\n\n\n\n\nCode\ntype(square)\n\n\nfunction\n\n\n\n\nCode\ndir(square)\n\n\n['__annotations__',\n '__builtins__',\n '__call__',\n '__class__',\n '__closure__',\n '__code__',\n '__defaults__',\n '__delattr__',\n '__dict__',\n '__dir__',\n '__doc__',\n '__eq__',\n '__format__',\n '__ge__',\n '__get__',\n '__getattribute__',\n '__getstate__',\n '__globals__',\n '__gt__',\n '__hash__',\n '__init__',\n '__init_subclass__',\n '__kwdefaults__',\n '__le__',\n '__lt__',\n '__module__',\n '__name__',\n '__ne__',\n '__new__',\n '__qualname__',\n '__reduce__',\n '__reduce_ex__',\n '__repr__',\n '__setattr__',\n '__sizeof__',\n '__str__',\n '__subclasshook__',\n '__type_params__']\n\n\n\n\nCode\ns = \"a\"\n\n\n\n\nCode\ntry:\n    square(\"a\")\nexcept TypeError:\n    print(\"TypeError: unsupported operand type(s) for ** or pow(): 'str' and 'int'\")\n\n\nTypeError: unsupported operand type(s) for ** or pow(): 'str' and 'int'\n\n\n\n\nCode\nsum2 = lambda a, b: a + b\nprint(sum2('Hello', ' world'))\nprint(sum2(1, 2))\n\n\nHello world\n3\n\n\nIntended for short and one-line function.\nMore complex functions use def (see below)"
  },
  {
    "objectID": "core/notebooks/notebook01_python.html#exercise",
    "href": "core/notebooks/notebook01_python.html#exercise",
    "title": "Introduction to Python",
    "section": "Exercise",
    "text": "Exercise\nPrint the squares of even numbers between 0 et 15\n\nUsing a list comprehension as before\nUsing map"
  },
  {
    "objectID": "core/notebooks/notebook01_python.html#brain-teasing-1",
    "href": "core/notebooks/notebook01_python.html#brain-teasing-1",
    "title": "Introduction to Python",
    "section": "Brain-teasing",
    "text": "Brain-teasing\nWhat is the output of\n\n\nCode\nreduce(lambda a, b: a + b[0] * b[1], enumerate('abcde'), 'A')"
  },
  {
    "objectID": "core/notebooks/notebook01_python.html#why-generators",
    "href": "core/notebooks/notebook01_python.html#why-generators",
    "title": "Introduction to Python",
    "section": "Why generators ?",
    "text": "Why generators ?\nThe memory used by range(i) does not scale linearly with i\nWhat is happening ?\n\nrange(n) does not allocate a list of n elements !\nIt generates on the fly the list of required integers\nWe say that such an object behaves like a generator in Python\nMany things in the Python standard library behaves like this\n\nWarning. Getting the real memory footprint of a Python object is difficult. Note that sizeof calls the __sizeof__ method of r, which does not give in general the actual memory used by an object. But nevermind here.\nThe following computation has no memory footprint:\n\n\nCode\nsum(range(10**8))\n\n\n4999999950000000\n\n\n\n\nCode\nmap(lambda x: x**2, range(10**7))\n\n\n&lt;map at 0x7a221b247bb0&gt;\n\n\nmap does not return a list for the same reason\n\n\nCode\nsum(map(lambda x: x**2, range(10**6)))\n\n\n333332833333500000"
  },
  {
    "objectID": "core/notebooks/notebook01_python.html#generator-expression",
    "href": "core/notebooks/notebook01_python.html#generator-expression",
    "title": "Introduction to Python",
    "section": "Generator expression",
    "text": "Generator expression\nNamely generators defined through comprehensions. Just replace [] by () in the comprehension.\nA generator can be iterated on only once\n\n\nCode\nrange(10)\n\n\nrange(0, 10)\n\n\n\n\nCode\ncarres = (i**2 for i in range(10))\n\n\n\n\nCode\ncarres\n\n\n&lt;generator object &lt;genexpr&gt; at 0x7a2271963510&gt;\n\n\n\n\nCode\nfor c in carres:\n    print(c)\n\n\n0\n1\n4\n9\n16\n25\n36\n49\n64\n81\n\n\n\n\nCode\nfor i in range(4):\n    for j in range(3):\n        print(i, j)\n\n\n0 0\n0 1\n0 2\n1 0\n1 1\n1 2\n2 0\n2 1\n2 2\n3 0\n3 1\n3 2\n\n\n\n\nCode\nfrom itertools import product\n\nfor t in product(range(4), range(3)):\n    print(t)\n\n\n(0, 0)\n(0, 1)\n(0, 2)\n(1, 0)\n(1, 1)\n(1, 2)\n(2, 0)\n(2, 1)\n(2, 2)\n(3, 0)\n(3, 1)\n(3, 2)\n\n\n\n\nCode\nfrom itertools import product\n\ngene = (i + j for i, j in product(range(3), range(3)))\ngene\n\n\n&lt;generator object &lt;genexpr&gt; at 0x7a2271963e00&gt;\n\n\n\n\nCode\nprint(list(gene))\nprint(list(gene))\n\n\n[0, 1, 2, 1, 2, 3, 2, 3, 4]\n[]"
  },
  {
    "objectID": "core/notebooks/notebook01_python.html#yield",
    "href": "core/notebooks/notebook01_python.html#yield",
    "title": "Introduction to Python",
    "section": "yield",
    "text": "yield\nSomething very powerful\n\n\nCode\ndef startswith(words, letter):\n    for word in words:\n        if word.startswith(letter):\n            yield word\n\n\n\n\nCode\nwords = [\n    'Python', \"is\", 'awesome', 'in', 'particular', 'generators', \n    'are', 'really', 'cool'\n]\n\n\n\n\nCode\nlist(word for word in words if word.startswith(\"a\"))\n\n\n['awesome', 'are']\n\n\n\n\nCode\na = 2\n\n\n\n\nCode\nfloat(a)\n\n\n2.0\n\n\nBut also with a for loop\n\n\nCode\nfor word in startswith(words, letter='a'):\n    print(word)\n\n\nawesome\nare\n\n\n\n\nCode\nit = startswith(words, letter='a')\n\n\n\n\nCode\ntype(it)\n\n\ngenerator\n\n\n\n\nCode\nnext(it)\n\n\n'awesome'\n\n\n\n\nCode\nnext(it)\n\n\n'are'\n\n\n\n\nCode\ntry:\n    next(it)\nexcept StopIteration:\n    print(\"StopIteration exception!\")\n\n\nStopIteration exception!"
  },
  {
    "objectID": "core/notebooks/notebook01_python.html#exercise-4",
    "href": "core/notebooks/notebook01_python.html#exercise-4",
    "title": "Introduction to Python",
    "section": "Exercise",
    "text": "Exercise\nCount the number of occurences of all the words in words.\nOutput must be a dictionary containg word: count\n\n\nCode\nprint(words)\n\n\n['Bonjour', 'Python', 'c', 'est', 'super', 'Python', 'ca', 'a', 'l', 'air', 'quand', 'même', 'un', 'peu', 'compliqué', 'Mais', 'bon', 'ca', 'a', 'l', 'air', 'pratique', 'Peut-être', 'que', 'je', 'pourrais', 'm', 'en', 'servir', 'pour', 'faire', 'des', 'trucs', 'super']"
  },
  {
    "objectID": "core/notebooks/notebook01_python.html#exercise-5",
    "href": "core/notebooks/notebook01_python.html#exercise-5",
    "title": "Introduction to Python",
    "section": "Exercise",
    "text": "Exercise\nCompute the number of occurences AND the length of each word in words.\nOutput must be a dictionary containing word: (count, length)"
  },
  {
    "objectID": "core/notebooks/notebook01_python.html#exercise-6",
    "href": "core/notebooks/notebook01_python.html#exercise-6",
    "title": "Introduction to Python",
    "section": "Exercise",
    "text": "Exercise\nCount the number of occurences of each word in the text file miserables.txt. We use a open context and the Counter from before."
  },
  {
    "objectID": "core/notebooks/notebook01_python.html#contexts",
    "href": "core/notebooks/notebook01_python.html#contexts",
    "title": "Introduction to Python",
    "section": "Contexts",
    "text": "Contexts\n\nA context in Python is something that we use with the with keyword.\nIt allows to deal automatically with the opening and the closing of the file.\n\nNote the for loop:\nfor line in f:\n    ...\nYou loop directly over the lines of the open file from within the open context"
  },
  {
    "objectID": "core/notebooks/notebook01_python.html#about-pickle",
    "href": "core/notebooks/notebook01_python.html#about-pickle",
    "title": "Introduction to Python",
    "section": "About pickle",
    "text": "About pickle\nYou can save your computation with pickle.\n\npickle is a way of saving almost anything with Python.\nIt serializes the object in a binary format, and is usually the simplest and fastest way to go.\n\n\n\nCode\nimport pickle as pkl\n\n# Let's save it\nwith open('miserable_word_counts.pkl', 'wb') as f:\n    pkl.dump(counter, f)\n\n# And read it again\nwith open('miserable_word_counts.pkl', 'rb') as f:\n    counter = pkl.load(f)\n\n\n\n\nCode\ncounter.most_common(10)\n\n\n[('{', 15),\n ('}', 15),\n ('0', 8),\n ('img', 6),\n ('margin:', 6),\n ('font', 6),\n ('logo', 6),\n ('only', 6),\n ('screen', 6),\n ('and', 6)]"
  },
  {
    "objectID": "core/notebooks/notebook01_python.html#function-definition",
    "href": "core/notebooks/notebook01_python.html#function-definition",
    "title": "Introduction to Python",
    "section": "Function definition",
    "text": "Function definition\nFunction blocks must be indented as other control-flow blocks.\n\n\nCode\ndef test():\n    return 'in test function'\n\ntest()\n\n\n'in test function'"
  },
  {
    "objectID": "core/notebooks/notebook01_python.html#return-statement",
    "href": "core/notebooks/notebook01_python.html#return-statement",
    "title": "Introduction to Python",
    "section": "Return statement",
    "text": "Return statement\nFunctions can optionally return values. By default, functions return None.\nThe syntax to define a function:\n\nthe def keyword;\nis followed by the function’s name, then\nthe arguments of the function are given between parentheses followed by a colon\nthe function body;\nand return object for optionally returning values.\n\n\n\nCode\nNone is None\n\n\nTrue\n\n\n\n\nCode\ndef f(x):\n    return x + 10\nf(20)\n\n\n30\n\n\nA function that returns several elements returns a tuple\n\n\nCode\ndef f(x):\n    return x + 1, x + 4\n\nf(5)\n\n\n(6, 9)\n\n\n\n\nCode\ntype(f)\n\n\nfunction\n\n\n\n\nCode\nf.truc = \"bonjour\"\n\n\n\n\nCode\ntype(f(5))\n\n\ntuple"
  },
  {
    "objectID": "core/notebooks/notebook01_python.html#parameters",
    "href": "core/notebooks/notebook01_python.html#parameters",
    "title": "Introduction to Python",
    "section": "Parameters",
    "text": "Parameters\nMandatory parameters (positional arguments)\n\n\nCode\ndef double_it(x):\n    return x * 2\n\ndouble_it(2)\n\n\n4\n\n\n\n\nCode\ntry:\n    double_it()\nexcept TypeError:\n    print(\"TypeError: double_it() missing 1 required positional argument: 'x'\")\n\n\nTypeError: double_it() missing 1 required positional argument: 'x'\n\n\nOptimal parameters\n\n\nCode\ndef double_it(x=2):\n    return x * 2\n\ndouble_it()\n\n\n4\n\n\n\n\nCode\ndouble_it(3)\n\n\n6\n\n\n\n\nCode\ndef f(x, y=2, z=10):\n    print(x, '+', y, '+', z, '=', x + y + z)\n\n\n\n\nCode\nf(5)\n\n\n5 + 2 + 10 = 17\n\n\n\n\nCode\nf(5, -2)\n\n\n5 + -2 + 10 = 13\n\n\n\n\nCode\nf(5, -2, 8)\n\n\n5 + -2 + 8 = 11\n\n\n\n\nCode\nf(z=5, x=-2, y=8)\n\n\n-2 + 8 + 5 = 11"
  },
  {
    "objectID": "core/notebooks/notebook01_python.html#argument-unpacking-and-keyword-argument-unpacking",
    "href": "core/notebooks/notebook01_python.html#argument-unpacking-and-keyword-argument-unpacking",
    "title": "Introduction to Python",
    "section": "Argument unpacking and keyword argument unpacking",
    "text": "Argument unpacking and keyword argument unpacking\nYou can do stuff like this, using unpacking * notation\n\n\nCode\na, *b, c = 1, 2, 3, 4, 5\na, b, c\n\n\n(1, [2, 3, 4], 5)\n\n\nBack to function f you can unpack a tuple as positional arguments\n\n\nCode\ntt = (1, 2, 3)\nf(*tt)\n\n\n1 + 2 + 3 = 6\n\n\n\n\nCode\ndd = {'y': 10, 'z': -5}\n\n\n\n\nCode\nf(3, **dd)\n\n\n3 + 10 + -5 = 8\n\n\n\n\nCode\ndef g(x, z, y, t=1, u=2):\n    print(x, '+', y, '+', z, '+', t, '+', \n          u, '=', x + y + z + t + u)\n\n\n\n\nCode\ntt = (1, -4, 2)\ndd = {'t': 10, 'u': -5}\ng(*tt, **dd)\n\n\n1 + 2 + -4 + 10 + -5 = 4"
  },
  {
    "objectID": "core/notebooks/notebook01_python.html#the-prototype-of-all-functions-in-python",
    "href": "core/notebooks/notebook01_python.html#the-prototype-of-all-functions-in-python",
    "title": "Introduction to Python",
    "section": "The prototype of all functions in Python",
    "text": "The prototype of all functions in Python\n\n\nCode\ndef f(*args, **kwargs):\n    print('args=', args)\n    print('kwargs=', kwargs)\n\nf(1, 2, 'truc', lastname='gaiffas', firstname='stephane')\n\n\nargs= (1, 2, 'truc')\nkwargs= {'lastname': 'gaiffas', 'firstname': 'stephane'}\n\n\n\nUses * for argument unpacking and ** for keyword argument unpacking\nThe names args and kwargs are a convention, not mandatory\n(but you are fired if you name these arguments otherwise)\n\n\n\nCode\n# How to get fired\ndef f(*aaa, **bbb):\n    print('args=', aaa)\n    print('kwargs=', bbb)\nf(1, 2, 'truc', lastname='gaiffas', firstname='stephane')    \n\n\nargs= (1, 2, 'truc')\nkwargs= {'lastname': 'gaiffas', 'firstname': 'stephane'}\n\n\nRemark. A function is a regular an object… you can add attributes on it !\n\n\nCode\nf.truc = 4\n\n\n\n\nCode\nf(1, 3)\n\n\nargs= (1, 3)\nkwargs= {}\n\n\n\n\nCode\nf(3, -2, y='truc')\n\n\nargs= (3, -2)\nkwargs= {'y': 'truc'}"
  },
  {
    "objectID": "core/notebooks/notebook01_python.html#exercise-7",
    "href": "core/notebooks/notebook01_python.html#exercise-7",
    "title": "Introduction to Python",
    "section": "Exercise",
    "text": "Exercise\nAdd a age method to the Student class that computes the age of the student. - You can (and should) use the datetime module. - Since we only know about the birth year, let’s assume that the day of the birth is January, 1st."
  },
  {
    "objectID": "core/notebooks/notebook01_python.html#properties",
    "href": "core/notebooks/notebook01_python.html#properties",
    "title": "Introduction to Python",
    "section": "Properties",
    "text": "Properties\nWe can make methods look like attributes using properties, as shown below\n\n\nCode\nclass Student(object):\n\n    def __init__(self, name, birthyear, major='computer science'):\n        self.name = name\n        self.birthyear = birthyear\n        self.major = major\n\n    def __repr__(self):\n        return \"Student(name='{name}', birthyear={birthyear}, major='{major}')\"\\\n                .format(name=self.name, birthyear=self.birthyear, major=self.major)\n\n    @property\n    def age(self):\n        return datetime.now().year - self.birthyear\n        \nanna = Student('anna', 1987)\nanna.age\n\n\n38"
  },
  {
    "objectID": "core/notebooks/notebook01_python.html#inheritance",
    "href": "core/notebooks/notebook01_python.html#inheritance",
    "title": "Introduction to Python",
    "section": "Inheritance",
    "text": "Inheritance\nA MasterStudent is a Student with a new extra mandatory internship attribute\n\n\nCode\n\"%d\" % 2\n\n\n'2'\n\n\n\n\nCode\nx = 2\n\nf\"truc {x}\"\n\n\n'truc 2'\n\n\n\n\nCode\nclass MasterStudent(Student):\n    \n    def __init__(self, name, age, internship, major='computer science'):\n        # Student.__init__(self, name, age, major)\n        Student.__init__(self, name, age, major)\n        self.internship = internship\n\n    def __repr__(self):\n        return f\"MasterStudent(name='{self.name}', internship={self.internship}, birthyear={self.birthyear}, major={self.major})\"\n    \nMasterStudent('djalil', 22, 'pwc')\n\n\nMasterStudent(name='djalil', internship=pwc, birthyear=22, major=computer science)\n\n\n\n\nCode\nclass MasterStudent(Student):\n    \n    def __init__(self, name, age, internship, major='computer science'):\n        # Student.__init__(self, name, age, major)\n        Student.__init__(self, name, age, major)\n        self.internship = internship\n\n    def __repr__(self):\n        return \"MasterStudent(name='{name}', internship='{internship}'\" \\\n               \", birthyear={birthyear}, major='{major}')\"\\\n                .format(name=self.name, internship=self.internship,\n                        birthyear=self.birthyear, major=self.major)\n    \ndjalil = MasterStudent('djalil', 1996, 'pwc')\n\n\n\n\nCode\ndjalil.__dict__\n\n\n{'name': 'djalil',\n 'birthyear': 1996,\n 'major': 'computer science',\n 'internship': 'pwc'}\n\n\n\n\nCode\ndjalil.birthyear\n\n\n1996\n\n\n\n\nCode\ndjalil.__dict__[\"birthyear\"]\n\n\n1996"
  },
  {
    "objectID": "core/notebooks/notebook01_python.html#monkey-patching",
    "href": "core/notebooks/notebook01_python.html#monkey-patching",
    "title": "Introduction to Python",
    "section": "Monkey patching",
    "text": "Monkey patching\n\nClasses in Python are objects and actually dicts under the hood…\nTherefore classes are objects that can be changed on the fly\n\n\n\nCode\nclass Monkey(object):\n    \n    def __init__(self, name):\n        self.name = name\n\n    def describe(self):\n        print(\"Old monkey %s\" % self.name)\n\ndef patch(self):\n    print(\"New monkey %s\" % self.name)\n\nmonkey = Monkey(\"Baloo\")\nmonkey.describe()\n\nMonkey.describe = patch\nmonkey.describe()\n\n\nOld monkey Baloo\nNew monkey Baloo\n\n\n\n\nCode\nmonkeys = [Monkey(\"Baloo\"), Monkey(\"Super singe\")]\n\n\nmonkey_name = monkey.name\n\nfor i in range(1000):    \n    monkey_name"
  },
  {
    "objectID": "core/notebooks/notebook01_python.html#data-classes",
    "href": "core/notebooks/notebook01_python.html#data-classes",
    "title": "Introduction to Python",
    "section": "Data classes",
    "text": "Data classes\nSince Python 3.7 you can use a dataclass for this\nDoes a lot of work for you (produces the __repr__ among many other things for you)\n\n\nCode\nfrom dataclasses import dataclass\nfrom datetime import datetime \n\n@dataclass\nclass Student(object):\n    name: str\n    birthyear: int\n    major: str = 'computer science'\n\n    @property\n    def age(self):\n        return datetime.now().year - self.birthyear\n        \nanna = Student(name=\"anna\", birthyear=1987)\nanna\n\n\nStudent(name='anna', birthyear=1987, major='computer science')\n\n\n\n\nCode\nprint(anna.age)\n\n\n38"
  },
  {
    "objectID": "core/notebooks/notebook01_python.html#using-a-mutable-value-as-a-default-value",
    "href": "core/notebooks/notebook01_python.html#using-a-mutable-value-as-a-default-value",
    "title": "Introduction to Python",
    "section": "Using a mutable value as a default value",
    "text": "Using a mutable value as a default value\n\n\nCode\ndef foo(bar=[]):\n    bar.append('oops')\n    return bar\n\nprint(foo())\nprint(foo())\nprint(foo())\n\nprint('-' * 8)\nprint(foo(['Ah ah']))\nprint(foo([]))\n\n\n['oops']\n['oops', 'oops']\n['oops', 'oops', 'oops']\n--------\n['Ah ah', 'oops']\n['oops']\n\n\n\n\nCode\nprint(foo.__defaults__)\nfoo()\nprint(foo.__defaults__)\n\n\n(['oops', 'oops', 'oops'],)\n(['oops', 'oops', 'oops', 'oops'],)\n\n\n\nThe default value for a function argument is evaluated once, when the function is defined\nthe bar argument is initialized to its default (i.e., an empty list) only when foo() is first defined\nsuccessive calls to foo() (with no a bar argument specified) use the same list!\n\nOne should use instead\n\n\nCode\ndef foo(bar=None):\n    if bar is None:\n        bar = []\n    bar.append('oops')\n    return bar\n\nprint(foo())\nprint(foo())\nprint(foo())\nprint(foo(['OK']))\n\n\n['oops']\n['oops']\n['oops']\n['OK', 'oops']\n\n\n\n\nCode\nprint(foo.__defaults__)\nfoo()\nprint(foo.__defaults__)\n\n\n(None,)\n(None,)\n\n\nNo problem with immutable types\n\n\nCode\ndef foo(bar=()):\n    bar += ('oops',)\n    return bar\n\nprint(foo())\nprint(foo())\nprint(foo())\n\n\n('oops',)\n('oops',)\n('oops',)\n\n\n\n\nCode\nprint(foo.__defaults__)\n\n\n((),)"
  },
  {
    "objectID": "core/notebooks/notebook01_python.html#class-attributes-vs-object-attributes",
    "href": "core/notebooks/notebook01_python.html#class-attributes-vs-object-attributes",
    "title": "Introduction to Python",
    "section": "Class attributes VS object attributes",
    "text": "Class attributes VS object attributes\n\n\nCode\nclass A(object):\n    x = 1\n\n    def __init__(self):\n        self.y = 2\n\nclass B(A):\n    def __init__(self):\n        super().__init__()\n\nclass C(A):\n    def __init__(self):\n        super().__init__()\n\na, b, c = A(), B(), C()\n\n\n\n\nCode\nprint(a.x, b.x, c.x)\nprint(a.y, b.y, c.y)\n\n\n1 1 1\n2 2 2\n\n\n\n\nCode\na.y = 3\nprint(a.y, b.y, c.y)\n\n\n3 2 2\n\n\n\n\nCode\na.x = 3  # Adds a new attribute named x in object a\nprint(a.x, b.x, c.x)\n\n\n3 1 1\n\n\n\n\nCode\nA.x = 4 # Changes the class attribute x of class A\nprint(a.x, b.x, c.x)\n\n\n3 4 4\n\n\n\nAttribute x is not an attribute of b nor c\nIt is also not a class attribute of classes B and C\nSo, it is is looked up in the base class A, which contains a class attribute x\n\nClasses and objects contain a hidden dict to store their attributes, and are accessed following a method resolution order (MRO)\n\n\nCode\na.__dict__, b.__dict__, c.__dict__\n\n\n({'y': 3, 'x': 3}, {'y': 2}, {'y': 2})\n\n\n\n\nCode\nA.__dict__, B.__dict__, C.__dict__\n\n\n(mappingproxy({'__module__': '__main__',\n               'x': 4,\n               '__init__': &lt;function __main__.A.__init__(self)&gt;,\n               '__dict__': &lt;attribute '__dict__' of 'A' objects&gt;,\n               '__weakref__': &lt;attribute '__weakref__' of 'A' objects&gt;,\n               '__doc__': None}),\n mappingproxy({'__module__': '__main__',\n               '__init__': &lt;function __main__.B.__init__(self)&gt;,\n               '__doc__': None}),\n mappingproxy({'__module__': '__main__',\n               '__init__': &lt;function __main__.C.__init__(self)&gt;,\n               '__doc__': None}))\n\n\nThis can lead to nasty errors when using class attributes: learn more about this"
  },
  {
    "objectID": "core/notebooks/notebook01_python.html#python-scope-rules",
    "href": "core/notebooks/notebook01_python.html#python-scope-rules",
    "title": "Introduction to Python",
    "section": "Python scope rules",
    "text": "Python scope rules\n\n\nCode\ntry:\n    ints += [4]\nexcept NameError:\n    print(\"NameError: name 'ints' is not defined\")\n\n\nNameError: name 'ints' is not defined\n\n\n\n\nCode\nints = [1]\n\ndef foo1():\n    ints.append(2)\n    return ints\n\ndef foo2():\n    ints += [2]\n    return ints\n\n\n\n\nCode\nfoo1()\n\n\n[1, 2]\n\n\n\n\nCode\ntry:    \n    foo2()\nexcept UnboundLocalError as inst:\n    print(inst)\n\n\ncannot access local variable 'ints' where it is not associated with a value\n\n\n\nWhat the hell ?\n\nAn assignment to a variable in a scope assumes that the variable is local to that scope\nand shadows any similarly named variable in any outer scope\n\nints += [2]\nmeans\nints = ints + [2]\nwhich is an assigment: ints must be defined in the local scope, but it is not, while\nints.append(2)\nis not an assignemnt"
  },
  {
    "objectID": "core/notebooks/notebook01_python.html#modify-a-list-while-iterating-over-it",
    "href": "core/notebooks/notebook01_python.html#modify-a-list-while-iterating-over-it",
    "title": "Introduction to Python",
    "section": "Modify a list while iterating over it",
    "text": "Modify a list while iterating over it\n\n\nCode\nodd = lambda x: bool(x % 2)\nnumbers = list(range(10))\n\ntry:\n  for i in range(len(numbers)):\n      if odd(numbers[i]):\n          del numbers[i]\nexcept IndexError as inst:\n    print(inst)\n\n\nlist index out of range\n\n\nTypically an example where one should use a list comprehension\n\n\nCode\n[number for number in numbers if not odd(number)]\n\n\n[0, 2, 4, 6, 8]"
  },
  {
    "objectID": "core/notebooks/notebook01_python.html#no-docstrings",
    "href": "core/notebooks/notebook01_python.html#no-docstrings",
    "title": "Introduction to Python",
    "section": "No docstrings",
    "text": "No docstrings\nAccept to spend time to write clean docstrings (look at numpydoc style)\n\n\nCode\ndef create_student(name, age, address, major='computer science'):\n    \"\"\"Add a student in the database\n    \n    Parameters\n    ----------\n    name: `str`\n        Name of the student\n    \n    age: `int`\n        Age of the student\n    \n    address: `str`\n        Address of the student\n    \n    major: `str`, default='computer science'\n        The major chosen by the student\n    \n    Returns\n    -------\n    output: `Student`\n        A fresh student\n    \"\"\"\n    pass\n\n\n\n\nCode\ncreate_student('Duduche', 28, 'Chalons')"
  },
  {
    "objectID": "core/notebooks/notebook01_python.html#not-using-available-methods-andor-the-simplest-solution",
    "href": "core/notebooks/notebook01_python.html#not-using-available-methods-andor-the-simplest-solution",
    "title": "Introduction to Python",
    "section": "Not using available methods and/or the simplest solution",
    "text": "Not using available methods and/or the simplest solution\n\n\nCode\ndd = {'stephane': 1234, 'gael': 4567, 'gontran': 891011}\n\n# Bad\nfor key in dd.keys():\n    print(key, dd[key])\n\nprint('-' * 8)\n\n# Good\nfor key, value in dd.items():\n    print(key, value)\n\n\nstephane 1234\ngael 4567\ngontran 891011\n--------\nstephane 1234\ngael 4567\ngontran 891011\n\n\n\n\nCode\ncolors = ['black', 'yellow', 'brown', 'red', 'pink']\n\n# Bad\nfor i in range(len(colors)):\n    print(i, colors[i])\n\nprint('-' * 8)\n\n# Good\nfor i, color in enumerate(colors):\n    print(i, color)\n\n\n0 black\n1 yellow\n2 brown\n3 red\n4 pink\n--------\n0 black\n1 yellow\n2 brown\n3 red\n4 pink"
  },
  {
    "objectID": "core/notebooks/notebook01_python.html#not-using-the-standard-library",
    "href": "core/notebooks/notebook01_python.html#not-using-the-standard-library",
    "title": "Introduction to Python",
    "section": "Not using the standard library",
    "text": "Not using the standard library\nWhile it’s always better than a hand-made solution\n\n\nCode\nlist1 = [1, 2]\nlist2 = [3, 4]\nlist3 = [5, 6, 7]\n\nfor a in list1:\n    for b in list2:\n        for c in list3:\n            print(a, b, c)\n\n\n1 3 5\n1 3 6\n1 3 7\n1 4 5\n1 4 6\n1 4 7\n2 3 5\n2 3 6\n2 3 7\n2 4 5\n2 4 6\n2 4 7\n\n\n\n\nCode\nfrom itertools import product\n\nfor a, b, c in product(list1, list2, list3):\n    print(a, b, c)\n\n\n1 3 5\n1 3 6\n1 3 7\n1 4 5\n1 4 6\n1 4 7\n2 3 5\n2 3 6\n2 3 7\n2 4 5\n2 4 6\n2 4 7"
  },
  {
    "objectID": "core/notebooks/notebook03_pandas.html",
    "href": "core/notebooks/notebook03_pandas.html",
    "title": "Introduction to pandas",
    "section": "",
    "text": "The pandas library (https://pandas.pydata.org) is one of the most used tool at the disposal of people working with data in python today."
  },
  {
    "objectID": "core/notebooks/notebook03_pandas.html#why",
    "href": "core/notebooks/notebook03_pandas.html#why",
    "title": "Introduction to pandas",
    "section": "Why ?",
    "text": "Why ?\nThrough pandas, you get acquainted with your data by analyzing it\n\nWhat’s the average, median, max, or min of each column?\nDoes column A correlate with column B?\nWhat does the distribution of data in column C look like?"
  },
  {
    "objectID": "core/notebooks/notebook03_pandas.html#why-cont",
    "href": "core/notebooks/notebook03_pandas.html#why-cont",
    "title": "Introduction to pandas",
    "section": "Why (con’t) ?",
    "text": "Why (con’t) ?\nyou get acquainted with your data by cleaning and transforming it\n\nRemoving missing values, filter rows or columns using some criteria\nStore the cleaned, transformed data back into virtually any format or database\nData visualization (when combined matplotlib or seaborn or others)"
  },
  {
    "objectID": "core/notebooks/notebook03_pandas.html#where",
    "href": "core/notebooks/notebook03_pandas.html#where",
    "title": "Introduction to pandas",
    "section": "Where ?",
    "text": "Where ?\npandas is a central component of the python “stack” for data science\n\npandas is built on top of numpy\noften used in conjunction with other libraries\na DataFrame is often fed to plotting functions or machine learning algorithms (such as scikit-learn)\nWell-interfaced with jupyter, leading to a nice interactive environment for data exploration and modeling"
  },
  {
    "objectID": "core/notebooks/notebook03_pandas.html#core-components-of-pandas",
    "href": "core/notebooks/notebook03_pandas.html#core-components-of-pandas",
    "title": "Introduction to pandas",
    "section": "Core components of pandas",
    "text": "Core components of pandas\nThe two primary components of pandas are the Series and DataFrame.\n\nA Series is essentially a column\nA DataFrame is a multi-dimensional table made up of a collection of Series with equal length"
  },
  {
    "objectID": "core/notebooks/notebook03_pandas.html#creating-a-dataframe-from-scratch",
    "href": "core/notebooks/notebook03_pandas.html#creating-a-dataframe-from-scratch",
    "title": "Introduction to pandas",
    "section": "Creating a DataFrame from scratch",
    "text": "Creating a DataFrame from scratch\n\n\nCode\nimport pandas as pd\n\nfruits = {\n    \"apples\": [3, 2, 0, 1],\n    \"oranges\": [0, 3, 7, 2]\n}\n\ndf_fruits = pd.DataFrame(fruits)\ndf_fruits\n\n\n\n\n\n\n\n\n\napples\noranges\n\n\n\n\n0\n3\n0\n\n\n1\n2\n3\n\n\n2\n0\n7\n\n\n3\n1\n2\n\n\n\n\n\n\n\n\n\nCode\ntype(df_fruits)\n\n\npandas.core.frame.DataFrame\n\n\n\n\nCode\ndf_fruits[\"apples\"]\n\n\n0    3\n1    2\n2    0\n3    1\nName: apples, dtype: int64\n\n\n\n\nCode\ntype(df_fruits[\"apples\"])\n\n\npandas.core.series.Series"
  },
  {
    "objectID": "core/notebooks/notebook03_pandas.html#indexing",
    "href": "core/notebooks/notebook03_pandas.html#indexing",
    "title": "Introduction to pandas",
    "section": "Indexing",
    "text": "Indexing\n\nBy default, a DataFrame uses a contiguous index\nBut what if we want to say who buys the fruits ?\n\n\n\nCode\ndf_fruits = pd.DataFrame(fruits, index=[\"Daniel\", \"Sean\", \"Pierce\", \"Roger\"])\ndf_fruits\n\n\n\n\n\n\n\n\n\napples\noranges\n\n\n\n\nDaniel\n3\n0\n\n\nSean\n2\n3\n\n\nPierce\n0\n7\n\n\nRoger\n1\n2"
  },
  {
    "objectID": "core/notebooks/notebook03_pandas.html#loc-versus-.iloc",
    "href": "core/notebooks/notebook03_pandas.html#loc-versus-.iloc",
    "title": "Introduction to pandas",
    "section": ".loc versus .iloc",
    "text": ".loc versus .iloc\n\n.loc locates by name\n.iloc locates by numerical index\n\n\n\nCode\ndf_fruits\n\n\n\n\n\n\n\n\n\napples\noranges\n\n\n\n\nDaniel\n3\n0\n\n\nSean\n2\n3\n\n\nPierce\n0\n7\n\n\nRoger\n1\n2\n\n\n\n\n\n\n\n\n\nCode\n# What's in Sean's basket ?\ndf_fruits.loc['Sean']\n\n\napples     2\noranges    3\nName: Sean, dtype: int64\n\n\n\n\nCode\n# Who has oranges ?\ndf_fruits.loc[:, 'oranges']\n\n\nDaniel    0\nSean      3\nPierce    7\nRoger     2\nName: oranges, dtype: int64\n\n\n\n\nCode\n# How many apples in Pierce's basket ?\ndf_fruits.loc['Pierce', 'apples']\n\n\nnp.int64(0)\n\n\n\n\nCode\ndf_fruits\n\n\n\n\n\n\n\n\n\napples\noranges\n\n\n\n\nDaniel\n3\n0\n\n\nSean\n2\n3\n\n\nPierce\n0\n7\n\n\nRoger\n1\n2\n\n\n\n\n\n\n\n\n\nCode\ndf_fruits.iloc[2, 1]\n\n\nnp.int64(7)"
  },
  {
    "objectID": "core/notebooks/notebook03_pandas.html#main-attributes-and-methods-of-a-dataframe",
    "href": "core/notebooks/notebook03_pandas.html#main-attributes-and-methods-of-a-dataframe",
    "title": "Introduction to pandas",
    "section": "Main attributes and methods of a DataFrame",
    "text": "Main attributes and methods of a DataFrame\nA DataFrame has many attributes\n\n\nCode\ndf_fruits.columns\n\n\nIndex(['apples', 'oranges'], dtype='object')\n\n\n\n\nCode\ndf_fruits.index\n\n\nIndex(['Daniel', 'Sean', 'Pierce', 'Roger'], dtype='object')\n\n\n\n\nCode\ndf_fruits.dtypes\n\n\napples     int64\noranges    int64\ndtype: object\n\n\nA DataFrame has many methods\n\n\nCode\ndf_fruits.info()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 4 entries, Daniel to Roger\nData columns (total 2 columns):\n #   Column   Non-Null Count  Dtype\n---  ------   --------------  -----\n 0   apples   4 non-null      int64\n 1   oranges  4 non-null      int64\ndtypes: int64(2)\nmemory usage: 268.0+ bytes\n\n\n\n\nCode\ndf_fruits.describe()\n\n\n\n\n\n\n\n\n\napples\noranges\n\n\n\n\ncount\n4.000000\n4.00000\n\n\nmean\n1.500000\n3.00000\n\n\nstd\n1.290994\n2.94392\n\n\nmin\n0.000000\n0.00000\n\n\n25%\n0.750000\n1.50000\n\n\n50%\n1.500000\n2.50000\n\n\n75%\n2.250000\n4.00000\n\n\nmax\n3.000000\n7.00000"
  },
  {
    "objectID": "core/notebooks/notebook03_pandas.html#missing-values",
    "href": "core/notebooks/notebook03_pandas.html#missing-values",
    "title": "Introduction to pandas",
    "section": "Missing values",
    "text": "Missing values\nWhat if we don’t know how many apples are in Sean’s basket ?\n\n\nCode\ndf_fruits.loc['Sean', 'apples'] = None\ndf_fruits\n\n\n\n\n\n\n\n\n\napples\noranges\n\n\n\n\nDaniel\n3.0\n0\n\n\nSean\nNaN\n3\n\n\nPierce\n0.0\n7\n\n\nRoger\n1.0\n2\n\n\n\n\n\n\n\n\n\nCode\ndf_fruits.describe()\n\n\n\n\n\n\n\n\n\napples\noranges\n\n\n\n\ncount\n3.000000\n4.00000\n\n\nmean\n1.333333\n3.00000\n\n\nstd\n1.527525\n2.94392\n\n\nmin\n0.000000\n0.00000\n\n\n25%\n0.500000\n1.50000\n\n\n50%\n1.000000\n2.50000\n\n\n75%\n2.000000\n4.00000\n\n\nmax\n3.000000\n7.00000\n\n\n\n\n\n\n\nNote that count is 3 for apples now, since we have 1 missing value among the 4\n\n\n\n\n\n\nNote\n\n\n\nTo review the members of objects of class pandas.DataFrame, dir() and module inspect are convenient.\n\n\n\n\nCode\n[x for x in dir(df_fruits) if not x.startswith('_') and not callable(x)]\n\n\n\n\nCode\nimport inspect\n\n# Get a list of methods\nmembres = inspect.getmembers(df_fruits)\n\nmethod_names = [m[0] for m in membres \n    if callable(m[1]) and not m[0].startswith('_')]\n\nprint(method_names)\n\n\n['abs', 'add', 'add_prefix', 'add_suffix', 'agg', 'aggregate', 'align', 'all', 'any', 'apply', 'applymap', 'asfreq', 'asof', 'assign', 'astype', 'at_time', 'backfill', 'between_time', 'bfill', 'bool', 'boxplot', 'clip', 'combine', 'combine_first', 'compare', 'convert_dtypes', 'copy', 'corr', 'corrwith', 'count', 'cov', 'cummax', 'cummin', 'cumprod', 'cumsum', 'describe', 'diff', 'div', 'divide', 'dot', 'drop', 'drop_duplicates', 'droplevel', 'dropna', 'duplicated', 'eq', 'equals', 'eval', 'ewm', 'expanding', 'explode', 'ffill', 'fillna', 'filter', 'first', 'first_valid_index', 'floordiv', 'from_dict', 'from_records', 'ge', 'get', 'groupby', 'gt', 'head', 'hist', 'idxmax', 'idxmin', 'iloc', 'infer_objects', 'info', 'insert', 'interpolate', 'isetitem', 'isin', 'isna', 'isnull', 'items', 'iterrows', 'itertuples', 'join', 'keys', 'kurt', 'kurtosis', 'last', 'last_valid_index', 'le', 'loc', 'lt', 'map', 'mask', 'max', 'mean', 'median', 'melt', 'memory_usage', 'merge', 'min', 'mod', 'mode', 'mul', 'multiply', 'ne', 'nlargest', 'notna', 'notnull', 'nsmallest', 'nunique', 'pad', 'pct_change', 'pipe', 'pivot', 'pivot_table', 'plot', 'pop', 'pow', 'prod', 'product', 'quantile', 'query', 'radd', 'rank', 'rdiv', 'reindex', 'reindex_like', 'rename', 'rename_axis', 'reorder_levels', 'replace', 'resample', 'reset_index', 'rfloordiv', 'rmod', 'rmul', 'rolling', 'round', 'rpow', 'rsub', 'rtruediv', 'sample', 'select_dtypes', 'sem', 'set_axis', 'set_flags', 'set_index', 'shift', 'skew', 'sort_index', 'sort_values', 'squeeze', 'stack', 'std', 'sub', 'subtract', 'sum', 'swapaxes', 'swaplevel', 'tail', 'take', 'to_clipboard', 'to_csv', 'to_dict', 'to_excel', 'to_feather', 'to_gbq', 'to_hdf', 'to_html', 'to_json', 'to_latex', 'to_markdown', 'to_numpy', 'to_orc', 'to_parquet', 'to_period', 'to_pickle', 'to_records', 'to_sql', 'to_stata', 'to_string', 'to_timestamp', 'to_xarray', 'to_xml', 'transform', 'transpose', 'truediv', 'truncate', 'tz_convert', 'tz_localize', 'unstack', 'update', 'value_counts', 'var', 'where', 'xs']\n\n\n\n\nCode\nothers = [x for x in membres\n    if not callable(x[1])]\n\n[x[0] for x in others if not x[0].startswith('_')]\n\n\n['T',\n 'apples',\n 'at',\n 'attrs',\n 'axes',\n 'columns',\n 'dtypes',\n 'empty',\n 'flags',\n 'iat',\n 'index',\n 'ndim',\n 'oranges',\n 'shape',\n 'size',\n 'style',\n 'values']"
  },
  {
    "objectID": "core/notebooks/notebook03_pandas.html#adding-a-column",
    "href": "core/notebooks/notebook03_pandas.html#adding-a-column",
    "title": "Introduction to pandas",
    "section": "Adding a column",
    "text": "Adding a column\nOoooops, we forgot about the bananas !\n\n\nCode\ndf_fruits[\"bananas\"] = [0, 2, 1, 6]\ndf_fruits\n\n\n\n\n\n\n\n\n\napples\noranges\nbananas\n\n\n\n\nDaniel\n3.0\n0\n0\n\n\nSean\nNaN\n3\n2\n\n\nPierce\n0.0\n7\n1\n\n\nRoger\n1.0\n2\n6"
  },
  {
    "objectID": "core/notebooks/notebook03_pandas.html#adding-a-column-with-the-date",
    "href": "core/notebooks/notebook03_pandas.html#adding-a-column-with-the-date",
    "title": "Introduction to pandas",
    "section": "Adding a column with the date",
    "text": "Adding a column with the date\nAnd we forgot the dates !\n\n\nCode\ndf_fruits['time'] = [\n    \"2020/10/08 12:13\", \"2020/10/07 11:37\", \n    \"2020/10/10 14:07\", \"2020/10/09 10:51\"\n]\ndf_fruits\n\n\n\n\n\n\n\n\n\napples\noranges\nbananas\ntime\n\n\n\n\nDaniel\n3.0\n0\n0\n2020/10/08 12:13\n\n\nSean\nNaN\n3\n2\n2020/10/07 11:37\n\n\nPierce\n0.0\n7\n1\n2020/10/10 14:07\n\n\nRoger\n1.0\n2\n6\n2020/10/09 10:51\n\n\n\n\n\n\n\n\n\nCode\ndf_fruits.dtypes\n\n\napples     float64\noranges      int64\nbananas      int64\ntime        object\ndtype: object\n\n\n\n\nCode\ntype(df_fruits.loc[\"Roger\", \"time\"])\n\n\nstr\n\n\nIt is not a date but a string (str) ! So we convert this column to something called datetime\n\n\nCode\ndf_fruits[\"time\"] = pd.to_datetime(df_fruits[\"time\"])\ndf_fruits\n\n\n\n\n\n\n\n\n\napples\noranges\nbananas\ntime\n\n\n\n\nDaniel\n3.0\n0\n0\n2020-10-08 12:13:00\n\n\nSean\nNaN\n3\n2\n2020-10-07 11:37:00\n\n\nPierce\n0.0\n7\n1\n2020-10-10 14:07:00\n\n\nRoger\n1.0\n2\n6\n2020-10-09 10:51:00\n\n\n\n\n\n\n\n\n\nCode\ndf_fruits.dtypes\n\n\napples            float64\noranges             int64\nbananas             int64\ntime       datetime64[ns]\ndtype: object\n\n\n\n\n\n\n\n\nNote\n\n\n\nEvery data science framework implements some datetime handling scheme. For Python see Python official documentation on datetime module\n\n\nWhat if we want to keep only the baskets after (including) October, 9th ?\n\n\nCode\ndf_fruits.loc[df_fruits[\"time\"] &gt;= pd.Timestamp(\"2020/10/09\")]\n\n\n\n\n\n\n\n\n\napples\noranges\nbananas\ntime\n\n\n\n\nPierce\n0.0\n7\n1\n2020-10-10 14:07:00\n\n\nRoger\n1.0\n2\n6\n2020-10-09 10:51:00"
  },
  {
    "objectID": "core/notebooks/notebook03_pandas.html#slices-and-subsets-of-rows-or-columns",
    "href": "core/notebooks/notebook03_pandas.html#slices-and-subsets-of-rows-or-columns",
    "title": "Introduction to pandas",
    "section": "Slices and subsets of rows or columns",
    "text": "Slices and subsets of rows or columns\n\n\nCode\ndf_fruits\n\n\n\n\n\n\n\n\n\napples\noranges\nbananas\ntime\n\n\n\n\nDaniel\n3.0\n0\n0\n2020-10-08 12:13:00\n\n\nSean\nNaN\n3\n2\n2020-10-07 11:37:00\n\n\nPierce\n0.0\n7\n1\n2020-10-10 14:07:00\n\n\nRoger\n1.0\n2\n6\n2020-10-09 10:51:00\n\n\n\n\n\n\n\n\n\nCode\ndf_fruits.loc[:, \"oranges\":\"time\"]\n\n\n\n\n\n\n\n\n\noranges\nbananas\ntime\n\n\n\n\nDaniel\n0\n0\n2020-10-08 12:13:00\n\n\nSean\n3\n2\n2020-10-07 11:37:00\n\n\nPierce\n7\n1\n2020-10-10 14:07:00\n\n\nRoger\n2\n6\n2020-10-09 10:51:00\n\n\n\n\n\n\n\n\n\nCode\ndf_fruits.loc[\"Daniel\":\"Sean\", \"apples\":\"bananas\"]\n\n\n\n\n\n\n\n\n\napples\noranges\nbananas\n\n\n\n\nDaniel\n3.0\n0\n0\n\n\nSean\nNaN\n3\n2\n\n\n\n\n\n\n\n\n\nCode\ndf_fruits[[\"apples\", \"time\"]]\n\n\n\n\n\n\n\n\n\napples\ntime\n\n\n\n\nDaniel\n3.0\n2020-10-08 12:13:00\n\n\nSean\nNaN\n2020-10-07 11:37:00\n\n\nPierce\n0.0\n2020-10-10 14:07:00\n\n\nRoger\n1.0\n2020-10-09 10:51:00"
  },
  {
    "objectID": "core/notebooks/notebook03_pandas.html#write-our-data-to-a-csv-file",
    "href": "core/notebooks/notebook03_pandas.html#write-our-data-to-a-csv-file",
    "title": "Introduction to pandas",
    "section": "Write our data to a CSV file",
    "text": "Write our data to a CSV file\nWhat if we want to write the file ?\n\n\nCode\ndf_fruits\n\n\n\n\n\n\n\n\n\napples\noranges\nbananas\ntime\n\n\n\n\nDaniel\n3.0\n0\n0\n2020-10-08 12:13:00\n\n\nSean\nNaN\n3\n2\n2020-10-07 11:37:00\n\n\nPierce\n0.0\n7\n1\n2020-10-10 14:07:00\n\n\nRoger\n1.0\n2\n6\n2020-10-09 10:51:00\n\n\n\n\n\n\n\n\n\nCode\ndf_fruits.to_csv(\"fruits.csv\")\n\n\n\n\nCode\n# Use !dir on windows\n!ls -alh | grep fru\n\n\n-rw-rw-r--  1 boucheron boucheron  163 avril  3 15:08 fruits.csv\n\n\n\n\nCode\n!head -n 5 fruits.csv\n\n\n,apples,oranges,bananas,time\nDaniel,3.0,0,0,2020-10-08 12:13:00\nSean,,3,2,2020-10-07 11:37:00\nPierce,0.0,7,1,2020-10-10 14:07:00\nRoger,1.0,2,6,2020-10-09 10:51:00"
  },
  {
    "objectID": "core/notebooks/notebook03_pandas.html#reading-data-and-working-with-it",
    "href": "core/notebooks/notebook03_pandas.html#reading-data-and-working-with-it",
    "title": "Introduction to pandas",
    "section": "Reading data and working with it",
    "text": "Reading data and working with it\n\n\n\n\n\n\nNote\n\n\n\nThe tips dataset comes through Kaggle\n\nThis dataset is a treasure trove of information from a collection of case studies for business statistics. Special thanks to Bryant and Smith for their diligent work:\n\n\nBryant, P. G. and Smith, M (1995) Practical Data Analysis: Case Studies in Business Statistics. Homewood, IL: Richard D. Irwin Publishing.\n\n\nYou can also access this dataset now through the Python package Seaborn.\n\n\n\nIt contains data about a restaurant: the bill, tip and some informations about the customers.\n\n\n\n\n\n\nA toy extraction pattern\n\n\n\nA data pipeline usually starts with Extraction, that is gathering data from some source, possibly in a galaxy far, far awy. Here follows a toy extraction pattern\n\nobtain the data from some URL using package requests\nsave the data on the hard drive\nload the data using Pandas\n\n\n\nCode\nimport requests\nimport os\n\n# The path containing your notebook\npath_data = './'\n# The name of the file\nfilename = 'tips.csv'\n\nif os.path.exists(os.path.join(path_data, filename)):\n    print('The file %s already exists.' % os.path.join(path_data, filename))\nelse:\n    url = 'https://raw.githubusercontent.com/mwaskom/seaborn-data/refs/heads/master/tips.csv'\n    r = requests.get(url)\n    with open(os.path.join(path_data, filename), 'wb') as f:\n        f.write(r.content)\n    print('Downloaded file %s.' % os.path.join(path_data, filename))\n\n\n\n\nCode\ndf = pd.read_csv(\n    \"tips.csv\", \n    delimiter=\",\"\n)\n\n\n\n\nThe data can be obtained from package seaborn.\n\n\nCode\nimport seaborn as sns\n\nsns_ds = sns.get_dataset_names()\n\n'tips' in sns_ds\n\ndf = sns.load_dataset('tips')\n\n\n\n\nCode\n# `.head()` shows the first rows of the dataframe\ndf.head(n=10)\n\n\n\n\n\n\n\n\n\ntotal_bill\ntip\nsex\nsmoker\nday\ntime\nsize\n\n\n\n\n0\n16.99\n1.01\nFemale\nNo\nSun\nDinner\n2\n\n\n1\n10.34\n1.66\nMale\nNo\nSun\nDinner\n3\n\n\n2\n21.01\n3.50\nMale\nNo\nSun\nDinner\n3\n\n\n3\n23.68\n3.31\nMale\nNo\nSun\nDinner\n2\n\n\n4\n24.59\n3.61\nFemale\nNo\nSun\nDinner\n4\n\n\n5\n25.29\n4.71\nMale\nNo\nSun\nDinner\n4\n\n\n6\n8.77\n2.00\nMale\nNo\nSun\nDinner\n2\n\n\n7\n26.88\n3.12\nMale\nNo\nSun\nDinner\n4\n\n\n8\n15.04\n1.96\nMale\nNo\nSun\nDinner\n2\n\n\n9\n14.78\n3.23\nMale\nNo\nSun\nDinner\n2\n\n\n\n\n\n\n\n\n\nCode\ndf.info()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 244 entries, 0 to 243\nData columns (total 7 columns):\n #   Column      Non-Null Count  Dtype   \n---  ------      --------------  -----   \n 0   total_bill  244 non-null    float64 \n 1   tip         244 non-null    float64 \n 2   sex         244 non-null    category\n 3   smoker      244 non-null    category\n 4   day         244 non-null    category\n 5   time        244 non-null    category\n 6   size        244 non-null    int64   \ndtypes: category(4), float64(2), int64(1)\nmemory usage: 7.4 KB\n\n\n\n\nCode\ndf.loc[42, \"day\"]\n\n\n'Sun'\n\n\n\n\nCode\ntype(df.loc[42, \"day\"])\n\n\nstr\n\n\nBy default, columns that are non-numerical contain strings (str type)"
  },
  {
    "objectID": "core/notebooks/notebook03_pandas.html#the-category-type",
    "href": "core/notebooks/notebook03_pandas.html#the-category-type",
    "title": "Introduction to pandas",
    "section": "The category type",
    "text": "The category type\nAn important type in pandas is category for variables that are non-numerical\nPro tip. It’s always a good idea to tell pandas which columns should be imported as categorical\nSo, let’s read again the file specifying some dtypes to the read_csv function\n\n\nCode\ndtypes = {\n    \"sex\": \"category\",\n    \"smoker\": \"category\",\n    \"day\": \"category\",\n    \"time\": \"category\"\n} \n\ndf = pd.read_csv(\"tips.csv\", dtype=dtypes)\n\n\n\n---------------------------------------------------------------------------\nFileNotFoundError                         Traceback (most recent call last)\nCell In[41], line 8\n      1 dtypes = {\n      2     \"sex\": \"category\",\n      3     \"smoker\": \"category\",\n      4     \"day\": \"category\",\n      5     \"time\": \"category\"\n      6 } \n----&gt; 8 df = pd.read_csv(\"tips.csv\", dtype=dtypes)\n\nFile ~/sandbox/IFEBY310/.venv/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1026, in read_csv(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\n   1013 kwds_defaults = _refine_defaults_read(\n   1014     dialect,\n   1015     delimiter,\n   (...)\n   1022     dtype_backend=dtype_backend,\n   1023 )\n   1024 kwds.update(kwds_defaults)\n-&gt; 1026 return _read(filepath_or_buffer, kwds)\n\nFile ~/sandbox/IFEBY310/.venv/lib/python3.12/site-packages/pandas/io/parsers/readers.py:620, in _read(filepath_or_buffer, kwds)\n    617 _validate_names(kwds.get(\"names\", None))\n    619 # Create the parser.\n--&gt; 620 parser = TextFileReader(filepath_or_buffer, **kwds)\n    622 if chunksize or iterator:\n    623     return parser\n\nFile ~/sandbox/IFEBY310/.venv/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1620, in TextFileReader.__init__(self, f, engine, **kwds)\n   1617     self.options[\"has_index_names\"] = kwds[\"has_index_names\"]\n   1619 self.handles: IOHandles | None = None\n-&gt; 1620 self._engine = self._make_engine(f, self.engine)\n\nFile ~/sandbox/IFEBY310/.venv/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1880, in TextFileReader._make_engine(self, f, engine)\n   1878     if \"b\" not in mode:\n   1879         mode += \"b\"\n-&gt; 1880 self.handles = get_handle(\n   1881     f,\n   1882     mode,\n   1883     encoding=self.options.get(\"encoding\", None),\n   1884     compression=self.options.get(\"compression\", None),\n   1885     memory_map=self.options.get(\"memory_map\", False),\n   1886     is_text=is_text,\n   1887     errors=self.options.get(\"encoding_errors\", \"strict\"),\n   1888     storage_options=self.options.get(\"storage_options\", None),\n   1889 )\n   1890 assert self.handles is not None\n   1891 f = self.handles.handle\n\nFile ~/sandbox/IFEBY310/.venv/lib/python3.12/site-packages/pandas/io/common.py:873, in get_handle(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\n    868 elif isinstance(handle, str):\n    869     # Check whether the filename is to be opened in binary mode.\n    870     # Binary mode does not support 'encoding' and 'newline'.\n    871     if ioargs.encoding and \"b\" not in ioargs.mode:\n    872         # Encoding\n--&gt; 873         handle = open(\n    874             handle,\n    875             ioargs.mode,\n    876             encoding=ioargs.encoding,\n    877             errors=errors,\n    878             newline=\"\",\n    879         )\n    880     else:\n    881         # Binary mode\n    882         handle = open(handle, ioargs.mode)\n\nFileNotFoundError: [Errno 2] No such file or directory: 'tips.csv'\n\n\n\n\n\nCode\ndf.dtypes\n\n\ntotal_bill     float64\ntip            float64\nsex           category\nsmoker        category\nday           category\ntime          category\nsize             int64\ndtype: object"
  },
  {
    "objectID": "core/notebooks/notebook03_pandas.html#computing-statistics",
    "href": "core/notebooks/notebook03_pandas.html#computing-statistics",
    "title": "Introduction to pandas",
    "section": "Computing statistics",
    "text": "Computing statistics\n\n\nCode\n# The describe method only shows statistics for the numerical columns by default\ndf.describe()\n\n\n\n\n\n\n\n\n\ntotal_bill\ntip\nsize\n\n\n\n\ncount\n244.000000\n244.000000\n244.000000\n\n\nmean\n19.785943\n2.998279\n2.569672\n\n\nstd\n8.902412\n1.383638\n0.951100\n\n\nmin\n3.070000\n1.000000\n1.000000\n\n\n25%\n13.347500\n2.000000\n2.000000\n\n\n50%\n17.795000\n2.900000\n2.000000\n\n\n75%\n24.127500\n3.562500\n3.000000\n\n\nmax\n50.810000\n10.000000\n6.000000\n\n\n\n\n\n\n\n\n\nCode\n# We use the include=\"all\" option to see everything\ndf.describe(include=\"all\")\n\n\n\n\n\n\n\n\n\ntotal_bill\ntip\nsex\nsmoker\nday\ntime\nsize\n\n\n\n\ncount\n244.000000\n244.000000\n244\n244\n244\n244\n244.000000\n\n\nunique\nNaN\nNaN\n2\n2\n4\n2\nNaN\n\n\ntop\nNaN\nNaN\nMale\nNo\nSat\nDinner\nNaN\n\n\nfreq\nNaN\nNaN\n157\n151\n87\n176\nNaN\n\n\nmean\n19.785943\n2.998279\nNaN\nNaN\nNaN\nNaN\n2.569672\n\n\nstd\n8.902412\n1.383638\nNaN\nNaN\nNaN\nNaN\n0.951100\n\n\nmin\n3.070000\n1.000000\nNaN\nNaN\nNaN\nNaN\n1.000000\n\n\n25%\n13.347500\n2.000000\nNaN\nNaN\nNaN\nNaN\n2.000000\n\n\n50%\n17.795000\n2.900000\nNaN\nNaN\nNaN\nNaN\n2.000000\n\n\n75%\n24.127500\n3.562500\nNaN\nNaN\nNaN\nNaN\n3.000000\n\n\nmax\n50.810000\n10.000000\nNaN\nNaN\nNaN\nNaN\n6.000000\n\n\n\n\n\n\n\n\n\nCode\n# Correlation between the numerical columns\ndf.corr(numeric_only = True)\n\n\n\n\n\n\n\n\n\ntotal_bill\ntip\nsize\n\n\n\n\ntotal_bill\n1.000000\n0.675734\n0.598315\n\n\ntip\n0.675734\n1.000000\n0.489299\n\n\nsize\n0.598315\n0.489299\n1.000000\n\n\n\n\n\n\n\n\n\nCode\n?df.corr"
  },
  {
    "objectID": "core/notebooks/notebook03_pandas.html#how-do-the-tip-depends-on-the-total-bill",
    "href": "core/notebooks/notebook03_pandas.html#how-do-the-tip-depends-on-the-total-bill",
    "title": "Introduction to pandas",
    "section": "How do the tip depends on the total bill ?",
    "text": "How do the tip depends on the total bill ?\n\n\nCode\nsns.jointplot(x=\"total_bill\", y=\"tip\", data=df)"
  },
  {
    "objectID": "core/notebooks/notebook03_pandas.html#when-do-customers-go-to-this-restaurant",
    "href": "core/notebooks/notebook03_pandas.html#when-do-customers-go-to-this-restaurant",
    "title": "Introduction to pandas",
    "section": "When do customers go to this restaurant ?",
    "text": "When do customers go to this restaurant ?\n\n\nCode\nsns.countplot(x='day', hue=\"time\", data=df)"
  },
  {
    "objectID": "core/notebooks/notebook03_pandas.html#when-do-customers-spend-the-most",
    "href": "core/notebooks/notebook03_pandas.html#when-do-customers-spend-the-most",
    "title": "Introduction to pandas",
    "section": "When do customers spend the most ?",
    "text": "When do customers spend the most ?\n\n\nCode\nplt.figure(figsize=(7, 5))\nsns.boxplot(x='day', y='total_bill', hue='time', data=df)\nplt.legend(loc=\"upper left\")\n\n\n\n\n\n\n\n\n\n\n\nCode\nplt.figure(figsize=(7, 5))\nsns.violinplot(x='day', y='total_bill', hue='time', split=True, data=df)\nplt.legend(loc=\"upper left\")"
  },
  {
    "objectID": "core/notebooks/notebook03_pandas.html#who-spends-the-most",
    "href": "core/notebooks/notebook03_pandas.html#who-spends-the-most",
    "title": "Introduction to pandas",
    "section": "Who spends the most ?",
    "text": "Who spends the most ?\n\n\nCode\nsns.boxplot(x='sex', y='total_bill', hue='smoker', data=df)"
  },
  {
    "objectID": "core/notebooks/notebook03_pandas.html#when-should-waiters-want-to-work",
    "href": "core/notebooks/notebook03_pandas.html#when-should-waiters-want-to-work",
    "title": "Introduction to pandas",
    "section": "When should waiters want to work ?",
    "text": "When should waiters want to work ?\n\n\nCode\nsns.boxplot(x='day', y='tip', hue='time', data=df)\n\n\n\n\n\n\n\n\n\n\n\nCode\nsns.violinplot(x='day', y='tip', hue='time', data=df)"
  },
  {
    "objectID": "core/notebooks/notebook03_pandas.html#computations-using-pandas-broadcasting",
    "href": "core/notebooks/notebook03_pandas.html#computations-using-pandas-broadcasting",
    "title": "Introduction to pandas",
    "section": "Computations using pandas : broadcasting",
    "text": "Computations using pandas : broadcasting\nLet’s add a column that contains the tip percentage\n\n\nCode\ndf[\"tip_percentage\"] = df[\"tip\"] / df[\"total_bill\"]\ndf.head()\n\n\n\n\n\n\n\n\n\ntotal_bill\ntip\nsex\nsmoker\nday\ntime\nsize\ntip_percentage\n\n\n\n\n0\n16.99\n1.01\nFemale\nNo\nSun\nDinner\n2\n0.059447\n\n\n1\n10.34\n1.66\nMale\nNo\nSun\nDinner\n3\n0.160542\n\n\n2\n21.01\n3.50\nMale\nNo\nSun\nDinner\n3\n0.166587\n\n\n3\n23.68\n3.31\nMale\nNo\nSun\nDinner\n2\n0.139780\n\n\n4\n24.59\n3.61\nFemale\nNo\nSun\nDinner\n4\n0.146808\n\n\n\n\n\n\n\nThe computation\n```{python}\ndf[\"tip\"] / df[\"total_bill\"]\n```\nuses a broadcast rule.\n\nWe can multiply, add, subtract, etc. together numpy arrays, Series or pandas dataframes when the computation makes sense in view of their respective shape\n\nThis principle is called broadcast or broadcasting.\n\n\n\n\n\n\nNote\n\n\n\nBroadcasting is a key feature of numpy ndarray, see\n\nNumpy User’s guide\nPandas book\n\n\n\n\n\nCode\ndf[\"tip\"].shape, df[\"total_bill\"].shape\n\n\n((244,), (244,))\n\n\nThe tip and total_billcolumns have the same shape, so broadcasting performs pairwise division.\nThis corresponds to the following “hand-crafted” approach with a for loop:\n\n\nCode\nfor i in range(df.shape[0]):\n    df.loc[i, \"tip_percentage\"] = df.loc[i, \"tip\"] / df.loc[i, \"total_bill\"]\n\n\nBut using such a loop is:\n\nlonger to write\nless readable\nprone to mistakes\nand slower :(\n\nNEVER use Python for-loops unless you need to !\n\n\nCode\n%%timeit -n 10\nfor i in range(df.shape[0]):\n    df.loc[i, \"tip_percentage\"] = df.loc[i, \"tip\"] / df.loc[i, \"total_bill\"]\n\n\n23.4 ms ± 68.3 μs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n\n\n\n\nCode\n%%timeit -n 10\ndf[\"tip_percentage\"] = df[\"tip\"] / df[\"total_bill\"]\n\n\n71.2 μs ± 13.3 μs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n\n\nThe for loop is \\(\\approx\\) 100 times slower ! (even worse on larger data)\n\nPitfall. Changing values in a DataFrame\nWhen you want to change a value in a DataFrame, never use\ndf[\"tip_percentage\"].loc[i] = 42\nbut use\ndf.loc[i, \"tip_percentage\"] = 42\n\n\n\n\n\n\nCaution\n\n\n\nUse a single loc or iloc statement. The first version might not work: it might modify a copy of the column and not the dataframe itself !\n\n\nAnother example of broadcasting is:\n\n\nCode\n(100 * df[[\"tip_percentage\"]]).head()\n\n\n\n\n\n\n\n\n\ntip_percentage\n\n\n\n\n0\n5.944673\n\n\n1\n16.054159\n\n\n2\n16.658734\n\n\n3\n13.978041\n\n\n4\n14.680765\n\n\n\n\n\n\n\nwhere we multiplied each entry of the tip_percentage column by 100.\n\n\n\n\n\n\nRemark\n\n\n\nNote the difference between\ndf[['tip_percentage']]\nwhich returns a DataFrame containing only the tip_percentage column and\ndf['tip_percentage']\nwhich returns a Series containing the data of the tip_percentage column"
  },
  {
    "objectID": "core/notebooks/notebook03_pandas.html#some-more-plots",
    "href": "core/notebooks/notebook03_pandas.html#some-more-plots",
    "title": "Introduction to pandas",
    "section": "Some more plots",
    "text": "Some more plots\n\nHow do the tip percentages relates to the total bill ?\n\n\nCode\nsns.jointplot(\n    x=\"total_bill\", \n    y=\"tip_percentage\", \n    data=df\n)\n\n\n\n\n\n\n\n\n\n\n\nWho tips best ?\n\n\nCode\nsns.boxplot(\n    x='sex', \n    y='tip_percentage', \n    hue='smoker', \n    data=df\n)\n\n\n\n\n\n\n\n\n\n\n\nWho tips best without the tip_percentage outliers ?\n\n\nCode\nsns.boxplot(\n    x='sex', \n    y='tip_percentage', \n    hue='smoker', \n    data=df.loc[df[\"tip_percentage\"] &lt;= 0.3]\n)\n\n\n\n\n\n\n\n\n\nObject identity\n\n\nCode\nid(df)\n\n\n131548824967296"
  },
  {
    "objectID": "core/notebooks/notebook03_pandas.html#the-all-mighty-groupby-and-aggregate",
    "href": "core/notebooks/notebook03_pandas.html#the-all-mighty-groupby-and-aggregate",
    "title": "Introduction to pandas",
    "section": "The all-mighty groupby and aggregate",
    "text": "The all-mighty groupby and aggregate\nMany computations can be formulated as a groupby followed by and aggregation.\n\nWhat is the mean tip and tip percentage each day ?\n\n\nCode\ndf.head()\n\n\n\n\n\n\n\n\n\ntotal_bill\ntip\nsex\nsmoker\nday\ntime\nsize\ntip_percentage\n\n\n\n\n0\n16.99\n1.01\nFemale\nNo\nSun\nDinner\n2\n0.059447\n\n\n1\n10.34\n1.66\nMale\nNo\nSun\nDinner\n3\n0.160542\n\n\n2\n21.01\n3.50\nMale\nNo\nSun\nDinner\n3\n0.166587\n\n\n3\n23.68\n3.31\nMale\nNo\nSun\nDinner\n2\n0.139780\n\n\n4\n24.59\n3.61\nFemale\nNo\nSun\nDinner\n4\n0.146808\n\n\n\n\n\n\n\n\n\nCode\ntry:\n\n    df.groupby(\"day\", observed=True).mean()\nexcept TypeError:\n    print('TypeError: category dtype does not support aggregation \"mean\"')\n\n\nTypeError: category dtype does not support aggregation \"mean\"\n\n\nBut we do not care about the size column here, so we can use instead\n\n\nCode\n(\n    df[[\"total_bill\", \"tip\", \"tip_percentage\", \"day\"]]\n        .groupby(\"day\")\n        .mean()\n)\n\n\n/tmp/ipykernel_74090/1740663163.py:3: FutureWarning:\n\nThe default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n\n\n\n\n\n\n\n\n\n\ntotal_bill\ntip\ntip_percentage\n\n\nday\n\n\n\n\n\n\n\nThur\n17.682742\n2.771452\n0.161276\n\n\nFri\n17.151579\n2.734737\n0.169913\n\n\nSat\n20.441379\n2.993103\n0.153152\n\n\nSun\n21.410000\n3.255132\n0.166897\n\n\n\n\n\n\n\nIf we want to be more precise, we can groupby using several columns\n\n\nCode\n(\n    df[[\"total_bill\", \"tip\", \"tip_percentage\", \"day\", \"time\"]]   # selection\n        .groupby([\"day\",\"time\"])                                # partition\n        .mean()                                                  # aggregation\n)\n\n\n/tmp/ipykernel_74090/391063870.py:3: FutureWarning:\n\nThe default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n\n\n\n\n\n\n\n\n\n\n\ntotal_bill\ntip\ntip_percentage\n\n\nday\ntime\n\n\n\n\n\n\n\nThur\nLunch\n17.664754\n2.767705\n0.161301\n\n\nDinner\n18.780000\n3.000000\n0.159744\n\n\nFri\nLunch\n12.845714\n2.382857\n0.188765\n\n\nDinner\n19.663333\n2.940000\n0.158916\n\n\nSat\nLunch\nNaN\nNaN\nNaN\n\n\nDinner\n20.441379\n2.993103\n0.153152\n\n\nSun\nLunch\nNaN\nNaN\nNaN\n\n\nDinner\n21.410000\n3.255132\n0.166897\n\n\n\n\n\n\n\n\n\n\n\n\n\nRemarks\n\n\n\n\nWe obtain a DataFrame with a two-level indexing: on the day and the time\nGroups must be homogeneous: we have NaN values for empty groups (e.g. Sat, Lunch)\n\n\n\n\n\nPro tip\nSometimes, it is more convenient to get the groups as columns instead of a multi-level index.\nFor this, use reset_index:\n\n\nCode\n(\n    df[[\"total_bill\", \"tip\", \"tip_percentage\", \"day\", \"time\"]]   # selection\n        .groupby([\"day\", \"time\"])                                # partition\n        .mean() # aggregation\n        .reset_index()   # ako ungroup\n)\n\n\n/tmp/ipykernel_74090/835267922.py:3: FutureWarning:\n\nThe default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n\n\n\n\n\n\n\n\n\n\nday\ntime\ntotal_bill\ntip\ntip_percentage\n\n\n\n\n0\nThur\nLunch\n17.664754\n2.767705\n0.161301\n\n\n1\nThur\nDinner\n18.780000\n3.000000\n0.159744\n\n\n2\nFri\nLunch\n12.845714\n2.382857\n0.188765\n\n\n3\nFri\nDinner\n19.663333\n2.940000\n0.158916\n\n\n4\nSat\nLunch\nNaN\nNaN\nNaN\n\n\n5\nSat\nDinner\n20.441379\n2.993103\n0.153152\n\n\n6\nSun\nLunch\nNaN\nNaN\nNaN\n\n\n7\nSun\nDinner\n21.410000\n3.255132\n0.166897\n\n\n\n\n\n\n\n\n\nAnother pro tip: care about code readers\nComputations with pandas can include many operations that are pipelined until the final computation.\nPipelining many operations is good practice and perfectly normal, but in order to make the code readable you can put it between parenthesis (python expression) as follows:\n\n\nCode\n(\n    df[[\"total_bill\", \"tip\", \"tip_percentage\", \"day\", \"time\"]]\n    .groupby([\"day\", \"time\"])\n    .mean()\n    .reset_index()\n    # and on top of all this we sort the dataframe with respect \n    # to the tip_percentage\n    .sort_values(\"tip_percentage\")\n)\n\n\n/tmp/ipykernel_74090/45053252.py:3: FutureWarning:\n\nThe default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n\n\n\n\n\n\n\n\n\n\nday\ntime\ntotal_bill\ntip\ntip_percentage\n\n\n\n\n5\nSat\nDinner\n20.441379\n2.993103\n0.153152\n\n\n3\nFri\nDinner\n19.663333\n2.940000\n0.158916\n\n\n1\nThur\nDinner\n18.780000\n3.000000\n0.159744\n\n\n0\nThur\nLunch\n17.664754\n2.767705\n0.161301\n\n\n7\nSun\nDinner\n21.410000\n3.255132\n0.166897\n\n\n2\nFri\nLunch\n12.845714\n2.382857\n0.188765\n\n\n4\nSat\nLunch\nNaN\nNaN\nNaN\n\n\n6\nSun\nLunch\nNaN\nNaN\nNaN"
  },
  {
    "objectID": "core/notebooks/notebook03_pandas.html#displaying-a-dataframe-with-style",
    "href": "core/notebooks/notebook03_pandas.html#displaying-a-dataframe-with-style",
    "title": "Introduction to pandas",
    "section": "Displaying a DataFrame with style",
    "text": "Displaying a DataFrame with style\nNow, we can answer, with style, to the question: what are the average tip percentages along the week ?\n\n\nCode\n(\n    df[[\"tip_percentage\", \"day\", \"time\"]]\n    .groupby([\"day\", \"time\"])\n    .mean()\n    # At the end of the pipeline you can use .style\n    .style\n    # Print numerical values as percentages \n    .format(\"{:.2%}\")\n    .background_gradient()\n)\n\n\n/tmp/ipykernel_74090/838795167.py:3: FutureWarning:\n\nThe default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n\n\n\n\n\n\n\n\n \n \ntip_percentage\n\n\nday\ntime\n \n\n\n\n\nThur\nLunch\n16.13%\n\n\nDinner\n15.97%\n\n\nFri\nLunch\n18.88%\n\n\nDinner\n15.89%\n\n\nSat\nLunch\nnan%\n\n\nDinner\n15.32%\n\n\nSun\nLunch\nnan%\n\n\nDinner\n16.69%"
  },
  {
    "objectID": "core/notebooks/notebook03_pandas.html#removing-the-nan-values",
    "href": "core/notebooks/notebook03_pandas.html#removing-the-nan-values",
    "title": "Introduction to pandas",
    "section": "Removing the NaN values",
    "text": "Removing the NaN values\nBut the NaN values are somewhat annoying. Let’s remove them\n\n\nCode\n(\n    df[[\"tip_percentage\", \"day\", \"time\"]]\n    .groupby([\"day\", \"time\"])\n    .mean()\n    # We just add this from the previous pipeline\n    .dropna()\n    .style\n    .format(\"{:.2%}\")\n    .background_gradient()\n)\n\n\n/tmp/ipykernel_74090/2662169510.py:3: FutureWarning:\n\nThe default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n\n\n\n\n\n\n\n\n \n \ntip_percentage\n\n\nday\ntime\n \n\n\n\n\nThur\nLunch\n16.13%\n\n\nDinner\n15.97%\n\n\nFri\nLunch\n18.88%\n\n\nDinner\n15.89%\n\n\nSat\nDinner\n15.32%\n\n\nSun\nDinner\n16.69%\n\n\n\n\n\nNow, we see when tip_percentage is maximal. But what about the standard deviation?\n\nWe used only .mean() for now, but we can use several aggregating function using .agg()\n\n\n\nCode\n(\n    df[[\"tip_percentage\", \"day\", \"time\"]]\n    .groupby([\"day\", \"time\"])\n    .agg([\"mean\", \"std\"])   # we feed `agg`  with a list of names of callables \n    .dropna()\n    .style\n    .format(\"{:.2%}\")\n    .background_gradient()\n)\n\n\n/tmp/ipykernel_74090/3957220442.py:3: FutureWarning:\n\nThe default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n\n\n\n\n\n\n\n\n \n \ntip_percentage\n\n\n \n \nmean\nstd\n\n\nday\ntime\n \n \n\n\n\n\nThur\nLunch\n16.13%\n3.90%\n\n\nFri\nLunch\n18.88%\n4.59%\n\n\nDinner\n15.89%\n4.70%\n\n\nSat\nDinner\n15.32%\n5.13%\n\n\nSun\nDinner\n16.69%\n8.47%\n\n\n\n\n\nAnd we can use also .describe() as aggregation function. Moreover we - use the subset option to specify which column we want to style - we use (\"tip_percentage\", \"count\") to access multi-level index\n\n\nCode\n(\n    df[[\"tip_percentage\", \"day\", \"time\"]]\n    .groupby([\"day\", \"time\"])\n    .describe()    # all-purpose summarising function\n)\n\n\n/tmp/ipykernel_74090/3924876303.py:3: FutureWarning:\n\nThe default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n\n\n\n\n\n\n\n\n\n\n\ntip_percentage\n\n\n\n\ncount\nmean\nstd\nmin\n25%\n50%\n75%\nmax\n\n\nday\ntime\n\n\n\n\n\n\n\n\n\n\n\n\nThur\nLunch\n61.0\n0.161301\n0.038972\n0.072961\n0.137741\n0.153846\n0.193424\n0.266312\n\n\nDinner\n1.0\n0.159744\nNaN\n0.159744\n0.159744\n0.159744\n0.159744\n0.159744\n\n\nFri\nLunch\n7.0\n0.188765\n0.045885\n0.117735\n0.167289\n0.187735\n0.210996\n0.259314\n\n\nDinner\n12.0\n0.158916\n0.047024\n0.103555\n0.123613\n0.144742\n0.179199\n0.263480\n\n\nSat\nDinner\n87.0\n0.153152\n0.051293\n0.035638\n0.123863\n0.151832\n0.188271\n0.325733\n\n\nSun\nDinner\n76.0\n0.166897\n0.084739\n0.059447\n0.119982\n0.161103\n0.187889\n0.710345\n\n\n\n\n\n\n\n\n\nCode\n(\n    df[[\"tip_percentage\", \"day\", \"time\"]]\n    .groupby([\"day\", \"time\"])\n    .describe()\n    .dropna()\n    .style\n    .bar(subset=[(\"tip_percentage\", \"count\")])\n    .background_gradient(subset=[(\"tip_percentage\", \"50%\")])\n)\n\n\n/tmp/ipykernel_74090/673231177.py:3: FutureWarning:\n\nThe default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n\n\n\n\n\n\n\n\n \n \ntip_percentage\n\n\n \n \ncount\nmean\nstd\nmin\n25%\n50%\n75%\nmax\n\n\nday\ntime\n \n \n \n \n \n \n \n \n\n\n\n\nThur\nLunch\n61.000000\n0.161301\n0.038972\n0.072961\n0.137741\n0.153846\n0.193424\n0.266312\n\n\nFri\nLunch\n7.000000\n0.188765\n0.045885\n0.117735\n0.167289\n0.187735\n0.210996\n0.259314\n\n\nDinner\n12.000000\n0.158916\n0.047024\n0.103555\n0.123613\n0.144742\n0.179199\n0.263480\n\n\nSat\nDinner\n87.000000\n0.153152\n0.051293\n0.035638\n0.123863\n0.151832\n0.188271\n0.325733\n\n\nSun\nDinner\n76.000000\n0.166897\n0.084739\n0.059447\n0.119982\n0.161103\n0.187889\n0.710345"
  },
  {
    "objectID": "core/notebooks/notebook03_pandas.html#supervised-learning-of-tip-based-on-the-total_bill",
    "href": "core/notebooks/notebook03_pandas.html#supervised-learning-of-tip-based-on-the-total_bill",
    "title": "Introduction to pandas",
    "section": "Supervised learning of tip based on the total_bill",
    "text": "Supervised learning of tip based on the total_bill\nAs an example of very simple machine-learning problem, let us try to understand how we can predict tip based on total_bill.\n\n\nCode\nimport numpy as np\n\nplt.scatter(df[\"total_bill\"], df[\"tip\"])\nplt.xlabel(\"total_bill\", fontsize=12)\nplt.ylabel(\"tip\", fontsize=12)\n\n\nText(0, 0.5, 'tip')\n\n\n\n\n\n\n\n\n\nThere’s a rough linear dependence between the two. Let us try to find it by hand! Namely, we look for numbers \\(b\\) and \\(w\\) such that\ntip ≈ b + w × total_bill\nfor all the examples of pairs of (tip, total_bill) we observe in the data.\nIn machine learning, we say that this is a very simple example of a supervised learning problem (here it is a regression problem), where tip is the label and where total_bill is the (only) feature, for which we intend to use a linear predictor.\n\n\nCode\nplt.scatter(df[\"total_bill\"], df[\"tip\"])\nplt.xlabel(\"total_bill\", fontsize=12)\nplt.ylabel(\"tip\", fontsize=12)\n\nslope = 1.0\nintercept = 0.0\n\nx = np.linspace(0, 50, 1000)\nplt.plot(x, intercept + slope * x, color=\"red\")\n\n\n\n\n\n\n\n\n\n\nA more interactive way\nThis might require\n\n\nCode\n# !pip install ipympl\n\n\n\n\nCode\nimport ipywidgets as widgets\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n%matplotlib widget\n%matplotlib inline\n\nx = np.linspace(0, 50, 1000)\n\n@widgets.interact(intercept=(-5, 5, 1.), slope=(0, 1, .05))\ndef update(intercept=0.0, slope=0.5):\n    plt.scatter(df[\"total_bill\"], df[\"tip\"])\n    plt.plot(x, intercept + slope * x, color=\"red\")\n    plt.xlim((0, 50))\n    plt.ylim((0, 10))\n    plt.xlabel(\"total_bill\", fontsize=12)\n    plt.ylabel(\"tip\", fontsize=12)\n\n\n\n\n\n\n\n\n\n\n\n\nThis is kind of tedious to do this by hand… it would be nice to come up with an automated way of doing this. Moreover:\n\nWe are using a linear function, while something more complicated (such as a polynomial) might be better\nMore importantly, we use only the total_bill column to predict the tip, while we know about many other things\n\n\n\nCode\ndf.head()\n\n\n\n\n\n\n\n\n\ntotal_bill\ntip\nsex\nsmoker\nday\ntime\nsize\ntip_percentage\n\n\n\n\n0\n16.99\n1.01\nFemale\nNo\nSun\nDinner\n2\n0.059447\n\n\n1\n10.34\n1.66\nMale\nNo\nSun\nDinner\n3\n0.160542\n\n\n2\n21.01\n3.50\nMale\nNo\nSun\nDinner\n3\n0.166587\n\n\n3\n23.68\n3.31\nMale\nNo\nSun\nDinner\n2\n0.139780\n\n\n4\n24.59\n3.61\nFemale\nNo\nSun\nDinner\n4\n0.146808"
  },
  {
    "objectID": "core/notebooks/notebook03_pandas.html#one-hot-encoding-of-categorical-variables",
    "href": "core/notebooks/notebook03_pandas.html#one-hot-encoding-of-categorical-variables",
    "title": "Introduction to pandas",
    "section": "One-hot encoding of categorical variables",
    "text": "One-hot encoding of categorical variables\nWe can’t perform computations (products and sums) with columns containing categorical variables. So, we can’t use them like this to predict the tip. We need to convert them to numbers somehow.\nThe most classical approach for this is one-hot encoding (or “create dummies” or “binarize”) of the categorical variables, which can be easily achieved with pandas.get_dummies\nWhy one-hot ? See wikipedia for a plausible explanation\n\n\nCode\ndf_one_hot = pd.get_dummies(df, prefix_sep='#')\ndf_one_hot.head(5)\n\n\n\n\n\n\n\n\n\ntotal_bill\ntip\nsize\ntip_percentage\nsex#Male\nsex#Female\nsmoker#Yes\nsmoker#No\nday#Thur\nday#Fri\nday#Sat\nday#Sun\ntime#Lunch\ntime#Dinner\n\n\n\n\n0\n16.99\n1.01\n2\n0.059447\nFalse\nTrue\nFalse\nTrue\nFalse\nFalse\nFalse\nTrue\nFalse\nTrue\n\n\n1\n10.34\n1.66\n3\n0.160542\nTrue\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nTrue\nFalse\nTrue\n\n\n2\n21.01\n3.50\n3\n0.166587\nTrue\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nTrue\nFalse\nTrue\n\n\n3\n23.68\n3.31\n2\n0.139780\nTrue\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nTrue\nFalse\nTrue\n\n\n4\n24.59\n3.61\n4\n0.146808\nFalse\nTrue\nFalse\nTrue\nFalse\nFalse\nFalse\nTrue\nFalse\nTrue\n\n\n\n\n\n\n\nOnly the categorical columns have been one-hot encoded. For instance, the \"day\" column is replaced by 4 columns named \"day#Thur\", \"day#Fri\", \"day#Sat\", \"day#Sun\", since \"day\" has 4 modalities (see next line).\n\n\nCode\ndf['day'].unique()\n\n\n['Sun', 'Sat', 'Thur', 'Fri']\nCategories (4, object): ['Thur', 'Fri', 'Sat', 'Sun']\n\n\n\n\nCode\ndf_one_hot.dtypes\n\n\ntotal_bill        float64\ntip               float64\nsize                int64\ntip_percentage    float64\nsex#Male             bool\nsex#Female           bool\nsmoker#Yes           bool\nsmoker#No            bool\nday#Thur             bool\nday#Fri              bool\nday#Sat              bool\nday#Sun              bool\ntime#Lunch           bool\ntime#Dinner          bool\ndtype: object"
  },
  {
    "objectID": "core/notebooks/notebook03_pandas.html#pitfall.-colinearities-with-one-hot-encoding",
    "href": "core/notebooks/notebook03_pandas.html#pitfall.-colinearities-with-one-hot-encoding",
    "title": "Introduction to pandas",
    "section": "Pitfall. Colinearities with one-hot encoding",
    "text": "Pitfall. Colinearities with one-hot encoding\nSums over dummies for sex, smoker, day, time and size are all equal to one (by constrution of the one-hot encoded vectors).\n\nLeads to colinearities in the matrix of features\nIt is much harder to train a linear regressor when the columns of the features matrix has colinearities\n\n\n\nCode\nday_cols = [col for col in df_one_hot.columns if col.startswith(\"day\")]\ndf_one_hot[day_cols].head()\ndf_one_hot[day_cols].sum(axis=1)\n\n\n0      1\n1      1\n2      1\n3      1\n4      1\n      ..\n239    1\n240    1\n241    1\n242    1\n243    1\nLength: 244, dtype: int64\n\n\n\n\nCode\nall(df_one_hot[day_cols].sum(axis=1) == 1)\n\n\nTrue\n\n\nThe most standard solution is to remove a modality (i.e. remove a one-hot encoding vector). Simply achieved by specifying drop_first=True in the get_dummies function.\n\n\nCode\ndf[\"day\"].unique()\n\n\n['Sun', 'Sat', 'Thur', 'Fri']\nCategories (4, object): ['Thur', 'Fri', 'Sat', 'Sun']\n\n\n\n\nCode\npd.get_dummies(df, prefix_sep='#', drop_first=True).head()\n\n\n\n\n\n\n\n\n\ntotal_bill\ntip\nsize\ntip_percentage\nsex#Female\nsmoker#No\nday#Fri\nday#Sat\nday#Sun\ntime#Dinner\n\n\n\n\n0\n16.99\n1.01\n2\n0.059447\nTrue\nTrue\nFalse\nFalse\nTrue\nTrue\n\n\n1\n10.34\n1.66\n3\n0.160542\nFalse\nTrue\nFalse\nFalse\nTrue\nTrue\n\n\n2\n21.01\n3.50\n3\n0.166587\nFalse\nTrue\nFalse\nFalse\nTrue\nTrue\n\n\n3\n23.68\n3.31\n2\n0.139780\nFalse\nTrue\nFalse\nFalse\nTrue\nTrue\n\n\n4\n24.59\n3.61\n4\n0.146808\nTrue\nTrue\nFalse\nFalse\nTrue\nTrue\n\n\n\n\n\n\n\nNow, if a categorical feature has \\(K\\) modalities, we use only \\(K-1\\) dummies. For instance, there is no more sex#Female binary column.\nQuestion. So, a linear regression won’t fit a weight for sex#Female. But, where do the model weights of the dropped binary columns go ?\nAnswer. They just “go” to the intercept: interpretation of the population bias depends on the “dropped” one-hot encodings.\nSo, we actually fit: \\[\\begin{array}{rl} \\texttt{tip} \\approx b & + w_1 \\times \\texttt{total_bill} + w_2 \\times \\texttt{size} \\\\ & + w_3 \\times \\texttt{sex#Male} + w_4 \\times \\texttt{smoker#Yes} \\\\ & + w_5 \\times \\texttt{day#Sat} + w_6 \\times \\texttt{day#Sun} + w_7 \\times \\texttt{day#Thur} \\\\ & + w_8 \\times \\texttt{time#Lunch} \\end{array}\\]"
  },
  {
    "objectID": "core/notebooks/notebook05_sparkrdd.html",
    "href": "core/notebooks/notebook05_sparkrdd.html",
    "title": "Introduction to Spark RDD",
    "section": "",
    "text": "Code\nimport numpy as np\nCode\nimport os\nimport sys\nimport inspect\n\nos.environ['PYSPARK_PYTHON'] = sys.executable\nos.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable\nCode\nfrom pyspark import SparkConf, SparkContext\n\nconf = SparkConf().setAppName(\"Spark RDD Course\")\nsc = SparkContext(conf=conf)\n\n\n25/04/03 15:08:23 WARN Utils: Your hostname, boucheron-Precision-5480 resolves to a loopback address: 127.0.1.1; using 172.23.32.10 instead (on interface eth0)\n25/04/03 15:08:23 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\nSetting default log level to \"WARN\".\nTo adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n25/04/03 15:08:24 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\nCode\nrdd = sc.parallelize(range(64))\nNote that parallelize takes an optional argument to choose the number of partitions\nCode\nrdd.getNumPartitions()\n\n\n20\nCode\nrdd = sc.parallelize(range(1000), 10)\nrdd.getNumPartitions()\n\n\n10"
  },
  {
    "objectID": "core/notebooks/notebook05_sparkrdd.html#transformations",
    "href": "core/notebooks/notebook05_sparkrdd.html#transformations",
    "title": "Introduction to Spark RDD",
    "section": "Transformations",
    "text": "Transformations\n\nmap\n\n\nCode\nrdd = sc.parallelize([2, 3, 4])\nrdd = rdd.map(lambda x: list(range(1, x)))\n\n\n\n\nCode\nrdd\n\n\nPythonRDD[3] at RDD at PythonRDD.scala:53\n\n\n\n\nCode\n(\n    sc.parallelize([2, 3, 4])\n      .map(lambda x: list(range(1, x)))\n)\n\n\nPythonRDD[5] at RDD at PythonRDD.scala:53\n\n\nmap is a transformation. It is lazily evaluated. Hence execution is delayed until an action is met in the DAG).\n\n\nCode\nrdd.collect()  # collect is an action \n\n\n[Stage 0:&gt;                                                        (0 + 20) / 20]                                                                                \n\n\n[[1], [1, 2], [1, 2, 3]]\n\n\n\n\nCode\n(\n    sc.parallelize([2, 3, 4])\n      .map(lambda x: list(range(1, x)))\n      .collect()\n)\n\n\n[[1], [1, 2], [1, 2, 3]]\n\n\n\n\nExercice: map with a method\nWarning. This example is a bad practice !!! Don’t do this at home\n\n\nCode\ndbtel = {'arthur': 1234, 'riad': 4567, 'anatole': 3615}\n\n\n\n\nCode\nclass TelephoneDB(object):\n    \n    def __init__(self):\n        self.tel = {'arthur': 1234, 'riad': 4567, 'anatole': 3615}\n   \n    def add_tel(self, name):\n        return name, self.tel.get(name)\n\n\n\n\nCode\ntel_db = TelephoneDB()\nnames = ['arthur', 'riad']\n\n\n\n\nCode\nrdd = (\n    sc\n        .parallelize(names)\n        .map(tel_db.add_tel)\n        .collect()\n)\n\nrdd\n\n\n[('arthur', 1234), ('riad', 4567)]\n\n\n\nReplace the tel dictionary by a defaultdict with default number 999\nUse it on a rdd containing names as above including an unknown one, and try it\n\n\n\nCode\nfrom collections import defaultdict\n\nclass TelephoneDefaultDB(object):\n    \n    def __init__(self):\n        self.tel = defaultdict(lambda: 999, {'arthur': 1234, 'riad': 4567, 'anatole': 3615})\n    \n    def add_tel(self, name):\n        return name, self.tel[name]\n    \n    def add_tel_rdd(self, rdd):  \n        return rdd.map(self.add_tel)\n\n\n\n\nCode\ntel_db = TelephoneDefaultDB()\nnames = ['riad', 'anatole', 'yiyang']\nrdd = (\n    sc\n        .parallelize(names)\n        .map(tel_db.add_tel)\n        .collect()\n)\nrdd\n\n\n[('riad', 4567), ('anatole', 3615), ('yiyang', 999)]\n\n\n\n\n\n\n\n\nCaution\n\n\n\nIt is a bad idea to pass methods to spark’s map. Since add_tel needs self, the whole object is serialized so that spark can use it.\nThis breaks if the tel is large, or if it is not serializable.\n\n\n\n\nflatMap\n\n\nCode\nrdd = sc.parallelize([2, 3, 4, 5])\n( \n    rdd\n        .flatMap(lambda x: range(1, x))\n        .collect()\n)\n\n\n[1, 1, 2, 1, 2, 3, 1, 2, 3, 4]\n\n\n\n\nfilter\n\n\nCode\nrdd = sc.parallelize(range(10))\n\nrdd\\\n    .filter(lambda x: x % 2 == 0)\\\n    .collect()\n\n\n[0, 2, 4, 6, 8]\n\n\n\n\ndistinct\n\n\nCode\nrdd = sc.parallelize([1, 1, 4, 2, 1, 3, 3])\nrdd.distinct().collect()\n\n\n[1, 2, 3, 4]\n\n\n\n\n“Pseudo-set” operations\n\n\nCode\nrdd1 = sc.parallelize(range(5))\nrdd2 = sc.parallelize(range(3, 9))\nrdd3 = rdd1.union(rdd2)\nrdd3.collect()\n\n\n[0, 1, 2, 3, 4, 3, 4, 5, 6, 7, 8]\n\n\n\n\nCode\nrdd3.distinct().collect()\n\n\n[0, 1, 2, 3, 4, 5, 6, 7, 8]\n\n\n\n\nCode\nrdd1 = sc.parallelize([1, 2])\nrdd2 = sc.parallelize([\"a\", \"b\"])\nrdd1.cartesian(rdd2).collect()\n\n\n[(1, 'a'), (1, 'b'), (2, 'a'), (2, 'b')]"
  },
  {
    "objectID": "core/notebooks/notebook05_sparkrdd.html#actions",
    "href": "core/notebooks/notebook05_sparkrdd.html#actions",
    "title": "Introduction to Spark RDD",
    "section": "Actions",
    "text": "Actions\ncollect is obviously an action…\n\ncount, countByValue\n\n\nCode\nrdd = sc.parallelize([1, 3, 1, 2, 2, 2])\nrdd.count()\n\n\n6\n\n\n\n\nCode\nrdd.countByValue()\n\n\ndefaultdict(int, {1: 2, 3: 1, 2: 3})\n\n\nWhy does countByValue() returns a dictionary?\nAre count() and countByValue() actions or transformations?\n\n\nCode\nu = np.int32((np.random.sample(100000) * 100000))  # 100000 random integers uniformly distributed on 0, ..., 100000\n\np = (\n    sc.parallelize(u)\n    .countByValue()\n)\n\nq = sorted(\n    p.items(), \n    key = lambda x : x[1], \n    reverse=True\n)\n\nq[0:10]\n\nq[0], 1 + np.log(len(u))/ np.log(np.log(len(u))), len(q)\n\n\n((np.int32(87583), 8), np.float64(5.711710714547694), 63259)\n\n\n\nHow many distinct values do you expect in u ?\nHow large is the largest value in \\(q\\) ?\n\n\n\nCode\nfrom scipy.stats import poisson \n\n( \n    len(q), \n    (1-np.exp(-1)) * len(u),\n    poisson.ppf(1.-1./len(u), 1)\n)\n\n\n(63259, np.float64(63212.05588285577), np.float64(8.0))\n\n\n\n\ntake, takeOrdered\n\n\nCode\nrdd = sc.parallelize([(3, 'a'), (1, 'b'), (2, 'd')])\n\n\n\n\nCode\n(1, 'b') &lt;=  (2, 'd') &lt;= (3, 'a')\n\n\nTrue\n\n\n\n\nCode\nrdd.takeOrdered(2)\n\n\n[(1, 'b'), (2, 'd')]\n\n\n\n\nCode\nrdd.takeOrdered(2, key=lambda x: x[1])\n\n\n[(3, 'a'), (1, 'b')]\n\n\n\n\nreduce, fold\n\n\nCode\nrdd = sc.range(1, 4)\nrdd.reduce(lambda a, b: a + b)\n\n\n6\n\n\n\n\nCode\nrdd = sc.range(1, 4, numSlices=7)\nrdd.reduce(lambda a, b: a + b)\n\n\n6\n\n\n\n\nCode\nrdd = sc.parallelize(range(1,4), 3)\nrdd.reduce(lambda a, b: a + b)\n\n\n6\n\n\n\n\nCode\n( \n    sc.parallelize(range(1, 4), 2)\n      .fold(0, lambda a, b: a + b)\n)\n\n\n6\n\n\n\n\nCode\n( \n    sc.parallelize(range(1, 4), 1)\n      .fold(3, lambda a, b: a + b)\n),( \n    sc.parallelize(range(1, 4), 2)\n      .fold(2, lambda a, b: a + b)\n)\n\n\n(12, 12)\n\n\n\n\nCode\nrdd =  sc.parallelize(range(1, 4),3)\n( \n    rdd.fold(1, lambda a, b: a + b), \n    rdd.getNumPartitions()\n)\n\n\n(10, 3)\n\n\n\n\nCode\nrdd =  sc.parallelize(range(1, 4),4)\n\n(\n    rdd.fold(1, lambda a, b: a + b), \n    rdd.getNumPartitions()\n)\n\n\n(11, 4)\n\n\n\n\nCode\nrdd = sc.parallelize([1, 2, 4], 2)\nrdd.fold(2, lambda a, b: a + b)\n\n\n13\n\n\n\n\nCode\nrdd = sc.parallelize([1, 2, 4], 3)\nrdd.fold(2, lambda a, b: a + b)\n\n\n15\n\n\n\n\nCode\nrdd.getNumPartitions()\n\n\n3\n\n\n\n\naggregate\n\n\nCode\nseqOp = lambda x, y: (x[0] + y, x[1] + 1)\ncombOp = lambda x, y: (x[0] + y[0], x[1] + y[1])\n\nrdd = sc.parallelize([1, 2, 3, 4], 8)\n(\n    rdd.aggregate((0, 0), seqOp, combOp), rdd.getNumPartitions()\n)\n\n\n((10, 4), 8)\n\n\n\n\nCode\nop = lambda x, y: x+y\nrdd = sc.parallelize([1, 2, 3, 4], 4)\n(\n    rdd.aggregate(0, op, op),\n    rdd.getNumPartitions()\n)\n\n\n(10, 4)\n\n\n\n\nExercice: sum of powers with aggregate\n\nUsing aggregate, compute the sum, the sum of squares \\(x^2\\) and the sum of cubes \\(x^3\\) for \\(x \\in \\{1, \\ldots, 10 \\}\\).\nCheck your computations using numpy\n\n\n\nCode\nseqOp = lambda x, y: (x[0] + y, x[1] + y ** 2, x[2] + y ** 3)\n\n\n\n\nCode\ncombOp = lambda x, y: (x[0] + y[0], x[1] + y[1], x[2] + y[2])\n\n\n\n\nCode\nsc.range(5)\n\n\nPythonRDD[68] at RDD at PythonRDD.scala:53\n\n\n\n\nCode\n( \n    sc\n        .range(1, 11)\n        .aggregate((0, 0, 0), seqOp, combOp)\n)\n\n\n(55, 385, 3025)\n\n\n\n\nCode\nimport numpy as np\n\nx = np.arange(1, 11)\nx\n\n\narray([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10])\n\n\n\n\nCode\nx.sum(), (x**2).sum(), (x**3).sum(), x.cumsum()\n\n\n(np.int64(55),\n np.int64(385),\n np.int64(3025),\n array([ 1,  3,  6, 10, 15, 21, 28, 36, 45, 55]))\n\n\n\n\nComputing an empirical variance with aggregate\nAssume a sample is stored as a RDD. Using aggregate, compute the sample variance \\(\\frac{1}{n}\\sum_{i=1}^n (x_i - \\overline{X}_n)^2\\) where \\(\\overline{X}_n = \\frac{1}{n} \\sum_{i=1}^n x_i\\)"
  },
  {
    "objectID": "core/notebooks/notebook05_sparkrdd.html#transformations-1",
    "href": "core/notebooks/notebook05_sparkrdd.html#transformations-1",
    "title": "Introduction to Spark RDD",
    "section": "Transformations",
    "text": "Transformations\n\nkeys, values\n\n\nCode\nrdd.keys().collect()\n\n\n[1, 2, 2]\n\n\n\n\nCode\nrdd.values().collect()\n\n\n[['a', 7], ['b', 13], ['c', 17]]\n\n\n\n\n\n\n\n\nWarning\n\n\n\nAll elements must be tuples with two elements (key and value)\n\n\n\n\nCode\nrdd = sc.parallelize([[1, \"a\", 7], [2, \"b\", 13], [2, \"c\", 17]])\nrdd.keys().collect()\n\n\n[1, 2, 2]\n\n\n\n\nCode\nrdd.values().collect()\n\n\n['a', 'b', 'c']\n\n\nThe values are not what we expected wrong… so we must do\n\n\nCode\nrdd = ( sc.parallelize([[1, \"a\", 7], [2, \"b\", 13], [2, \"c\", 17]])\n          .map(lambda x: (x[0], x[1:]))\n      )\nrdd.keys().collect()\n\n\n[1, 2, 2]\n\n\n\n\nCode\nrdd.values().collect()\n\n\n[['a', 7], ['b', 13], ['c', 17]]\n\n\nNow, the values are correct.\n\n\nmapValues, flatMapValues\n\n\nCode\nrdd = sc.parallelize([(\"a\", \"x y z\"), (\"b\", \"p r\")])\n\nrdd.mapValues(lambda v: v.split(' ')).collect(), rdd.collect()\n\n\n([('a', ['x', 'y', 'z']), ('b', ['p', 'r'])], [('a', 'x y z'), ('b', 'p r')])\n\n\n\n\nCode\nrdd.flatMapValues(lambda v: v.split(' ')).collect()\n\n\n[('a', 'x'), ('a', 'y'), ('a', 'z'), ('b', 'p'), ('b', 'r')]\n\n\n\n\ngroupByKey\n\n\nCode\nrdd = sc.parallelize([(\"a\", 1), (\"b\", 1), (\"a\", 1), (\"b\", 3), (\"c\", 42)])\n( \n    rdd.groupByKey()\n       .mapValues(list)\n       .collect()\n)\n\n\n[('b', [1, 3]), ('c', [42]), ('a', [1, 1])]\n\n\n\n\nCode\nrdd.groupByKey().collect()\n\n\n[('b', &lt;pyspark.resultiterable.ResultIterable at 0x7058255e9520&gt;),\n ('c', &lt;pyspark.resultiterable.ResultIterable at 0x7058256348f0&gt;),\n ('a', &lt;pyspark.resultiterable.ResultIterable at 0x7058256349b0&gt;)]\n\n\n\n\nreduceByKey\n\n\nCode\nrdd = sc.parallelize([(\"a\", 1), (\"b\", 1), (\"a\", 1)])\nrdd.reduceByKey(lambda a, b: a + b).collect()\n\n\n[('b', 1), ('a', 2)]\n\n\n\n\ncombineByKey\n\n\nCode\nrdd = sc.parallelize([('a', 1), ('b', 2), ('a', 13)])\n\ndef add(a, b): \n    return a + str(b)\n\nrdd.combineByKey(str, add, add).collect()\n\n\n[('b', '2'), ('a', '113')]\n\n\n\n\njoin, rightOuterJoin, leftOuterJoin\n\n\nCode\nemployees = sc.parallelize([\n    (31, \"Rafferty\"),\n    (33, \"Jones\"),\n    (33, \"Heisenberg\"),\n    (34, \"Robinson\"),\n    (34, \"Smith\"),\n    (None, \"Williams\")\n])\n\n\n\n\nCode\ndepartments = sc.parallelize([\n    (31, \"Sales\"),\n    (33, \"Engineering\"),\n    (34, \"Clerical\"),\n    (35, \"Marketing\")\n])\n\n\n\n\nCode\n( \n    employees\n        .join(departments)\n        .sortByKey()\n        .collect()\n)\n\n\n[(31, ('Rafferty', 'Sales')),\n (33, ('Jones', 'Engineering')),\n (33, ('Heisenberg', 'Engineering')),\n (34, ('Robinson', 'Clerical')),\n (34, ('Smith', 'Clerical'))]\n\n\n\n\nCode\n( \n    employees\n        .rightOuterJoin(departments)\n        .sortByKey()\n        .collect()\n)\n\n\n[(31, ('Rafferty', 'Sales')),\n (33, ('Jones', 'Engineering')),\n (33, ('Heisenberg', 'Engineering')),\n (34, ('Robinson', 'Clerical')),\n (34, ('Smith', 'Clerical')),\n (35, (None, 'Marketing'))]\n\n\n\n\nCode\n(\n    employees\n        .leftOuterJoin(departments)\n        .collect()\n)\n\n\n[(None, ('Williams', None)),\n (31, ('Rafferty', 'Sales')),\n (33, ('Jones', 'Engineering')),\n (33, ('Heisenberg', 'Engineering')),\n (34, ('Robinson', 'Clerical')),\n (34, ('Smith', 'Clerical'))]"
  },
  {
    "objectID": "core/notebooks/notebook05_sparkrdd.html#actions-1",
    "href": "core/notebooks/notebook05_sparkrdd.html#actions-1",
    "title": "Introduction to Spark RDD",
    "section": "Actions",
    "text": "Actions\n\n\nCode\nemployees.countByKey()\n\n\ndefaultdict(int, {31: 1, 33: 2, 34: 2, None: 1})\n\n\n\n\nCode\nemployees.lookup(33)\n\n\n['Jones', 'Heisenberg']\n\n\n\n\nCode\nemployees.lookup(None)\n\n\n['Williams']\n\n\n\n\nCode\nemployees.collectAsMap()\n\n\n{31: 'Rafferty', 33: 'Heisenberg', 34: 'Smith', None: 'Williams'}"
  },
  {
    "objectID": "core/notebooks/notebook05_sparkrdd.html#references",
    "href": "core/notebooks/notebook05_sparkrdd.html#references",
    "title": "Introduction to Spark RDD",
    "section": "References",
    "text": "References\nSpark Core reference"
  },
  {
    "objectID": "core/notebooks/notebook07_json-format.html",
    "href": "core/notebooks/notebook07_json-format.html",
    "title": "Using JSON data with Python",
    "section": "",
    "text": "This notebook is concerned with JSON a format that serves many purposes. Just as csv files, json files are important sources and sinks for Spark. As a exchange format, JSON is also a serialization tool for Python and many other languages. JSON provides a way to accomodate semi-structured data in otherwise tabular environments (dataframes and databases tables).\nThe notebook is organized in the following way:"
  },
  {
    "objectID": "core/notebooks/notebook07_json-format.html#serialization-and-deserialization-of-built-in-types",
    "href": "core/notebooks/notebook07_json-format.html#serialization-and-deserialization-of-built-in-types",
    "title": "Using JSON data with Python",
    "section": "Serialization and deserialization of built-in types",
    "text": "Serialization and deserialization of built-in types\n\n\nCode\nimport json\n\nobj = {\n    \"name\": \"Foo Bar\",\n    \"age\": 78,\n    \"friends\": [\"Jane\",\"John\"],\n    \"balance\": 345.80,\n    \"other_names\":(\"Doe\",\"Joe\"),\n    \"active\": True,\n    \"spouse\": None\n}\n\nprint(json.dumps(obj, sort_keys=True, indent=4))\n\n\n{\n    \"active\": true,\n    \"age\": 78,\n    \"balance\": 345.8,\n    \"friends\": [\n        \"Jane\",\n        \"John\"\n    ],\n    \"name\": \"Foo Bar\",\n    \"other_names\": [\n        \"Doe\",\n        \"Joe\"\n    ],\n    \"spouse\": null\n}\n\n\n\n\n\n\n\n\nNote\n\n\n\njson.dumps() outputs a JSON formatted string.\nNot every type of object can be fed to json.dumps().\n\n\n\n\nCode\nwith open('user.json','w') as file:\n    json.dump(obj, file, sort_keys=True, indent=4)\n\n\n\n\nCode\n!cat user.json\n\n\n{\n    \"active\": true,\n    \"age\": 78,\n    \"balance\": 345.8,\n    \"friends\": [\n        \"Jane\",\n        \"John\"\n    ],\n    \"name\": \"Foo Bar\",\n    \"other_names\": [\n        \"Doe\",\n        \"Joe\"\n    ],\n    \"spouse\": null\n}\n\n\n\n\nCode\njson.loads('{\"active\": true, \"age\": 78, \"balance\": 345.8, \"friends\": [\"Jane\",\"John\"], \"name\": \"Foo Bar\", \"other_names\": [\"Doe\",\"Joe\"],\"spouse\":null}')\n\n\n{'active': True,\n 'age': 78,\n 'balance': 345.8,\n 'friends': ['Jane', 'John'],\n 'name': 'Foo Bar',\n 'other_names': ['Doe', 'Joe'],\n 'spouse': None}\n\n\n\n\nCode\nwith open('user.json', 'r') as file:\n    user_data = json.load(file)\n\nprint(user_data)\n\n\n{'active': True, 'age': 78, 'balance': 345.8, 'friends': ['Jane', 'John'], 'name': 'Foo Bar', 'other_names': ['Doe', 'Joe'], 'spouse': None}\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nWhat happens if we feed json.dumps() with a numpy array?\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nWhat happens if we feed json.dumps() with a datatime object?"
  },
  {
    "objectID": "core/notebooks/notebook07_json-format.html#serialization-and-deserialization-of-custom-objects",
    "href": "core/notebooks/notebook07_json-format.html#serialization-and-deserialization-of-custom-objects",
    "title": "Using JSON data with Python",
    "section": "Serialization and deserialization of custom objects",
    "text": "Serialization and deserialization of custom objects\n\n\nCode\nclass User(object):\n    \"\"\"Custom User Class\n    \"\"\"\n    def __init__(self, name, age, active, balance, \n                 other_names, friends, spouse):\n        self.name = name\n        self.age = age\n        self.active = active\n        self.balance = balance\n        self.other_names = other_names\n        self.friends = friends\n        self.spouse = spouse\n            \n    def __repr__(self):\n        s = \"User(\"\n        s += \"name=\" + repr(self.name)\n        s += \", age=\" + repr(self.age)\n        s += \", active=\" + repr(self.active)\n        s += \", other_names=\" + repr(self.other_names)\n        s += \", friends=\" + repr(self.friends)\n        s += \", spouse=\" + repr(self.spouse) + \")\"\n        return s\n\n\n\n\n\n\n\n\nTip\n\n\n\nBrush up your dunder/magic methods, for example in Fluent Python by Ramalho (Chapter I: The Python data model, Section Overview of Special Methods)\n\n\n\n\nCode\nnew_user = User(\n    name = \"Foo Bar\",\n    age = 78,\n    friends = [\"Jane\", \"John\"],\n    balance = 345.80,\n    other_names = (\"Doe\", \"Joe\"),\n    active = True,\n    spouse = None\n)\n\nnew_user\n\n\nUser(name='Foo Bar', age=78, active=True, other_names=('Doe', 'Joe'), friends=['Jane', 'John'], spouse=None)\n\n\n\n\n\n\n\n\nNote\n\n\n\nUncomment to see what happens\n\n\n\n\nCode\n# This will raise a TypeError\n# json.dumps(new_user)\n\n\nAs expected, the custom object new_user is not JSON serializable. So let’s build a method that does that for us.\n\nThis comes as no surprise to us, since earlier on we observed that the json module only handles the built-in types, and User is not one.\nWe need to send our user data to a client over a network, so how do we get ourselves out of this error state?\nA simple solution would be to convert our custom type into a serializable type that is a built-in type. We can conveniently define a method convert_to_dict() that returns a dictionary representation of our object. json.dumps() takes in a optional argument, default, which specifies a function to be called if the object is not serializable. This function returns a JSON encodable version of the object.\n\nRecall that class obj has a dunder attribute __dict__ that provides a basis for obtaining a dictionary with the attributes of any object:\n\n\nCode\nnew_user.__dict__\n\n\n{'name': 'Foo Bar',\n 'age': 78,\n 'active': True,\n 'balance': 345.8,\n 'other_names': ('Doe', 'Joe'),\n 'friends': ['Jane', 'John'],\n 'spouse': None}\n\n\n\n\nCode\ndef obj_to_dict(obj):\n    \"\"\"Converts an object to a dictionary representation of the object including \n    meta-data information about the object's module and class name.\n\n    Parameters\n    ----------\n    obj : `object`\n        A python object to be converted into a dictionary representation\n\n    Returns\n    -------\n    output : `dict`\n        A dictionary representation of the object\n    \"\"\"\n    # Add object meta data \n    obj_dict = {\n        \"__class__\": obj.__class__.__name__,\n        \"__module__\": obj.__module__\n    }\n    # Add the object properties\n    return obj_dict | obj.__dict__\n\n\n\n\nCode\nobj_to_dict(new_user)\n\n\n{'__class__': 'User',\n '__module__': '__main__',\n 'name': 'Foo Bar',\n 'age': 78,\n 'active': True,\n 'balance': 345.8,\n 'other_names': ('Doe', 'Joe'),\n 'friends': ['Jane', 'John'],\n 'spouse': None}\n\n\nThe function convert_to_dict does the following:\n\ncreate a dictionary named obj_dict to act as the dict representation of our object.\ndunder attributes __class__.__name__ and __module__ provide crucial metadata on the object: the class name and the module name\nadd the instance attributes of the object using obj.__dict__ (Python stores instance attributes in a dictionary)\n\nThe resulting obj_dict is now serializable (provided all attributes of our object are).\nNow we can comfortably call json.dumps() on the object and pass default=convert_to_dict\n\n\n\n\n\n\nNote\n\n\n\nObviously this fails if one of the attributes is not JSON serializable\n\n\n\n\nCode\nprint(json.dumps(new_user, default=obj_to_dict, indent=4, sort_keys=True))\n\n\n{\n    \"__class__\": \"User\",\n    \"__module__\": \"__main__\",\n    \"active\": true,\n    \"age\": 78,\n    \"balance\": 345.8,\n    \"friends\": [\n        \"Jane\",\n        \"John\"\n    ],\n    \"name\": \"Foo Bar\",\n    \"other_names\": [\n        \"Doe\",\n        \"Joe\"\n    ],\n    \"spouse\": null\n}\n\n\nNow, if we want to decode (deserialiaze) a custom object, and create the correct object type, we need a function that does the inverse of obj_to_dict, since json.loads simply returns a dict:\n\n\nCode\nuser_data = json.loads(json.dumps(new_user, default=obj_to_dict))\nprint(user_data)\n\n\n{'__class__': 'User', '__module__': '__main__', 'name': 'Foo Bar', 'age': 78, 'active': True, 'balance': 345.8, 'other_names': ['Doe', 'Joe'], 'friends': ['Jane', 'John'], 'spouse': None}\n\n\n\n\n\n\n\n\nNote\n\n\n\nWe need json.loads() to reconstruct a User object from this dictionary: json.loads() takes an optional argument object_hook which specifies a function that returns the desired custom object, given the decoded output (which in this case, is a dict).\n\n\n\n\nCode\ndef dict_to_obj(input_dict):\n    \"\"\"Converts a dictionary representation of an object to an instance of the object.\n\n    Parameters\n    ----------\n    input_dict : `dict`\n        A dictionary representation of the object, containing \"__module__\" \n        and \"__class__\" metadata\n\n    Returns\n    -------    \n    obj : `object`\n        A python object constructed from the dictionary representation    \n    \"\"\"\n    assert \"__class__\" in input_dict and \"__module__\" in input_dict\n    class_name = input_dict.pop(\"__class__\")\n    module_name = input_dict.pop(\"__module__\")\n    module = __import__(module_name)\n    class_ = getattr(module, class_name)\n    obj = class_(**input_dict)\n    return obj\n\n\nThis function does the following:\n\nExtract the class name from the dictionary under the key __class__\nExtract the module name from the dictionary under the key __module__\nImports the module and get the class\nInstantiate the class by giving to the class constructor all the instance arguments through dictionary unpacking\n\n\n\nCode\nobj_data = json.dumps(new_user, default=obj_to_dict)\nnew_object = json.loads(obj_data, object_hook=dict_to_obj)\nnew_object\n\n\nUser(name='Foo Bar', age=78, active=True, other_names=['Doe', 'Joe'], friends=['Jane', 'John'], spouse=None)\n\n\n\n\nCode\ntype(new_object)\n\n\n__main__.User\n\n\n\n\nCode\nnew_object.age\n\n\n78\n\n\n\n\n\n\n\n\nNote\n\n\n\nFunctions obj_to_dict() and dict_to_obj() are showcases for special/magic/dunder methods.\nIn the definition of class User, two special methods were explicitly defined: __init__() and __repr__(). But many more are available, including __dir__().\nRemember that some dunder members of the object are not callable.\n\n\n\n\nCode\n[dude for dude in dir(new_object) if dude.startswith('__') and callable(getattr(new_object, dude))]\n\n\n['__class__',\n '__delattr__',\n '__dir__',\n '__eq__',\n '__format__',\n '__ge__',\n '__getattribute__',\n '__getstate__',\n '__gt__',\n '__hash__',\n '__init__',\n '__init_subclass__',\n '__le__',\n '__lt__',\n '__ne__',\n '__new__',\n '__reduce__',\n '__reduce_ex__',\n '__repr__',\n '__setattr__',\n '__sizeof__',\n '__str__',\n '__subclasshook__']\n\n\n\n\nCode\n[dude for dude in dir(new_object) if dude.startswith('__') and not callable(getattr(new_object, dude))]\n\n\n['__dict__', '__doc__', '__module__', '__weakref__']\n\n\n\n\nCode\nnew_object.__getattribute__('age')\n\ngetattr(new_object, 'age')\n\n\n78\n\n\n\n\n\n\n\n\nNote\n\n\n\nClass User could have been implemented as a dataclass\n\n\n\n\nCode\nfrom dataclasses import dataclass\n\n@dataclass\nclass UserBis(object):\n    \"\"\"Custom User Class\n    \"\"\"\n    name: str \n    age: int\n    active: bool\n    balance: float\n    other_names: list[str]\n    friends: list[str]\n    spouse: str\n\n\n\n\n\n\n\n\nNote\n\n\n\n@dataclass is a decorator. Have a look at the chapter on decorators in [Fluent Python] by Ramalho\n\n\n\n\nCode\nother_user = UserBis(**(new_user.__dict__))\n\n\n\n\nCode\nrepr(other_user)\n\n\n\"UserBis(name='Foo Bar', age=78, active=True, balance=345.8, other_names=('Doe', 'Joe'), friends=['Jane', 'John'], spouse=None)\"\n\n\n\n\nCode\n{dude for dude in dir(other_user) if dude.startswith('__')} -  {dude for dude in dir(new_user) if dude.startswith('__')}\n\n\n{'__annotations__',\n '__dataclass_fields__',\n '__dataclass_params__',\n '__match_args__'}\n\n\n\n\n\n\n\n\nNote\n\n\n\nHave a look at dataclasses documentation.\nSee also Chapter 5: Data class builders in Fluent Python"
  },
  {
    "objectID": "core/notebooks/notebook07_json-format.html#reading-a-json-dataset-with-spark",
    "href": "core/notebooks/notebook07_json-format.html#reading-a-json-dataset-with-spark",
    "title": "Using JSON data with Python",
    "section": "Reading a JSON dataset with Spark",
    "text": "Reading a JSON dataset with Spark\n\n\nCode\nfrom pyspark import SparkConf, SparkContext\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql import functions as fn\nfrom pyspark.sql.functions import col\n\nspark = (SparkSession\n    .builder\n    .appName(\"Spark JSON\")\n    .getOrCreate()\n)\n\nsc = spark._sc\n\n\n25/04/03 15:09:09 WARN Utils: Your hostname, boucheron-Precision-5480 resolves to a loopback address: 127.0.1.1; using 172.23.32.10 instead (on interface eth0)\n25/04/03 15:09:09 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\nSetting default log level to \"WARN\".\nTo adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n25/04/03 15:09:10 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n\n\n\n\nCode\nfilename = \"drug-enforcement.json\"\n\n\nFirst, lets look at the data. It’s a large set of JSON records about drugs enforcement.\n\n\nCode\n!head -n 100 drug-enforcement.json\n\n\n[\n    {\n      \"classification\": \"Class II\",\n      \"center_classification_date\": \"20121025\",\n      \"report_date\": \"20121031\",\n      \"postal_code\": \"08816-2108\",\n      \"termination_date\": \"20141007\",\n      \"recall_initiation_date\": \"20120904\",\n      \"recall_number\": \"D-026-2013\",\n      \"city\": \"East Brunswick\",\n      \"event_id\": \"63384\",\n      \"distribution_pattern\": \"Nationwide\",\n      \"openfda\": {},\n      \"recalling_firm\": \"Raritan Pharmaceuticals, Inc.\",\n      \"voluntary_mandated\": \"Voluntary: Firm Initiated\",\n      \"state\": \"NJ\",\n      \"reason_for_recall\": \"Microbial Contamination of Non-Sterile Products: Product is being recalled due to possible microbial contamination by C. difficile discovered in the raw material.\",\n      \"initial_firm_notification\": \"E-Mail\",\n      \"status\": \"Terminated\",\n      \"product_type\": \"Drugs\",\n      \"country\": \"United States\",\n      \"product_description\": \"Wal-Mucil 100% Natural Fiber, 100% Natural Psyllium Seed Husk, Fiber Laxative/Supplement, a)160 capsules per bottle (item #503663), and b) 320 capsules per bottle (Item #586143), Distributed by: Walgreen Co., 200 Wilmot Road, Deerfield, IL 60015-4616, www.walgreens.com, a) UPC 3-11917-08151-9, b) UPC 3-11917-07658-4\",\n      \"code_info\": \"Lots a) 15952, 16270,16425, Exp 06/15; b)16459, 16466, 16467, Exp 07/15\",\n      \"address_1\": \"8 Joanna Ct\",\n      \"address_2\": \"\",\n      \"product_quantity\": \"56,808 bottles\"\n    },\n    {\n      \"classification\": \"Class II\",\n      \"center_classification_date\": \"20121025\",\n      \"report_date\": \"20121031\",\n      \"postal_code\": \"08816-2108\",\n      \"termination_date\": \"20141007\",\n      \"recall_initiation_date\": \"20120904\",\n      \"recall_number\": \"D-031-2013\",\n      \"city\": \"East Brunswick\",\n      \"event_id\": \"63384\",\n      \"distribution_pattern\": \"Nationwide\",\n      \"openfda\": {},\n      \"recalling_firm\": \"Raritan Pharmaceuticals, Inc.\",\n      \"voluntary_mandated\": \"Voluntary: Firm Initiated\",\n      \"state\": \"NJ\",\n      \"reason_for_recall\": \"Microbial Contamination of Non-Sterile Products: Product is being recalled due to possible microbial contamination by C. difficile discovered in the raw material.\",\n      \"initial_firm_notification\": \"E-Mail\",\n      \"status\": \"Terminated\",\n      \"product_type\": \"Drugs\",\n      \"country\": \"United States\",\n      \"product_description\": \"Premier Value Fiber Plus Calcium Supplement Capsules, 120 capsules per bottle, Distributed by: Chain Drug Consortium, LLC, Boca Raton, FL, UPC 8-40986-01987-6\",\n      \"code_info\": \"Lot 15087, Exp 08/15\",\n      \"address_1\": \"8 Joanna Ct\",\n      \"address_2\": \"\",\n      \"product_quantity\": \"96 bottles\"\n    },\n    {\n      \"classification\": \"Class III\",\n      \"center_classification_date\": \"20121106\",\n      \"report_date\": \"20121114\",\n      \"postal_code\": \"08807\",\n      \"termination_date\": \"20130325\",\n      \"recall_initiation_date\": \"20121015\",\n      \"recall_number\": \"D-047-2013\",\n      \"city\": \"Bridgewater\",\n      \"event_id\": \"63488\",\n      \"distribution_pattern\": \"Nationwide\",\n      \"openfda\": {},\n      \"recalling_firm\": \"Valeant Pharmaceuticals\",\n      \"voluntary_mandated\": \"Voluntary: Firm Initiated\",\n      \"state\": \"NJ\",\n      \"reason_for_recall\": \"Subpotent (Single Ingredient) Drug: This product was found to be subpotent for the salicylic acid ingredient.  Additionally, this product is mislabeled because the label either omits or erroneously added inactive ingredients to the label.\",\n      \"initial_firm_notification\": \"Letter\",\n      \"status\": \"Terminated\",\n      \"product_type\": \"Drugs\",\n      \"country\": \"United States\",\n      \"product_description\": \"AcneFree 3-in-1 Acne Night Repair Foam (retinol + salicylic acid 1.5% w/v), 3 oz (85 g) canister, Dist. by: University Medical Pharmaceuticals Corp., Irvine, CA  92618, UPC 7 88521 13548 6.\",\n      \"code_info\": \"All lots with expiration dates between 10/10/12 through 10/10/14 of UPC 7 88521 13548 6\",\n      \"address_1\": \"700 Rte 206 North\",\n      \"address_2\": \"\",\n      \"product_quantity\": \"81,319 canisters\"\n    },\n    {\n      \"classification\": \"Class III\",\n      \"center_classification_date\": \"20121220\",\n      \"report_date\": \"20121226\",\n      \"postal_code\": \"08558-1311\",\n      \"termination_date\": \"20140429\",\n      \"recall_initiation_date\": \"20121204\",\n      \"recall_number\": \"D-098-2013\",\n      \"city\": \"Skillman\",\n      \"more_code_info\": null,\n      \"event_id\": \"63787\",\n      \"distribution_pattern\": \"Nationwide\",\n      \"openfda\": {},\n      \"recalling_firm\": \"Johnson & Johnson\",\n      \"voluntary_mandated\": \"Voluntary: Firm Initiated\",\n      \"state\": \"NJ\",\n      \"reason_for_recall\": \"Superpotent (Single Ingredient Drug): salicylic acid\",\n      \"initial_firm_notification\": \"Letter\",\n      \"status\": \"Terminated\",\n      \"product_type\": \"Drugs\",\n      \"country\": \"United States\",\n\n\n\n\n\n\n\n\nNote\n\n\n\nWe need to tell spark that rows span on several lines with the multLine option\n\n\n\n\nCode\ndf = spark.read.json(filename, multiLine=True)\n\n\n\n\nCode\ndf.printSchema()\n\n\nroot\n |-- address_1: string (nullable = true)\n |-- address_2: string (nullable = true)\n |-- center_classification_date: string (nullable = true)\n |-- city: string (nullable = true)\n |-- classification: string (nullable = true)\n |-- code_info: string (nullable = true)\n |-- country: string (nullable = true)\n |-- distribution_pattern: string (nullable = true)\n |-- event_id: string (nullable = true)\n |-- initial_firm_notification: string (nullable = true)\n |-- more_code_info: string (nullable = true)\n |-- openfda: struct (nullable = true)\n |    |-- application_number: array (nullable = true)\n |    |    |-- element: string (containsNull = true)\n |    |-- brand_name: array (nullable = true)\n |    |    |-- element: string (containsNull = true)\n |    |-- generic_name: array (nullable = true)\n |    |    |-- element: string (containsNull = true)\n |    |-- is_original_packager: array (nullable = true)\n |    |    |-- element: boolean (containsNull = true)\n |    |-- manufacturer_name: array (nullable = true)\n |    |    |-- element: string (containsNull = true)\n |    |-- nui: array (nullable = true)\n |    |    |-- element: string (containsNull = true)\n |    |-- original_packager_product_ndc: array (nullable = true)\n |    |    |-- element: string (containsNull = true)\n |    |-- package_ndc: array (nullable = true)\n |    |    |-- element: string (containsNull = true)\n |    |-- pharm_class_cs: array (nullable = true)\n |    |    |-- element: string (containsNull = true)\n |    |-- pharm_class_epc: array (nullable = true)\n |    |    |-- element: string (containsNull = true)\n |    |-- pharm_class_moa: array (nullable = true)\n |    |    |-- element: string (containsNull = true)\n |    |-- pharm_class_pe: array (nullable = true)\n |    |    |-- element: string (containsNull = true)\n |    |-- product_ndc: array (nullable = true)\n |    |    |-- element: string (containsNull = true)\n |    |-- product_type: array (nullable = true)\n |    |    |-- element: string (containsNull = true)\n |    |-- route: array (nullable = true)\n |    |    |-- element: string (containsNull = true)\n |    |-- rxcui: array (nullable = true)\n |    |    |-- element: string (containsNull = true)\n |    |-- spl_id: array (nullable = true)\n |    |    |-- element: string (containsNull = true)\n |    |-- spl_set_id: array (nullable = true)\n |    |    |-- element: string (containsNull = true)\n |    |-- substance_name: array (nullable = true)\n |    |    |-- element: string (containsNull = true)\n |    |-- unii: array (nullable = true)\n |    |    |-- element: string (containsNull = true)\n |    |-- upc: array (nullable = true)\n |    |    |-- element: string (containsNull = true)\n |-- postal_code: string (nullable = true)\n |-- product_description: string (nullable = true)\n |-- product_quantity: string (nullable = true)\n |-- product_type: string (nullable = true)\n |-- reason_for_recall: string (nullable = true)\n |-- recall_initiation_date: string (nullable = true)\n |-- recall_number: string (nullable = true)\n |-- recalling_firm: string (nullable = true)\n |-- report_date: string (nullable = true)\n |-- state: string (nullable = true)\n |-- status: string (nullable = true)\n |-- termination_date: string (nullable = true)\n |-- voluntary_mandated: string (nullable = true)\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nIn a less user-friendly format:\n\n\n\n\nCode\ndf.schema\n\n\nStructType([StructField('address_1', StringType(), True), StructField('address_2', StringType(), True), StructField('center_classification_date', StringType(), True), StructField('city', StringType(), True), StructField('classification', StringType(), True), StructField('code_info', StringType(), True), StructField('country', StringType(), True), StructField('distribution_pattern', StringType(), True), StructField('event_id', StringType(), True), StructField('initial_firm_notification', StringType(), True), StructField('more_code_info', StringType(), True), StructField('openfda', StructType([StructField('application_number', ArrayType(StringType(), True), True), StructField('brand_name', ArrayType(StringType(), True), True), StructField('generic_name', ArrayType(StringType(), True), True), StructField('is_original_packager', ArrayType(BooleanType(), True), True), StructField('manufacturer_name', ArrayType(StringType(), True), True), StructField('nui', ArrayType(StringType(), True), True), StructField('original_packager_product_ndc', ArrayType(StringType(), True), True), StructField('package_ndc', ArrayType(StringType(), True), True), StructField('pharm_class_cs', ArrayType(StringType(), True), True), StructField('pharm_class_epc', ArrayType(StringType(), True), True), StructField('pharm_class_moa', ArrayType(StringType(), True), True), StructField('pharm_class_pe', ArrayType(StringType(), True), True), StructField('product_ndc', ArrayType(StringType(), True), True), StructField('product_type', ArrayType(StringType(), True), True), StructField('route', ArrayType(StringType(), True), True), StructField('rxcui', ArrayType(StringType(), True), True), StructField('spl_id', ArrayType(StringType(), True), True), StructField('spl_set_id', ArrayType(StringType(), True), True), StructField('substance_name', ArrayType(StringType(), True), True), StructField('unii', ArrayType(StringType(), True), True), StructField('upc', ArrayType(StringType(), True), True)]), True), StructField('postal_code', StringType(), True), StructField('product_description', StringType(), True), StructField('product_quantity', StringType(), True), StructField('product_type', StringType(), True), StructField('reason_for_recall', StringType(), True), StructField('recall_initiation_date', StringType(), True), StructField('recall_number', StringType(), True), StructField('recalling_firm', StringType(), True), StructField('report_date', StringType(), True), StructField('state', StringType(), True), StructField('status', StringType(), True), StructField('termination_date', StringType(), True), StructField('voluntary_mandated', StringType(), True)])\n\n\n\n\n\n\n\n\nNote\n\n\n\nThis dataset is a little bit of a mess!\nThis should not be surprising. The data used to populate the Spark dataframe are not classically tabular but what people call semi-structured. Json is well-suited to store, represent, and exchange such data.\nIn the classical age of tabular data (according to Codd’s principles), a table cell could only hold a scalar value (numeric, logical, text, date, timestamp, …), nowadays Relational Database Management Systems handle Arrays, Composite Types, Range Types, …, and Json (see PostgreSQL).\nSpark, R, and Pandas, and modern relational databases also allow us to work with complex types.\nModern column oriented file format like parquet also work with nested structures.\n\n\n\nFirst, there is a nested opendfa dictionary. Each element of the dictionary is an array\nA first good idea is to “flatten” the schema of the DataFrame, so that there are no nested types any more."
  },
  {
    "objectID": "core/notebooks/notebook07_json-format.html#flattening-the-schema",
    "href": "core/notebooks/notebook07_json-format.html#flattening-the-schema",
    "title": "Using JSON data with Python",
    "section": "Flattening the schema",
    "text": "Flattening the schema\nAll the columns in the nested structure openfda are put up in the schema. These columns nested in the openfda are as follows:\n\n\nCode\ndf.select('openfda.*').columns\n\n\n['application_number',\n 'brand_name',\n 'generic_name',\n 'is_original_packager',\n 'manufacturer_name',\n 'nui',\n 'original_packager_product_ndc',\n 'package_ndc',\n 'pharm_class_cs',\n 'pharm_class_epc',\n 'pharm_class_moa',\n 'pharm_class_pe',\n 'product_ndc',\n 'product_type',\n 'route',\n 'rxcui',\n 'spl_id',\n 'spl_set_id',\n 'substance_name',\n 'unii',\n 'upc']\n\n\n\n\nCode\ndf.select(\"openfda.*\").head(2)\n\n\n[Row(application_number=None, brand_name=None, generic_name=None, is_original_packager=None, manufacturer_name=None, nui=None, original_packager_product_ndc=None, package_ndc=None, pharm_class_cs=None, pharm_class_epc=None, pharm_class_moa=None, pharm_class_pe=None, product_ndc=None, product_type=None, route=None, rxcui=None, spl_id=None, spl_set_id=None, substance_name=None, unii=None, upc=None),\n Row(application_number=None, brand_name=None, generic_name=None, is_original_packager=None, manufacturer_name=None, nui=None, original_packager_product_ndc=None, package_ndc=None, pharm_class_cs=None, pharm_class_epc=None, pharm_class_moa=None, pharm_class_pe=None, product_ndc=None, product_type=None, route=None, rxcui=None, spl_id=None, spl_set_id=None, substance_name=None, unii=None, upc=None)]\n\n\n\n\nCode\nfor c in df.select(\"openfda.*\").columns:\n    df = df.withColumn(\"openfda_\" + c, col(\"openfda.\" + c))\n\n\n\n\nCode\ndf = df.select([c for c in df.columns if c != \"openfda\"])\n\n\n\n\nCode\ndf.printSchema()\n\n\nroot\n |-- address_1: string (nullable = true)\n |-- address_2: string (nullable = true)\n |-- center_classification_date: string (nullable = true)\n |-- city: string (nullable = true)\n |-- classification: string (nullable = true)\n |-- code_info: string (nullable = true)\n |-- country: string (nullable = true)\n |-- distribution_pattern: string (nullable = true)\n |-- event_id: string (nullable = true)\n |-- initial_firm_notification: string (nullable = true)\n |-- more_code_info: string (nullable = true)\n |-- postal_code: string (nullable = true)\n |-- product_description: string (nullable = true)\n |-- product_quantity: string (nullable = true)\n |-- product_type: string (nullable = true)\n |-- reason_for_recall: string (nullable = true)\n |-- recall_initiation_date: string (nullable = true)\n |-- recall_number: string (nullable = true)\n |-- recalling_firm: string (nullable = true)\n |-- report_date: string (nullable = true)\n |-- state: string (nullable = true)\n |-- status: string (nullable = true)\n |-- termination_date: string (nullable = true)\n |-- voluntary_mandated: string (nullable = true)\n |-- openfda_application_number: array (nullable = true)\n |    |-- element: string (containsNull = true)\n |-- openfda_brand_name: array (nullable = true)\n |    |-- element: string (containsNull = true)\n |-- openfda_generic_name: array (nullable = true)\n |    |-- element: string (containsNull = true)\n |-- openfda_is_original_packager: array (nullable = true)\n |    |-- element: boolean (containsNull = true)\n |-- openfda_manufacturer_name: array (nullable = true)\n |    |-- element: string (containsNull = true)\n |-- openfda_nui: array (nullable = true)\n |    |-- element: string (containsNull = true)\n |-- openfda_original_packager_product_ndc: array (nullable = true)\n |    |-- element: string (containsNull = true)\n |-- openfda_package_ndc: array (nullable = true)\n |    |-- element: string (containsNull = true)\n |-- openfda_pharm_class_cs: array (nullable = true)\n |    |-- element: string (containsNull = true)\n |-- openfda_pharm_class_epc: array (nullable = true)\n |    |-- element: string (containsNull = true)\n |-- openfda_pharm_class_moa: array (nullable = true)\n |    |-- element: string (containsNull = true)\n |-- openfda_pharm_class_pe: array (nullable = true)\n |    |-- element: string (containsNull = true)\n |-- openfda_product_ndc: array (nullable = true)\n |    |-- element: string (containsNull = true)\n |-- openfda_product_type: array (nullable = true)\n |    |-- element: string (containsNull = true)\n |-- openfda_route: array (nullable = true)\n |    |-- element: string (containsNull = true)\n |-- openfda_rxcui: array (nullable = true)\n |    |-- element: string (containsNull = true)\n |-- openfda_spl_id: array (nullable = true)\n |    |-- element: string (containsNull = true)\n |-- openfda_spl_set_id: array (nullable = true)\n |    |-- element: string (containsNull = true)\n |-- openfda_substance_name: array (nullable = true)\n |    |-- element: string (containsNull = true)\n |-- openfda_unii: array (nullable = true)\n |    |-- element: string (containsNull = true)\n |-- openfda_upc: array (nullable = true)\n |    |-- element: string (containsNull = true)\n\n\n\n\n\nCode\ndf.head(2)\n\n\n25/04/03 15:09:13 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n\n\n[Row(address_1='8 Joanna Ct', address_2='', center_classification_date='20121025', city='East Brunswick', classification='Class II', code_info='Lots a) 15952, 16270,16425, Exp 06/15; b)16459, 16466, 16467, Exp 07/15', country='United States', distribution_pattern='Nationwide', event_id='63384', initial_firm_notification='E-Mail', more_code_info=None, postal_code='08816-2108', product_description='Wal-Mucil 100% Natural Fiber, 100% Natural Psyllium Seed Husk, Fiber Laxative/Supplement, a)160 capsules per bottle (item #503663), and b) 320 capsules per bottle (Item #586143), Distributed by: Walgreen Co., 200 Wilmot Road, Deerfield, IL 60015-4616, www.walgreens.com, a) UPC 3-11917-08151-9, b) UPC 3-11917-07658-4', product_quantity='56,808 bottles', product_type='Drugs', reason_for_recall='Microbial Contamination of Non-Sterile Products: Product is being recalled due to possible microbial contamination by C. difficile discovered in the raw material.', recall_initiation_date='20120904', recall_number='D-026-2013', recalling_firm='Raritan Pharmaceuticals, Inc.', report_date='20121031', state='NJ', status='Terminated', termination_date='20141007', voluntary_mandated='Voluntary: Firm Initiated', openfda_application_number=None, openfda_brand_name=None, openfda_generic_name=None, openfda_is_original_packager=None, openfda_manufacturer_name=None, openfda_nui=None, openfda_original_packager_product_ndc=None, openfda_package_ndc=None, openfda_pharm_class_cs=None, openfda_pharm_class_epc=None, openfda_pharm_class_moa=None, openfda_pharm_class_pe=None, openfda_product_ndc=None, openfda_product_type=None, openfda_route=None, openfda_rxcui=None, openfda_spl_id=None, openfda_spl_set_id=None, openfda_substance_name=None, openfda_unii=None, openfda_upc=None),\n Row(address_1='8 Joanna Ct', address_2='', center_classification_date='20121025', city='East Brunswick', classification='Class II', code_info='Lot 15087, Exp 08/15', country='United States', distribution_pattern='Nationwide', event_id='63384', initial_firm_notification='E-Mail', more_code_info=None, postal_code='08816-2108', product_description='Premier Value Fiber Plus Calcium Supplement Capsules, 120 capsules per bottle, Distributed by: Chain Drug Consortium, LLC, Boca Raton, FL, UPC 8-40986-01987-6', product_quantity='96 bottles', product_type='Drugs', reason_for_recall='Microbial Contamination of Non-Sterile Products: Product is being recalled due to possible microbial contamination by C. difficile discovered in the raw material.', recall_initiation_date='20120904', recall_number='D-031-2013', recalling_firm='Raritan Pharmaceuticals, Inc.', report_date='20121031', state='NJ', status='Terminated', termination_date='20141007', voluntary_mandated='Voluntary: Firm Initiated', openfda_application_number=None, openfda_brand_name=None, openfda_generic_name=None, openfda_is_original_packager=None, openfda_manufacturer_name=None, openfda_nui=None, openfda_original_packager_product_ndc=None, openfda_package_ndc=None, openfda_pharm_class_cs=None, openfda_pharm_class_epc=None, openfda_pharm_class_moa=None, openfda_pharm_class_pe=None, openfda_product_ndc=None, openfda_product_type=None, openfda_route=None, openfda_rxcui=None, openfda_spl_id=None, openfda_spl_set_id=None, openfda_substance_name=None, openfda_unii=None, openfda_upc=None)]\n\n\nNote that the display of the DataFrame is not as usual… it displays the dataframe like a list of Row, since the columns “openfda*” contain arrays of varying length\n\n\n\n\n\n\nNote\n\n\n\nA principled approach to schema flattening is embodied in the next chunk.\ndf.schema allows us to perform flattening in a programmatic way.\n\n\n\n\nCode\nfrom pyspark.sql.types import StructType\nfrom pyspark.sql.functions import col\n\ndef flatten_schema(df):\n    # Get fields and their data types\n    fields = df.schema.fields\n    \n    # Flatten array of column names\n    flat_cols = []\n    \n    for field in fields:\n        # Handle nested structures\n        if isinstance(field.dataType, StructType):\n            nested = df.select(field.name + \".*\").columns\n            flat_cols.extend([field.name + \".\" + x for x in nested])\n        else:\n            flat_cols.append(field.name)\n    \n    # Select all flattened columns\n    df_flattened = df.select([col(x).alias(x.replace(\".\",\"_\")) for x in flat_cols])\n    \n    return df_flattened\n\n\n\n\n\n\n\n\nNote\n\n\n\nThis function definition is from copilot under the following prompt:\nHow can I flatten the schema of a spark dataframe?\n\n\n\n\nCode\ndf = spark.read.json(filename, multiLine=True)\n\ndf_flat = flatten_schema(df)\n\ndf_flat.printSchema()\n\n\nroot\n |-- address_1: string (nullable = true)\n |-- address_2: string (nullable = true)\n |-- center_classification_date: string (nullable = true)\n |-- city: string (nullable = true)\n |-- classification: string (nullable = true)\n |-- code_info: string (nullable = true)\n |-- country: string (nullable = true)\n |-- distribution_pattern: string (nullable = true)\n |-- event_id: string (nullable = true)\n |-- initial_firm_notification: string (nullable = true)\n |-- more_code_info: string (nullable = true)\n |-- openfda_application_number: array (nullable = true)\n |    |-- element: string (containsNull = true)\n |-- openfda_brand_name: array (nullable = true)\n |    |-- element: string (containsNull = true)\n |-- openfda_generic_name: array (nullable = true)\n |    |-- element: string (containsNull = true)\n |-- openfda_is_original_packager: array (nullable = true)\n |    |-- element: boolean (containsNull = true)\n |-- openfda_manufacturer_name: array (nullable = true)\n |    |-- element: string (containsNull = true)\n |-- openfda_nui: array (nullable = true)\n |    |-- element: string (containsNull = true)\n |-- openfda_original_packager_product_ndc: array (nullable = true)\n |    |-- element: string (containsNull = true)\n |-- openfda_package_ndc: array (nullable = true)\n |    |-- element: string (containsNull = true)\n |-- openfda_pharm_class_cs: array (nullable = true)\n |    |-- element: string (containsNull = true)\n |-- openfda_pharm_class_epc: array (nullable = true)\n |    |-- element: string (containsNull = true)\n |-- openfda_pharm_class_moa: array (nullable = true)\n |    |-- element: string (containsNull = true)\n |-- openfda_pharm_class_pe: array (nullable = true)\n |    |-- element: string (containsNull = true)\n |-- openfda_product_ndc: array (nullable = true)\n |    |-- element: string (containsNull = true)\n |-- openfda_product_type: array (nullable = true)\n |    |-- element: string (containsNull = true)\n |-- openfda_route: array (nullable = true)\n |    |-- element: string (containsNull = true)\n |-- openfda_rxcui: array (nullable = true)\n |    |-- element: string (containsNull = true)\n |-- openfda_spl_id: array (nullable = true)\n |    |-- element: string (containsNull = true)\n |-- openfda_spl_set_id: array (nullable = true)\n |    |-- element: string (containsNull = true)\n |-- openfda_substance_name: array (nullable = true)\n |    |-- element: string (containsNull = true)\n |-- openfda_unii: array (nullable = true)\n |    |-- element: string (containsNull = true)\n |-- openfda_upc: array (nullable = true)\n |    |-- element: string (containsNull = true)\n |-- postal_code: string (nullable = true)\n |-- product_description: string (nullable = true)\n |-- product_quantity: string (nullable = true)\n |-- product_type: string (nullable = true)\n |-- reason_for_recall: string (nullable = true)\n |-- recall_initiation_date: string (nullable = true)\n |-- recall_number: string (nullable = true)\n |-- recalling_firm: string (nullable = true)\n |-- report_date: string (nullable = true)\n |-- state: string (nullable = true)\n |-- status: string (nullable = true)\n |-- termination_date: string (nullable = true)\n |-- voluntary_mandated: string (nullable = true)\n\n\n\n\n\nCode\nfrom pyspark.sql.types import StructType, StructField, StringType, IntegerType\n\nmessy_schema = StructType([\n    StructField(\"id\", IntegerType()),\n    StructField(\"info\", StructType([\n        StructField(\"name\", StringType()),\n        StructField(\"age\", IntegerType()),\n        StructField(\"zoo\", StructType([\n            StructField(\"cat\", StringType()),\n            StructField(\"dog\", StringType())\n        ]))\n    ]))\n])\n\n\n\n\n\n\n\n\nNote\n\n\n\nThis principled approach is not the end of the story. If the schema exhibits hierarchical nesting, flatten_schema() only removes one level of nesting.\n\n\n\n\nCode\ndata = [(1, (\"John\", 30, (\"Fritz\", \"Medor\"))), (2, (\"Jane\", 25, (\"Grominet\", \"Goofy\")))]\n\nvery_nested_df = spark.createDataFrame(data, messy_schema)\n\n\n\n\nCode\nflatten_schema(very_nested_df).show()\n\n\n+---+---------+--------+-----------------+\n| id|info_name|info_age|         info_zoo|\n+---+---------+--------+-----------------+\n|  1|     John|      30|   {Fritz, Medor}|\n|  2|     Jane|      25|{Grominet, Goofy}|\n+---+---------+--------+-----------------+\n\n\n\n\n\nCode\nflatten_schema(very_nested_df).printSchema()\n\n\nroot\n |-- id: integer (nullable = true)\n |-- info_name: string (nullable = true)\n |-- info_age: integer (nullable = true)\n |-- info_zoo: struct (nullable = true)\n |    |-- cat: string (nullable = true)\n |    |-- dog: string (nullable = true)\n\n\n\n\n\n\n\n\n\nWarning\n\n\n\ncopilot pretends that the flattening function above handles nested structure recursively. This is not the case.\nFix this"
  },
  {
    "objectID": "core/notebooks/notebook07_json-format.html#missing-data",
    "href": "core/notebooks/notebook07_json-format.html#missing-data",
    "title": "Using JSON data with Python",
    "section": "Missing data",
    "text": "Missing data\nA strategy can be to remove rows with missing data. dropna() has several options, explained below.\n\n\nCode\ndf.dropna().count()\n\n\n2\n\n\nIf we remove all lines with at least one missing value, we end up with an empty dataframe !\n\n\nCode\ndf.dropna(how='all').count()\n\n\n11292\n\n\ndropna() accepts the following arguments\n\nhow: can be 'any' or 'all'. If 'any', rows containing any null values will be dropped entirely (this is the default). If 'all', only rows which are entirely empty will be dropped.\nthresh: accepts an integer representing the “threshold” for how many empty cells a row must have before being dropped. tresh is a middle ground between how='any' and how='all'. As a result, the presence of thresh will override how\nsubset: accepts a list of column names. When a subset is present, N/A values will only be checked against the columns whose names are provided.\n\n\n\nCode\nn_columns = len(df.columns)\n\n\n\n\nCode\ndf.dropna(thresh=n_columns).count()\n\n\n2\n\n\n\n\nCode\ndf.dropna(thresh=n_columns-1).count()\n\n\n7550\n\n\n\n\nCode\ndf.dropna(thresh=n_columns-10).count()\n\n\n11292\n\n\n\n\nCode\ndf = df.dropna(subset=['postal_code', 'city', 'country', 'address_1'])\ndf.count()\n\n\n11292\n\n\nBut before this, let’s count the number of missing value for each column\n\n\nCode\n# For each column we create a new column containing 1 if the value is null and 0 otherwise.\n# We need to bast Boolean to Int so that we can use fn.sum after\nfor c in df.columns:\n    # Do not do this for _isnull columns (just in case you run this cell twice...)\n    if not c.endswith(\"_isnull\"):\n        df = df.withColumn(c + \"_isnull\", fn.isnull(col(c)).cast('int'))\n\n\n\n\nCode\ndf.head()\n\n\nRow(address_1='8 Joanna Ct', address_2='', center_classification_date='20121025', city='East Brunswick', classification='Class II', code_info='Lots a) 15952, 16270,16425, Exp 06/15; b)16459, 16466, 16467, Exp 07/15', country='United States', distribution_pattern='Nationwide', event_id='63384', initial_firm_notification='E-Mail', more_code_info=None, openfda=Row(application_number=None, brand_name=None, generic_name=None, is_original_packager=None, manufacturer_name=None, nui=None, original_packager_product_ndc=None, package_ndc=None, pharm_class_cs=None, pharm_class_epc=None, pharm_class_moa=None, pharm_class_pe=None, product_ndc=None, product_type=None, route=None, rxcui=None, spl_id=None, spl_set_id=None, substance_name=None, unii=None, upc=None), postal_code='08816-2108', product_description='Wal-Mucil 100% Natural Fiber, 100% Natural Psyllium Seed Husk, Fiber Laxative/Supplement, a)160 capsules per bottle (item #503663), and b) 320 capsules per bottle (Item #586143), Distributed by: Walgreen Co., 200 Wilmot Road, Deerfield, IL 60015-4616, www.walgreens.com, a) UPC 3-11917-08151-9, b) UPC 3-11917-07658-4', product_quantity='56,808 bottles', product_type='Drugs', reason_for_recall='Microbial Contamination of Non-Sterile Products: Product is being recalled due to possible microbial contamination by C. difficile discovered in the raw material.', recall_initiation_date='20120904', recall_number='D-026-2013', recalling_firm='Raritan Pharmaceuticals, Inc.', report_date='20121031', state='NJ', status='Terminated', termination_date='20141007', voluntary_mandated='Voluntary: Firm Initiated', address_1_isnull=0, address_2_isnull=0, center_classification_date_isnull=0, city_isnull=0, classification_isnull=0, code_info_isnull=0, country_isnull=0, distribution_pattern_isnull=0, event_id_isnull=0, initial_firm_notification_isnull=0, more_code_info_isnull=1, openfda_isnull=0, postal_code_isnull=0, product_description_isnull=0, product_quantity_isnull=0, product_type_isnull=0, reason_for_recall_isnull=0, recall_initiation_date_isnull=0, recall_number_isnull=0, recalling_firm_isnull=0, report_date_isnull=0, state_isnull=0, status_isnull=0, termination_date_isnull=0, voluntary_mandated_isnull=0)\n\n\n\n\nCode\n# Get the list of _isnull columns\nisnull_columns = [c for c in df.columns if c.endswith(\"_isnull\")]\n\n# On the _isnull columns :\n#  - we compute the sum to have the number of null values and rename the column\n#  - convert to pandas for better readability\n#  - transpose the pandas dataframe for better readability\nmissing_values = df.select(isnull_columns)\\\n    .agg(*[fn.sum(c).alias(c.replace(\"_isnull\", \"\")) for c in isnull_columns])\\\n    .toPandas()\n\nmissing_values.T\\\n    .rename({0: \"missing values\"}, axis=\"columns\")\n\n\n\n\n\n\n\n\n\nmissing values\n\n\n\n\naddress_1\n0\n\n\naddress_2\n0\n\n\ncenter_classification_date\n47\n\n\ncity\n0\n\n\nclassification\n0\n\n\ncode_info\n0\n\n\ncountry\n0\n\n\ndistribution_pattern\n0\n\n\nevent_id\n0\n\n\ninitial_firm_notification\n0\n\n\nmore_code_info\n11290\n\n\nopenfda\n0\n\n\npostal_code\n0\n\n\nproduct_description\n0\n\n\nproduct_quantity\n0\n\n\nproduct_type\n0\n\n\nreason_for_recall\n0\n\n\nrecall_initiation_date\n0\n\n\nrecall_number\n0\n\n\nrecalling_firm\n0\n\n\nreport_date\n0\n\n\nstate\n0\n\n\nstatus\n0\n\n\ntermination_date\n3741\n\n\nvoluntary_mandated\n0\n\n\n\n\n\n\n\nWe see that more_code_info is always null and that termination_date if often null. Most of the openfda* columns are also almost always empty.\nWe can keep only the columns with no missing values\n\n\nCode\n# This line can seem complicated, run pieces of each to understand\nkept_columns = list(\n    missing_values.columns[(missing_values.iloc[0] == 0).values]\n)\n\n\n\n\nCode\ndf_kept = df.select(kept_columns)\n\n\n\n\nCode\ndf_kept.head(2)\n\n\n[Row(address_1='8 Joanna Ct', address_2='', city='East Brunswick', classification='Class II', code_info='Lots a) 15952, 16270,16425, Exp 06/15; b)16459, 16466, 16467, Exp 07/15', country='United States', distribution_pattern='Nationwide', event_id='63384', initial_firm_notification='E-Mail', openfda=Row(application_number=None, brand_name=None, generic_name=None, is_original_packager=None, manufacturer_name=None, nui=None, original_packager_product_ndc=None, package_ndc=None, pharm_class_cs=None, pharm_class_epc=None, pharm_class_moa=None, pharm_class_pe=None, product_ndc=None, product_type=None, route=None, rxcui=None, spl_id=None, spl_set_id=None, substance_name=None, unii=None, upc=None), postal_code='08816-2108', product_description='Wal-Mucil 100% Natural Fiber, 100% Natural Psyllium Seed Husk, Fiber Laxative/Supplement, a)160 capsules per bottle (item #503663), and b) 320 capsules per bottle (Item #586143), Distributed by: Walgreen Co., 200 Wilmot Road, Deerfield, IL 60015-4616, www.walgreens.com, a) UPC 3-11917-08151-9, b) UPC 3-11917-07658-4', product_quantity='56,808 bottles', product_type='Drugs', reason_for_recall='Microbial Contamination of Non-Sterile Products: Product is being recalled due to possible microbial contamination by C. difficile discovered in the raw material.', recall_initiation_date='20120904', recall_number='D-026-2013', recalling_firm='Raritan Pharmaceuticals, Inc.', report_date='20121031', state='NJ', status='Terminated', voluntary_mandated='Voluntary: Firm Initiated'),\n Row(address_1='8 Joanna Ct', address_2='', city='East Brunswick', classification='Class II', code_info='Lot 15087, Exp 08/15', country='United States', distribution_pattern='Nationwide', event_id='63384', initial_firm_notification='E-Mail', openfda=Row(application_number=None, brand_name=None, generic_name=None, is_original_packager=None, manufacturer_name=None, nui=None, original_packager_product_ndc=None, package_ndc=None, pharm_class_cs=None, pharm_class_epc=None, pharm_class_moa=None, pharm_class_pe=None, product_ndc=None, product_type=None, route=None, rxcui=None, spl_id=None, spl_set_id=None, substance_name=None, unii=None, upc=None), postal_code='08816-2108', product_description='Premier Value Fiber Plus Calcium Supplement Capsules, 120 capsules per bottle, Distributed by: Chain Drug Consortium, LLC, Boca Raton, FL, UPC 8-40986-01987-6', product_quantity='96 bottles', product_type='Drugs', reason_for_recall='Microbial Contamination of Non-Sterile Products: Product is being recalled due to possible microbial contamination by C. difficile discovered in the raw material.', recall_initiation_date='20120904', recall_number='D-031-2013', recalling_firm='Raritan Pharmaceuticals, Inc.', report_date='20121031', state='NJ', status='Terminated', voluntary_mandated='Voluntary: Firm Initiated')]\n\n\n\n\nCode\ndf_kept.printSchema()\n\n\nroot\n |-- address_1: string (nullable = true)\n |-- address_2: string (nullable = true)\n |-- city: string (nullable = true)\n |-- classification: string (nullable = true)\n |-- code_info: string (nullable = true)\n |-- country: string (nullable = true)\n |-- distribution_pattern: string (nullable = true)\n |-- event_id: string (nullable = true)\n |-- initial_firm_notification: string (nullable = true)\n |-- openfda: struct (nullable = true)\n |    |-- application_number: array (nullable = true)\n |    |    |-- element: string (containsNull = true)\n |    |-- brand_name: array (nullable = true)\n |    |    |-- element: string (containsNull = true)\n |    |-- generic_name: array (nullable = true)\n |    |    |-- element: string (containsNull = true)\n |    |-- is_original_packager: array (nullable = true)\n |    |    |-- element: boolean (containsNull = true)\n |    |-- manufacturer_name: array (nullable = true)\n |    |    |-- element: string (containsNull = true)\n |    |-- nui: array (nullable = true)\n |    |    |-- element: string (containsNull = true)\n |    |-- original_packager_product_ndc: array (nullable = true)\n |    |    |-- element: string (containsNull = true)\n |    |-- package_ndc: array (nullable = true)\n |    |    |-- element: string (containsNull = true)\n |    |-- pharm_class_cs: array (nullable = true)\n |    |    |-- element: string (containsNull = true)\n |    |-- pharm_class_epc: array (nullable = true)\n |    |    |-- element: string (containsNull = true)\n |    |-- pharm_class_moa: array (nullable = true)\n |    |    |-- element: string (containsNull = true)\n |    |-- pharm_class_pe: array (nullable = true)\n |    |    |-- element: string (containsNull = true)\n |    |-- product_ndc: array (nullable = true)\n |    |    |-- element: string (containsNull = true)\n |    |-- product_type: array (nullable = true)\n |    |    |-- element: string (containsNull = true)\n |    |-- route: array (nullable = true)\n |    |    |-- element: string (containsNull = true)\n |    |-- rxcui: array (nullable = true)\n |    |    |-- element: string (containsNull = true)\n |    |-- spl_id: array (nullable = true)\n |    |    |-- element: string (containsNull = true)\n |    |-- spl_set_id: array (nullable = true)\n |    |    |-- element: string (containsNull = true)\n |    |-- substance_name: array (nullable = true)\n |    |    |-- element: string (containsNull = true)\n |    |-- unii: array (nullable = true)\n |    |    |-- element: string (containsNull = true)\n |    |-- upc: array (nullable = true)\n |    |    |-- element: string (containsNull = true)\n |-- postal_code: string (nullable = true)\n |-- product_description: string (nullable = true)\n |-- product_quantity: string (nullable = true)\n |-- product_type: string (nullable = true)\n |-- reason_for_recall: string (nullable = true)\n |-- recall_initiation_date: string (nullable = true)\n |-- recall_number: string (nullable = true)\n |-- recalling_firm: string (nullable = true)\n |-- report_date: string (nullable = true)\n |-- state: string (nullable = true)\n |-- status: string (nullable = true)\n |-- voluntary_mandated: string (nullable = true)\n\n\n\n\n\nCode\ndf_kept.count()\n\n\n11292"
  },
  {
    "objectID": "core/notebooks/notebook07_json-format.html#filtering-by-string-values",
    "href": "core/notebooks/notebook07_json-format.html#filtering-by-string-values",
    "title": "Using JSON data with Python",
    "section": "Filtering by string values",
    "text": "Filtering by string values\nCases from South San Francisco\n\n\nCode\ndf.filter(df.city == \"South San Francisco\")\\\n    .toPandas()\n\n\n\n\n\n\n\n\n\naddress_1\naddress_2\ncenter_classification_date\ncity\nclassification\ncode_info\ncountry\ndistribution_pattern\nevent_id\ninitial_firm_notification\n...\nproduct_type_isnull\nreason_for_recall_isnull\nrecall_initiation_date_isnull\nrecall_number_isnull\nrecalling_firm_isnull\nreport_date_isnull\nstate_isnull\nstatus_isnull\ntermination_date_isnull\nvoluntary_mandated_isnull\n\n\n\n\n0\n1 DNA Way\n\n20130122\nSouth San Francisco\nClass I\nLot #: 454138, Exp 07/14 (containing Trastuzum...\nUnited States\nNationwide\n63258\nE-Mail\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n1\n1 DNA Way\n\n20121204\nSouth San Francisco\nClass II\nLot# 936674 Exp. 09/30/13\nUnited States\nNationwide\n63243\nLetter\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n2\n1 DNA Way\n\n20121204\nSouth San Francisco\nClass II\nLot # 936670 Exp. 09/30/13\nUnited States\nNationwide\n63243\nLetter\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n3\n1 DNA Way\n\n20121220\nSouth San Francisco\nClass III\nLot #: a) M1365B01, Exp 04/15; b) M1365, Exp 0...\nUnited States\nNationwide\n63874\nLetter\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n4\n1 Dna Way\n\n20180213\nSouth San Francisco\nClass III\nLot # 3141989, EXP 08/31/2019\nUnited States\nDistributed throughout the United States\n79175\nLetter\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n5\n5000 Shoreline Ct. Ste 200\n\n20130201\nSouth San Francisco\nClass III\nLot #: MA00AD5, Exp: 11/30/2014\nUnited States\nCA & VA\n64151\nLetter\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n6\n1 Dna Way\n\n20171201\nSouth San Francisco\nClass I\nLot# 3128243, 3141239, EXP. 9/30/2018; 3166728...\nUnited States\nNationwide in the USA\n78088\nLetter\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n7\n1 Dna Way\n\n20170405\nSouth San Francisco\nClass II\nB1009MC, B1009M9, B1009MA; Exp. 02/18 B1009MT...\nUnited States\nNJ and IL\n76726\nLetter\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n\n\n8 rows × 50 columns\n\n\n\n\n\n\n\n\n\nCaution\n\n\n\nOnce again, we use .toPandas() to pretty format the results in the notebook.\nBut it’s a BAD idea to do this if the spark DataFrame is large, since it requires a collect()\n\n\nAside from filtering strings by a perfect match, there are plenty of other powerful ways to filter by strings in pyspark :\n\ndf.filter(df.city.contains('San Francisco')): returns rows where strings of a column contain a provided substring. In our example, filtering by rows which contain the substring “San Francisco” would be a good way to get all rows in San Francisco, instead of just “South San Francisco”.\ndf.filter(df.city.startswith('San')): Returns rows where a string starts with a provided substring.\ndf.filter(df.city.endswith('ice')): Returns rows where a string starts with a provided substring.\ndf.filter(df.city.isNull()): Returns rows where values in a provided column are null.\ndf.filter(df.city.isNotNull()): Opposite of the above.\ndf.filter(df.city.like('San%')): Performs a SQL-like query containing the LIKE clause.\ndf.filter(df.city.rlike('[A-Z]*ice$')): Performs a regexp filter.\ndf.filter(df.city.isin('San Francisco', 'Los Angeles')): Looks for rows where the string value of a column matches any of the provided strings exactly.\n\nYou can try some of these to understand\n\n\nCode\ndf.filter(df.city.contains('San Francisco'))\\\n    .toPandas()\n\n\n\n\n\n\n\n\n\naddress_1\naddress_2\ncenter_classification_date\ncity\nclassification\ncode_info\ncountry\ndistribution_pattern\nevent_id\ninitial_firm_notification\n...\nproduct_type_isnull\nreason_for_recall_isnull\nrecall_initiation_date_isnull\nrecall_number_isnull\nrecalling_firm_isnull\nreport_date_isnull\nstate_isnull\nstatus_isnull\ntermination_date_isnull\nvoluntary_mandated_isnull\n\n\n\n\n0\n1 DNA Way\n\n20130122\nSouth San Francisco\nClass I\nLot #: 454138, Exp 07/14 (containing Trastuzum...\nUnited States\nNationwide\n63258\nE-Mail\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n1\n1 DNA Way\n\n20121204\nSouth San Francisco\nClass II\nLot# 936674 Exp. 09/30/13\nUnited States\nNationwide\n63243\nLetter\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n2\n1 DNA Way\n\n20121204\nSouth San Francisco\nClass II\nLot # 936670 Exp. 09/30/13\nUnited States\nNationwide\n63243\nLetter\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n3\n1 DNA Way\n\n20121220\nSouth San Francisco\nClass III\nLot #: a) M1365B01, Exp 04/15; b) M1365, Exp 0...\nUnited States\nNationwide\n63874\nLetter\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n4\n801 Irving St\n\n20160706\nSan Francisco\nClass II\nAll lots compounded between 03/24/2015 and 03/...\nUnited States\nU.S. Including: CA, HI, NM\n73662\nPress Release\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n73\n801 Irving St\n\n20160706\nSan Francisco\nClass II\nAll lots compounded between 03/24/2015 and 03/...\nUnited States\nU.S. Including: CA, HI, NM\n73662\nPress Release\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n74\n801 Irving St\n\n20160706\nSan Francisco\nClass II\nAll lots compounded between 03/24/2015 and 03/...\nUnited States\nU.S. Including: CA, HI, NM\n73662\nPress Release\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n75\n5000 Shoreline Ct. Ste 200\n\n20130201\nSouth San Francisco\nClass III\nLot #: MA00AD5, Exp: 11/30/2014\nUnited States\nCA & VA\n64151\nLetter\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n76\n1 Dna Way\n\n20171201\nSouth San Francisco\nClass I\nLot# 3128243, 3141239, EXP. 9/30/2018; 3166728...\nUnited States\nNationwide in the USA\n78088\nLetter\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n77\n1 Dna Way\n\n20170405\nSouth San Francisco\nClass II\nB1009MC, B1009M9, B1009MA; Exp. 02/18 B1009MT...\nUnited States\nNJ and IL\n76726\nLetter\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n\n\n78 rows × 50 columns\n\n\n\n\n\nCode\n(\n    df.filter(df.city.isin('San Francisco', 'Los Angeles'))\n      .toPandas()\n)\n\n\n\n\n\n\n\n\n\naddress_1\naddress_2\ncenter_classification_date\ncity\nclassification\ncode_info\ncountry\ndistribution_pattern\nevent_id\ninitial_firm_notification\n...\nproduct_type_isnull\nreason_for_recall_isnull\nrecall_initiation_date_isnull\nrecall_number_isnull\nrecalling_firm_isnull\nreport_date_isnull\nstate_isnull\nstatus_isnull\ntermination_date_isnull\nvoluntary_mandated_isnull\n\n\n\n\n0\n1990 Westwood Blvd Ste 135\n\n20170731\nLos Angeles\nClass II\nLot # 03022017+44906; BUD 08/29/17\nUnited States\nThere were only one customer in California\n77757\nLetter\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n1\n450 N Van Ness Ave Apt 107\n\n20180411\nLos Angeles\nClass I\nUPC # 891656002209, exp date 12/31/2021\nUnited States\nProduct was distributed online via eBay.\n79476\nE-Mail\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n2\n801 Irving St\n\n20160706\nSan Francisco\nClass II\nAll lots compounded between 03/24/2015 and 03/...\nUnited States\nU.S. Including: CA, HI, NM\n73662\nPress Release\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n3\n801 Irving St\n\n20160706\nSan Francisco\nClass II\nAll lots compounded between 03/24/2015 and 03/...\nUnited States\nU.S. Including: CA, HI, NM\n73662\nPress Release\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n4\n801 Irving St\n\n20160706\nSan Francisco\nClass II\nAll lots compounded between 03/24/2015 and 03/...\nUnited States\nU.S. Including: CA, HI, NM\n73662\nPress Release\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n70\n801 Irving St\n\n20160706\nSan Francisco\nClass II\nAll lots compounded between 03/24/2015 and 03/...\nUnited States\nU.S. Including: CA, HI, NM\n73662\nPress Release\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n71\n801 Irving St\n\n20160706\nSan Francisco\nClass II\nAll lots compounded between 03/24/2015 and 03/...\nUnited States\nU.S. Including: CA, HI, NM\n73662\nPress Release\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n72\n154 W 131st St\n\n20170524\nLos Angeles\nClass I\nAll lots within expiry\nUnited States\nNationwide in the USA and Puerto Rico.\n77009\nPress Release\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n73\n154 W 131st St\n\n20170524\nLos Angeles\nClass I\nAll lots within expiry\nUnited States\nNationwide in the USA and Puerto Rico.\n77009\nPress Release\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n74\n304 E 11th St\n\n20180313\nLos Angeles\nClass I\nLot #: MFD:10.15.2017, Exp.10/14/2019.\nUnited States\nProduct was distributed in California to onlin...\n79327\nPress Release\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n\n\n75 rows × 50 columns"
  },
  {
    "objectID": "core/notebooks/notebook07_json-format.html#filtering-by-date-values",
    "href": "core/notebooks/notebook07_json-format.html#filtering-by-date-values",
    "title": "Using JSON data with Python",
    "section": "Filtering by Date Values",
    "text": "Filtering by Date Values\nIn addition to filtering by strings, we can also filter by columns where the values are stored as dates or datetimes (or strings that can be inferred as dates). Perhaps the most useful way to filter dates is by using the between() method, which allows us to find results within a certain date range. Here we find all the results which were reported in the years 2013 and 2014:\n\n\nCode\n( \n    df\n        .filter(df.city == \"South San Francisco\")\n        .filter(df.report_date.between('2013-01-01 00:00:00','2015-03-11 00:00:00'))\n        .toPandas()\n)\n\n\n\n\n\n\n\n\n\naddress_1\naddress_2\ncenter_classification_date\ncity\nclassification\ncode_info\ncountry\ndistribution_pattern\nevent_id\ninitial_firm_notification\n...\nproduct_type_isnull\nreason_for_recall_isnull\nrecall_initiation_date_isnull\nrecall_number_isnull\nrecalling_firm_isnull\nreport_date_isnull\nstate_isnull\nstatus_isnull\ntermination_date_isnull\nvoluntary_mandated_isnull\n\n\n\n\n0\n1 DNA Way\n\n20130122\nSouth San Francisco\nClass I\nLot #: 454138, Exp 07/14 (containing Trastuzum...\nUnited States\nNationwide\n63258\nE-Mail\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n1\n5000 Shoreline Ct. Ste 200\n\n20130201\nSouth San Francisco\nClass III\nLot #: MA00AD5, Exp: 11/30/2014\nUnited States\nCA & VA\n64151\nLetter\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n\n\n2 rows × 50 columns\n\n\n\n\n\n\n\n\n\nCaution\n\n\n\nIs Spark smart enough to understand that the string in column report_date contains a date?\n\n\n\n\nCode\ndf.filter(df.city == \"South San Francisco\")\\\n    .filter(df.center_classification_date.between('2013-01-01 00:00:00','2013-12-31 00:00:00'))\\\n    .toPandas()\n\n\n\n\n\n\n\n\n\naddress_1\naddress_2\ncenter_classification_date\ncity\nclassification\ncode_info\ncountry\ndistribution_pattern\nevent_id\ninitial_firm_notification\n...\nproduct_type_isnull\nreason_for_recall_isnull\nrecall_initiation_date_isnull\nrecall_number_isnull\nrecalling_firm_isnull\nreport_date_isnull\nstate_isnull\nstatus_isnull\ntermination_date_isnull\nvoluntary_mandated_isnull\n\n\n\n\n\n\n0 rows × 50 columns\n\n\n\n\n\nCode\ndf_dates = df.select([c for c in df.columns if c.endswith(\"date\")])\n\ndf_dates.printSchema()\n\n\nroot\n |-- center_classification_date: string (nullable = true)\n |-- recall_initiation_date: string (nullable = true)\n |-- report_date: string (nullable = true)\n |-- termination_date: string (nullable = true)\n\n\n\n\n\nCode\ndf_dates.show(5)\n\n\n+--------------------------+----------------------+-----------+----------------+\n|center_classification_date|recall_initiation_date|report_date|termination_date|\n+--------------------------+----------------------+-----------+----------------+\n|                  20121025|              20120904|   20121031|        20141007|\n|                  20121025|              20120904|   20121031|        20141007|\n|                  20121106|              20121015|   20121114|        20130325|\n|                  20121220|              20121204|   20121226|        20140429|\n|                  20121231|              20120926|   20130109|        20161007|\n+--------------------------+----------------------+-----------+----------------+\nonly showing top 5 rows\n\n\n\nColumns are not dates (DateType) but strings (StringType). When comparing report_date with '2013-01-01 00:00:00' and '2015-03-11 00:00:00', we are comparing strings and are lucky enough that in unicode '-' &lt; '0' &lt; '...' &lt; '9' so that 2013-.... is less that any string starting with 20130..., while any string starting with 2013... is less than any string starting with 2015...\n\n\n\n\n\n\nCaution\n\n\n\nIf some field in a Json string is meant to represent a date or a datetime object, spark should be given a hint.\nJson loaders (from Python) as well as the Spark Json reader have optional arguments that can be used to indicate the date parser to be used.\n\n\n\n\n\n\n\n\nNote\n\n\n\nWe have to tell the json loader about two things:\n\nwhich columns should be read as dates\nwhich format should be used for those columns\n\nThe first point can be settled using the schema argument of .json() method (see Documentation)\n\n\n\n\nCode\nze_schema = df.schema \n\nlist_fields = []\n\nfor f in ze_schema.fields:\n  if f.name.endswith('date'):\n    list_fields.append(StructField(f.name, DateType(), True))\n  else:\n    list_fields.append(f)\n\nze_schema = StructType(list_fields)\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[67], line 7\n      5 for f in ze_schema.fields:\n      6   if f.name.endswith('date'):\n----&gt; 7     list_fields.append(StructField(f.name, DateType(), True))\n      8   else:\n      9     list_fields.append(f)\n\nNameError: name 'DateType' is not defined\n\n\n\n\n\nCode\n# Alternative syntax using a dictionary of options\noptions = {\n    \"dateFormat\": \"yyyyMMdd\",\n    \"multiLine\": \"true\"\n}\n\ndf = (\n    spark.read\n        .options(**options)\n        .json(filename, ze_schema)\n)\n\n\n\n\nCode\ndf.select([c for c in df.columns if c.endswith(\"date\")]).printSchema()\n\n\nroot\n |-- center_classification_date: string (nullable = true)\n |-- recall_initiation_date: string (nullable = true)\n |-- report_date: string (nullable = true)\n |-- termination_date: string (nullable = true)\n\n\n\n\n\nCode\n(\ndf.filter(df.city == \"South San Francisco\")\n  .filter(df.center_classification_date.between('2013-01-01 00:00:00','2013-12-31 00:00:00'))\n  .toPandas()\n)\n\n\n\n\n\n\n\n\n\naddress_1\naddress_2\ncenter_classification_date\ncity\nclassification\ncode_info\ncountry\ndistribution_pattern\nevent_id\ninitial_firm_notification\n...\nproduct_type_isnull\nreason_for_recall_isnull\nrecall_initiation_date_isnull\nrecall_number_isnull\nrecalling_firm_isnull\nreport_date_isnull\nstate_isnull\nstatus_isnull\ntermination_date_isnull\nvoluntary_mandated_isnull\n\n\n\n\n\n\n0 rows × 50 columns"
  },
  {
    "objectID": "core/notebooks/notebook07_json-format.html#handling-complex-types",
    "href": "core/notebooks/notebook07_json-format.html#handling-complex-types",
    "title": "Using JSON data with Python",
    "section": "Handling complex types",
    "text": "Handling complex types\nBridging the gap between tabular and semi-structured data.\n\n\n\n\n\n\nNote\n\n\n\nSQL, R, Pandas …\n\n\nstruct, array, map\n\n\nCode\n# struct\n\n\nThe problems we faced after loading data from the json file pertained to the fact that column fda was of complex StrucType() type. We shall revisit this dataframe.\n\n\nCode\ndf = spark.read.json(filename, multiLine=True)\n\n\nThe dataframe schema df.schema which is of type StructType (defined in pyspark.sql.types) can be converted to a json string which in turn can be converted into a Python dictionary.\n\n\nCode\ndf = spark.read.json(filename, multiLine=True)\n\nsj = json.loads(df.schema.json())\n\n\nWe equip the dataframe with a primary key\n\n\nCode\nfrom pyspark.sql import Window\n\nw = Window.orderBy(col(\"center_classification_date\"))\n\ndf = (\n  df\n    .withColumn(\"row_id\", fn.row_number().over(w))\n)\n\n\n\n\nCode\n[(f['name'], f['type'])  \n    for f in sj['fields'] if not isinstance(f['type'], str)]\n\n\n[('openfda',\n  {'fields': [{'metadata': {},\n     'name': 'application_number',\n     'nullable': True,\n     'type': {'containsNull': True, 'elementType': 'string', 'type': 'array'}},\n    {'metadata': {},\n     'name': 'brand_name',\n     'nullable': True,\n     'type': {'containsNull': True, 'elementType': 'string', 'type': 'array'}},\n    {'metadata': {},\n     'name': 'generic_name',\n     'nullable': True,\n     'type': {'containsNull': True, 'elementType': 'string', 'type': 'array'}},\n    {'metadata': {},\n     'name': 'is_original_packager',\n     'nullable': True,\n     'type': {'containsNull': True,\n      'elementType': 'boolean',\n      'type': 'array'}},\n    {'metadata': {},\n     'name': 'manufacturer_name',\n     'nullable': True,\n     'type': {'containsNull': True, 'elementType': 'string', 'type': 'array'}},\n    {'metadata': {},\n     'name': 'nui',\n     'nullable': True,\n     'type': {'containsNull': True, 'elementType': 'string', 'type': 'array'}},\n    {'metadata': {},\n     'name': 'original_packager_product_ndc',\n     'nullable': True,\n     'type': {'containsNull': True, 'elementType': 'string', 'type': 'array'}},\n    {'metadata': {},\n     'name': 'package_ndc',\n     'nullable': True,\n     'type': {'containsNull': True, 'elementType': 'string', 'type': 'array'}},\n    {'metadata': {},\n     'name': 'pharm_class_cs',\n     'nullable': True,\n     'type': {'containsNull': True, 'elementType': 'string', 'type': 'array'}},\n    {'metadata': {},\n     'name': 'pharm_class_epc',\n     'nullable': True,\n     'type': {'containsNull': True, 'elementType': 'string', 'type': 'array'}},\n    {'metadata': {},\n     'name': 'pharm_class_moa',\n     'nullable': True,\n     'type': {'containsNull': True, 'elementType': 'string', 'type': 'array'}},\n    {'metadata': {},\n     'name': 'pharm_class_pe',\n     'nullable': True,\n     'type': {'containsNull': True, 'elementType': 'string', 'type': 'array'}},\n    {'metadata': {},\n     'name': 'product_ndc',\n     'nullable': True,\n     'type': {'containsNull': True, 'elementType': 'string', 'type': 'array'}},\n    {'metadata': {},\n     'name': 'product_type',\n     'nullable': True,\n     'type': {'containsNull': True, 'elementType': 'string', 'type': 'array'}},\n    {'metadata': {},\n     'name': 'route',\n     'nullable': True,\n     'type': {'containsNull': True, 'elementType': 'string', 'type': 'array'}},\n    {'metadata': {},\n     'name': 'rxcui',\n     'nullable': True,\n     'type': {'containsNull': True, 'elementType': 'string', 'type': 'array'}},\n    {'metadata': {},\n     'name': 'spl_id',\n     'nullable': True,\n     'type': {'containsNull': True, 'elementType': 'string', 'type': 'array'}},\n    {'metadata': {},\n     'name': 'spl_set_id',\n     'nullable': True,\n     'type': {'containsNull': True, 'elementType': 'string', 'type': 'array'}},\n    {'metadata': {},\n     'name': 'substance_name',\n     'nullable': True,\n     'type': {'containsNull': True, 'elementType': 'string', 'type': 'array'}},\n    {'metadata': {},\n     'name': 'unii',\n     'nullable': True,\n     'type': {'containsNull': True, 'elementType': 'string', 'type': 'array'}},\n    {'metadata': {},\n     'name': 'upc',\n     'nullable': True,\n     'type': {'containsNull': True,\n      'elementType': 'string',\n      'type': 'array'}}],\n   'type': 'struct'})]\n\n\nColumn openfda has type StrucType() with fields with composite type.\n\n\nCode\n{f.dataType  for f in df.schema.fields if not f.dataType==StringType()}\n\n\n{IntegerType(),\n StructType([StructField('application_number', ArrayType(StringType(), True), True), StructField('brand_name', ArrayType(StringType(), True), True), StructField('generic_name', ArrayType(StringType(), True), True), StructField('is_original_packager', ArrayType(BooleanType(), True), True), StructField('manufacturer_name', ArrayType(StringType(), True), True), StructField('nui', ArrayType(StringType(), True), True), StructField('original_packager_product_ndc', ArrayType(StringType(), True), True), StructField('package_ndc', ArrayType(StringType(), True), True), StructField('pharm_class_cs', ArrayType(StringType(), True), True), StructField('pharm_class_epc', ArrayType(StringType(), True), True), StructField('pharm_class_moa', ArrayType(StringType(), True), True), StructField('pharm_class_pe', ArrayType(StringType(), True), True), StructField('product_ndc', ArrayType(StringType(), True), True), StructField('product_type', ArrayType(StringType(), True), True), StructField('route', ArrayType(StringType(), True), True), StructField('rxcui', ArrayType(StringType(), True), True), StructField('spl_id', ArrayType(StringType(), True), True), StructField('spl_set_id', ArrayType(StringType(), True), True), StructField('substance_name', ArrayType(StringType(), True), True), StructField('unii', ArrayType(StringType(), True), True), StructField('upc', ArrayType(StringType(), True), True)])}\n\n\n\n\nCode\n{f['type']['type']\n    for f in sj['fields'] if not isinstance(f['type'], str)}\n\n\n{'struct'}\n\n\nProjecting on row_id and openfda.* leads to a (partially) flattened datafame, that, thanks to the row_id column can be joined with the original dataframe.\n\n\nCode\ndf_proj = df.select('row_id', 'openfda.*')\n\ndf_proj.printSchema()\n\n\nroot\n |-- row_id: integer (nullable = false)\n |-- application_number: array (nullable = true)\n |    |-- element: string (containsNull = true)\n |-- brand_name: array (nullable = true)\n |    |-- element: string (containsNull = true)\n |-- generic_name: array (nullable = true)\n |    |-- element: string (containsNull = true)\n |-- is_original_packager: array (nullable = true)\n |    |-- element: boolean (containsNull = true)\n |-- manufacturer_name: array (nullable = true)\n |    |-- element: string (containsNull = true)\n |-- nui: array (nullable = true)\n |    |-- element: string (containsNull = true)\n |-- original_packager_product_ndc: array (nullable = true)\n |    |-- element: string (containsNull = true)\n |-- package_ndc: array (nullable = true)\n |    |-- element: string (containsNull = true)\n |-- pharm_class_cs: array (nullable = true)\n |    |-- element: string (containsNull = true)\n |-- pharm_class_epc: array (nullable = true)\n |    |-- element: string (containsNull = true)\n |-- pharm_class_moa: array (nullable = true)\n |    |-- element: string (containsNull = true)\n |-- pharm_class_pe: array (nullable = true)\n |    |-- element: string (containsNull = true)\n |-- product_ndc: array (nullable = true)\n |    |-- element: string (containsNull = true)\n |-- product_type: array (nullable = true)\n |    |-- element: string (containsNull = true)\n |-- route: array (nullable = true)\n |    |-- element: string (containsNull = true)\n |-- rxcui: array (nullable = true)\n |    |-- element: string (containsNull = true)\n |-- spl_id: array (nullable = true)\n |    |-- element: string (containsNull = true)\n |-- spl_set_id: array (nullable = true)\n |    |-- element: string (containsNull = true)\n |-- substance_name: array (nullable = true)\n |    |-- element: string (containsNull = true)\n |-- unii: array (nullable = true)\n |    |-- element: string (containsNull = true)\n |-- upc: array (nullable = true)\n |    |-- element: string (containsNull = true)\n\n\n\nWe can inspect the length of the arrays.\n\n\nCode\n# array\ndf_proj.select(\n    fn.max(fn.size(col(\"application_number\"))).alias(\"Max\"), \n    fn.min(fn.size(col(\"application_number\"))).alias(\"min\"), \n    fn.avg(fn.size(col(\"application_number\"))).alias(\"Mean\")).show(1)\n\n\n+---+---+-------------------+\n|Max|min|               Mean|\n+---+---+-------------------+\n|  2| -1|-0.7380446333687567|\n+---+---+-------------------+\n\n\n\nIn some rows, the size of the array is -1 because the field is NULL.\n\n\nCode\n(\n  df_proj\n    .where(fn.size(col(\"application_number\"))&gt;1)\n    .select(\"row_id\")\n    .show(5)\n)\n\n\n25/04/03 15:09:19 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n25/04/03 15:09:19 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n25/04/03 15:09:19 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n25/04/03 15:09:19 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n25/04/03 15:09:19 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n\n\n+------+\n|row_id|\n+------+\n|  7159|\n|  7344|\n+------+\n\n\n\nAn array column can be exploded. This is like pivoting into long form. The result contains one row per item in the array.\n\n\nCode\n(\n  df_proj\n    .select('row_id', 'application_number')\n    .withColumn(\"exploded\", \n                fn.explode(col(\"application_number\")))\n    .select('row_id', 'exploded')\n    .groupBy('row_id')\n    .agg(fn.count('exploded').alias(\"n_lignes\"))\n    .where(\"n_lignes &gt; 1\")\n    .show(5)\n)\n\n\n25/04/03 15:09:19 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n25/04/03 15:09:19 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n25/04/03 15:09:19 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n25/04/03 15:09:19 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n\n\n+------+--------+\n|row_id|n_lignes|\n+------+--------+\n|  7159|       2|\n|  7344|       2|\n+------+--------+"
  },
  {
    "objectID": "core/notebooks/notebook08_webdata.html",
    "href": "core/notebooks/notebook08_webdata.html",
    "title": "Using with pyspark for data preprocessing",
    "section": "",
    "text": "We want to use pyspark to preprocess a potentially huge dataset used for web-marketing."
  },
  {
    "objectID": "core/notebooks/notebook08_webdata.html#data-description",
    "href": "core/notebooks/notebook08_webdata.html#data-description",
    "title": "Using with pyspark for data preprocessing",
    "section": "Data description",
    "text": "Data description\nThe data is a parquet file which contains a dataframe with 8 columns:\n\nxid: unique user id\naction: type of action. ‘C’ is a click, ‘O’ or ‘VSL’ is a web-display\ndate: date of the action\nwebsite_id: unique id of the website\nurl: url of the webpage\ncategory_id: id of the display\nzipcode: postal zipcode of the user\ndevice: type of device used by the user"
  },
  {
    "objectID": "core/notebooks/notebook08_webdata.html#q1.-some-statistics-computations",
    "href": "core/notebooks/notebook08_webdata.html#q1.-some-statistics-computations",
    "title": "Using with pyspark for data preprocessing",
    "section": "Q1. Some statistics / computations",
    "text": "Q1. Some statistics / computations\nUsing pyspark.sql we want to do the following things:\n\nCompute the total number of unique users\nConstruct a column containing the total number of actions per user\nConstruct a column containing the number of days since the last action of the user\nConstruct a column containing the number of actions of each user for each modality of device"
  },
  {
    "objectID": "core/notebooks/notebook08_webdata.html#q2.-binary-classification",
    "href": "core/notebooks/notebook08_webdata.html#q2.-binary-classification",
    "title": "Using with pyspark for data preprocessing",
    "section": "Q2. Binary classification",
    "text": "Q2. Binary classification\nThen, we want to construct a classifier to predict the click on the category 1204. Here is an agenda for this:\n\nConstruction of a features matrix for which each line corresponds to the information concerning a user.\nIn this matrix, we need to keep only the users that have been exposed to the display in category 1204\nUsing this training dataset, train a binary classifier, and evaluate your classifier using a precision / recall curve computed on test data."
  },
  {
    "objectID": "core/notebooks/notebook08_webdata.html#compute-the-total-number-of-unique-users",
    "href": "core/notebooks/notebook08_webdata.html#compute-the-total-number-of-unique-users",
    "title": "Using with pyspark for data preprocessing",
    "section": "Compute the total number of unique users",
    "text": "Compute the total number of unique users\n\n\nCode\n( \n    df.select('xid')\n      .distinct()\n      .count()\n)\n\n\n473761\n\n\n\n\nCode\ndef foo(x):\n   c = len(set(x))\n   print(c)\n   return c\n\n\n\n\nCode\nfoo([1, 1, 2])\n\n\n2\n\n\n2\n\n\n\n\nCode\ndf.rdd.map(lambda x : x.xid).foreachPartition(foo)\n\n\n0\n0\n0\n0\n0\n0\n[Stage 9:=============================&gt;                            (6 + 6) / 12]78120\n[Stage 9:=================================&gt;                        (7 + 5) / 12]78865\n78636\n79296\n79090\n79754\n                                                                                \n\n\n\n\nCode\n78120 + 78636 + 79090 + 78865 + 79296 + 79754\n\n\n473761\n\n\nThis might pump up some computational resources\n\n\nCode\n( \n    df.select('xid')\n      .distinct() \n      .explain()\n)\n\n\n== Physical Plan ==\nAdaptiveSparkPlan isFinalPlan=false\n+- HashAggregate(keys=[xid#0], functions=[])\n   +- Exchange hashpartitioning(xid#0, 200), ENSURE_REQUIREMENTS, [plan_id=136]\n      +- HashAggregate(keys=[xid#0], functions=[])\n         +- FileScan parquet [xid#0] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/home/boucheron/sandbox/IFEBY310/core/notebooks/data/webdata.parq..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct&lt;xid:string&gt;\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe distinct values of xid seem to be evenly spread among the six files making the parquet directory"
  },
  {
    "objectID": "core/notebooks/notebook08_webdata.html#construct-a-column-containing-the-total-number-of-actions-per-user",
    "href": "core/notebooks/notebook08_webdata.html#construct-a-column-containing-the-total-number-of-actions-per-user",
    "title": "Using with pyspark for data preprocessing",
    "section": "Construct a column containing the total number of actions per user",
    "text": "Construct a column containing the total number of actions per user\n\n\nCode\nxid_partition = Window.partitionBy('xid')\n\nn_events = func.count(col('action')).over(xid_partition)\n\ndf = df.withColumn('n_events', n_events)\n\ndf.show(n=2)\n\n\n+--------------------+------+-------------------+----------+--------------------+-----------+-------+------+--------+\n|                 xid|action|               date|website_id|                 url|category_id|zipcode|device|n_events|\n+--------------------+------+-------------------+----------+--------------------+-----------+-------+------+--------+\n|0006cea7-1679-426...|     O|2016-12-26 13:41:08|        51|https://www.footl...|     1002.0|  34290|   TAB|       1|\n|000893c8-a14b-4f3...|     O|2016-12-23 16:18:37|        56|http://blague.dum...|     1002.0|   NULL|   DSK|       1|\n+--------------------+------+-------------------+----------+--------------------+-----------+-------+------+--------+\nonly showing top 2 rows\n\n\n\n\n\nCode\n( \n  df\n    .groupBy('xid')\n    .agg(func.count('action'))\n    .show(n=5)\n)\n\n\n+--------------------+-------------+\n|                 xid|count(action)|\n+--------------------+-------------+\n|001c4a21-52c6-489...|            1|\n|0024344b-7ee2-4fc...|            4|\n|004564e3-87c1-4e1...|            1|\n|006d807f-91c3-415...|            1|\n|006e0463-b24c-499...|            1|\n+--------------------+-------------+\nonly showing top 5 rows\n\n\n\nVisualize the distribution of the number of users per number of actions.\n\n\n\n\n\n\nQuestion\n\n\n\nConstruct a column containing the number of days since the last action of the user\n\n\n\n\nCode\n# xid_partition = Window.partitionBy('xid')\n\nmax_date = (\n  func\n    .max(col('date'))\n    .over(xid_partition)\n)\n\nn_days_since_last_event = func.datediff(func.current_date(), max_date)\n\ndf = df.withColumn('n_days_since_last_event',\n                   n_days_since_last_event)\n\ndf.show(n=2)\n\n\n+--------------------+------+-------------------+----------+--------------------+-----------+-------+------+--------+-----------------------+\n|                 xid|action|               date|website_id|                 url|category_id|zipcode|device|n_events|n_days_since_last_event|\n+--------------------+------+-------------------+----------+--------------------+-----------+-------+------+--------+-----------------------+\n|0006cea7-1679-426...|     O|2016-12-26 13:41:08|        51|https://www.footl...|     1002.0|  34290|   TAB|       1|                   3020|\n|000893c8-a14b-4f3...|     O|2016-12-23 16:18:37|        56|http://blague.dum...|     1002.0|   NULL|   DSK|       1|                   3023|\n+--------------------+------+-------------------+----------+--------------------+-----------+-------+------+--------+-----------------------+\nonly showing top 2 rows\n\n\n\n\n\nCode\ndf.printSchema()\n\n\nroot\n |-- xid: string (nullable = true)\n |-- action: string (nullable = true)\n |-- date: timestamp (nullable = true)\n |-- website_id: string (nullable = true)\n |-- url: string (nullable = true)\n |-- category_id: float (nullable = true)\n |-- zipcode: string (nullable = true)\n |-- device: string (nullable = true)\n |-- n_events: long (nullable = false)\n |-- n_days_since_last_event: integer (nullable = true)"
  },
  {
    "objectID": "core/notebooks/notebook08_webdata.html#construct-a-column-containing-the-number-of-actions-of-each-user-for-each-modality-of-device",
    "href": "core/notebooks/notebook08_webdata.html#construct-a-column-containing-the-number-of-actions-of-each-user-for-each-modality-of-device",
    "title": "Using with pyspark for data preprocessing",
    "section": "Construct a column containing the number of actions of each user for each modality of device",
    "text": "Construct a column containing the number of actions of each user for each modality of device\n\n\nCode\nxid_device_partition = xid_partition.partitionBy('device')\n\nn_events_per_device = func.count(col('action')).over(xid_device_partition)\n\ndf = df.withColumn('n_events_per_device', n_events_per_device)\n\ndf.head(n=2)\n\n\n[Stage 21:&gt;                                                       (0 + 20) / 22][Stage 21:==================================================&gt;     (20 + 2) / 22][Stage 24:&gt;                                                         (0 + 1) / 1]                                                                                \n\n\n[Row(xid='000893c8-a14b-4f33-858f-210440f37def', action='O', date=datetime.datetime(2016, 12, 23, 16, 18, 37), website_id='56', url='http://blague.dumatin.fr/', category_id=1002.0, zipcode=None, device='DSK', n_events=1, n_days_since_last_event=3023, n_events_per_device=1021837),\n Row(xid='0008c5d2-c263-4b55-ae7d-82c4bf566cc4', action='O', date=datetime.datetime(2017, 1, 16, 4, 26, 21), website_id='74', url='http://www.realite-virtuelle.com/meilleure-videos-360-vr', category_id=1002.0, zipcode=None, device='DSK', n_events=1, n_days_since_last_event=2999, n_events_per_device=1021837)]"
  },
  {
    "objectID": "core/notebooks/notebook08_webdata.html#number-of-devices-per-user",
    "href": "core/notebooks/notebook08_webdata.html#number-of-devices-per-user",
    "title": "Using with pyspark for data preprocessing",
    "section": "Number of devices per user ",
    "text": "Number of devices per user \n\n\nCode\nxid_partition = Window.partitionBy('xid')\n\nrank_device = (\n  func\n    .dense_rank()\n    .over(xid_partition.orderBy('device'))\n)\n\nn_unique_device = (\n    func\n      .last(rank_device)\n      .over(xid_partition)\n)\n\ndf = df.withColumn('n_device', n_unique_device)\n\ndf.head(n=2)\n\n\n[Stage 30:======================================&gt;                   (2 + 1) / 3]                                                                                \n\n\n[Row(xid='0006cea7-1679-4264-bdef-0cd089749ede', action='O', date=datetime.datetime(2016, 12, 26, 13, 41, 8), website_id='51', url='https://www.footlegende.fr/mercato-psg-coutinho-10166', category_id=1002.0, zipcode='34290', device='TAB', n_events=1, n_days_since_last_event=3020, n_events_per_device=132013, n_device=1),\n Row(xid='000893c8-a14b-4f33-858f-210440f37def', action='O', date=datetime.datetime(2016, 12, 23, 16, 18, 37), website_id='56', url='http://blague.dumatin.fr/', category_id=1002.0, zipcode=None, device='DSK', n_events=1, n_days_since_last_event=3023, n_events_per_device=1021837, n_device=1)]\n\n\n\n\nCode\ndf\\\n    .where(col('n_device') &gt; 1)\\\n    .select('xid', 'device', 'n_events',  'n_device', 'n_events_per_device')\\\n    .head(n=8)\n\n\n[Stage 40:=============================&gt;                            (1 + 1) / 2]                                                                                \n\n\n[Row(xid='4c1dc79d-a140-4da9-ae28-540b4503c3b8', device='DSK', n_events=6, n_device=2, n_events_per_device=1021837),\n Row(xid='4c1dc79d-a140-4da9-ae28-540b4503c3b8', device='DSK', n_events=6, n_device=2, n_events_per_device=1021837),\n Row(xid='4c1dc79d-a140-4da9-ae28-540b4503c3b8', device='DSK', n_events=6, n_device=2, n_events_per_device=1021837),\n Row(xid='4c1dc79d-a140-4da9-ae28-540b4503c3b8', device='DSK', n_events=6, n_device=2, n_events_per_device=1021837),\n Row(xid='4c1dc79d-a140-4da9-ae28-540b4503c3b8', device='DSK', n_events=6, n_device=2, n_events_per_device=1021837),\n Row(xid='4c1dc79d-a140-4da9-ae28-540b4503c3b8', device='MOB', n_events=6, n_device=2, n_events_per_device=1564),\n Row(xid='78156cdf-7229-46eb-bb6b-92d384f9a6fa', device='DSK', n_events=6, n_device=2, n_events_per_device=1021837),\n Row(xid='78156cdf-7229-46eb-bb6b-92d384f9a6fa', device='DSK', n_events=6, n_device=2, n_events_per_device=1021837)]\n\n\n\n\nCode\ndf\\\n    .where(col('n_device') &gt; 1)\\\n    .select('xid', 'device', 'n_events',  'n_device', 'n_events_per_device')\\\n    .count()\n\n\n3153"
  },
  {
    "objectID": "core/notebooks/notebook08_webdata.html#extraction",
    "href": "core/notebooks/notebook08_webdata.html#extraction",
    "title": "Using with pyspark for data preprocessing",
    "section": "Extraction",
    "text": "Extraction\nHere extraction is just about reading the data\n\n\nCode\ndf = spark.read.parquet(file_path)\ndf.show(n=3)\n\n\n+--------------------+------+-------------------+----------+--------------------+-----------+-------+------+\n|                 xid|action|               date|website_id|                 url|category_id|zipcode|device|\n+--------------------+------+-------------------+----------+--------------------+-----------+-------+------+\n|001ff9b6-5383-422...|     O|2017-01-25 07:02:18|         3|http://www.8chanc...|     1002.0|  11370|   SMP|\n|0056ab7a-3cba-4ed...|     O|2016-12-28 09:47:08|        54|http://www.salair...|     1002.0|  86000|   DSK|\n|005ae4ab-363a-41a...|     O|2017-01-27 22:21:06|        74|http://www.realit...|     1002.0|  49700|   DSK|\n+--------------------+------+-------------------+----------+--------------------+-----------+-------+------+\nonly showing top 3 rows"
  },
  {
    "objectID": "core/notebooks/notebook08_webdata.html#transformation-of-the-data",
    "href": "core/notebooks/notebook08_webdata.html#transformation-of-the-data",
    "title": "Using with pyspark for data preprocessing",
    "section": "Transformation of the data",
    "text": "Transformation of the data\nAt this step we compute a lot of extra things from the data. The aim is to build features that describe users.\n\n\nCode\ndef n_events_transformer(df):\n    xid_partition = Window.partitionBy('xid')\n    n_events = func.count(col('action')).over(xid_partition)\n    \n    df = df.withColumn('n_events', n_events)\n\n    return df\n\n\n\n\nCode\ndef n_events_per_action_transformer(df):\n    xid_action_partition = Window.partitionBy('xid', 'action')\n    n_events_per_action = func.count(col('action')).over(xid_action_partition)\n\n    df = df.withColumn('n_events_per_action', n_events_per_action)\n    \n    return df\n\n\n\n\nCode\ndef hour_transformer(df):\n    hour = func.hour(col('date'))\n    df = df.withColumn('hour', hour)\n    return df\n\ndef weekday_transformer(df):\n    weekday = func.date_format(col('date'), 'EEEE')\n    df = df.withColumn('weekday', weekday)\n    return df\n\ndef n_events_per_hour_transformer(df):\n    xid_hour_partition = Window.partitionBy('xid', 'hour')\n    n_events_per_hour = func.count(col('action')).over(xid_hour_partition)\n    df = df.withColumn('n_events_per_hour', n_events_per_hour)\n    return df\n\ndef n_events_per_weekday_transformer(df):\n    xid_weekday_partition = Window.partitionBy('xid', 'weekday')\n    n_events_per_weekday = func.count(col('action')).over(xid_weekday_partition)\n    df = df.withColumn('n_events_per_weekday', n_events_per_weekday)\n    return df\n\ndef n_days_since_last_event_transformer(df):\n    xid_partition = Window.partitionBy('xid')\n    max_date = func.max(col('date')).over(xid_partition)\n    n_days_since_last_event = func.datediff(func.current_date(), max_date)\n    df = df.withColumn('n_days_since_last_event',\n                       n_days_since_last_event + lit(0.1))\n    return df\n\ndef n_days_since_last_action_transformer(df):\n    xid_partition_action = Window.partitionBy('xid', 'action')\n    max_date = func.max(col('date')).over(xid_partition_action)\n    n_days_since_last_action = func.datediff(func.current_date(),\n                                                        max_date)\n    df = df.withColumn('n_days_since_last_action',\n                       n_days_since_last_action + lit(0.1))\n    return df\n\ndef n_unique_day_transformer(df):\n    xid_partition = Window.partitionBy('xid')\n    dayofyear = func.dayofyear(col('date'))\n    rank_day = func.dense_rank().over(xid_partition.orderBy(dayofyear))\n    n_unique_day = func.last(rank_day).over(xid_partition)\n    df = df.withColumn('n_unique_day', n_unique_day)\n    return df\n\ndef n_unique_hour_transformer(df):\n    xid_partition = Window.partitionBy('xid')\n    rank_hour = func.dense_rank().over(xid_partition.orderBy('hour'))\n    n_unique_hour = func.last(rank_hour).over(xid_partition)\n    df = df.withColumn('n_unique_hour', n_unique_hour)\n    return df\n\ndef n_events_per_device_transformer(df):\n    xid_device_partition = Window.partitionBy('xid', 'device')\n    n_events_per_device = func.count(func.col('device')) \\\n        .over(xid_device_partition)\n    df = df.withColumn('n_events_per_device', n_events_per_device)\n    return df\n\ndef n_unique_device_transformer(df):\n    xid_partition = Window.partitionBy('xid')\n    rank_device = func.dense_rank().over(xid_partition.orderBy('device'))\n    n_unique_device = func.last(rank_device).over(xid_partition)\n    df = df.withColumn('n_device', n_unique_device)\n    return df\n\ndef n_actions_per_category_id_transformer(df):\n    xid_category_id_partition = Window.partitionBy('xid', 'category_id',\n                                                   'action')\n    n_actions_per_category_id = func.count(func.col('action')) \\\n        .over(xid_category_id_partition)\n    df = df.withColumn('n_actions_per_category_id', n_actions_per_category_id)\n    return df\n\ndef n_unique_category_id_transformer(df):\n    xid_partition = Window.partitionBy('xid')\n    rank_category_id = func.dense_rank().over(xid_partition\\\n                                              .orderBy('category_id'))\n    n_unique_category_id = func.last(rank_category_id).over(xid_partition)\n    df = df.withColumn('n_unique_category_id', n_unique_category_id)\n    return df\n\ndef n_events_per_category_id_transformer(df):\n    xid_category_id_partition = Window.partitionBy('xid', 'category_id')\n    n_events_per_category_id = func.count(func.col('action')) \\\n        .over(xid_category_id_partition)\n    df = df.withColumn('n_events_per_category_id', n_events_per_category_id)\n    return df\n\ndef n_events_per_website_id_transformer(df):\n    xid_website_id_partition = Window.partitionBy('xid', 'website_id')\n    n_events_per_website_id = func.count(col('action'))\\\n        .over(xid_website_id_partition)\n    df = df.withColumn('n_events_per_website_id', n_events_per_website_id)\n    return df\n\n\n\n\nCode\ntransformers = [\n    hour_transformer,\n    weekday_transformer,\n    n_events_per_hour_transformer,\n    n_events_per_weekday_transformer,\n    n_days_since_last_event_transformer,\n    n_days_since_last_action_transformer,\n    n_unique_day_transformer,\n    n_unique_hour_transformer,\n    n_events_per_device_transformer,\n    n_unique_device_transformer,\n    n_actions_per_category_id_transformer,\n    n_events_per_category_id_transformer,\n    n_events_per_website_id_transformer,\n]\n\n\n\n\nCode\nN = 10000\n\n\n\n\nCode\nsample_df = df.sample(withReplacement=False, fraction=.05)\n\n\n\n\nCode\nsample_df.count()\n\n\n58801\n\n\n\n\nCode\nfor transformer in transformers:\n    df = transformer(df)\n\ndf.head(n=1)\n\n\n[Stage 56:=================================&gt;                       (7 + 5) / 12][Stage 58:&gt;                                                         (0 + 1) / 1]                                                                                \n\n\n[Row(xid='0006cea7-1679-4264-bdef-0cd089749ede', action='O', date=datetime.datetime(2016, 12, 26, 13, 41, 8), website_id='51', url='https://www.footlegende.fr/mercato-psg-coutinho-10166', category_id=1002.0, zipcode='34290', device='TAB', hour=13, weekday='Monday', n_events_per_hour=1, n_events_per_weekday=1, n_days_since_last_event=3020.1, n_days_since_last_action=3020.1, n_unique_day=1, n_unique_hour=1, n_events_per_device=1, n_device=1, n_actions_per_category_id=1, n_events_per_category_id=1, n_events_per_website_id=1)]\n\n\n\n\nCode\nfor transformer in transformers:\n    sample_df = transformer(sample_df)\n\nsample_df.head(n=1)\n\n\n[Row(xid='000fcd14-ce41-461d-a2aa-cb407630a03a', action='O', date=datetime.datetime(2017, 1, 15, 16, 3, 41), website_id='3', url='http://www.8chances.com/grille', category_id=1002.0, zipcode='43000', device='DSK', hour=16, weekday='Sunday', n_events_per_hour=1, n_events_per_weekday=1, n_days_since_last_event=3000.1, n_days_since_last_action=3000.1, n_unique_day=1, n_unique_hour=1, n_events_per_device=1, n_device=1, n_actions_per_category_id=1, n_events_per_category_id=1, n_events_per_website_id=1)]\n\n\n\n\nCode\ndf = sample_df\n\n\n\n\nCode\nsorted(df.columns)\n\n\n['action',\n 'category_id',\n 'date',\n 'device',\n 'hour',\n 'n_actions_per_category_id',\n 'n_days_since_last_action',\n 'n_days_since_last_event',\n 'n_device',\n 'n_events_per_category_id',\n 'n_events_per_device',\n 'n_events_per_hour',\n 'n_events_per_website_id',\n 'n_events_per_weekday',\n 'n_unique_day',\n 'n_unique_hour',\n 'url',\n 'website_id',\n 'weekday',\n 'xid',\n 'zipcode']"
  },
  {
    "objectID": "core/notebooks/notebook08_webdata.html#load-step",
    "href": "core/notebooks/notebook08_webdata.html#load-step",
    "title": "Using with pyspark for data preprocessing",
    "section": "Load step",
    "text": "Load step\nHere, we use all the previous computations (saved in the columns of the dataframe) to compute aggregated informations about each user.\n\n\nCode\ndef n_events_per_hour_loader(df):\n    csr = df\\\n        .select('xid', 'hour', 'n_events_per_hour')\\\n        .withColumnRenamed('n_events_per_hour', 'value')\\\n        .distinct()     # action\n    feature_name = func.concat(lit('n_events_per_hour#'), col('hour'))\n    csr = csr\\\n        .withColumn('feature_name', feature_name)\\\n        .drop('hour')\n    return csr\n\ndef n_events_per_website_id_loader(df):\n    csr = df.select('xid', 'website_id', 'n_events_per_website_id')\\\n        .withColumnRenamed('n_events_per_hour', 'value')\\\n        .distinct()\n    feature_name = func.concat(lit('n_events_per_website_id#'),\n                               col('website_id'))\n    csr = csr\\\n        .withColumn('feature_name', feature_name)\\\n        .drop('website_id')\n    return csr\n\ndef n_events_per_hour_loader(df):\n    csr = df\\\n        .select('xid', 'hour', 'n_events_per_hour')\\\n        .withColumnRenamed('n_events_per_hour', 'value')\\\n        .distinct()\n    feature_name = func.concat(lit('n_events_per_hour#'), col('hour'))\n    csr = csr\\\n        .withColumn('feature_name', feature_name)\\\n        .drop('hour')\n    return csr\n\ndef n_events_per_weekday_loader(df):\n    csr = df\\\n        .select('xid', 'weekday', 'n_events_per_weekday')\\\n        .withColumnRenamed('n_events_per_weekday', 'value')\\\n        .distinct()\n    feature_name = func.concat(lit('n_events_per_weekday#'), col('weekday'))\n    csr = csr\\\n        .withColumn('feature_name', feature_name)\\\n        .drop('weekday')\n    return csr\n\ndef n_days_since_last_event_loader(df):\n    csr = df.select('xid',  'n_days_since_last_event')\\\n        .withColumnRenamed('n_days_since_last_event#', 'value')\\\n        .distinct()\n    feature_name = lit('n_days_since_last_event')\n    csr = csr\\\n        .withColumn('feature_name', feature_name)\n    return csr\n\ndef n_days_since_last_action_loader(df):\n    csr = df.select('xid', 'action', 'n_days_since_last_action')\\\n        .withColumnRenamed('n_days_since_last_action', 'value')\\\n        .distinct()\n    feature_name = func.concat(lit('n_days_since_last_action#'), col('action'))\n    csr = csr\\\n        .withColumn('feature_name', feature_name)\\\n        .drop('action')\n    return csr\n\ndef n_unique_day_loader(df):\n    csr = df.select('xid', 'n_unique_day')\\\n        .withColumnRenamed('n_unique_day', 'value')\\\n        .distinct()\n    feature_name = lit('n_unique_day')\n    csr = csr\\\n        .withColumn('feature_name', feature_name)\n    return csr\n\ndef n_unique_hour_loader(df):\n    csr = df.select('xid', 'n_unique_hour')\\\n        .withColumnRenamed('n_unique_hour', 'value')\\\n        .distinct()\n    feature_name = lit('n_unique_hour')\n    csr = csr\\\n        .withColumn('feature_name', feature_name)\n    return csr\n\ndef n_events_per_device_loader(df):\n    csr = df\\\n        .select('xid', 'device', 'n_events_per_device')\\\n        .withColumnRenamed('n_events_per_device', 'value')\\\n        .distinct()\n    feature_name = func.concat(lit('n_events_per_device#'), col('device'))\n    csr = csr\\\n        .withColumn('feature_name', feature_name)\\\n        .drop('device')\n    return csr\n\ndef n_unique_device_loader(df):\n    csr = df.select('xid', 'n_device')\\\n        .withColumnRenamed('n_device', 'value')\\\n        .distinct()\n    feature_name = lit('n_device')\n    csr = csr\\\n        .withColumn('feature_name', feature_name)\n    return csr\n\ndef n_events_per_category_id_loader(df):\n    csr = df.select('xid', 'category_id', 'n_events_per_category_id')\\\n        .withColumnRenamed('n_events_per_category_id', 'value')\\\n        .distinct()\n    feature_name = func.concat(lit('n_events_per_category_id#'),\n                               col('category_id'))\n    csr = csr\\\n        .withColumn('feature_name', feature_name)\\\n        .drop('category_id')\n    return csr\n\ndef n_actions_per_category_id_loader(df):\n    csr = df.select('xid', 'category_id', 'action', 'n_actions_per_category_id')\\\n        .withColumnRenamed('n_actions_per_category_id', 'value')\\\n        .distinct()\n    feature_name = func.concat(lit('n_actions_per_category_id#'),\n                               col('action'), lit('#'), \n                               col('category_id'))\n    csr = csr\\\n        .withColumn('feature_name', feature_name)\\\n        .drop('category_id')\\\n        .drop('action')\n    return csr\n\ndef n_events_per_website_id_loader(df):\n    csr = df.select('xid', 'website_id', 'n_events_per_website_id')\\\n        .withColumnRenamed('n_events_per_website_id', 'value')\\\n        .distinct()\n    feature_name = func.concat(lit('n_events_per_website_id#'),\n                               col('website_id'))\n    csr = csr\\\n        .withColumn('feature_name', feature_name)\\\n        .drop('website_id')\n    return csr\n\n\n\n\nCode\nfrom functools import reduce\n\n\n\n\nCode\nloaders = [\n    n_events_per_hour_loader,\n    n_events_per_website_id_loader,\n    n_events_per_hour_loader,\n    n_events_per_weekday_loader,\n    n_days_since_last_event_loader,\n    n_days_since_last_action_loader,\n    n_unique_day_loader,\n    n_unique_hour_loader,\n    n_events_per_device_loader,\n    n_unique_device_loader,\n    n_events_per_category_id_loader,\n    n_actions_per_category_id_loader,\n    n_events_per_website_id_loader,\n]\n\n\n\n\nCode\ndef union(df, other):\n    return df.union(other)\n\n\n\n\n\n\n\n\nAbout DataFrame.union()\n\n\n\nThis method performs a SQL-style set union of the rows from both DataFrame objects, with no automatic deduplication of elements.\nUse the distinct() method to perform deduplication of rows.\nThe method resolves columns by position (not by name), following the standard behavior in SQL.\n\n\n\n\nCode\nspam = [loader(df) for loader in loaders]\n\n\n\n\nCode\nspam[0].printSchema()\n\n\nroot\n |-- xid: string (nullable = true)\n |-- value: long (nullable = false)\n |-- feature_name: string (nullable = true)\n\n\n\n\n\nCode\nlen(spam)\n\n\n13\n\n\n\n\nCode\ncsr = reduce(\n    lambda df1, df2: df1.union(df2),\n    spam\n)\n\ncsr.head(n=3)\n\n\n[Row(xid='000fcd14-ce41-461d-a2aa-cb407630a03a', value=1.0, feature_name='n_events_per_hour#16'),\n Row(xid='0013cccf-fd6b-4794-8951-ab362875b6d2', value=1.0, feature_name='n_events_per_hour#1'),\n Row(xid='00142eb6-ade4-4f51-b945-32aaddf9c550', value=1.0, feature_name='n_events_per_hour#9')]\n\n\n\n\nCode\ncsr.columns\n\n\n['xid', 'value', 'feature_name']\n\n\n\n\nCode\ncsr.show(5)\n\n\n+--------------------+-----+--------------------+\n|                 xid|value|        feature_name|\n+--------------------+-----+--------------------+\n|000fcd14-ce41-461...|  1.0|n_events_per_hour#16|\n|0013cccf-fd6b-479...|  1.0| n_events_per_hour#1|\n|00142eb6-ade4-4f5...|  1.0| n_events_per_hour#9|\n|0015f89f-cc8d-4fb...|  2.0| n_events_per_hour#8|\n|0015f89f-cc8d-4fb...|  4.0|n_events_per_hour#17|\n+--------------------+-----+--------------------+\nonly showing top 5 rows\n\n\n\n\n\nCode\ncsr.rdd.getNumPartitions()\n\n\n17\n\n\n\n\nCode\n# Replace features names and xid by a unique number\nfeature_name_partition = Window().orderBy('feature_name')\nxid_partition = Window().orderBy('xid')\n\ncol_idx = func.dense_rank().over(feature_name_partition)\nrow_idx = func.dense_rank().over(xid_partition)\n\n\n\n\nCode\ncsr = csr.withColumn('col', col_idx)\\\n    .withColumn('row', row_idx)\n\ncsr = csr.na.drop('any')\n\ncsr.head(n=5)\n\n\n25/04/03 15:10:28 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n25/04/03 15:10:28 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n25/04/03 15:10:28 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n25/04/03 15:10:28 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n25/04/03 15:10:28 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n25/04/03 15:10:29 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n25/04/03 15:10:29 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n25/04/03 15:10:29 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n25/04/03 15:10:29 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n25/04/03 15:10:29 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n25/04/03 15:10:29 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n25/04/03 15:10:29 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n25/04/03 15:10:29 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n25/04/03 15:10:29 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n25/04/03 15:10:29 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n25/04/03 15:10:29 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n25/04/03 15:10:29 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n25/04/03 15:10:29 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n25/04/03 15:10:29 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n25/04/03 15:10:29 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n25/04/03 15:10:29 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n25/04/03 15:10:29 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n25/04/03 15:10:29 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n25/04/03 15:10:29 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n25/04/03 15:10:29 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n25/04/03 15:10:29 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n25/04/03 15:10:29 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n25/04/03 15:10:29 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n25/04/03 15:10:29 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n25/04/03 15:10:29 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n25/04/03 15:10:29 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n25/04/03 15:10:29 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n25/04/03 15:10:29 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n25/04/03 15:10:29 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n25/04/03 15:10:29 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n25/04/03 15:10:29 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n25/04/03 15:10:29 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n25/04/03 15:10:29 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n25/04/03 15:10:29 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n25/04/03 15:10:29 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n25/04/03 15:10:29 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n25/04/03 15:10:29 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n25/04/03 15:10:29 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n25/04/03 15:10:29 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n25/04/03 15:10:29 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n25/04/03 15:10:30 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n25/04/03 15:10:30 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n25/04/03 15:10:30 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n25/04/03 15:10:30 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n\n\n[Row(xid='00032aad-44f9-4784-ae36-4fa30a0ad491', value=1.0, feature_name='n_actions_per_category_id#O#1204.0', col=4, row=1),\n Row(xid='00032aad-44f9-4784-ae36-4fa30a0ad491', value=3030.1, feature_name='n_days_since_last_action#O', col=6, row=1),\n Row(xid='00032aad-44f9-4784-ae36-4fa30a0ad491', value=3030.1, feature_name='n_days_since_last_event', col=7, row=1),\n Row(xid='00032aad-44f9-4784-ae36-4fa30a0ad491', value=1.0, feature_name='n_device', col=8, row=1),\n Row(xid='00032aad-44f9-4784-ae36-4fa30a0ad491', value=1.0, feature_name='n_events_per_category_id#1204.0', col=10, row=1)]\n\n\n\n\nCode\n# Let's save the result of our hard work into a new parquet file\noutput_path = Path('./data')\noutput_file = str(output_path / 'csr.parquet')\ncsr.write.parquet(output_file, mode='overwrite')\n\n\n25/04/03 15:10:31 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n25/04/03 15:10:31 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n25/04/03 15:10:31 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n25/04/03 15:10:31 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n25/04/03 15:10:31 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n25/04/03 15:10:31 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n25/04/03 15:10:31 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n25/04/03 15:10:31 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n25/04/03 15:10:31 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n25/04/03 15:10:31 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n25/04/03 15:10:31 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n25/04/03 15:10:31 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n25/04/03 15:10:31 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n25/04/03 15:10:31 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n25/04/03 15:10:31 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n25/04/03 15:10:31 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n25/04/03 15:10:31 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n25/04/03 15:10:31 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n25/04/03 15:10:31 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n25/04/03 15:10:31 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n25/04/03 15:10:31 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n25/04/03 15:10:31 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n25/04/03 15:10:31 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n25/04/03 15:10:31 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n25/04/03 15:10:31 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n25/04/03 15:10:31 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n25/04/03 15:10:31 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n25/04/03 15:10:31 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n25/04/03 15:10:31 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n25/04/03 15:10:32 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n25/04/03 15:10:32 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n25/04/03 15:10:32 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n25/04/03 15:10:32 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n25/04/03 15:10:32 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n25/04/03 15:10:32 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n25/04/03 15:10:32 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n25/04/03 15:10:32 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n25/04/03 15:10:32 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n25/04/03 15:10:32 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n25/04/03 15:10:32 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n25/04/03 15:10:32 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n25/04/03 15:10:32 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n25/04/03 15:10:32 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n25/04/03 15:10:32 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n25/04/03 15:10:32 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n25/04/03 15:10:32 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n25/04/03 15:10:32 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n25/04/03 15:10:32 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n25/04/03 15:10:32 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n25/04/03 15:10:33 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n25/04/03 15:10:33 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n25/04/03 15:10:33 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n25/04/03 15:10:33 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n[Stage 222:&gt;                                                        (0 + 1) / 1]"
  },
  {
    "objectID": "core/notebooks/notebook08_webdata.html#finally",
    "href": "core/notebooks/notebook08_webdata.html#finally",
    "title": "Using with pyspark for data preprocessing",
    "section": "Finally !!",
    "text": "Finally !!\nWow ! That was a lot of work. Now we have a features matrix \\(X\\) and a vector of labels \\(y\\).\n\n\nCode\nX.indices\n\n\narray([ 3,  4,  5, ..., 69, 75, 76], shape=(119137,), dtype=int32)\n\n\n\n\nCode\nX.indptr\n\n\narray([     0,     10,     20, ..., 119117, 119127, 119137],\n      shape=(10844,), dtype=int32)\n\n\n\n\nCode\nX.shape, X.nnz\n\n\n((10843, 77), 119137)\n\n\n\n\nCode\ny.shape, y.sum()\n\n\n((10843,), np.int64(81))"
  },
  {
    "objectID": "core/notebooks/notebook11_dive.html",
    "href": "core/notebooks/notebook11_dive.html",
    "title": "Diving deeer",
    "section": "",
    "text": "Code\nimport os\nimport sys\n\nos.environ['PYSPARK_PYTHON'] = sys.executable\nos.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable\nCode\nfrom pyspark import SparkConf, SparkContext\nfrom pyspark.sql import SparkSession\n\nconf = SparkConf().setAppName(\"Spark SQL Course\")\nsc = SparkContext(conf=conf)  # no need for Spark 3...\n\nspark = (SparkSession\n    .builder\n    .appName(\"Spark SQL Course\")\n    .getOrCreate()\n)\n\n\n25/04/03 15:10:49 WARN Utils: Your hostname, boucheron-Precision-5480 resolves to a loopback address: 127.0.1.1; using 172.23.32.10 instead (on interface eth0)\n25/04/03 15:10:49 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\nSetting default log level to \"WARN\".\nTo adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n25/04/03 15:10:50 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\nCode\nsc = spark._sc\nCode\nrdd = sc.parallelize([(\"a\", 1), (\"b\", 1), (\"a\", 1)])\nCode\nrdd.reduceByKey(lambda a, b: a + b).collect()\n\n\n[Stage 0:&gt;                                                        (0 + 20) / 20]                                                                                \n\n\n[('b', 1), ('a', 2)]\nCode\nimport requests, zipfile, io\nfrom pathlib import Path\n\npath = Path('webdata.parquet')\nif not path.exists():\n    url = \"https://stephanegaiffas.github.io/big_data_course/data/webdata.parquet.zip\"\n    r = requests.get(url)\n    z = zipfile.ZipFile(io.BytesIO(r.content))\n    z.extractall(path='./')\nCode\ninput_path = './'\n\ninput_file = os.path.join(input_path, 'webdata.parquet')\ndf = spark.read.parquet(input_file)\nCode\ndf.head(6)\n\n\n[Row(xid='001ff9b6-5383-4221-812d-58c2c3f234cc', action='O', date=datetime.datetime(2017, 1, 25, 7, 2, 18), website_id='3', url='http://www.8chances.com/grille', category_id=1002.0, zipcode='11370', device='SMP'),\n Row(xid='0056ab7a-3cba-4ed5-a495-3d4abf79ab66', action='O', date=datetime.datetime(2016, 12, 28, 9, 47, 8), website_id='54', url='http://www.salaire-brut-en-net.fr/differences-brut-net/', category_id=1002.0, zipcode='86000', device='DSK'),\n Row(xid='005ae4ab-363a-41a0-b8f9-faee47d622a4', action='O', date=datetime.datetime(2017, 1, 27, 22, 21, 6), website_id='74', url='http://www.realite-virtuelle.com/top-applications-horreur-vr-halloween', category_id=1002.0, zipcode='49700', device='DSK'),\n Row(xid='006f867c-70cb-41f0-82af-f3688fa719c5', action='O', date=datetime.datetime(2016, 12, 20, 12, 45, 14), website_id='43', url='http://www.frenchblues.fr/', category_id=1002.0, zipcode='42660', device='DSK'),\n Row(xid='006f867c-70cb-41f0-82af-f3688fa719c5', action='O', date=datetime.datetime(2016, 12, 20, 12, 56, 50), website_id='43', url='http://www.frenchblues.fr/', category_id=1002.0, zipcode='42660', device='DSK'),\n Row(xid='006f867c-70cb-41f0-82af-f3688fa719c5', action='O', date=datetime.datetime(2016, 12, 20, 12, 56, 53), website_id='43', url='http://www.frenchblues.fr/contact/', category_id=1002.0, zipcode='42660', device='DSK')]\nCode\ndf.describe()\n\n\nDataFrame[summary: string, xid: string, action: string, website_id: string, url: string, category_id: string, zipcode: string, device: string]\nCode\ndf.count()\n\n\n1179532"
  },
  {
    "objectID": "core/notebooks/notebook11_dive.html#compute-the-total-number-of-unique-users",
    "href": "core/notebooks/notebook11_dive.html#compute-the-total-number-of-unique-users",
    "title": "Diving deeer",
    "section": "Compute the total number of unique users",
    "text": "Compute the total number of unique users\n\n\nCode\ndf.select('xid').distinct().count()\n\n\n473761"
  },
  {
    "objectID": "core/notebooks/notebook11_dive.html#construct-a-column-containing-the-total-number-of-actions-per-user",
    "href": "core/notebooks/notebook11_dive.html#construct-a-column-containing-the-total-number-of-actions-per-user",
    "title": "Diving deeer",
    "section": "Construct a column containing the total number of actions per user",
    "text": "Construct a column containing the total number of actions per user\n\n\nCode\nxid_partition = Window.partitionBy('xid')\nn_events = func.count(col('action')).over(xid_partition)\ndf = df.withColumn('n_events', n_events)\ndf.head(n=2)\n\n\n[Row(xid='0006cea7-1679-4264-bdef-0cd089749ede', action='O', date=datetime.datetime(2016, 12, 26, 13, 41, 8), website_id='51', url='https://www.footlegende.fr/mercato-psg-coutinho-10166', category_id=1002.0, zipcode='34290', device='TAB', n_events=1),\n Row(xid='000893c8-a14b-4f33-858f-210440f37def', action='O', date=datetime.datetime(2016, 12, 23, 16, 18, 37), website_id='56', url='http://blague.dumatin.fr/', category_id=1002.0, zipcode=None, device='DSK', n_events=1)]\n\n\n\n\nCode\ndf.groupBy('xid').agg(func.count('action')).head(5)\n\n\n[Row(xid='001c4a21-52c6-4890-b6ce-2b9d4ba06a56', count(action)=1),\n Row(xid='0024344b-7ee2-4fcd-a0b4-bec26d8c8b0e', count(action)=4),\n Row(xid='004564e3-87c1-4e16-ad2c-0e96afc3d617', count(action)=1),\n Row(xid='006d807f-91c3-415a-bb5e-6b9f7e6517a1', count(action)=1),\n Row(xid='006e0463-b24c-4996-84ab-d6d0d65a52aa', count(action)=1)]"
  },
  {
    "objectID": "core/notebooks/notebook11_dive.html#construct-a-column-containing-the-number-of-days-since-the-last-action-of-the-user",
    "href": "core/notebooks/notebook11_dive.html#construct-a-column-containing-the-number-of-days-since-the-last-action-of-the-user",
    "title": "Diving deeer",
    "section": "Construct a column containing the number of days since the last action of the user",
    "text": "Construct a column containing the number of days since the last action of the user\n\n\nCode\nxid_partition = Window.partitionBy('xid')\nmax_date = func.max(col('date')).over(xid_partition)\nn_days_since_last_event = func.datediff(func.current_date(), max_date)\ndf = df.withColumn('n_days_since_last_event',\n                   n_days_since_last_event)\ndf.head(n=2)\n\n\n[Row(xid='0006cea7-1679-4264-bdef-0cd089749ede', action='O', date=datetime.datetime(2016, 12, 26, 13, 41, 8), website_id='51', url='https://www.footlegende.fr/mercato-psg-coutinho-10166', category_id=1002.0, zipcode='34290', device='TAB', n_events=1, n_days_since_last_event=3020),\n Row(xid='000893c8-a14b-4f33-858f-210440f37def', action='O', date=datetime.datetime(2016, 12, 23, 16, 18, 37), website_id='56', url='http://blague.dumatin.fr/', category_id=1002.0, zipcode=None, device='DSK', n_events=1, n_days_since_last_event=3023)]"
  },
  {
    "objectID": "core/notebooks/notebook11_dive.html#construct-a-column-containing-the-number-of-actions-of-each-user-for-each-modality-of-device",
    "href": "core/notebooks/notebook11_dive.html#construct-a-column-containing-the-number-of-actions-of-each-user-for-each-modality-of-device",
    "title": "Diving deeer",
    "section": "Construct a column containing the number of actions of each user for each modality of device",
    "text": "Construct a column containing the number of actions of each user for each modality of device\n\n\nCode\nxid_device_partition = Window.partitionBy('xid', 'device')\nn_events_per_device = func.count(col('action')).over(xid_device_partition)\ndf = df.withColumn('n_events_per_device', n_events_per_device)\ndf.head(n=2)\n\n\n[Row(xid='0006cea7-1679-4264-bdef-0cd089749ede', action='O', date=datetime.datetime(2016, 12, 26, 13, 41, 8), website_id='51', url='https://www.footlegende.fr/mercato-psg-coutinho-10166', category_id=1002.0, zipcode='34290', device='TAB', n_events=1, n_days_since_last_event=3020, n_events_per_device=1),\n Row(xid='000893c8-a14b-4f33-858f-210440f37def', action='O', date=datetime.datetime(2016, 12, 23, 16, 18, 37), website_id='56', url='http://blague.dumatin.fr/', category_id=1002.0, zipcode=None, device='DSK', n_events=1, n_days_since_last_event=3023, n_events_per_device=1)]"
  },
  {
    "objectID": "core/notebooks/notebook11_dive.html#number-of-device-per-user-some-mental-gymnastics",
    "href": "core/notebooks/notebook11_dive.html#number-of-device-per-user-some-mental-gymnastics",
    "title": "Diving deeer",
    "section": "Number of device per user: some mental gymnastics",
    "text": "Number of device per user: some mental gymnastics\n\n\nCode\nxid_partition = Window.partitionBy('xid')\nrank_device = func.dense_rank().over(xid_partition.orderBy('device'))\nn_unique_device = func.last(rank_device).over(xid_partition)\ndf = df.withColumn('n_device', n_unique_device)\ndf.head(n=2)\n\n\n[Row(xid='0006cea7-1679-4264-bdef-0cd089749ede', action='O', date=datetime.datetime(2016, 12, 26, 13, 41, 8), website_id='51', url='https://www.footlegende.fr/mercato-psg-coutinho-10166', category_id=1002.0, zipcode='34290', device='TAB', n_events=1, n_days_since_last_event=3020, n_events_per_device=1, n_device=1),\n Row(xid='000893c8-a14b-4f33-858f-210440f37def', action='O', date=datetime.datetime(2016, 12, 23, 16, 18, 37), website_id='56', url='http://blague.dumatin.fr/', category_id=1002.0, zipcode=None, device='DSK', n_events=1, n_days_since_last_event=3023, n_events_per_device=1, n_device=1)]\n\n\n\n\nCode\ndf\\\n    .where(col('n_device') &gt; 1)\\\n    .select('xid', 'device', 'n_events',  'n_device', 'n_events_per_device')\\\n    .head(n=8)\n\n\n[Row(xid='4c1dc79d-a140-4da9-ae28-540b4503c3b8', device='DSK', n_events=6, n_device=2, n_events_per_device=5),\n Row(xid='4c1dc79d-a140-4da9-ae28-540b4503c3b8', device='DSK', n_events=6, n_device=2, n_events_per_device=5),\n Row(xid='4c1dc79d-a140-4da9-ae28-540b4503c3b8', device='DSK', n_events=6, n_device=2, n_events_per_device=5),\n Row(xid='4c1dc79d-a140-4da9-ae28-540b4503c3b8', device='DSK', n_events=6, n_device=2, n_events_per_device=5),\n Row(xid='4c1dc79d-a140-4da9-ae28-540b4503c3b8', device='DSK', n_events=6, n_device=2, n_events_per_device=5),\n Row(xid='4c1dc79d-a140-4da9-ae28-540b4503c3b8', device='MOB', n_events=6, n_device=2, n_events_per_device=1),\n Row(xid='78156cdf-7229-46eb-bb6b-92d384f9a6fa', device='DSK', n_events=6, n_device=2, n_events_per_device=5),\n Row(xid='78156cdf-7229-46eb-bb6b-92d384f9a6fa', device='DSK', n_events=6, n_device=2, n_events_per_device=5)]"
  },
  {
    "objectID": "core/notebooks/notebook11_dive.html#extraction",
    "href": "core/notebooks/notebook11_dive.html#extraction",
    "title": "Diving deeer",
    "section": "Extraction",
    "text": "Extraction\nExtraction is easy here, it’s just about reading the data\n\n\nCode\ndf = spark.read.parquet(input_file)\ndf.head(n=3)\n\n\n[Row(xid='001ff9b6-5383-4221-812d-58c2c3f234cc', action='O', date=datetime.datetime(2017, 1, 25, 7, 2, 18), website_id='3', url='http://www.8chances.com/grille', category_id=1002.0, zipcode='11370', device='SMP'),\n Row(xid='0056ab7a-3cba-4ed5-a495-3d4abf79ab66', action='O', date=datetime.datetime(2016, 12, 28, 9, 47, 8), website_id='54', url='http://www.salaire-brut-en-net.fr/differences-brut-net/', category_id=1002.0, zipcode='86000', device='DSK'),\n Row(xid='005ae4ab-363a-41a0-b8f9-faee47d622a4', action='O', date=datetime.datetime(2017, 1, 27, 22, 21, 6), website_id='74', url='http://www.realite-virtuelle.com/top-applications-horreur-vr-halloween', category_id=1002.0, zipcode='49700', device='DSK')]"
  },
  {
    "objectID": "core/notebooks/notebook11_dive.html#transformation-of-the-data",
    "href": "core/notebooks/notebook11_dive.html#transformation-of-the-data",
    "title": "Diving deeer",
    "section": "Transformation of the data",
    "text": "Transformation of the data\nAt this step we compute a lot of extra things from the data. The aim is to build features that describe users.\n\n\nCode\ndef n_events_transformer(df):\n    xid_partition = Window.partitionBy('xid')\n    n_events = func.count(col('action')).over(xid_partition)\n    df = df.withColumn('n_events', n_events)\n    return df\n\ndef n_events_per_action_transformer(df):\n    xid_action_partition = Window.partitionBy('xid', 'action')\n    n_events_per_action = func.count(col('action')).over(xid_action_partition)\n    df = df.withColumn('n_events_per_action', n_events_per_action)\n    return df\n\ndef hour_transformer(df):\n    hour = func.hour(col('date'))\n    df = df.withColumn('hour', hour)\n    return df\n\ndef weekday_transformer(df):\n    weekday = func.date_format(col('date'), 'EEEE')\n    df = df.withColumn('weekday', weekday)\n    return df\n\ndef n_events_per_hour_transformer(df):\n    xid_hour_partition = Window.partitionBy('xid', 'hour')\n    n_events_per_hour = func.count(col('action')).over(xid_hour_partition)\n    df = df.withColumn('n_events_per_hour', n_events_per_hour)\n    return df\n\ndef n_events_per_weekday_transformer(df):\n    xid_weekday_partition = Window.partitionBy('xid', 'weekday')\n    n_events_per_weekday = func.count(col('action')).over(xid_weekday_partition)\n    df = df.withColumn('n_events_per_weekday', n_events_per_weekday)\n    return df\n\ndef n_days_since_last_event_transformer(df):\n    xid_partition = Window.partitionBy('xid')\n    max_date = func.max(col('date')).over(xid_partition)\n    n_days_since_last_event = func.datediff(func.current_date(), max_date)\n    df = df.withColumn('n_days_since_last_event',\n                       n_days_since_last_event + lit(0.1))\n    return df\n\ndef n_days_since_last_action_transformer(df):\n    xid_partition_action = Window.partitionBy('xid', 'action')\n    max_date = func.max(col('date')).over(xid_partition_action)\n    n_days_since_last_action = func.datediff(func.current_date(),\n                                                        max_date)\n    df = df.withColumn('n_days_since_last_action',\n                       n_days_since_last_action + lit(0.1))\n    return df\n\ndef n_unique_day_transformer(df):\n    xid_partition = Window.partitionBy('xid')\n    dayofyear = func.dayofyear(col('date'))\n    rank_day = func.dense_rank().over(xid_partition.orderBy(dayofyear))\n    n_unique_day = func.last(rank_day).over(xid_partition)\n    df = df.withColumn('n_unique_day', n_unique_day)\n    return df\n\ndef n_unique_hour_transformer(df):\n    xid_partition = Window.partitionBy('xid')\n    rank_hour = func.dense_rank().over(xid_partition.orderBy('hour'))\n    n_unique_hour = func.last(rank_hour).over(xid_partition)\n    df = df.withColumn('n_unique_hour', n_unique_hour)\n    return df\n\ndef n_events_per_device_transformer(df):\n    xid_device_partition = Window.partitionBy('xid', 'device')\n    n_events_per_device = func.count(func.col('device')) \\\n        .over(xid_device_partition)\n    df = df.withColumn('n_events_per_device', n_events_per_device)\n    return df\n\ndef n_unique_device_transformer(df):\n    xid_partition = Window.partitionBy('xid')\n    rank_device = func.dense_rank().over(xid_partition.orderBy('device'))\n    n_unique_device = func.last(rank_device).over(xid_partition)\n    df = df.withColumn('n_device', n_unique_device)\n    return df\n\ndef n_actions_per_category_id_transformer(df):\n    xid_category_id_partition = Window.partitionBy('xid', 'category_id',\n                                                   'action')\n    n_actions_per_category_id = func.count(func.col('action')) \\\n        .over(xid_category_id_partition)\n    df = df.withColumn('n_actions_per_category_id', n_actions_per_category_id)\n    return df\n\ndef n_unique_category_id_transformer(df):\n    xid_partition = Window.partitionBy('xid')\n    rank_category_id = func.dense_rank().over(xid_partition\\\n                                              .orderBy('category_id'))\n    n_unique_category_id = func.last(rank_category_id).over(xid_partition)\n    df = df.withColumn('n_unique_category_id', n_unique_category_id)\n    return df\n\ndef n_events_per_category_id_transformer(df):\n    xid_category_id_partition = Window.partitionBy('xid', 'category_id')\n    n_events_per_category_id = func.count(func.col('action')) \\\n        .over(xid_category_id_partition)\n    df = df.withColumn('n_events_per_category_id', n_events_per_category_id)\n    return df\n\ndef n_events_per_website_id_transformer(df):\n    xid_website_id_partition = Window.partitionBy('xid', 'website_id')\n    n_events_per_website_id = func.count(col('action'))\\\n        .over(xid_website_id_partition)\n    df = df.withColumn('n_events_per_website_id', n_events_per_website_id)\n    return df\n\n\n\n\nCode\ntransformers = [\n    hour_transformer,\n    weekday_transformer,\n    n_events_per_hour_transformer,\n    n_events_per_weekday_transformer,\n    n_days_since_last_event_transformer,\n    n_days_since_last_action_transformer,\n    n_unique_day_transformer,\n    n_unique_hour_transformer,\n    n_events_per_device_transformer,\n    n_unique_device_transformer,\n    n_actions_per_category_id_transformer,\n    n_events_per_category_id_transformer,\n    n_events_per_website_id_transformer,\n]\n\nfor transformer in transformers:\n    df = transformer(df)\n\ndf.head(n=1)\n\n\n[Stage 35:&gt;                                                         (0 + 1) / 1]                                                                                \n\n\n[Row(xid='0006cea7-1679-4264-bdef-0cd089749ede', action='O', date=datetime.datetime(2016, 12, 26, 13, 41, 8), website_id='51', url='https://www.footlegende.fr/mercato-psg-coutinho-10166', category_id=1002.0, zipcode='34290', device='TAB', hour=13, weekday='Monday', n_events_per_hour=1, n_events_per_weekday=1, n_days_since_last_event=3020.1, n_days_since_last_action=3020.1, n_unique_day=1, n_unique_hour=1, n_events_per_device=1, n_device=1, n_actions_per_category_id=1, n_events_per_category_id=1, n_events_per_website_id=1)]\n\n\n\n\nCode\nsorted(df.columns)\n\n\n['action',\n 'category_id',\n 'date',\n 'device',\n 'hour',\n 'n_actions_per_category_id',\n 'n_days_since_last_action',\n 'n_days_since_last_event',\n 'n_device',\n 'n_events_per_category_id',\n 'n_events_per_device',\n 'n_events_per_hour',\n 'n_events_per_website_id',\n 'n_events_per_weekday',\n 'n_unique_day',\n 'n_unique_hour',\n 'url',\n 'website_id',\n 'weekday',\n 'xid',\n 'zipcode']"
  },
  {
    "objectID": "core/notebooks/notebook11_dive.html#load-step",
    "href": "core/notebooks/notebook11_dive.html#load-step",
    "title": "Diving deeer",
    "section": "Load step",
    "text": "Load step\nHere, we use all the previous computations (saved in the columns of the dataframe) to compute aggregated informations about each user.\n\n\nCode\ndef n_events_per_hour_loader(df):\n    csr = df\\\n        .select('xid', 'hour', 'n_events_per_hour')\\\n        .withColumnRenamed('n_events_per_hour', 'value')\\\n        .distinct()\n    feature_name = func.concat(lit('n_events_per_hour#'), col('hour'))\n    csr = csr\\\n        .withColumn('feature_name', feature_name)\\\n        .drop('hour')\n    return csr\n\ndef n_events_per_website_id_loader(df):\n    csr = df.select('xid', 'website_id', 'n_events_per_website_id')\\\n        .withColumnRenamed('n_events_per_hour', 'value')\\\n        .distinct()\n    feature_name = func.concat(lit('n_events_per_website_id#'),\n                               col('website_id'))\n    csr = csr\\\n        .withColumn('feature_name', feature_name)\\\n        .drop('website_id')\n    return csr\n\ndef n_events_per_hour_loader(df):\n    csr = df\\\n        .select('xid', 'hour', 'n_events_per_hour')\\\n        .withColumnRenamed('n_events_per_hour', 'value')\\\n        .distinct()\n    feature_name = func.concat(lit('n_events_per_hour#'), col('hour'))\n    csr = csr\\\n        .withColumn('feature_name', feature_name)\\\n        .drop('hour')\n    return csr\n\ndef n_events_per_weekday_loader(df):\n    csr = df\\\n        .select('xid', 'weekday', 'n_events_per_weekday')\\\n        .withColumnRenamed('n_events_per_weekday', 'value')\\\n        .distinct()\n    feature_name = func.concat(lit('n_events_per_weekday#'), col('weekday'))\n    csr = csr\\\n        .withColumn('feature_name', feature_name)\\\n        .drop('weekday')\n    return csr\n\ndef n_days_since_last_event_loader(df):\n    csr = df.select('xid',  'n_days_since_last_event')\\\n        .withColumnRenamed('n_days_since_last_event#', 'value')\\\n        .distinct()\n    feature_name = lit('n_days_since_last_event')\n    csr = csr\\\n        .withColumn('feature_name', feature_name)\n    return csr\n\ndef n_days_since_last_action_loader(df):\n    csr = df.select('xid', 'action', 'n_days_since_last_action')\\\n        .withColumnRenamed('n_days_since_last_action', 'value')\\\n        .distinct()\n    feature_name = func.concat(lit('n_days_since_last_action#'), col('action'))\n    csr = csr\\\n        .withColumn('feature_name', feature_name)\\\n        .drop('action')\n    return csr\n\ndef n_unique_day_loader(df):\n    csr = df.select('xid', 'n_unique_day')\\\n        .withColumnRenamed('n_unique_day', 'value')\\\n        .distinct()\n    feature_name = lit('n_unique_day')\n    csr = csr\\\n        .withColumn('feature_name', feature_name)\n    return csr\n\ndef n_unique_hour_loader(df):\n    csr = df.select('xid', 'n_unique_hour')\\\n        .withColumnRenamed('n_unique_hour', 'value')\\\n        .distinct()\n    feature_name = lit('n_unique_hour')\n    csr = csr\\\n        .withColumn('feature_name', feature_name)\n    return csr\n\ndef n_events_per_device_loader(df):\n    csr = df\\\n        .select('xid', 'device', 'n_events_per_device')\\\n        .withColumnRenamed('n_events_per_device', 'value')\\\n        .distinct()\n    feature_name = func.concat(lit('n_events_per_device#'), col('device'))\n    csr = csr\\\n        .withColumn('feature_name', feature_name)\\\n        .drop('device')\n    return csr\n\ndef n_unique_device_loader(df):\n    csr = df.select('xid', 'n_device')\\\n        .withColumnRenamed('n_device', 'value')\\\n        .distinct()\n    feature_name = lit('n_device')\n    csr = csr\\\n        .withColumn('feature_name', feature_name)\n    return csr\n\ndef n_events_per_category_id_loader(df):\n    csr = df.select('xid', 'category_id', 'n_events_per_category_id')\\\n        .withColumnRenamed('n_events_per_category_id', 'value')\\\n        .distinct()\n    feature_name = func.concat(lit('n_events_per_category_id#'),\n                               col('category_id'))\n    csr = csr\\\n        .withColumn('feature_name', feature_name)\\\n        .drop('category_id')\n    return csr\n\ndef n_actions_per_category_id_loader(df):\n    csr = df.select('xid', 'category_id', 'action', 'n_actions_per_category_id')\\\n        .withColumnRenamed('n_actions_per_category_id', 'value')\\\n        .distinct()\n    feature_name = func.concat(lit('n_actions_per_category_id#'),\n                               col('action'), lit('#'), \n                               col('category_id'))\n    csr = csr\\\n        .withColumn('feature_name', feature_name)\\\n        .drop('category_id')\\\n        .drop('action')\n    return csr\n\ndef n_events_per_website_id_loader(df):\n    csr = df.select('xid', 'website_id', 'n_events_per_website_id')\\\n        .withColumnRenamed('n_events_per_website_id', 'value')\\\n        .distinct()\n    feature_name = func.concat(lit('n_events_per_website_id#'),\n                               col('website_id'))\n    csr = csr\\\n        .withColumn('feature_name', feature_name)\\\n        .drop('website_id')\n    return csr\n\n\n\n\nCode\nfrom functools import reduce\n\nloaders = [\n    n_events_per_hour_loader,\n    n_events_per_website_id_loader,\n    n_events_per_hour_loader,\n    n_events_per_weekday_loader,\n    n_days_since_last_event_loader,\n    n_days_since_last_action_loader,\n    n_unique_day_loader,\n    n_unique_hour_loader,\n    n_events_per_device_loader,\n    n_unique_device_loader,\n    n_events_per_category_id_loader,\n    n_actions_per_category_id_loader,\n    n_events_per_website_id_loader,\n]\n\ndef union(df, other):\n    return df.union(other)\n\ncsr = reduce(\n    lambda df1, df2: df1.union(df2),\n    [loader(df) for loader in loaders]\n)\n\ncsr.head(n=3)\n\n\n[Stage 38:=&gt;(10 + 2) / 12][Stage 39:==&gt;(9 + 3) / 12][Stage 40:=&gt; (6 + 6) / 12][Stage 40:=&gt; (6 + 6) / 12][Stage 41:=&gt; (6 + 6) / 12][Stage 42:=&gt; (6 + 6) / 12][Stage 43:==&gt;(8 + 4) / 12][Stage 44:=&gt; (6 + 6) / 12][Stage 45:&gt;  (3 + 9) / 12][Stage 44:=&gt;(10 + 2) / 12][Stage 45:=&gt; (6 + 6) / 12][Stage 46:=&gt; (6 + 6) / 12][Stage 48:&gt;               (0 + 19) / 19][Stage 50:&gt;                (1 + 1) / 20][Stage 50:==============&gt;                                         (5 + 15) / 20][Stage 64:&gt;                                                         (0 + 1) / 1]                                                                                \n\n\n[Row(xid='000095cc-9a61-49b5-8ad5-83442daa93d6', value=2.0, feature_name='n_events_per_hour#21'),\n Row(xid='0000fa20-47ca-4548-82e9-78d81aa83fba', value=1.0, feature_name='n_events_per_hour#23'),\n Row(xid='00010386-a996-48ad-9888-4df5440188f2', value=1.0, feature_name='n_events_per_hour#21')]\n\n\n\n\nCode\ncsr.columns\n\n\n['xid', 'value', 'feature_name']\n\n\n\n\nCode\ncsr.count()\n\n\n[Stage 65:=&gt;(10 + 2) / 12][Stage 67:=&gt; (7 + 5) / 12][Stage 68:==&gt;(9 + 3) / 12][Stage 69:=========&gt;       (7 + 5) / 12][Stage 70:===========&gt;     (8 + 4) / 12][Stage 72:=&gt; (7 + 5) / 12][Stage 73:=&gt; (6 + 6) / 12][Stage 74:=&gt; (6 + 6) / 12][Stage 75:==========================================&gt;              (9 + 3) / 12]25/04/03 15:11:06 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n25/04/03 15:11:06 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n[Stage 93:&gt;                                                       (0 + 20) / 20]25/04/03 15:11:06 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n25/04/03 15:11:06 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n25/04/03 15:11:07 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n25/04/03 15:11:07 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n25/04/03 15:11:08 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n25/04/03 15:11:08 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n25/04/03 15:11:08 WARN TaskMemoryManager: Failed to allocate a page (4194304 bytes), try again.\n25/04/03 15:11:08 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n25/04/03 15:11:08 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n25/04/03 15:11:08 WARN TaskMemoryManager: Failed to allocate a page (4194304 bytes), try again.\n\n\n[19,167s][warning][gc,alloc] Executor task launch worker for task 7.0 in stage 93.0 (TID 527): Retried waiting for GCLocker too often allocating 524290 words\n[19,195s][warning][gc,alloc] Executor task launch worker for task 9.0 in stage 93.0 (TID 529): Retried waiting for GCLocker too often allocating 524290 words\n[19,195s][warning][gc,alloc] Executor task launch worker for task 7.0 in stage 93.0 (TID 527): Retried waiting for GCLocker too often allocating 524290 words\n[19,214s][warning][gc,alloc] Executor task launch worker for task 7.0 in stage 93.0 (TID 527): Retried waiting for GCLocker too often allocating 486289 words\n[19,215s][warning][gc,alloc] Executor task launch worker for task 9.0 in stage 93.0 (TID 529): Retried waiting for GCLocker too often allocating 131074 words\n[19,215s][warning][gc,alloc] Executor task launch worker for task 1.0 in stage 93.0 (TID 521): Retried waiting for GCLocker too often allocating 131074 words\n[19,218s][warning][gc,alloc] Executor task launch worker for task 3.0 in stage 93.0 (TID 523): Retried waiting for GCLocker too often allocating 524290 words\n[19,225s][warning][gc,alloc] Executor task launch worker for task 3.0 in stage 93.0 (TID 523): Retried waiting for GCLocker too often allocating 131074 words\n\n\n25/04/03 15:11:08 WARN TaskMemoryManager: Failed to allocate a page (4194304 bytes), try again.\n25/04/03 15:11:08 WARN TaskMemoryManager: Failed to allocate a page (4194304 bytes), try again.\n25/04/03 15:11:08 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n25/04/03 15:11:08 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n25/04/03 15:11:08 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n25/04/03 15:11:08 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n25/04/03 15:11:08 WARN TaskMemoryManager: Failed to allocate a page (3890290 bytes), try again.\n25/04/03 15:11:08 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n25/04/03 15:11:08 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n25/04/03 15:11:08 WARN TaskMemoryManager: Failed to allocate a page (4194304 bytes), try again.\n25/04/03 15:11:08 ERROR Executor: Exception in task 9.0 in stage 93.0 (TID 529)\njava.lang.OutOfMemoryError: Java heap space\n25/04/03 15:11:08 ERROR Executor: Exception in task 3.0 in stage 93.0 (TID 523)\njava.lang.OutOfMemoryError: Java heap space\n25/04/03 15:11:08 ERROR Executor: Exception in task 1.0 in stage 93.0 (TID 521)\njava.lang.OutOfMemoryError: Java heap space\n25/04/03 15:11:08 ERROR SparkUncaughtExceptionHandler: Uncaught exception in thread Thread[Executor task launch worker for task 1.0 in stage 93.0 (TID 521),5,main]\njava.lang.OutOfMemoryError: Java heap space\n25/04/03 15:11:08 ERROR SparkUncaughtExceptionHandler: [Container in shutdown] Uncaught exception in thread Thread[Executor task launch worker for task 9.0 in stage 93.0 (TID 529),5,main]\njava.lang.OutOfMemoryError: Java heap space\n25/04/03 15:11:08 ERROR SparkUncaughtExceptionHandler: [Container in shutdown] Uncaught exception in thread Thread[Executor task launch worker for task 3.0 in stage 93.0 (TID 523),5,main]\njava.lang.OutOfMemoryError: Java heap space\n25/04/03 15:11:08 WARN TaskSetManager: Lost task 1.0 in stage 93.0 (TID 521) (host-32-10.sg.lan executor driver): java.lang.OutOfMemoryError: Java heap space\n\n25/04/03 15:11:08 ERROR TaskSetManager: Task 1 in stage 93.0 failed 1 times; aborting job\n25/04/03 15:11:08 WARN TaskSetManager: Lost task 18.0 in stage 93.0 (TID 538) (host-32-10.sg.lan executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 1 in stage 93.0 failed 1 times, most recent failure: Lost task 1.0 in stage 93.0 (TID 521) (host-32-10.sg.lan executor driver): java.lang.OutOfMemoryError: Java heap space\n\nDriver stacktrace:)\n25/04/03 15:11:08 WARN TaskSetManager: Lost task 14.0 in stage 93.0 (TID 534) (host-32-10.sg.lan executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 1 in stage 93.0 failed 1 times, most recent failure: Lost task 1.0 in stage 93.0 (TID 521) (host-32-10.sg.lan executor driver): java.lang.OutOfMemoryError: Java heap space\n\nDriver stacktrace:)\n25/04/03 15:11:08 WARN TaskSetManager: Lost task 4.0 in stage 93.0 (TID 524) (host-32-10.sg.lan executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 1 in stage 93.0 failed 1 times, most recent failure: Lost task 1.0 in stage 93.0 (TID 521) (host-32-10.sg.lan executor driver): java.lang.OutOfMemoryError: Java heap space\n\nDriver stacktrace:)\n25/04/03 15:11:08 WARN TaskSetManager: Lost task 15.0 in stage 93.0 (TID 535) (host-32-10.sg.lan executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 1 in stage 93.0 failed 1 times, most recent failure: Lost task 1.0 in stage 93.0 (TID 521) (host-32-10.sg.lan executor driver): java.lang.OutOfMemoryError: Java heap space\n\nDriver stacktrace:)\n25/04/03 15:11:08 WARN TaskSetManager: Lost task 12.0 in stage 93.0 (TID 532) (host-32-10.sg.lan executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 1 in stage 93.0 failed 1 times, most recent failure: Lost task 1.0 in stage 93.0 (TID 521) (host-32-10.sg.lan executor driver): java.lang.OutOfMemoryError: Java heap space\n\nDriver stacktrace:)\n25/04/03 15:11:08 WARN TaskSetManager: Lost task 5.0 in stage 93.0 (TID 525) (host-32-10.sg.lan executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 1 in stage 93.0 failed 1 times, most recent failure: Lost task 1.0 in stage 93.0 (TID 521) (host-32-10.sg.lan executor driver): java.lang.OutOfMemoryError: Java heap space\n\nDriver stacktrace:)\n25/04/03 15:11:08 WARN TaskSetManager: Lost task 7.0 in stage 93.0 (TID 527) (host-32-10.sg.lan executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 1 in stage 93.0 failed 1 times, most recent failure: Lost task 1.0 in stage 93.0 (TID 521) (host-32-10.sg.lan executor driver): java.lang.OutOfMemoryError: Java heap space\n\nDriver stacktrace:)\n25/04/03 15:11:08 WARN TaskSetManager: Lost task 0.0 in stage 93.0 (TID 520) (host-32-10.sg.lan executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 1 in stage 93.0 failed 1 times, most recent failure: Lost task 1.0 in stage 93.0 (TID 521) (host-32-10.sg.lan executor driver): java.lang.OutOfMemoryError: Java heap space\n\nDriver stacktrace:)\n25/04/03 15:11:08 WARN TaskSetManager: Lost task 2.0 in stage 93.0 (TID 522) (host-32-10.sg.lan executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 1 in stage 93.0 failed 1 times, most recent failure: Lost task 1.0 in stage 93.0 (TID 521) (host-32-10.sg.lan executor driver): java.lang.OutOfMemoryError: Java heap space\n\nDriver stacktrace:)\n25/04/03 15:11:08 WARN TaskSetManager: Lost task 17.0 in stage 93.0 (TID 537) (host-32-10.sg.lan executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 1 in stage 93.0 failed 1 times, most recent failure: Lost task 1.0 in stage 93.0 (TID 521) (host-32-10.sg.lan executor driver): java.lang.OutOfMemoryError: Java heap space\n\nDriver stacktrace:)\n25/04/03 15:11:08 WARN TaskSetManager: Lost task 13.0 in stage 93.0 (TID 533) (host-32-10.sg.lan executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 1 in stage 93.0 failed 1 times, most recent failure: Lost task 1.0 in stage 93.0 (TID 521) (host-32-10.sg.lan executor driver): java.lang.OutOfMemoryError: Java heap space\n\nDriver stacktrace:)\n25/04/03 15:11:08 WARN TaskSetManager: Lost task 8.0 in stage 93.0 (TID 528) (host-32-10.sg.lan executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 1 in stage 93.0 failed 1 times, most recent failure: Lost task 1.0 in stage 93.0 (TID 521) (host-32-10.sg.lan executor driver): java.lang.OutOfMemoryError: Java heap space\n\nDriver stacktrace:)\n25/04/03 15:11:08 WARN TaskSetManager: Lost task 19.0 in stage 93.0 (TID 539) (host-32-10.sg.lan executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 1 in stage 93.0 failed 1 times, most recent failure: Lost task 1.0 in stage 93.0 (TID 521) (host-32-10.sg.lan executor driver): java.lang.OutOfMemoryError: Java heap space\n\nDriver stacktrace:)\n25/04/03 15:11:08 WARN TaskSetManager: Lost task 11.0 in stage 93.0 (TID 531) (host-32-10.sg.lan executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 1 in stage 93.0 failed 1 times, most recent failure: Lost task 1.0 in stage 93.0 (TID 521) (host-32-10.sg.lan executor driver): java.lang.OutOfMemoryError: Java heap space\n\nDriver stacktrace:)\n25/04/03 15:11:08 WARN TaskSetManager: Lost task 6.0 in stage 93.0 (TID 526) (host-32-10.sg.lan executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 1 in stage 93.0 failed 1 times, most recent failure: Lost task 1.0 in stage 93.0 (TID 521) (host-32-10.sg.lan executor driver): java.lang.OutOfMemoryError: Java heap space\n\nDriver stacktrace:)\n25/04/03 15:11:08 WARN TaskSetManager: Lost task 16.0 in stage 93.0 (TID 536) (host-32-10.sg.lan executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 1 in stage 93.0 failed 1 times, most recent failure: Lost task 1.0 in stage 93.0 (TID 521) (host-32-10.sg.lan executor driver): java.lang.OutOfMemoryError: Java heap space\n\nDriver stacktrace:)\n25/04/03 15:11:08 WARN TaskSetManager: Lost task 10.0 in stage 93.0 (TID 530) (host-32-10.sg.lan executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 1 in stage 93.0 failed 1 times, most recent failure: Lost task 1.0 in stage 93.0 (TID 521) (host-32-10.sg.lan executor driver): java.lang.OutOfMemoryError: Java heap space\n\nDriver stacktrace:)\nERROR:root:Exception while sending command.\nTraceback (most recent call last):\n  File \"/home/boucheron/sandbox/IFEBY310/.venv/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3577, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"/tmp/ipykernel_87987/3594614522.py\", line 1, in &lt;module&gt;\n    csr.count()\n  File \"/home/boucheron/sandbox/IFEBY310/.venv/lib/python3.12/site-packages/pyspark/sql/dataframe.py\", line 1240, in count\n    return int(self._jdf.count())\n               ^^^^^^^^^^^^^^^^^\n  File \"/home/boucheron/sandbox/IFEBY310/.venv/lib/python3.12/site-packages/py4j/java_gateway.py\", line 1322, in __call__\n    return_value = get_return_value(\n                   ^^^^^^^^^^^^^^^^^\n  File \"/home/boucheron/sandbox/IFEBY310/.venv/lib/python3.12/site-packages/pyspark/errors/exceptions/captured.py\", line 179, in deco\n    return f(*a, **kw)\n           ^^^^^^^^^^^\n  File \"/home/boucheron/sandbox/IFEBY310/.venv/lib/python3.12/site-packages/py4j/protocol.py\", line 326, in get_return_value\n    raise Py4JJavaError(\npy4j.protocol.Py4JJavaError: &lt;exception str() failed&gt;\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/home/boucheron/sandbox/IFEBY310/.venv/lib/python3.12/site-packages/py4j/clientserver.py\", line 516, in send_command\n    raise Py4JNetworkError(\"Answer from Java side is empty\")\npy4j.protocol.Py4JNetworkError: Answer from Java side is empty\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/home/boucheron/sandbox/IFEBY310/.venv/lib/python3.12/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n    response = connection.send_command(command)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/boucheron/sandbox/IFEBY310/.venv/lib/python3.12/site-packages/py4j/clientserver.py\", line 539, in send_command\n    raise Py4JNetworkError(\npy4j.protocol.Py4JNetworkError: Error while sending or receiving\nERROR:root:Exception while sending command.\nTraceback (most recent call last):\n  File \"/home/boucheron/sandbox/IFEBY310/.venv/lib/python3.12/site-packages/py4j/clientserver.py\", line 511, in send_command\n    answer = smart_decode(self.stream.readline()[:-1])\n                          ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.12/socket.py\", line 707, in readinto\n    return self._sock.recv_into(b)\n           ^^^^^^^^^^^^^^^^^^^^^^^\nConnectionResetError: [Errno 104] Connection reset by peer\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/home/boucheron/sandbox/IFEBY310/.venv/lib/python3.12/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n    response = connection.send_command(command)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/boucheron/sandbox/IFEBY310/.venv/lib/python3.12/site-packages/py4j/clientserver.py\", line 539, in send_command\n    raise Py4JNetworkError(\npy4j.protocol.Py4JNetworkError: Error while sending or receiving\n\n\n\n---------------------------------------------------------------------------\nPy4JJavaError                             Traceback (most recent call last)\n    [... skipping hidden 1 frame]\n\nCell In[26], line 1\n----&gt; 1 csr.count()\n\nFile ~/sandbox/IFEBY310/.venv/lib/python3.12/site-packages/pyspark/sql/dataframe.py:1240, in DataFrame.count(self)\n   1218 \"\"\"Returns the number of rows in this :class:`DataFrame`.\n   1219 \n   1220 .. versionadded:: 1.3.0\n   (...)\n   1238 3\n   1239 \"\"\"\n-&gt; 1240 return int(self._jdf.count())\n\nFile ~/sandbox/IFEBY310/.venv/lib/python3.12/site-packages/py4j/java_gateway.py:1322, in JavaMember.__call__(self, *args)\n   1321 answer = self.gateway_client.send_command(command)\n-&gt; 1322 return_value = get_return_value(\n   1323     answer, self.gateway_client, self.target_id, self.name)\n   1325 for temp_arg in temp_args:\n\nFile ~/sandbox/IFEBY310/.venv/lib/python3.12/site-packages/pyspark/errors/exceptions/captured.py:179, in capture_sql_exception.&lt;locals&gt;.deco(*a, **kw)\n    178 try:\n--&gt; 179     return f(*a, **kw)\n    180 except Py4JJavaError as e:\n\nFile ~/sandbox/IFEBY310/.venv/lib/python3.12/site-packages/py4j/protocol.py:326, in get_return_value(answer, gateway_client, target_id, name)\n    325 if answer[1] == REFERENCE_TYPE:\n--&gt; 326     raise Py4JJavaError(\n    327         \"An error occurred while calling {0}{1}{2}.\\n\".\n    328         format(target_id, \".\", name), value)\n    329 else:\n\n&lt;class 'str'&gt;: (&lt;class 'ConnectionRefusedError'&gt;, ConnectionRefusedError(111, 'Connection refused'))\n\nDuring handling of the above exception, another exception occurred:\n\nConnectionRefusedError                    Traceback (most recent call last)\n    [... skipping hidden 1 frame]\n\nFile ~/sandbox/IFEBY310/.venv/lib/python3.12/site-packages/IPython/core/interactiveshell.py:2179, in InteractiveShell.showtraceback(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\n   2176         traceback.print_exc()\n   2177         return None\n-&gt; 2179     self._showtraceback(etype, value, stb)\n   2180 if self.call_pdb:\n   2181     # drop into debugger\n   2182     self.debugger(force=True)\n\nFile ~/sandbox/IFEBY310/.venv/lib/python3.12/site-packages/ipykernel/zmqshell.py:559, in ZMQInteractiveShell._showtraceback(self, etype, evalue, stb)\n    553 sys.stdout.flush()\n    554 sys.stderr.flush()\n    556 exc_content = {\n    557     \"traceback\": stb,\n    558     \"ename\": str(etype.__name__),\n--&gt; 559     \"evalue\": str(evalue),\n    560 }\n    562 dh = self.displayhook\n    563 # Send exception info over pub socket for other clients than the caller\n    564 # to pick up\n\nFile ~/sandbox/IFEBY310/.venv/lib/python3.12/site-packages/py4j/protocol.py:471, in Py4JJavaError.__str__(self)\n    469 def __str__(self):\n    470     gateway_client = self.java_exception._gateway_client\n--&gt; 471     answer = gateway_client.send_command(self.exception_cmd)\n    472     return_value = get_return_value(answer, gateway_client, None, None)\n    473     # Note: technically this should return a bytestring 'str' rather than\n    474     # unicodes in Python 2; however, it can return unicodes for now.\n    475     # See https://github.com/bartdag/py4j/issues/306 for more details.\n\nFile ~/sandbox/IFEBY310/.venv/lib/python3.12/site-packages/py4j/java_gateway.py:1036, in GatewayClient.send_command(self, command, retry, binary)\n   1015 def send_command(self, command, retry=True, binary=False):\n   1016     \"\"\"Sends a command to the JVM. This method is not intended to be\n   1017        called directly by Py4J users. It is usually called by\n   1018        :class:`JavaMember` instances.\n   (...)\n   1034      if `binary` is `True`.\n   1035     \"\"\"\n-&gt; 1036     connection = self._get_connection()\n   1037     try:\n   1038         response = connection.send_command(command)\n\nFile ~/sandbox/IFEBY310/.venv/lib/python3.12/site-packages/py4j/clientserver.py:284, in JavaClient._get_connection(self)\n    281     pass\n    283 if connection is None or connection.socket is None:\n--&gt; 284     connection = self._create_new_connection()\n    285 return connection\n\nFile ~/sandbox/IFEBY310/.venv/lib/python3.12/site-packages/py4j/clientserver.py:291, in JavaClient._create_new_connection(self)\n    287 def _create_new_connection(self):\n    288     connection = ClientServerConnection(\n    289         self.java_parameters, self.python_parameters,\n    290         self.gateway_property, self)\n--&gt; 291     connection.connect_to_java_server()\n    292     self.set_thread_connection(connection)\n    293     return connection\n\nFile ~/sandbox/IFEBY310/.venv/lib/python3.12/site-packages/py4j/clientserver.py:438, in ClientServerConnection.connect_to_java_server(self)\n    435 if self.ssl_context:\n    436     self.socket = self.ssl_context.wrap_socket(\n    437         self.socket, server_hostname=self.java_address)\n--&gt; 438 self.socket.connect((self.java_address, self.java_port))\n    439 self.stream = self.socket.makefile(\"rb\")\n    440 self.is_connected = True\n\nConnectionRefusedError: [Errno 111] Connection refused\n\n\n\n\n\nCode\n# Replace features names and xid by a unique number\nfeature_name_partition = Window().orderBy('feature_name')\nxid_partition = Window().orderBy('xid')\n\ncol_idx = func.dense_rank().over(feature_name_partition)\nrow_idx = func.dense_rank().over(xid_partition)\n\ncsr = csr.withColumn('col', col_idx)\\\n    .withColumn('row', row_idx)\n\ncsr = csr.na.drop('any')\n\ncsr.head(n=5)\n\n\n\n---------------------------------------------------------------------------\nConnectionRefusedError                    Traceback (most recent call last)\nCell In[27], line 2\n      1 # Replace features names and xid by a unique number\n----&gt; 2 feature_name_partition = Window().orderBy('feature_name')\n      3 xid_partition = Window().orderBy('xid')\n      5 col_idx = func.dense_rank().over(feature_name_partition)\n\nFile ~/sandbox/IFEBY310/.venv/lib/python3.12/site-packages/pyspark/sql/utils.py:222, in try_remote_window.&lt;locals&gt;.wrapped(*args, **kwargs)\n    220     return getattr(Window, f.__name__)(*args, **kwargs)\n    221 else:\n--&gt; 222     return f(*args, **kwargs)\n\nFile ~/sandbox/IFEBY310/.venv/lib/python3.12/site-packages/pyspark/sql/window.py:186, in Window.orderBy(*cols)\n    137 \"\"\"\n    138 Creates a :class:`WindowSpec` with the ordering defined.\n    139 \n   (...)\n    183 +---+--------+----------+\n    184 \"\"\"\n    185 sc = get_active_spark_context()\n--&gt; 186 jspec = cast(JVMView, sc._jvm).org.apache.spark.sql.expressions.Window.orderBy(\n    187     _to_java_cols(cols)\n    188 )\n    189 return WindowSpec(jspec)\n\nFile ~/sandbox/IFEBY310/.venv/lib/python3.12/site-packages/py4j/java_gateway.py:1712, in JVMView.__getattr__(self, name)\n   1709 if name == UserHelpAutoCompletion.KEY:\n   1710     return UserHelpAutoCompletion()\n-&gt; 1712 answer = self._gateway_client.send_command(\n   1713     proto.REFLECTION_COMMAND_NAME +\n   1714     proto.REFL_GET_UNKNOWN_SUB_COMMAND_NAME + name + \"\\n\" + self._id +\n   1715     \"\\n\" + proto.END_COMMAND_PART)\n   1716 if answer == proto.SUCCESS_PACKAGE:\n   1717     return JavaPackage(name, self._gateway_client, jvm_id=self._id)\n\nFile ~/sandbox/IFEBY310/.venv/lib/python3.12/site-packages/py4j/java_gateway.py:1036, in GatewayClient.send_command(self, command, retry, binary)\n   1015 def send_command(self, command, retry=True, binary=False):\n   1016     \"\"\"Sends a command to the JVM. This method is not intended to be\n   1017        called directly by Py4J users. It is usually called by\n   1018        :class:`JavaMember` instances.\n   (...)\n   1034      if `binary` is `True`.\n   1035     \"\"\"\n-&gt; 1036     connection = self._get_connection()\n   1037     try:\n   1038         response = connection.send_command(command)\n\nFile ~/sandbox/IFEBY310/.venv/lib/python3.12/site-packages/py4j/clientserver.py:284, in JavaClient._get_connection(self)\n    281     pass\n    283 if connection is None or connection.socket is None:\n--&gt; 284     connection = self._create_new_connection()\n    285 return connection\n\nFile ~/sandbox/IFEBY310/.venv/lib/python3.12/site-packages/py4j/clientserver.py:291, in JavaClient._create_new_connection(self)\n    287 def _create_new_connection(self):\n    288     connection = ClientServerConnection(\n    289         self.java_parameters, self.python_parameters,\n    290         self.gateway_property, self)\n--&gt; 291     connection.connect_to_java_server()\n    292     self.set_thread_connection(connection)\n    293     return connection\n\nFile ~/sandbox/IFEBY310/.venv/lib/python3.12/site-packages/py4j/clientserver.py:438, in ClientServerConnection.connect_to_java_server(self)\n    435 if self.ssl_context:\n    436     self.socket = self.ssl_context.wrap_socket(\n    437         self.socket, server_hostname=self.java_address)\n--&gt; 438 self.socket.connect((self.java_address, self.java_port))\n    439 self.stream = self.socket.makefile(\"rb\")\n    440 self.is_connected = True\n\nConnectionRefusedError: [Errno 111] Connection refused\n\n\n\n\n\nCode\n# Let's save the result of our hard work into a new parquet file\noutput_path = './'\noutput_file = os.path.join(output_path, 'csr.parquet')\ncsr.write.parquet(output_file, mode='overwrite')\n\n\n\n---------------------------------------------------------------------------\nConnectionRefusedError                    Traceback (most recent call last)\nCell In[28], line 4\n      2 output_path = './'\n      3 output_file = os.path.join(output_path, 'csr.parquet')\n----&gt; 4 csr.write.parquet(output_file, mode='overwrite')\n\nFile ~/sandbox/IFEBY310/.venv/lib/python3.12/site-packages/pyspark/sql/dataframe.py:509, in DataFrame.write(self)\n    482 @property\n    483 def write(self) -&gt; DataFrameWriter:\n    484     \"\"\"\n    485     Interface for saving the content of the non-streaming :class:`DataFrame` out into external\n    486     storage.\n   (...)\n    507     &gt;&gt;&gt; _ = spark.sql(\"DROP TABLE tab2\")\n    508     \"\"\"\n--&gt; 509     return DataFrameWriter(self)\n\nFile ~/sandbox/IFEBY310/.venv/lib/python3.12/site-packages/pyspark/sql/readwriter.py:964, in DataFrameWriter.__init__(self, df)\n    962 self._df = df\n    963 self._spark = df.sparkSession\n--&gt; 964 self._jwrite = df._jdf.write()\n\nFile ~/sandbox/IFEBY310/.venv/lib/python3.12/site-packages/py4j/java_gateway.py:1321, in JavaMember.__call__(self, *args)\n   1314 args_command, temp_args = self._build_args(*args)\n   1316 command = proto.CALL_COMMAND_NAME +\\\n   1317     self.command_header +\\\n   1318     args_command +\\\n   1319     proto.END_COMMAND_PART\n-&gt; 1321 answer = self.gateway_client.send_command(command)\n   1322 return_value = get_return_value(\n   1323     answer, self.gateway_client, self.target_id, self.name)\n   1325 for temp_arg in temp_args:\n\nFile ~/sandbox/IFEBY310/.venv/lib/python3.12/site-packages/py4j/java_gateway.py:1036, in GatewayClient.send_command(self, command, retry, binary)\n   1015 def send_command(self, command, retry=True, binary=False):\n   1016     \"\"\"Sends a command to the JVM. This method is not intended to be\n   1017        called directly by Py4J users. It is usually called by\n   1018        :class:`JavaMember` instances.\n   (...)\n   1034      if `binary` is `True`.\n   1035     \"\"\"\n-&gt; 1036     connection = self._get_connection()\n   1037     try:\n   1038         response = connection.send_command(command)\n\nFile ~/sandbox/IFEBY310/.venv/lib/python3.12/site-packages/py4j/clientserver.py:284, in JavaClient._get_connection(self)\n    281     pass\n    283 if connection is None or connection.socket is None:\n--&gt; 284     connection = self._create_new_connection()\n    285 return connection\n\nFile ~/sandbox/IFEBY310/.venv/lib/python3.12/site-packages/py4j/clientserver.py:291, in JavaClient._create_new_connection(self)\n    287 def _create_new_connection(self):\n    288     connection = ClientServerConnection(\n    289         self.java_parameters, self.python_parameters,\n    290         self.gateway_property, self)\n--&gt; 291     connection.connect_to_java_server()\n    292     self.set_thread_connection(connection)\n    293     return connection\n\nFile ~/sandbox/IFEBY310/.venv/lib/python3.12/site-packages/py4j/clientserver.py:438, in ClientServerConnection.connect_to_java_server(self)\n    435 if self.ssl_context:\n    436     self.socket = self.ssl_context.wrap_socket(\n    437         self.socket, server_hostname=self.java_address)\n--&gt; 438 self.socket.connect((self.java_address, self.java_port))\n    439 self.stream = self.socket.makefile(\"rb\")\n    440 self.is_connected = True\n\nConnectionRefusedError: [Errno 111] Connection refused"
  },
  {
    "objectID": "core/notebooks/notebook11_dive.html#finally",
    "href": "core/notebooks/notebook11_dive.html#finally",
    "title": "Diving deeer",
    "section": "Finally !!",
    "text": "Finally !!\nWow ! That was a lot of work. Now we have a features matrix \\(X\\) and a vector of labels \\(y\\).\n\n\nCode\nX.indices\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[51], line 1\n----&gt; 1 X.indices\n\nNameError: name 'X' is not defined\n\n\n\n\n\nCode\nX.indptr\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[52], line 1\n----&gt; 1 X.indptr\n\nNameError: name 'X' is not defined\n\n\n\n\n\nCode\nX.shape, X.nnz\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[53], line 1\n----&gt; 1 X.shape, X.nnz\n\nNameError: name 'X' is not defined\n\n\n\n\n\nCode\ny.shape, y.sum()\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[54], line 1\n----&gt; 1 y.shape, y.sum()\n\nNameError: name 'y' is not defined"
  },
  {
    "objectID": "core/notebooks/notebookxx_pg_pandas_spark.html",
    "href": "core/notebooks/notebookxx_pg_pandas_spark.html",
    "title": "PostgreSQL and Spark",
    "section": "",
    "text": "Reading and sriting Spark Dataframes from and to Databases\nCode\nimport pyspark\nfrom pyspark import SparkConf\nfrom pyspark.sql import SparkSession\n\nimport os\nimport getpass"
  },
  {
    "objectID": "core/notebooks/notebookxx_pg_pandas_spark.html#connect-to-pg-server",
    "href": "core/notebooks/notebookxx_pg_pandas_spark.html#connect-to-pg-server",
    "title": "PostgreSQL and Spark",
    "section": "Connect to Pg server",
    "text": "Connect to Pg server\n\n\nCode\nulogin = getpass.getuser()\npw = getpass.getpass()\n\n\n\n---------------------------------------------------------------------------\nStdinNotImplementedError                  Traceback (most recent call last)\nCell In[2], line 2\n      1 ulogin = getpass.getuser()\n----&gt; 2 pw = getpass.getpass()\n\nFile ~/sandbox/IFEBY310/.venv/lib/python3.12/site-packages/ipykernel/kernelbase.py:1256, in Kernel.getpass(self, prompt, stream)\n   1254 if not self._allow_stdin:\n   1255     msg = \"getpass was called, but this frontend does not support input requests.\"\n-&gt; 1256     raise StdinNotImplementedError(msg)\n   1257 if stream is not None:\n   1258     import warnings\n\nStdinNotImplementedError: getpass was called, but this frontend does not support input requests.\n\n\n\nSpark jdbc readers and writers rely on a collection of options. Some options are used repeatedly. In order to avoid cut and paste, we pack them in a dictionary.\n\n\nCode\ndico_jdbc_pg = {\n    \"url\":  \"jdbc:postgresql://localhost:5434/bd_2023-24\",  \n    \"user\":  ulogin, \n    \"password\":  pw, \n    \"driver\":  \"org.postgresql.Driver\"\n}\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[3], line 4\n      1 dico_jdbc_pg = {\n      2     \"url\":  \"jdbc:postgresql://localhost:5434/bd_2023-24\",  \n      3     \"user\":  ulogin, \n----&gt; 4     \"password\":  pw, \n      5     \"driver\":  \"org.postgresql.Driver\"\n      6 }\n\nNameError: name 'pw' is not defined\n\n\n\n\n\nCode\ndbschema = 'nycflights'"
  },
  {
    "objectID": "core/notebooks/notebookxx_pg_pandas_spark.html#reading-spark-dataframes-from-a-postgresql-database",
    "href": "core/notebooks/notebookxx_pg_pandas_spark.html#reading-spark-dataframes-from-a-postgresql-database",
    "title": "PostgreSQL and Spark",
    "section": "Reading Spark Dataframes from a PostgreSQL database",
    "text": "Reading Spark Dataframes from a PostgreSQL database\n\n\nCode\nspark_home = \"/home/boucheron/.local/share/spark-3.5.0-bin-hadoop3\"\n\n\n\nTo get started you will need to include the JDBC driver for your particular database on the spark classpath.\n\n\n\nCode\nspark = (\n  SparkSession \n    .builder \n    .appName(\"Python Spark SQL basic example\") \n    .config(\"spark.jars\", \n            spark_home + \"/jars/\" + \"postgresql-42.7.2.jar\") \n    .getOrCreate()\n)\n\n\n25/04/03 15:11:34 WARN Utils: Your hostname, boucheron-Precision-5480 resolves to a loopback address: 127.0.1.1; using 172.23.32.10 instead (on interface eth0)\n25/04/03 15:11:34 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n25/04/03 15:11:34 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\nSetting default log level to \"WARN\".\nTo adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel)."
  },
  {
    "objectID": "core/notebooks/notebookxx_pg_pandas_spark.html#downloading-a-table-to-spark",
    "href": "core/notebooks/notebookxx_pg_pandas_spark.html#downloading-a-table-to-spark",
    "title": "PostgreSQL and Spark",
    "section": "Downloading a table to Spark",
    "text": "Downloading a table to Spark\nWe rely on dictionary union and dictionary unpacking to set the options.\n\n\nCode\ndf_airlines = (\n  spark\n    .read\n    .format(\"jdbc\")\n    .options(**(dico_jdbc_pg | {'dbtable': 'nycflights.airlines'}))\n    .load()\n)\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[7], line 5\n      1 df_airlines = (\n      2   spark\n      3     .read\n      4     .format(\"jdbc\")\n----&gt; 5     .options(**(dico_jdbc_pg | {'dbtable': 'nycflights.airlines'}))\n      6     .load()\n      7 )\n\nNameError: name 'dico_jdbc_pg' is not defined\n\n\n\n\n\nCode\ndf_airlines.show(5)\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[8], line 1\n----&gt; 1 df_airlines.show(5)\n\nNameError: name 'df_airlines' is not defined"
  },
  {
    "objectID": "core/notebooks/notebookxx_pg_pandas_spark.html#querying-the-database",
    "href": "core/notebooks/notebookxx_pg_pandas_spark.html#querying-the-database",
    "title": "PostgreSQL and Spark",
    "section": "Querying the database",
    "text": "Querying the database\n\n\nCode\nquery = \"\"\"\n    SELECT DISTINCT fl.carrier, al.name, fl.origin, fl.dest\n    FROM nycflights.airlines al JOIN \n        nycflights.flights fl ON (fl.carrier=al.carrier)\n\"\"\"\n\n\n\n\nCode\ndf_query = (\n  spark\n    .read\n    .format(\"jdbc\")\n    .options(**(dico_jdbc_pg | {'query': query}))\n    .load()\n)\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[10], line 5\n      1 df_query = (\n      2   spark\n      3     .read\n      4     .format(\"jdbc\")\n----&gt; 5     .options(**(dico_jdbc_pg | {'query': query}))\n      6     .load()\n      7 )\n\nNameError: name 'dico_jdbc_pg' is not defined\n\n\n\n\n\nCode\ndf_query.show()\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[11], line 1\n----&gt; 1 df_query.show()\n\nNameError: name 'df_query' is not defined"
  },
  {
    "objectID": "core/notebooks/notebookxx_pg_pandas_spark.html#the-end",
    "href": "core/notebooks/notebookxx_pg_pandas_spark.html#the-end",
    "title": "PostgreSQL and Spark",
    "section": "The end",
    "text": "The end\n\n\nCode\nspark.stop()"
  },
  {
    "objectID": "core/notebooks/xcitibike_spark.html",
    "href": "core/notebooks/xcitibike_spark.html",
    "title": "Building parquet dataset from extracted csv files",
    "section": "",
    "text": "Code\nimport os\nimport sys\nimport re \nimport pandas as pd\nimport datetime\nfrom tqdm import tqdm\n\nos.environ['PYSPARK_PYTHON'] = sys.executable\nos.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable\nCode\ndata_dir = \"../data\"\n# os.path.exists(data_dir)\n\nextract_dir = os.path.join(data_dir, \"xcitibike\")\nif not os.path.exists(extract_dir):\n    os.mkdir(extract_dir)\n\nparquet_dir = os.path.join(data_dir, \"pq_citibike\")\nif not os.path.exists(parquet_dir):\n    os.mkdir(parquet_dir)\n\ncheckpoint_dir = os.path.join(data_dir, \"citibike_charlie\")\nif not os.path.exists(checkpoint_dir):\n    os.mkdir(checkpoint_dir)\n\n\n\n---------------------------------------------------------------------------\nFileNotFoundError                         Traceback (most recent call last)\nCell In[2], line 6\n      4 extract_dir = os.path.join(data_dir, \"xcitibike\")\n      5 if not os.path.exists(extract_dir):\n----&gt; 6     os.mkdir(extract_dir)\n      8 parquet_dir = os.path.join(data_dir, \"pq_citibike\")\n      9 if not os.path.exists(parquet_dir):\n\nFileNotFoundError: [Errno 2] No such file or directory: '../data/xcitibike'\nCode\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql import functions as fn\nfrom pyspark.sql.functions import pandas_udf\nfrom pyspark.sql.types import BooleanType\nfrom pyspark.sql.functions import PandasUDFType\nCode\nspark = (SparkSession\n    .builder\n    .appName(\"Spark building citibike parquet file\")\n    .getOrCreate()\n)\n\n\n25/04/03 15:11:49 WARN Utils: Your hostname, boucheron-Precision-5480 resolves to a loopback address: 127.0.1.1; using 172.23.32.10 instead (on interface eth0)\n25/04/03 15:11:49 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\nSetting default log level to \"WARN\".\nTo adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n25/04/03 15:11:50 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\nCode\nspark.sparkContext.setCheckpointDir(checkpoint_dir)\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[5], line 1\n----&gt; 1 spark.sparkContext.setCheckpointDir(checkpoint_dir)\n\nNameError: name 'checkpoint_dir' is not defined\nCode\n@pandas_udf(BooleanType())\ndef detect_non_ISO(s: pd.Series) -&gt; bool:\n    r = s.str.match(r\"\\d+/\\d+/\\d+\").any()\n    return r\n\n@pandas_udf(\"string\")\ndef make_iso(s: pd.Series) -&gt; pd.Series:\n    t = s.str.split(' ', expand=True)\n    u = t[0].str.split('/')\n    v = u.map(lambda x  : [x[2], x[0], x[1]]).str.join('-')\n    w = v.combine(t[1], lambda x, y : ' '.join([x, y]))\n    return w\nCode\ndicts_rename = {\n    1: {\n 'tripduration': 'trip_duration',\n 'starttime': 'started_at',\n 'stoptime': 'ended_at',\n 'bikeid': 'bike_id',\n 'usertype': 'user_type',\n 'start station latitude': 'start_lat',\n 'start station longitude': 'start_lng',\n 'end station latitude': 'end_lat',\n 'end station longitude': 'end_lng'\n} ,\n  2:  {\n 'Trip Duration': 'trip_duration',\n  'Start Time': 'started_at',\n  'Stop Time': 'ended_at',\n  'Start Station Latitude': 'start_lat',\n  'Start Station Longitude': 'start_lng',\n  'End Station Latitude': 'end_lat',\n  'End Station Longitude': 'end_lng'    \n}\n}\nCode\nfor (root, dirs ,files) in tqdm(os.walk(extract_dir, topdown=True)):\n    if dirs:\n        continue\n\n    for flnm in files:\n        if not flnm.endswith('.csv'):  \n            continue\n\n        fpath = os.path.join(root, flnm)\n        df = spark.read.option(\"header\",\"true\").csv(fpath)\n\n        df = (\n            df.withColumnsRenamed(dicts_rename[1])\n            .withColumnsRenamed(dicts_rename[2])\n        )\n\n        df = df.toDF(*[c.replace(' ','_').lower() for c in df.columns])\n\n        if re.match(r\"\\d+/\\d+/\\d+\", df.select(\"started_at\").first()[0]):\n            df = (\n                   df\n                    .withColumn('started_at', make_iso(fn.col(\"started_at\")))\n                    .withColumn('ended_at', make_iso(fn.col(\"ended_at\")))\n            ) \n\n        df = df.withColumns(\n                {\n                'started_at': fn.to_timestamp(fn.col(\"started_at\")),\n                'ended_at': fn.to_timestamp(fn.col(\"ended_at\"))\n                }\n            )   \n\n        df = df.withColumns(\n                {\n                    'start_year': fn.year(fn.col('started_at')),\n                    'start_month': fn.month(fn.col('ended_at'))\n                }\n            )\n\n        df.checkpoint(eager=True)\n\n        # df.printSchema()\n\n        df.write.parquet(\n            parquet_dir, \n            partitionBy=['start_year', 'start_month'], \n            mode=\"append\"\n        )\n\n\n0it [00:00, ?it/s]0it [00:00, ?it/s]\nCode\n# spark.stop()"
  },
  {
    "objectID": "core/notebooks/xcitibike_spark.html#references",
    "href": "core/notebooks/xcitibike_spark.html#references",
    "title": "Building parquet dataset from extracted csv files",
    "section": "References",
    "text": "References\nPython vectorized string computations"
  },
  {
    "objectID": "cours-syllabus.html",
    "href": "cours-syllabus.html",
    "title": "IFEBY310 Syllabus",
    "section": "",
    "text": "Organization\n\n\n\nWe will have one weeky lecture. Each lecture is organized around Slides and Notebooks. We will switch from blackboard to laptop and back. You are invited to bring your laptop to the lectures.\n\n\n\n\n\n\n\n\n\n\n\nDay\nHour\nRoom\nStart\n\n\n\n\nLecture\nFriday\n15:45 - 17:46\nSophie Germain 014\n2025-01-17 –2025-02-21\n\n\nLecture\nFriday\n13:30 - 15:30\nSophie Germain 1005\n2025-03-07-…\n\n\n\n\n\n We will not attempt to complete the notebooks during the sessions. You are expected to complete the noteboks on your own time. Solutions (at least partial solutions) are available on the course website.\nYou can fork the course repository and post issues, comments, and corrections.\n\n\n\n\n\n\n\n\n\nObjectives\n\n\n\nDuring this course, you shall learn to:\n\nHandle middlesize data using Python Data Stack: Numpy/Scipy/Pandas\nScale up and down with Dask\nHandle Big Data with Spark (PySpark)\nManage and store data using dedicated columnar formats (Parquet, ORC, Avro, Arrow)\n\n\n\n\n\n\n\n\n\nCommunication \n\n\n\nCourse material: s-v-b.github.io/IFEBY310 Fork the repo, use github issues to send feedback (no email please)\nAlerts are spread through Moodle\nRegister at Moodle portal to be updated\n\n\n\n\n\n\n\n\nComputing environment \n\n\n\n We use the PostGreSQL server from UFR de Mathématiques. To obtain an ENT account  follow procedure Moodle\n You may install and use\n\nPython\nJupyter\nPandas\nNumpy\nScipy\nPlotly\nAltair\nDask\nSpark\nParquet\n‘Arrow’\nDocker\nQuarto\nR\npsql\nVS Code\n\n\n\n\n\n\n\n\n\nRéférences \n\n\n\n\nPandas Book\nPython Data Science Handbook\nDask\nSpark\nData pipelines\nData pipelines\nAlice\nDocumentation PostGres\nNext Generation Databases NoSQLand Big Data, Guy Harrison\nGuy Harrison Blog\nDatabases trends and applications\nUpcoming book “Principles of Databases”, by Marcelo Arenas, Pablo Barcelo, Leonid Libkin, Wim Martens, and Andreas Pieris.\n\n\n\n\n\n\n\n\n\n\nCourses \n\n\n\nslides\nNotebooks (html and ipynb)\n\n\n\n\n\n\n\n\n\nEvaluation \n\n\n\n\nTwo homeworks/projects\nGrading\n\n\n\n\n\n\n\n\n\n Trucs\n\n\n\n\nHave a look at slides before the course\nDon’t jump to corrections\nUse online help (StackOverflow, ChatGPT, copilot, …)\nRead error messages\n\n\n\n\n\n\n\n\n\nCode of conduct\n\n\n\nTL;DR: No cheating!\n\n\n\n\n\n\n\n\nSave the dates ! \n\n\n\n\nJanuary 17: Course kick-off\nFebruary 14: No session\nFebruary 28: Winter Holidays\nMarch 7: Session 6\nApril 4: Room 2017\nApril 18: Eastern Holidays\nApril 25: Eastern Holidays\nMay 2: Session 12\n\nUniversité Paris Cité calendar\nM1 MIDS calendar\n\n\n\n\n\n\n\n\nUniversité Paris Cité\n\n\n\nUseful links:\n\nCharte Université Paris Cité\nDémarches et accessibilité\nCentre de contact\nRelais handicap",
    "crumbs": [
      "Information",
      "Syllabus"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "IFEBY310: Big Data Technologies",
    "section": "",
    "text": "moodle\n  \n  \n    \n     Github\n  \n\n  \n  \n\n\nCourse IFEBY310: Big Data Technologies\nfor Master MIDS (M1) at Université Paris Cité introduces a collection of software technologies dedicated to Big Data management. This course is designed for students with dual background in Mathematics and Computer Science.\n\n\n\nDuring this course, you shall learn to:\n\nHandle middlesize data using Python Data Stack: Numpy/Scipy/Pandas\nScale up and down with Dask\nHandle Big Data with Spark (PySpark)\nManage and store data using dedicated columnar formats (Parquet, ORC, Avro, Arrow)\n\n\n\n\nWe will work on Spark using Docker images. It is a good idea to install Python and the Python data stack on your laptop.\n\n\n\n\n\nCourse Syllabus\nCurrent and past teams\nSoftware tips\nSlides\nNotebooks\nProjects",
    "crumbs": [
      "Information",
      "Glimpse"
    ]
  },
  {
    "objectID": "projects-listings.html",
    "href": "projects-listings.html",
    "title": "Projects",
    "section": "",
    "text": "Modus operandi\n\n\n\nCourse evaluation is based on Projects \n\n Find a friend : all work done by pairs of students\n Create a single private GitHub repository for each project and each pair of students.\n Grant me access to these repositories\n All work is transmitted through your private repository and nowhere else\n No emails for project submission\n All projects deliverables consist of Quarto files\n The report should be rendered at least in HTML format, and possibly also in PDF format\n Do not forget to disclose your name and first name in the report\n\n\n\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Description\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\nDate\n\n\nTitle\n\n\nDescription\n\n\n\n\n\n\nMar 10, 2025\n\n\nTable wrangling, visualization\n\n\nPython, NLP, Spacy, Dask\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "Homeworks"
    ]
  },
  {
    "objectID": "slides-listings.html",
    "href": "slides-listings.html",
    "title": "Slides",
    "section": "",
    "text": "Slides provide the framework for the course. You can use them to familiarise yourself with the material for the next lesson, and/or to review previous lessons.\nIcon  points to chalkboard arguments.\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Titre\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Description\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\nDate\n\n\nTitre\n\n\nDescription\n\n\n\n\n\n\nJan 17, 2025\n\n\nIntroduction to Big Data\n\n\n\n\n\n\n\nJan 17, 2025\n\n\nPython data stack : Numpy et Scipy\n\n\n\n\n\n\n\nJan 24, 2025\n\n\nPython data stack : Pandas \n\n\n\n\n\n\n\nJan 31, 2025\n\n\nPython data stack : Dask\n\n\n\n\n\n\n\nFeb 7, 2025\n\n\nSpark : RDDs\n\n\n\n\n\n\n\nFeb 20, 2025\n\n\nSpark : SQL\n\n\n\n\n\n\n\nFeb 21, 2025\n\n\nPandas on Spark and sparklyr\n\n\n\n\n\n\n\nMar 7, 2025\n\n\nJSON\n\n\n\n\n\n\n\nMar 14, 2025\n\n\nFile formats for Big Data\n\n\n\n\n\n\n\nMar 21, 2025\n\n\nSpark: a deeper dive\n\n\n\n\n\n\n\nMar 28, 2025\n\n\nSpark: tips\n\n\n\n\n\n\n\nApr 4, 2025\n\n\nSpark: Applications\n\n\n\n\n\n\n\nApr 11, 2025\n\n\nSpark: Applications\n\n\n\n\n\n\n\n\nNo matching items\n\n\n\n\n\n\n\n\nMode d’emploi\n\n\n\nSlides use revealjs de . They are best viewed using a browser.\nKeyboard shortcut  takes you to the Help page.",
    "crumbs": [
      "Slides"
    ]
  },
  {
    "objectID": "weeks/week-1.html",
    "href": "weeks/week-1.html",
    "title": "Week 1",
    "section": "",
    "text": "Important\n\n\n\n\nRoom: Sophie Germain 0014\nFriday 17 January 2025 15h45-17h45\n Calendar"
  },
  {
    "objectID": "weeks/week-1.html#lecture-slides",
    "href": "weeks/week-1.html#lecture-slides",
    "title": "Week 1",
    "section": "Lecture : slides",
    "text": "Lecture : slides\nWe went (rapidly) through the two decks of slides :\n\n Introduction to Big Data\n Python Data Science Stack\n\nWe shall come back to the description of Spark in a couple of weeks\nDuring the next two weeks, we shall explore the Python Data Stack."
  },
  {
    "objectID": "weeks/week-1.html#notebooks",
    "href": "weeks/week-1.html#notebooks",
    "title": "Week 1",
    "section": "Notebooks",
    "text": "Notebooks\n\n Jupyter notebook I : tour of Python\n html: tour of Python\n Jupyter notebook II : tour of numpy\n html: tour of numpy"
  },
  {
    "objectID": "weeks/week-1.html#references",
    "href": "weeks/week-1.html#references",
    "title": "Week 1",
    "section": "References",
    "text": "References\nYou can watch the Introduction to NumPy conference presented at EuroSciPy 2023"
  },
  {
    "objectID": "weeks/week-1.html#logistics",
    "href": "weeks/week-1.html#logistics",
    "title": "Week 1",
    "section": "Logistics",
    "text": "Logistics\n To work the jupyter notebooks, install python 3, and modules related to jupyter: jupyter-cache, jupyter_client, jupyter_core, jupyterlab_widgets (this induces the installation of dependencies).\nhttps://jupyter.org\nDownload the jupyter notebooks from notebooks listings.\n If you do not already have an ENT account, follow instructions on Moodle to get one. You shall need this account to connect to PostGres cluster.\n\n\nBack to Agenda ⏎"
  },
  {
    "objectID": "weeks/week-11.html",
    "href": "weeks/week-11.html",
    "title": "Week 11",
    "section": "",
    "text": "Important\n\n\n\n\nApril the 4th 2025 Sophie Germain 1005 13h30-15h30\n Calendar"
  },
  {
    "objectID": "weeks/week-11.html#lecture-slides",
    "href": "weeks/week-11.html#lecture-slides",
    "title": "Week 11",
    "section": "Lecture : slides",
    "text": "Lecture : slides\n\n Spark tips\n Deeper dive\n\nWe may come back to several parts of\n\n Json format\n Spark high level APIs: SQL\n Spark low level APIs: RDD\n Python Data Science Stack\n Dask\n File formats"
  },
  {
    "objectID": "weeks/week-11.html#notebooks",
    "href": "weeks/week-11.html#notebooks",
    "title": "Week 11",
    "section": "Notebooks",
    "text": "Notebooks\n\n Jupyter notebook VIII : wrangling\n webdata\n\nYou shall have gone through (on your own)\n\n Jupyter notebook VII : JSON\n html: JSON\n\n Jupyter notebook VI : Spark SQL\n Jupyter notebook V : Spark RDD\n html: Spark RDD\n\n Jupyter notebook I : tour of Python\n html: tour of Python\n\n Jupyter notebook II : tour of numpy\n html: tour of numpy\n\n Jupyter notebook III: tour of pandas\n html: tour of Pandas"
  },
  {
    "objectID": "weeks/week-11.html#references",
    "href": "weeks/week-11.html#references",
    "title": "Week 11",
    "section": "References",
    "text": "References\n\n\n\nSpark Definitive Guide\nSpark project\nSpark documentation\nData pipelines\nNext Generation Databases NoSQLand Big Data, Guy Harrison\nMastering Spark on R\n\n\n\nParquet\n‘Arrow’\nDask Docs\nDask Examples\nDask Code\nDask Blog\nPandas cheatsheet"
  },
  {
    "objectID": "weeks/week-11.html#logistics",
    "href": "weeks/week-11.html#logistics",
    "title": "Week 11",
    "section": "Logistics",
    "text": "Logistics\n pyspark\n To work the jupyter notebooks, install python 3, and modules related to jupyter: jupyter-cache, jupyter_client, jupyter_core, jupyterlab_widgets (this induces the installation of dependencies).\nhttps://jupyter.org\nDownload the jupyter notebooks from notebooks listings.\n If you do not already have an ENT account, follow instructions on Moodle to get one. You shall need this account to connect to PostGres cluster.\n\n\nBack to Agenda ⏎"
  },
  {
    "objectID": "weeks/week-3.html",
    "href": "weeks/week-3.html",
    "title": "Week 3",
    "section": "",
    "text": "Important\n\n\n\n\nRoom: Sophie Germain 0014\nFriday 31 January 2025 15h45-17h45\n Calendar"
  },
  {
    "objectID": "weeks/week-3.html#lecture-slides",
    "href": "weeks/week-3.html#lecture-slides",
    "title": "Week 3",
    "section": "Lecture : slides",
    "text": "Lecture : slides\n\n Dask\n\nWe shall briefly come back to several parts of\n\n Python Data Science Stack\n\nData processing\nVisualization"
  },
  {
    "objectID": "weeks/week-3.html#notebooks",
    "href": "weeks/week-3.html#notebooks",
    "title": "Week 3",
    "section": "Notebooks",
    "text": "Notebooks\nWe shall spend most of the lecture on\nDask Tutorial\nYou shall have gone through (on your own)\n\n Jupyter notebook I : tour of Python\n html: tour of Python\n\n Jupyter notebook II : tour of numpy\n html: tour of numpy\n\n Jupyter notebook III: tour of pandas\n html: tour of Pandas"
  },
  {
    "objectID": "weeks/week-3.html#references",
    "href": "weeks/week-3.html#references",
    "title": "Week 3",
    "section": "References",
    "text": "References\n\nDask Docs\nDask Examples\nDask Code\nDask Blog\nPandas cheatsheet"
  },
  {
    "objectID": "weeks/week-3.html#logistics",
    "href": "weeks/week-3.html#logistics",
    "title": "Week 3",
    "section": "Logistics",
    "text": "Logistics\n To work the jupyter notebooks, install python 3, and modules related to jupyter: jupyter-cache, jupyter_client, jupyter_core, jupyterlab_widgets (this induces the installation of dependencies).\nhttps://jupyter.org\nDownload the jupyter notebooks from notebooks listings.\n If you do not already have an ENT account, follow instructions on Moodle to get one. You shall need this account to connect to PostGres cluster.\n\n\nBack to Agenda ⏎"
  },
  {
    "objectID": "weeks/week-6.html",
    "href": "weeks/week-6.html",
    "title": "Weeks 6",
    "section": "",
    "text": "Important\n\n\n\n2 Sessions\n\nThursday 20 February 2025 Olympes de Gouges 358 10h45-12h45\nFriday 21 February 2025 Sophie Germain 0014 15h45-17h45\n Calendar"
  },
  {
    "objectID": "weeks/week-6.html#lecture-slides",
    "href": "weeks/week-6.html#lecture-slides",
    "title": "Weeks 6",
    "section": "Lecture : slides",
    "text": "Lecture : slides\n\n Spark high level APIs: SQL\n\nWe may come back to several parts of\n\n Spark low level APIs: RDD\n Python Data Science Stack\n Dask"
  },
  {
    "objectID": "weeks/week-6.html#notebooks",
    "href": "weeks/week-6.html#notebooks",
    "title": "Weeks 6",
    "section": "Notebooks",
    "text": "Notebooks\nWe shall spend most of the lectures on\n\n Jupyter notebook VI : Spark SQL\n html: Spark SQL\n\n Jupyter notebook VII : JSON\n html: JSON\n\nand possibly compare with:\nDask Tutorial\nYou shall have gone through (on your own)\n\n Jupyter notebook V : Spark RDD\n html: Spark RDD\n\n Jupyter notebook I : tour of Python\n html: tour of Python\n\n Jupyter notebook II : tour of numpy\n html: tour of numpy\n\n Jupyter notebook III: tour of pandas\n html: tour of Pandas"
  },
  {
    "objectID": "weeks/week-6.html#references",
    "href": "weeks/week-6.html#references",
    "title": "Weeks 6",
    "section": "References",
    "text": "References\n\n\n\nSpark Definitive Guide\nSpark\nSpark\nData pipelines\nNext Generation Databases NoSQLand Big Data, Guy Harrison\nMastering Spark on R\n\n\n\nParquet\n‘Arrow’\nDask Docs\nDask Examples\nDask Code\nDask Blog\nPandas cheatsheet"
  },
  {
    "objectID": "weeks/week-6.html#logistics",
    "href": "weeks/week-6.html#logistics",
    "title": "Weeks 6",
    "section": "Logistics",
    "text": "Logistics\n pyspark\n To work the jupyter notebooks, install python 3, and modules related to jupyter: jupyter-cache, jupyter_client, jupyter_core, jupyterlab_widgets (this induces the installation of dependencies).\nhttps://jupyter.org\nDownload the jupyter notebooks from notebooks listings.\n If you do not already have an ENT account, follow instructions on Moodle to get one. You shall need this account to connect to PostGres cluster.\n\n\nBack to Agenda ⏎"
  },
  {
    "objectID": "weeks/week-8.html",
    "href": "weeks/week-8.html",
    "title": "Week 8",
    "section": "",
    "text": "Important\n\n\n\n1 Session\n\nFriday 14 March 2025 Sophie Germain 1005 13h30-15h30\n Calendar"
  },
  {
    "objectID": "weeks/week-8.html#lecture-slides",
    "href": "weeks/week-8.html#lecture-slides",
    "title": "Week 8",
    "section": "Lecture : slides",
    "text": "Lecture : slides\n\n File formats\n\nWe may come back to several parts of\n\n Json format\n Spark high level APIs: SQL\n Spark low level APIs: RDD\n Python Data Science Stack\n Dask"
  },
  {
    "objectID": "weeks/week-8.html#notebooks",
    "href": "weeks/week-8.html#notebooks",
    "title": "Week 8",
    "section": "Notebooks",
    "text": "Notebooks\nWe shall spend most of the lectures on\n\n Jupyter notebook VII : JSON\n html: JSON\n\nYou shall have gone through (on your own)\n\n Jupyter notebook VII : JSON\n html: JSON\n\n Jupyter notebook VI : Spark SQL\n Jupyter notebook V : Spark RDD\n html: Spark RDD\n\n Jupyter notebook I : tour of Python\n html: tour of Python\n\n Jupyter notebook II : tour of numpy\n html: tour of numpy\n\n Jupyter notebook III: tour of pandas\n html: tour of Pandas"
  },
  {
    "objectID": "weeks/week-8.html#references",
    "href": "weeks/week-8.html#references",
    "title": "Week 8",
    "section": "References",
    "text": "References\n\n\n\nSpark Definitive Guide\nSpark project\nSpark documentation\nData pipelines\nNext Generation Databases NoSQLand Big Data, Guy Harrison\nMastering Spark on R\n\n\n\nParquet\n‘Arrow’\nDask Docs\nDask Examples\nDask Code\nDask Blog\nPandas cheatsheet"
  },
  {
    "objectID": "weeks/week-8.html#logistics",
    "href": "weeks/week-8.html#logistics",
    "title": "Week 8",
    "section": "Logistics",
    "text": "Logistics\n pyspark\n To work the jupyter notebooks, install python 3, and modules related to jupyter: jupyter-cache, jupyter_client, jupyter_core, jupyterlab_widgets (this induces the installation of dependencies).\nhttps://jupyter.org\nDownload the jupyter notebooks from notebooks listings.\n If you do not already have an ENT account, follow instructions on Moodle to get one. You shall need this account to connect to PostGres cluster.\n\n\nBack to Agenda ⏎"
  }
]