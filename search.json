[
  {
    "objectID": "weeks/week-3.html",
    "href": "weeks/week-3.html",
    "title": "Week 3",
    "section": "",
    "text": "Important\n\n\n\n\nRoom: Sophie Germain 0014\nFriday 31 January 2025 15h45-17h45\n Calendar",
    "crumbs": [
      "Journal",
      "Week 3"
    ]
  },
  {
    "objectID": "weeks/week-3.html#lecture-slides",
    "href": "weeks/week-3.html#lecture-slides",
    "title": "Week 3",
    "section": "Lecture : slides",
    "text": "Lecture : slides\n\n Dask\n\nWe shall briefly come back to several parts of\n\n Python Data Science Stack\n\nData processing\nVisualization",
    "crumbs": [
      "Journal",
      "Week 3"
    ]
  },
  {
    "objectID": "weeks/week-3.html#notebooks",
    "href": "weeks/week-3.html#notebooks",
    "title": "Week 3",
    "section": "Notebooks",
    "text": "Notebooks\nWe shall spend most of the lecture on\nDask Tutorial\nYou shall have gone through (on your own)\n\n Jupyter notebook I : tour of Python\n html: tour of Python\n\n Jupyter notebook II : tour of numpy\n html: tour of numpy\n\n Jupyter notebook III: tour of pandas\n html: tour of Pandas",
    "crumbs": [
      "Journal",
      "Week 3"
    ]
  },
  {
    "objectID": "weeks/week-3.html#references",
    "href": "weeks/week-3.html#references",
    "title": "Week 3",
    "section": "References",
    "text": "References\n\nDask Docs\nDask Examples\nDask Code\nDask Blog\nPandas cheatsheet",
    "crumbs": [
      "Journal",
      "Week 3"
    ]
  },
  {
    "objectID": "weeks/week-3.html#logistics",
    "href": "weeks/week-3.html#logistics",
    "title": "Week 3",
    "section": "Logistics",
    "text": "Logistics\n To work the jupyter notebooks, install python 3, and modules related to jupyter: jupyter-cache, jupyter_client, jupyter_core, jupyterlab_widgets (this induces the installation of dependencies).\nhttps://jupyter.org\nDownload the jupyter notebooks from notebooks listings.\n If you do not already have an ENT account, follow instructions on Moodle to get one. You shall need this account to connect to PostGres cluster.\n\n\nBack to Agenda ⏎",
    "crumbs": [
      "Journal",
      "Week 3"
    ]
  },
  {
    "objectID": "weeks/week-1.html",
    "href": "weeks/week-1.html",
    "title": "Week 1",
    "section": "",
    "text": "Important\n\n\n\n\nRoom: Sophie Germain 0014\nFriday 17 January 2025 15h45-17h45\n Calendar",
    "crumbs": [
      "Journal",
      "Week 1"
    ]
  },
  {
    "objectID": "weeks/week-1.html#lecture-slides",
    "href": "weeks/week-1.html#lecture-slides",
    "title": "Week 1",
    "section": "Lecture : slides",
    "text": "Lecture : slides\nWe went (rapidly) through the two decks of slides :\n\n Introduction to Big Data\n Python Data Science Stack\n\nWe shall come back to the description of Spark in a couple of weeks\nDuring the next two weeks, we shall explore the Python Data Stack.",
    "crumbs": [
      "Journal",
      "Week 1"
    ]
  },
  {
    "objectID": "weeks/week-1.html#notebooks",
    "href": "weeks/week-1.html#notebooks",
    "title": "Week 1",
    "section": "Notebooks",
    "text": "Notebooks\n\n Jupyter notebook I : tour of Python\n html: tour of Python\n Jupyter notebook II : tour of numpy\n html: tour of numpy",
    "crumbs": [
      "Journal",
      "Week 1"
    ]
  },
  {
    "objectID": "weeks/week-1.html#references",
    "href": "weeks/week-1.html#references",
    "title": "Week 1",
    "section": "References",
    "text": "References\nYou can watch the Introduction to NumPy conference presented at EuroSciPy 2023",
    "crumbs": [
      "Journal",
      "Week 1"
    ]
  },
  {
    "objectID": "weeks/week-1.html#logistics",
    "href": "weeks/week-1.html#logistics",
    "title": "Week 1",
    "section": "Logistics",
    "text": "Logistics\n To work the jupyter notebooks, install python 3, and modules related to jupyter: jupyter-cache, jupyter_client, jupyter_core, jupyterlab_widgets (this induces the installation of dependencies).\nhttps://jupyter.org\nDownload the jupyter notebooks from notebooks listings.\n If you do not already have an ENT account, follow instructions on Moodle to get one. You shall need this account to connect to PostGres cluster.\n\n\nBack to Agenda ⏎",
    "crumbs": [
      "Journal",
      "Week 1"
    ]
  },
  {
    "objectID": "quarto-format.html#a-translator",
    "href": "quarto-format.html#a-translator",
    "title": "Quarto",
    "section": "A translator",
    "text": "A translator",
    "crumbs": [
      "Support",
      "Quarto"
    ]
  },
  {
    "objectID": "quarto-format.html#quarto-and-rstudio",
    "href": "quarto-format.html#quarto-and-rstudio",
    "title": "Quarto",
    "section": "Quarto and rstudio",
    "text": "Quarto and rstudio",
    "crumbs": [
      "Support",
      "Quarto"
    ]
  },
  {
    "objectID": "quarto-format.html#quarto-and-vs-code",
    "href": "quarto-format.html#quarto-and-vs-code",
    "title": "Quarto",
    "section": "Quarto and vs code",
    "text": "Quarto and vs code",
    "crumbs": [
      "Support",
      "Quarto"
    ]
  },
  {
    "objectID": "quarto-format.html#command-line-tool",
    "href": "quarto-format.html#command-line-tool",
    "title": "Quarto",
    "section": "Command Line Tool",
    "text": "Command Line Tool",
    "crumbs": [
      "Support",
      "Quarto"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "IFEBY310: Big Data Technologies",
    "section": "",
    "text": "moodle\n  \n  \n    \n     Github\n  \n\n  \n  \n\n\nCourse IFEBY310: Big Data Technologies\nfor Master MIDS (M1) at Université Paris Cité introduces a collection of software technologies dedicated to Big Data management. This course is designed for students with dual background in Mathematics and Computer Science.\n\n\n\nDuring this course, you shall learn to:\n\nHandle middlesize data using Python Data Stack: Numpy/Scipy/Pandas\nScale up and down with Dask\nHandle Big Data with Spark (PySpark)\nManage and store data using dedicated columnar formats (Parquet, ORC, Avro, Arrow)\n\n\n\n\nWe will work on Spark using Docker images. It is a good idea to install Python and the Python data stack on your laptop.\n\n\n\n\n\nCourse Syllabus\nCurrent and past teams\nSoftware tips\nSlides\nNotebooks\nProjects",
    "crumbs": [
      "Information",
      "Glimpse"
    ]
  },
  {
    "objectID": "cours-syllabus.html",
    "href": "cours-syllabus.html",
    "title": "IFEBY310 Syllabus",
    "section": "",
    "text": "Organization\n\n\n\nWe will have one weeky lecture. Each lecture is organized around Slides and Notebooks. We will switch from blackboard to laptop and back. You are invited to bring your laptop to the lectures.\n\n\n\n\n\n\n\n\n\n\n\nDay\nHour\nRoom\nStart\n\n\n\n\nLecture\nFriday\n15:45 - 17:46\nSophie Germain 014\n2025-01-17\n\n\n\n\n\n We will not attempt to complete the notebooks during the sessions. You are expected to complete the noteboks on your own time. Solutions (at least partial solutions) are available on the course website.\nYou can fork the course repository and post issues, comments, and corrections.\n\n\n\n\n\n\n\n\n\nObjectives\n\n\n\nDuring this course, you shall learn to:\n\nHandle middlesize data using Python Data Stack: Numpy/Scipy/Pandas\nScale up and down with Dask\nHandle Big Data with Spark (PySpark)\nManage and store data using dedicated columnar formats (Parquet, ORC, Avro, Arrow)\n\n\n\n\n\n\n\n\n\nCommunication \n\n\n\nCourse material: s-v-b.github.io/IFEBY310 Fork the repo, use github issues to send feedback (no email please)\nAlerts are spread through Moodle\nRegister at Moodle portal to be updated\n\n\n\n\n\n\n\n\nComputing environment \n\n\n\n We use the PostGreSQL server from UFR de Mathématiques. To obtain an ENT account  follow procedure Moodle\n You may install and use\n\nPython\nJupyter\nPandas\nNumpy\nScipy\nPlotly\nAltair\nDask\nSpark\nParquet\n‘Arrow’\nDocker\nQuarto\nR\npsql\nVS Code\n\n\n\n\n\n\n\n\n\nRéférences \n\n\n\n\nPandas Book\nPython Data Science Handbook\nDask\nSpark\nData pipelines\nData pipelines\nAlice\nDocumentation PostGres\nNext Generation Databases NoSQLand Big Data, Guy Harrison\nGuy Harrison Blog\nDatabases trends and applications\nUpcoming book “Principles of Databases”, by Marcelo Arenas, Pablo Barcelo, Leonid Libkin, Wim Martens, and Andreas Pieris.\n\n\n\n\n\n\n\n\n\n\nCourses \n\n\n\nslides\nNotebooks (html and ipynb)\n\n\n\n\n\n\n\n\n\nEvaluation \n\n\n\n\nTwo homeworks/projects\nGrading\n\n\n\n\n\n\n\n\n\n Trucs\n\n\n\n\nHave a look at slides before the course\nDon’t jump to corrections\nUse online help (StackOverflow, ChatGPT, copilot, …)\nRead error messages\n\n\n\n\n\n\n\n\n\nCode of conduct\n\n\n\nTL;DR: No cheating!\n\n\n\n\n\n\n\n\nSave the dates ! \n\n\n\n\nJanuary 17: Course kick-off\nFebruary 14: No session\nFebruary 28: Winter Holidays\nMarch 7: Session 6\nApril 4: Room 2017\nApril 18: Eastern Holidays\nApril 25: Eastern Holidays\nMay 2: Session 12\n\nUniversité Paris Cité calendar\nM1 MIDS calendar\n\n\n\n\n\n\n\n\nUniversité Paris Cité\n\n\n\nUseful links:\n\nCharte Université Paris Cité\nDémarches et accessibilité\nCentre de contact\nRelais handicap",
    "crumbs": [
      "Information",
      "Syllabus"
    ]
  },
  {
    "objectID": "core/projects/hmw_table_wrangling.html",
    "href": "core/projects/hmw_table_wrangling.html",
    "title": "Hmw I : Tables and visualization",
    "section": "",
    "text": "Important\n\n\n\n\nDue : March 7, 2025\nWork in pairs\nDeliver your work as a qmd file through a github  repository\nUse the quarto package for reproducible research\nThe report should be rendered at least in HTML format, and possibly also in PDF format"
  },
  {
    "objectID": "core/projects/hmw_table_wrangling.html#report-organization",
    "href": "core/projects/hmw_table_wrangling.html#report-organization",
    "title": "Hmw I : Tables and visualization",
    "section": " Report organization",
    "text": "Report organization\nThe first part (introduction) of the report shall be dedicated to the description of the data to be extracted and to the extraction pipeline.\nThe second part of the report shall be dedicated to the description of load/transform pipeline\nThe third part of the report shall be dedicated to the description of the annotation pipeline\nThe fourth part of the report shall be dedicated to the stylometric analysis: which questions did you pick up (and why?), plots, summary tables and comments. Refrain from overplaying your hand: yours plots are not likely to provide a new literary interpretation of Balzac opera. Comment the data, all the data, and nothing but the data.\nThe fifth part is the appendix. The first four parts should be mostly text and plots. The fifth part should be code only.\nThe appendix shall be dedicated to the details of the pipelines. Yu shall give the code.\nYou shall also give the code of the graphical pipelines in the appendix.\nYou shall avoid copy-paste coding. Don’t Repeat Yourself. knitr provide the tools to organize the Quarto file so that you can write your code once and use it many times, once for data wrangling and plotting (without echoing), then for listing and explanation.\n\n\n\n\n\n\nOrganizing a report using the jupyter engine"
  },
  {
    "objectID": "core/projects/hmw_table_wrangling.html#references",
    "href": "core/projects/hmw_table_wrangling.html#references",
    "title": "Hmw I : Tables and visualization",
    "section": " References",
    "text": "References\n\nData Humanities with R\nSpacy\nSpark NLP\nWeb scrapping\nscrapy"
  },
  {
    "objectID": "core/projects/hmw_table_wrangling.html#grading-criteria",
    "href": "core/projects/hmw_table_wrangling.html#grading-criteria",
    "title": "Hmw I : Tables and visualization",
    "section": " Grading criteria",
    "text": "Grading criteria\n\n\n\nCriterion\nPoints\nDetails\n\n\n\n\nNarrative, spelling and syntax\n25%\nEnglish/French \n\n\nPlots correction\n15%\nchoice of aesthetics, geom, scale … \n\n\nPlot style\n10%\nTitles, legends, labels, breaks … \n\n\nETL\n15%\nETL, SQL like manipulations \n\n\nAnnotation\n10%\nAnnotations \n\n\nComputing Statistics\n5%\n… \n\n\nDRY compliance\n20%\nDRY principle at  Wikipedia"
  },
  {
    "objectID": "core/notebooks/xciti_pandas.html",
    "href": "core/notebooks/xciti_pandas.html",
    "title": "Imports",
    "section": "",
    "text": "Code\nimport glob\n\nimport os\nimport sys\nimport re \nimport shutils\nimport logging \n\nimport pandas as pd\nimport numpy as np\n\n\n\nimport datetime\n\n# from functools import reduce\nimport itertools\n\nimport zipfile\nfrom zipfile import ZipFile\nfrom tqdm import tqdm\n\nimport pyarrow as pa\nimport comet    as co\nimport pyarrow.parquet as pq\nimport pyarrow.dataset as ds\n\n\nimport dask\n\nos.environ['PYSPARK_PYTHON'] = sys.executable\nos.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable\n\n\n\n---------------------------------------------------------------------------\nModuleNotFoundError                       Traceback (most recent call last)\nCell In[1], line 6\n      4 import sys\n      5 import re \n----&gt; 6 import shutils\n      7 import logging \n      9 import pandas as pd\n\nModuleNotFoundError: No module named 'shutils'\nCode\nfrom dask.distributed import Client\n\nclient = Client(n_workers=20, threads_per_worker=2, memory_limit=\"2GB\")\nclient\n\n\n\n---------------------------------------------------------------------------\nModuleNotFoundError                       Traceback (most recent call last)\nFile ~/Documents/IFEBY310/.venv/lib/python3.12/site-packages/dask/distributed.py:13\n     12 try:\n---&gt; 13     from distributed import *\n     14 except ImportError as e:\n\nModuleNotFoundError: No module named 'distributed'\n\nThe above exception was the direct cause of the following exception:\n\nImportError                               Traceback (most recent call last)\nCell In[2], line 1\n----&gt; 1 from dask.distributed import Client\n      3 client = Client(n_workers=20, threads_per_worker=2, memory_limit=\"2GB\")\n      4 client\n\nFile ~/Documents/IFEBY310/.venv/lib/python3.12/site-packages/dask/distributed.py:16\n     14 except ImportError as e:\n     15     if e.msg == \"No module named 'distributed'\":\n---&gt; 16         raise ImportError(_import_error_message) from e\n     17     else:\n     18         raise\n\nImportError: dask.distributed is not installed.\n\nPlease either conda or pip install distributed:\n\n  conda install dask distributed             # either conda install\n  python -m pip install \"dask[distributed]\" --upgrade    # or pip install\nCode\nlogger = logging.getLogger(__name__)\nlogging.basicConfig(filename='example.log', encoding='utf-8', level=logging.DEBUG)\nlogger.debug('This message should go to the log file')\nlogger.info('So should this')\nlogger.warning('And this, too')\nlogger.error('And non-ASCII stuff, too, like Øresund and Malmö')\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[3], line 1\n----&gt; 1 logger = logging.getLogger(__name__)\n      2 logging.basicConfig(filename='example.log', encoding='utf-8', level=logging.DEBUG)\n      3 logger.debug('This message should go to the log file')\n\nNameError: name 'logging' is not defined"
  },
  {
    "objectID": "core/notebooks/xciti_pandas.html#paths",
    "href": "core/notebooks/xciti_pandas.html#paths",
    "title": "Imports",
    "section": "Paths",
    "text": "Paths\nDownloaded zip archives are in data_dir\nExtracted csv files are in extract_dir\nParquet files are in parquet_dir\n\n\nCode\ndata_dir = '../data'\nos.path.exists(data_dir)\n\nextract_dir = os.path.join(data_dir, 'xcitibike')\nif not os.path.exists(extract_dir):\n    os.mkdir(extract_dir)\n\nparquet_dir = os.path.join(data_dir, 'pq_citibike')\nif not os.path.exists(parquet_dir):\n    os.mkdir(parquet_dir)\n\n\n\n---------------------------------------------------------------------------\nFileNotFoundError                         Traceback (most recent call last)\nCell In[4], line 6\n      4 extract_dir = os.path.join(data_dir, 'xcitibike')\n      5 if not os.path.exists(extract_dir):\n----&gt; 6     os.mkdir(extract_dir)\n      8 parquet_dir = os.path.join(data_dir, 'pq_citibike')\n      9 if not os.path.exists(parquet_dir):\n\nFileNotFoundError: [Errno 2] No such file or directory: '../data/xcitibike'"
  },
  {
    "objectID": "core/notebooks/xciti_pandas.html#extracting-archives",
    "href": "core/notebooks/xciti_pandas.html#extracting-archives",
    "title": "Imports",
    "section": "Extracting archives",
    "text": "Extracting archives\nZip archive files contain directory trees where the csv files are to be found.\n\n\nCode\ncitibike_archives_paths = sorted(glob.glob(data_dir + '/*-citibike-tripdata.zip'))\n\n\nTODO: - parallelize part of the extraction process - one thread per element in citibike_archives_paths - should be doable with dask\n\n\nCode\nfor ar_path in tqdm(citibike_archives_paths):\n    myzip = ZipFile(ar_path)\n    to_extract = [elt  for elt in myzip.namelist() if (elt.endswith(\".csv\") and not (elt.startswith('__MACOSX')))]\n    myzip.extractall(path=extract_dir,\n                     members=to_extract)\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[6], line 1\n----&gt; 1 for ar_path in tqdm(citibike_archives_paths):\n      2     myzip = ZipFile(ar_path)\n      3     to_extract = [elt  for elt in myzip.namelist() if (elt.endswith(\".csv\") and not (elt.startswith('__MACOSX')))]\n\nNameError: name 'tqdm' is not defined"
  },
  {
    "objectID": "core/notebooks/xciti_pandas.html#collecting-headers",
    "href": "core/notebooks/xciti_pandas.html#collecting-headers",
    "title": "Imports",
    "section": "Collecting headers",
    "text": "Collecting headers\nThe extracted csv files do not share the same schema and the same datetime encoding format.\nWalking through the csv files in extract_dir, allows to gather the three different column naming patterns.\nTODO: - Save the schemata with inferred types in a json file. Different typing patterns may correspond to the same column naming pattern - For each csv file spot the column naming pattern, the datetime encoding format\n\n\nCode\n# schemata_names = set()\n\n# for (root, dirs ,files) in os.walk(extract_dir, topdown=True):\n#    if dirs:\n#        continue\n#    for fn in files:\n#        if fn.endswith('.csv'):\n#            with open(os.path.join(root, fn), 'r') as fd:\n#                schemata_names.add(fd.readline())\n\n# schemata_names = [s.replace('\\n', '').split(',') for s in schemata_names]\n\n\n\n\nCode\nschemata_names = [\n    ['ride_id',\n  'rideable_type',\n  'started_at',\n  'ended_at',\n  'start_station_name',\n  'start_station_id',\n  'end_station_name',\n  'end_station_id',\n  'start_lat',\n  'start_lng',\n  'end_lat',\n  'end_lng',\n  'member_casual'],\n ['tripduration',\n  'starttime',\n  'stoptime',\n  'start station id',\n  'start station name',\n  'start station latitude',\n  'start station longitude',\n  'end station id',\n  'end station name',\n  'end station latitude',\n  'end station longitude',\n  'bikeid',\n  'usertype',\n  'birth year',\n  'gender'],\n ['Trip Duration',\n  'Start Time',\n  'Stop Time',\n  'Start Station ID',\n  'Start Station Name',\n  'Start Station Latitude',\n  'Start Station Longitude',\n  'End Station ID',\n  'End Station Name',\n  'End Station Latitude',\n  'End Station Longitude',\n  'Bike ID',\n  'User Type',\n  'Birth Year',\n  'Gender']\n]\n\n\nFor each csv file, find the column naming pattern, build a dictionary with this information.\nTODO: - should done during the first walk.\n\n\nCode\nschemata_numbers = {}\n\nfor (root, dirs ,files) in os.walk(extract_dir, topdown=True):\n    if dirs:\n        continue\n    for fn in files:\n        if fn.endswith('.csv'):        \n            with open(os.path.join(root, fn), 'r') as fd:\n                col_names = fd.readline().replace('\\n', '').split(',')\n                schemata_numbers[fn] = schemata_names.index(col_names)"
  },
  {
    "objectID": "core/notebooks/xciti_pandas.html#building-renaming-dictionaries",
    "href": "core/notebooks/xciti_pandas.html#building-renaming-dictionaries",
    "title": "Imports",
    "section": "Building renaming dictionaries",
    "text": "Building renaming dictionaries\n\nFrom 0\nNothing to do\n\n\nFrom 1\nUse\n{\n 'tripduration': 'trip_duration',\n 'starttime': 'started_at',\n 'stoptime': 'ended_at',\n 'bikeid': 'bike_id',\n 'usertype': 'user_type',\n 'start station latitude': 'start_lat',\n 'start station longitude': 'start_lng',\n 'end station latitude': 'end_lat',\n 'end station longitude': 'end_lng'\n}\nand replace  with ’_’.\n\n\nFrom 2\n{\n 'Trip Duration': 'trip_duration',\n  'Start Time': 'started_at',\n  'Stop Time': 'ended_at',\n  'Start Station Latitude': 'start_lat',\n  'Start Station Longitude': 'start_lng',\n  'End Station Latitude': 'end_lat',\n  'End Station Longitude': 'end_lng'    \n}\n\nand replace  with ’_’, use lower().\n\n\nCode\ndicts_rename = {\n    1: {\n 'tripduration': 'trip_duration',\n 'starttime': 'started_at',\n 'stoptime': 'ended_at',\n 'bikeid': 'bike_id',\n 'usertype': 'user_type',\n 'start station latitude': 'start_lat',\n 'start station longitude': 'start_lng',\n 'end station latitude': 'end_lat',\n 'end station longitude': 'end_lng'\n} ,\n  2:  {\n 'Trip Duration': 'trip_duration',\n  'Start Time': 'started_at',\n  'Stop Time': 'ended_at',\n  'Start Station Latitude': 'start_lat',\n  'Start Station Longitude': 'start_lng',\n  'End Station Latitude': 'end_lat',\n  'End Station Longitude': 'end_lng'    \n}\n}\n\n\nAnother problem.\nstart_station_id, end_station_id is not consistently formatted."
  },
  {
    "objectID": "core/notebooks/xciti_pandas.html#building-a-parquet-replica",
    "href": "core/notebooks/xciti_pandas.html#building-a-parquet-replica",
    "title": "Imports",
    "section": "Building a parquet replica",
    "text": "Building a parquet replica\nTODO: - explain why engine='pyarrow' is useful when using pd.read_csv() - clean up the renaming schemes\nDatetime hand-made parsing for non ISO compliant csv file\n\n\nCode\ndef my_parse(s):\n    \"\"\"datetime parsing for non-ISO enco\n\n    Args:\n        s (str): a datetime encoding string '%m/%d/%Y H:M[:S]'\n\n    Returns:\n        datetime: a datetime object without timezone\n    \"\"\"\n    rem = re.compile(r\"(\\d+)/(\\d+)/(\\d+) (\\d+)?(:\\d+)?(:\\d+)?\")\n\n    matches = rem.search(s).groups()\n    month, day, year, hours, mins, secs = [int(x.replace(':','')) if x else 0 for x in matches]\n\n    zdt = datetime.datetime(year, month, day, hours, mins, secs)\n    return zdt\n\n\nTODO: - parallelize this\n\n\nCode\ndef csv2pq(root, dirs, files):\n    if dirs:\n        return\n\n    for fn in files:\n        if not fn.endswith('.csv'):  \n            continue\n\n        df = pd.read_csv(\n                    os.path.join(root, fn),\n                    engine = 'pyarrow'\n            )\n                    \n        if 1==schemata_numbers[fn]: \n            df = ( \n                df\n                    .rename(columns=dicts_rename[1])\n                    .rename(mapper= lambda s : s.replace(' ', '_'), axis='columns')\n            )                \n        elif 2==schemata_numbers[fn]:\n            df = ( \n                df\n                    .rename(columns=dicts_rename[2])\n                    .rename(mapper= lambda s : s.lower().replace(' ', '_'), axis='columns')\n            )\n\n        if (str(df.dtypes.loc['ended_at'])=='object'):\n            # Format is either '%m/%d/%Y %H:%M:%S'or '%m/%d/%Y %H:%M'\n            try:\n                df['ended_at'] = pd.to_datetime(df.ended_at, format='%m/%d/%Y %H:%M:%S')\n                df['started_at'] = pd.to_datetime(df.started_at, format='%m/%d/%Y %H:%M:%S')\n            except ValueError:\n                df['ended_at'] = pd.to_datetime(df.ended_at, format='%m/%d/%Y %H:%M')\n                df['started_at'] = pd.to_datetime(df.started_at, format='%m/%d/%Y %H:%M')\n            except:\n                df['ended_at'] = df.ended_at.map(my_parse)\n                df['started_at'] = df.started_at.map(my_parse)\n\n\n        # if df.start_station_id.dtype != np.dtype('O'):\n\n        df['start_station_id'] = df.start_station_id.astype(np.dtype('O'))  \n        df['end_station_id'] = df.end_station_id.astype(np.dtype('O')) \n            \n        df['start_year'] = df.started_at.dt.year\n        df['start_month'] = df.started_at.dt.month  \n        \n        table = pa.Table.from_pandas(df)\n\n        logger.info('writing: ' + fn)\n\n        pq.write_to_dataset(\n                table,\n                parquet_dir,\n                partition_cols=[\"start_year\", \"start_month\"],\n        )    \n\n    return root \n\n\n\n\nCode\ntodo = dask.delayed([dask.delayed(csv2pq)(root, dirs ,files)\n    for (root, dirs ,files) in os.walk(extract_dir, topdown=True)\n ])\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[13], line 1\n----&gt; 1 todo = dask.delayed([dask.delayed(csv2pq)(root, dirs ,files)\n      2     for (root, dirs ,files) in os.walk(extract_dir, topdown=True)\n      3  ])\n\nNameError: name 'dask' is not defined\n\n\n\n\n\nCode\nfoo = todo.compute()\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[14], line 1\n----&gt; 1 foo = todo.compute()\n\nNameError: name 'todo' is not defined\n\n\n\n\n\nCode\n[x  for x in foo if x]\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[15], line 1\n----&gt; 1 [x  for x in foo if x]\n\nNameError: name 'foo' is not defined\n\n\n\n\n\nCode\nlist(schemata_numbers.keys())\n\n\n[]\n\n\n\n\nCode\nfor (root, dirs ,files) in tqdm(os.walk(extract_dir, topdown=True)):\n    if dirs:\n        continue\n    for fn in files:\n        if not fn.endswith('.csv'):  \n            continue\n\n        df = pd.read_csv(\n                os.path.join(root, fn),\n                engine = 'pyarrow'\n        )\n                \n        if 1==schemata_numbers[fn]: \n            df = ( \n                df\n                  .rename(columns=dicts_rename[1])\n                  .rename(mapper= lambda s : s.replace(' ', '_'), axis='columns')\n            )                \n        elif 2==schemata_numbers[fn]:\n            df = ( \n                df\n                  .rename(columns=dicts_rename[2])\n                  .rename(mapper= lambda s : s.lower().replace(' ', '_'), axis='columns')\n            )\n        \n        if (str(df.dtypes.loc['ended_at'])=='object'):\n            # Format is either '%m/%d/%Y %H:%M:%S'or '%m/%d/%Y %H:%M'\n            try:\n                df['ended_at'] = pd.to_datetime(df.ended_at, format='%m/%d/%Y %H:%M:%S')\n                df['started_at'] = pd.to_datetime(df.started_at, format='%m/%d/%Y %H:%M:%S')\n            except ValueError:\n                df['ended_at'] = pd.to_datetime(df.ended_at, format='%m/%d/%Y %H:%M')\n                df['started_at'] = pd.to_datetime(df.started_at, format='%m/%d/%Y %H:%M')\n            except:\n                df['ended_at'] = df.ended_at.map(my_parse)\n                df['started_at'] = df.started_at.map(my_parse) \n\n        \n\n        if df.start_station_id.dtype != np.dtype('O'):\n            df['start_station_id'] = df.start_station_id.astype(np.dtype('O'))  \n            df['end_station_id'] = df.end_station_id.astype(np.dtype('O')) \n         \n        df['start_year'] = df.started_at.dt.year\n        df['start_month'] = df.started_at.dt.month  \n      \n        table = pa.Table.from_pandas(df)\n\n        pa.schema(table)\n\n        pq.write_to_dataset(\n             table,\n             parquet_dir,\n             partition_cols=[\"start_year\", \"start_month\"],\n        )\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[17], line 1\n----&gt; 1 for (root, dirs ,files) in tqdm(os.walk(extract_dir, topdown=True)):\n      2     if dirs:\n      3         continue\n\nNameError: name 'tqdm' is not defined\n\n\n\n\n\nCode\ndf.start_station_id.astype(np.dtype('O'))\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[18], line 1\n----&gt; 1 df.start_station_id.astype(np.dtype('O'))\n\nNameError: name 'df' is not defined"
  },
  {
    "objectID": "core/notebooks/xciti_pandas.html#todos",
    "href": "core/notebooks/xciti_pandas.html#todos",
    "title": "Imports",
    "section": "TODOs",
    "text": "TODOs\n\nHandling schema evolution\nSchema changed between 2021 January and 2021 February\n\n\n\n\n\n\n\n\nOld Column\nNew Column\nAction\n\n\n\n\n\nride_id\nPrimary key ?\n\n\n\nride_type\ndocked_bike\n\n\ntripduration\n\nIn seconds, can be recovered from started_at/ended_at\n\n\nstarttime\nstarted_at\nNo need for microseconds before 2021 January\n\n\nstoptime\nended_at\nNo need for microseconds before 2021 January\n\n\nstart station id\nstart_station_id\nOrder mismatch, code mismatch. Before : int. After:\n\n\nstart station name\nstart_station_name\nCheck consistency\n\n\nstart station latitude\nstart_lat\nCheck consistency\n\n\nstart station longitude\nstart_lng\nCheck consistency\n\n\n\n\n\n\n\n\n\n\n\nOld Column\nNew Column\nAction\n\n\n\n\nend station id\nend_station_id\nCheck consistency\n\n\nend station name\nend_station_name\nCheck consistency\n\n\nend station latitude\nend_lat\nCheck consistency\n\n\nend station longitude\nend_lng\nCheck consistency\n\n\nbikeid\n\n\n\n\nusertype\n\nSubscriber/Customer\n\n\nbirth year\n\n\n\n\ngender\n\n0, 1\n\n\n\nmember_casual\ncasual/member\n\n\n\n\nreading side: just read start_time, end_time, start_station_id, end_station_id,\nstart_at and end_at must be translated to start_time, end_time\ntrip_duration, user_type, bike_id, member_casual\nPrepare for a dimension table for stations\n\nid\nname\nlat\nlon\nmore\n\n\n\n\nSelect from the colum names\nCan we read directly as a pyarrow table ? Yes, but Pandas is convenient for datetime manipulations, and possibly for renaming\n\n\nUsage pyarrow.unify_schemas\n\n\nParsing dates\nFor some files, timestamps are not in ISO format.\nFrom 2014-09-01 till 2016-09-.., started_at and ended_at do not abide ISO format, but %m/%d/%Y %H:%M:%S.\nTry to use pd.to_datetime(). If failure, use regular expression to parse the putative date column. Handle the optional field that way.\nBetter ask forgiveness than permission.\n\n\nCode\nroot, dirs, fn = next(os.walk(os.path.join(parquet_dir, 'start_year=2013', 'start_month=9')))\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[19], line 1\n----&gt; 1 root, dirs, fn = next(os.walk(os.path.join(parquet_dir, 'start_year=2013', 'start_month=9')))\n\nNameError: name 'parquet_dir' is not defined\n\n\n\n\n\nCode\nspam = pq.read_metadata(os.path.join(root, fn[0]))\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[20], line 1\n----&gt; 1 spam = pq.read_metadata(os.path.join(root, fn[0]))\n\nNameError: name 'pq' is not defined\n\n\n\n\n\nCode\nfrom  dask import dataframe as dd\n\n\n/home/boucheron/Documents/IFEBY310/.venv/lib/python3.12/site-packages/dask/dataframe/__init__.py:49: FutureWarning:\n\n\nDask dataframe query planning is disabled because dask-expr is not installed.\n\nYou can install it with `pip install dask[dataframe]` or `conda install dask`.\nThis will raise in a future version.\n\n\n\n\n\n\nCode\nspam = dd.read_parquet(os.path.join(parquet_dir, 'start_year=2023' ))\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[22], line 1\n----&gt; 1 spam = dd.read_parquet(os.path.join(parquet_dir, 'start_year=2023' ))\n\nNameError: name 'parquet_dir' is not defined\n\n\n\n\n\nCode\nspam.dtypes\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[23], line 1\n----&gt; 1 spam.dtypes\n\nNameError: name 'spam' is not defined\n\n\n\n\n\nCode\nfoo_path = os.path.join(parquet_dir, 'start_year=2023', 'start_month=1')\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[24], line 1\n----&gt; 1 foo_path = os.path.join(parquet_dir, 'start_year=2023', 'start_month=1')\n\nNameError: name 'parquet_dir' is not defined\n\n\n\n\n\nCode\nroot, dirs, fn = next(os.walk(foo_path))\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[25], line 1\n----&gt; 1 root, dirs, fn = next(os.walk(foo_path))\n\nNameError: name 'foo_path' is not defined\n\n\n\n\n\nCode\nparquet_file = pq.ParquetFile(os.path.join(root, fn[0]))\nschema = parquet_file.schema\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[26], line 1\n----&gt; 1 parquet_file = pq.ParquetFile(os.path.join(root, fn[0]))\n      2 schema = parquet_file.schema\n\nNameError: name 'pq' is not defined\n\n\n\n\n\nCode\nschema\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[27], line 1\n----&gt; 1 schema\n\nNameError: name 'schema' is not defined"
  },
  {
    "objectID": "core/notebooks/notebook15_polars.html",
    "href": "core/notebooks/notebook15_polars.html",
    "title": "Polars",
    "section": "",
    "text": "Code\n!pip install polars\n\n\nRequirement already satisfied: polars in /home/boucheron/Documents/IFEBY310/.venv/lib/python3.12/site-packages (1.20.0)\nCode\nimport pyarrow as pa\nimport numpy as np\nimport polars as pl\nCode\ndf = pl.read_csv('./tips.csv')\nhttps://github.com/mattharrison/datasets\nCode\ntips\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[4], line 1\n----&gt; 1 tips\n\nNameError: name 'tips' is not defined"
  },
  {
    "objectID": "core/notebooks/notebook15_polars.html#arrow",
    "href": "core/notebooks/notebook15_polars.html#arrow",
    "title": "Polars",
    "section": "Arrow",
    "text": "Arrow\nLibrary for In Memory management of tabular data.\nPyArrow types (not numpy types?)"
  },
  {
    "objectID": "core/notebooks/notebook15_polars.html#rust",
    "href": "core/notebooks/notebook15_polars.html#rust",
    "title": "Polars",
    "section": "Rust",
    "text": "Rust\nA language."
  },
  {
    "objectID": "core/notebooks/notebook15_polars.html#polars-api",
    "href": "core/notebooks/notebook15_polars.html#polars-api",
    "title": "Polars",
    "section": "Polars API",
    "text": "Polars API\ndtypes, columns,\n\n\nCode\nimport inspect\n\n\n\n\nCode\ndf.estimated_size()\n\n\n9777\n\n\n\n\nCode\ndf.describe()\n\n\n\nshape: (9, 8)\n\n\n\nstatistic\ntotal_bill\ntip\nsex\nsmoker\nday\ntime\nsize\n\n\nstr\nf64\nf64\nstr\nstr\nstr\nstr\nf64\n\n\n\n\n\"count\"\n244.0\n244.0\n\"244\"\n\"244\"\n\"244\"\n\"244\"\n244.0\n\n\n\"null_count\"\n0.0\n0.0\n\"0\"\n\"0\"\n\"0\"\n\"0\"\n0.0\n\n\n\"mean\"\n19.785943\n2.998279\nnull\nnull\nnull\nnull\n2.569672\n\n\n\"std\"\n8.902412\n1.383638\nnull\nnull\nnull\nnull\n0.9511\n\n\n\"min\"\n3.07\n1.0\n\"Female\"\n\"No\"\n\"Fri\"\n\"Dinner\"\n1.0\n\n\n\"25%\"\n13.37\n2.0\nnull\nnull\nnull\nnull\n2.0\n\n\n\"50%\"\n17.81\n2.92\nnull\nnull\nnull\nnull\n2.0\n\n\n\"75%\"\n24.08\n3.55\nnull\nnull\nnull\nnull\n3.0\n\n\n\"max\"\n50.81\n10.0\n\"Male\"\n\"Yes\"\n\"Thur\"\n\"Lunch\"\n6.0"
  },
  {
    "objectID": "core/notebooks/notebook15_polars.html#expressions",
    "href": "core/notebooks/notebook15_polars.html#expressions",
    "title": "Polars",
    "section": "Expressions",
    "text": "Expressions\n\n\nCode\ntype(pl.col('sex'))\n\n\npolars.expr.expr.Expr\n\n\nSelect numerical columns\n\n\nCode\n(\n  df\n    .select(pl.col(pl.Float64))\n)\n\n\n\nshape: (244, 2)\n\n\n\ntotal_bill\ntip\n\n\nf64\nf64\n\n\n\n\n16.99\n1.01\n\n\n10.34\n1.66\n\n\n21.01\n3.5\n\n\n23.68\n3.31\n\n\n24.59\n3.61\n\n\n…\n…\n\n\n29.03\n5.92\n\n\n27.18\n2.0\n\n\n22.67\n2.0\n\n\n17.82\n1.75\n\n\n18.78\n3.0\n\n\n\n\n\n\n\n\nCode\n(\n  df\n    .with_columns(\n      pl.lit('spam').alias('egg')\n      )\n)\n\n\n\nshape: (244, 8)\n\n\n\ntotal_bill\ntip\nsex\nsmoker\nday\ntime\nsize\negg\n\n\nf64\nf64\nstr\nstr\nstr\nstr\ni64\nstr\n\n\n\n\n16.99\n1.01\n\"Female\"\n\"No\"\n\"Sun\"\n\"Dinner\"\n2\n\"spam\"\n\n\n10.34\n1.66\n\"Male\"\n\"No\"\n\"Sun\"\n\"Dinner\"\n3\n\"spam\"\n\n\n21.01\n3.5\n\"Male\"\n\"No\"\n\"Sun\"\n\"Dinner\"\n3\n\"spam\"\n\n\n23.68\n3.31\n\"Male\"\n\"No\"\n\"Sun\"\n\"Dinner\"\n2\n\"spam\"\n\n\n24.59\n3.61\n\"Female\"\n\"No\"\n\"Sun\"\n\"Dinner\"\n4\n\"spam\"\n\n\n…\n…\n…\n…\n…\n…\n…\n…\n\n\n29.03\n5.92\n\"Male\"\n\"No\"\n\"Sat\"\n\"Dinner\"\n3\n\"spam\"\n\n\n27.18\n2.0\n\"Female\"\n\"Yes\"\n\"Sat\"\n\"Dinner\"\n2\n\"spam\"\n\n\n22.67\n2.0\n\"Male\"\n\"Yes\"\n\"Sat\"\n\"Dinner\"\n2\n\"spam\"\n\n\n17.82\n1.75\n\"Male\"\n\"No\"\n\"Sat\"\n\"Dinner\"\n2\n\"spam\"\n\n\n18.78\n3.0\n\"Female\"\n\"No\"\n\"Thur\"\n\"Dinner\"\n2\n\"spam\"\n\n\n\n\n\n\nIf possible stay in the rust.\nTidy selection using regular expressions."
  },
  {
    "objectID": "core/notebooks/notebook15_polars.html#in-memory-or-not",
    "href": "core/notebooks/notebook15_polars.html#in-memory-or-not",
    "title": "Polars",
    "section": "In memory or not",
    "text": "In memory or not"
  },
  {
    "objectID": "core/notebooks/notebook15_polars.html#streaming",
    "href": "core/notebooks/notebook15_polars.html#streaming",
    "title": "Polars",
    "section": "Streaming",
    "text": "Streaming"
  },
  {
    "objectID": "core/notebooks/notebook15_polars.html#comparisons",
    "href": "core/notebooks/notebook15_polars.html#comparisons",
    "title": "Polars",
    "section": "Comparisons",
    "text": "Comparisons\n\nPandas\nModen\nDask\nPolars"
  },
  {
    "objectID": "core/notebooks/notebook11_dive.html",
    "href": "core/notebooks/notebook11_dive.html",
    "title": "Diving deeer",
    "section": "",
    "text": "Code\nimport os\nimport sys\n\nos.environ['PYSPARK_PYTHON'] = sys.executable\nos.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable\nCode\nfrom pyspark import SparkConf, SparkContext\nfrom pyspark.sql import SparkSession\n\nconf = SparkConf().setAppName(\"Spark SQL Course\")\nsc = SparkContext(conf=conf)  # no need for Spark 3...\n\nspark = (SparkSession\n    .builder\n    .appName(\"Spark SQL Course\")\n    .getOrCreate()\n)\n\n\n25/02/03 23:21:16 WARN Utils: Your hostname, boucheron-Precision-5480 resolves to a loopback address: 127.0.1.1; using 192.168.10.120 instead (on interface wlp0s20f3)\n25/02/03 23:21:16 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\nSetting default log level to \"WARN\".\nTo adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n25/02/03 23:21:16 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n25/02/03 23:21:17 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\nCode\nsc = spark._sc\nCode\nrdd = sc.parallelize([(\"a\", 1), (\"b\", 1), (\"a\", 1)])\nCode\nrdd.reduceByKey(lambda a, b: a + b).collect()\n\n\n[Stage 0:&gt;                                                        (0 + 20) / 20]                                                                                \n\n\n[('b', 1), ('a', 2)]\nCode\nimport requests, zipfile, io\nfrom pathlib import Path\n\npath = Path('webdata.parquet')\nif not path.exists():\n    url = \"https://stephanegaiffas.github.io/big_data_course/data/webdata.parquet.zip\"\n    r = requests.get(url)\n    z = zipfile.ZipFile(io.BytesIO(r.content))\n    z.extractall(path='./')\n\n\n\n---------------------------------------------------------------------------\nBadZipFile                                Traceback (most recent call last)\nCell In[6], line 8\n      6 url = \"https://stephanegaiffas.github.io/big_data_course/data/webdata.parquet.zip\"\n      7 r = requests.get(url)\n----&gt; 8 z = zipfile.ZipFile(io.BytesIO(r.content))\n      9 z.extractall(path='./')\n\nFile /usr/lib/python3.12/zipfile/__init__.py:1349, in ZipFile.__init__(self, file, mode, compression, allowZip64, compresslevel, strict_timestamps, metadata_encoding)\n   1347 try:\n   1348     if mode == 'r':\n-&gt; 1349         self._RealGetContents()\n   1350     elif mode in ('w', 'x'):\n   1351         # set the modified flag so central directory gets written\n   1352         # even if no files are added to the archive\n   1353         self._didModify = True\n\nFile /usr/lib/python3.12/zipfile/__init__.py:1416, in ZipFile._RealGetContents(self)\n   1414     raise BadZipFile(\"File is not a zip file\")\n   1415 if not endrec:\n-&gt; 1416     raise BadZipFile(\"File is not a zip file\")\n   1417 if self.debug &gt; 1:\n   1418     print(endrec)\n\nBadZipFile: File is not a zip file\nCode\ninput_path = './'\n\ninput_file = os.path.join(input_path, 'webdata.parquet')\ndf = spark.read.parquet(input_file)\n\n\n\n---------------------------------------------------------------------------\nAnalysisException                         Traceback (most recent call last)\nCell In[7], line 4\n      1 input_path = './'\n      3 input_file = os.path.join(input_path, 'webdata.parquet')\n----&gt; 4 df = spark.read.parquet(input_file)\n\nFile ~/Documents/IFEBY310/.venv/lib/python3.12/site-packages/pyspark/sql/readwriter.py:544, in DataFrameReader.parquet(self, *paths, **options)\n    533 int96RebaseMode = options.get(\"int96RebaseMode\", None)\n    534 self._set_opts(\n    535     mergeSchema=mergeSchema,\n    536     pathGlobFilter=pathGlobFilter,\n   (...)\n    541     int96RebaseMode=int96RebaseMode,\n    542 )\n--&gt; 544 return self._df(self._jreader.parquet(_to_seq(self._spark._sc, paths)))\n\nFile ~/Documents/IFEBY310/.venv/lib/python3.12/site-packages/py4j/java_gateway.py:1322, in JavaMember.__call__(self, *args)\n   1316 command = proto.CALL_COMMAND_NAME +\\\n   1317     self.command_header +\\\n   1318     args_command +\\\n   1319     proto.END_COMMAND_PART\n   1321 answer = self.gateway_client.send_command(command)\n-&gt; 1322 return_value = get_return_value(\n   1323     answer, self.gateway_client, self.target_id, self.name)\n   1325 for temp_arg in temp_args:\n   1326     if hasattr(temp_arg, \"_detach\"):\n\nFile ~/Documents/IFEBY310/.venv/lib/python3.12/site-packages/pyspark/errors/exceptions/captured.py:185, in capture_sql_exception.&lt;locals&gt;.deco(*a, **kw)\n    181 converted = convert_exception(e.java_exception)\n    182 if not isinstance(converted, UnknownException):\n    183     # Hide where the exception came from that shows a non-Pythonic\n    184     # JVM exception message.\n--&gt; 185     raise converted from None\n    186 else:\n    187     raise\n\nAnalysisException: [PATH_NOT_FOUND] Path does not exist: file:/home/boucheron/Documents/IFEBY310/core/notebooks/webdata.parquet.\nCode\ndf.head(6)\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[8], line 1\n----&gt; 1 df.head(6)\n\nNameError: name 'df' is not defined\nCode\ndf.describe()\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[9], line 1\n----&gt; 1 df.describe()\n\nNameError: name 'df' is not defined\nCode\ndf.count()\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[10], line 1\n----&gt; 1 df.count()\n\nNameError: name 'df' is not defined"
  },
  {
    "objectID": "core/notebooks/notebook11_dive.html#compute-the-total-number-of-unique-users",
    "href": "core/notebooks/notebook11_dive.html#compute-the-total-number-of-unique-users",
    "title": "Diving deeer",
    "section": "Compute the total number of unique users",
    "text": "Compute the total number of unique users\n\n\nCode\ndf.select('xid').distinct().count()\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[12], line 1\n----&gt; 1 df.select('xid').distinct().count()\n\nNameError: name 'df' is not defined"
  },
  {
    "objectID": "core/notebooks/notebook11_dive.html#construct-a-column-containing-the-total-number-of-actions-per-user",
    "href": "core/notebooks/notebook11_dive.html#construct-a-column-containing-the-total-number-of-actions-per-user",
    "title": "Diving deeer",
    "section": "Construct a column containing the total number of actions per user",
    "text": "Construct a column containing the total number of actions per user\n\n\nCode\nxid_partition = Window.partitionBy('xid')\nn_events = func.count(col('action')).over(xid_partition)\ndf = df.withColumn('n_events', n_events)\ndf.head(n=2)\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[13], line 3\n      1 xid_partition = Window.partitionBy('xid')\n      2 n_events = func.count(col('action')).over(xid_partition)\n----&gt; 3 df = df.withColumn('n_events', n_events)\n      4 df.head(n=2)\n\nNameError: name 'df' is not defined\n\n\n\n\n\nCode\ndf.groupBy('xid').agg(func.count('action')).head(5)\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[14], line 1\n----&gt; 1 df.groupBy('xid').agg(func.count('action')).head(5)\n\nNameError: name 'df' is not defined"
  },
  {
    "objectID": "core/notebooks/notebook11_dive.html#construct-a-column-containing-the-number-of-days-since-the-last-action-of-the-user",
    "href": "core/notebooks/notebook11_dive.html#construct-a-column-containing-the-number-of-days-since-the-last-action-of-the-user",
    "title": "Diving deeer",
    "section": "Construct a column containing the number of days since the last action of the user",
    "text": "Construct a column containing the number of days since the last action of the user\n\n\nCode\nxid_partition = Window.partitionBy('xid')\nmax_date = func.max(col('date')).over(xid_partition)\nn_days_since_last_event = func.datediff(func.current_date(), max_date)\ndf = df.withColumn('n_days_since_last_event',\n                   n_days_since_last_event)\ndf.head(n=2)\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[15], line 4\n      2 max_date = func.max(col('date')).over(xid_partition)\n      3 n_days_since_last_event = func.datediff(func.current_date(), max_date)\n----&gt; 4 df = df.withColumn('n_days_since_last_event',\n      5                    n_days_since_last_event)\n      6 df.head(n=2)\n\nNameError: name 'df' is not defined"
  },
  {
    "objectID": "core/notebooks/notebook11_dive.html#construct-a-column-containing-the-number-of-actions-of-each-user-for-each-modality-of-device",
    "href": "core/notebooks/notebook11_dive.html#construct-a-column-containing-the-number-of-actions-of-each-user-for-each-modality-of-device",
    "title": "Diving deeer",
    "section": "Construct a column containing the number of actions of each user for each modality of device",
    "text": "Construct a column containing the number of actions of each user for each modality of device\n\n\nCode\nxid_device_partition = Window.partitionBy('xid', 'device')\nn_events_per_device = func.count(col('action')).over(xid_device_partition)\ndf = df.withColumn('n_events_per_device', n_events_per_device)\ndf.head(n=2)\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[16], line 3\n      1 xid_device_partition = Window.partitionBy('xid', 'device')\n      2 n_events_per_device = func.count(col('action')).over(xid_device_partition)\n----&gt; 3 df = df.withColumn('n_events_per_device', n_events_per_device)\n      4 df.head(n=2)\n\nNameError: name 'df' is not defined"
  },
  {
    "objectID": "core/notebooks/notebook11_dive.html#number-of-device-per-user-some-mental-gymnastics",
    "href": "core/notebooks/notebook11_dive.html#number-of-device-per-user-some-mental-gymnastics",
    "title": "Diving deeer",
    "section": "Number of device per user: some mental gymnastics",
    "text": "Number of device per user: some mental gymnastics\n\n\nCode\nxid_partition = Window.partitionBy('xid')\nrank_device = func.dense_rank().over(xid_partition.orderBy('device'))\nn_unique_device = func.last(rank_device).over(xid_partition)\ndf = df.withColumn('n_device', n_unique_device)\ndf.head(n=2)\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[17], line 4\n      2 rank_device = func.dense_rank().over(xid_partition.orderBy('device'))\n      3 n_unique_device = func.last(rank_device).over(xid_partition)\n----&gt; 4 df = df.withColumn('n_device', n_unique_device)\n      5 df.head(n=2)\n\nNameError: name 'df' is not defined\n\n\n\n\n\nCode\ndf\\\n    .where(col('n_device') &gt; 1)\\\n    .select('xid', 'device', 'n_events',  'n_device', 'n_events_per_device')\\\n    .head(n=8)\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[18], line 1\n----&gt; 1 df\\\n      2     .where(col('n_device') &gt; 1)\\\n      3     .select('xid', 'device', 'n_events',  'n_device', 'n_events_per_device')\\\n      4     .head(n=8)\n\nNameError: name 'df' is not defined"
  },
  {
    "objectID": "core/notebooks/notebook11_dive.html#extraction",
    "href": "core/notebooks/notebook11_dive.html#extraction",
    "title": "Diving deeer",
    "section": "Extraction",
    "text": "Extraction\nExtraction is easy here, it’s just about reading the data\n\n\nCode\ndf = spark.read.parquet(input_file)\ndf.head(n=3)\n\n\n\n---------------------------------------------------------------------------\nAnalysisException                         Traceback (most recent call last)\nCell In[19], line 1\n----&gt; 1 df = spark.read.parquet(input_file)\n      2 df.head(n=3)\n\nFile ~/Documents/IFEBY310/.venv/lib/python3.12/site-packages/pyspark/sql/readwriter.py:544, in DataFrameReader.parquet(self, *paths, **options)\n    533 int96RebaseMode = options.get(\"int96RebaseMode\", None)\n    534 self._set_opts(\n    535     mergeSchema=mergeSchema,\n    536     pathGlobFilter=pathGlobFilter,\n   (...)\n    541     int96RebaseMode=int96RebaseMode,\n    542 )\n--&gt; 544 return self._df(self._jreader.parquet(_to_seq(self._spark._sc, paths)))\n\nFile ~/Documents/IFEBY310/.venv/lib/python3.12/site-packages/py4j/java_gateway.py:1322, in JavaMember.__call__(self, *args)\n   1316 command = proto.CALL_COMMAND_NAME +\\\n   1317     self.command_header +\\\n   1318     args_command +\\\n   1319     proto.END_COMMAND_PART\n   1321 answer = self.gateway_client.send_command(command)\n-&gt; 1322 return_value = get_return_value(\n   1323     answer, self.gateway_client, self.target_id, self.name)\n   1325 for temp_arg in temp_args:\n   1326     if hasattr(temp_arg, \"_detach\"):\n\nFile ~/Documents/IFEBY310/.venv/lib/python3.12/site-packages/pyspark/errors/exceptions/captured.py:185, in capture_sql_exception.&lt;locals&gt;.deco(*a, **kw)\n    181 converted = convert_exception(e.java_exception)\n    182 if not isinstance(converted, UnknownException):\n    183     # Hide where the exception came from that shows a non-Pythonic\n    184     # JVM exception message.\n--&gt; 185     raise converted from None\n    186 else:\n    187     raise\n\nAnalysisException: [PATH_NOT_FOUND] Path does not exist: file:/home/boucheron/Documents/IFEBY310/core/notebooks/webdata.parquet."
  },
  {
    "objectID": "core/notebooks/notebook11_dive.html#transformation-of-the-data",
    "href": "core/notebooks/notebook11_dive.html#transformation-of-the-data",
    "title": "Diving deeer",
    "section": "Transformation of the data",
    "text": "Transformation of the data\nAt this step we compute a lot of extra things from the data. The aim is to build features that describe users.\n\n\nCode\ndef n_events_transformer(df):\n    xid_partition = Window.partitionBy('xid')\n    n_events = func.count(col('action')).over(xid_partition)\n    df = df.withColumn('n_events', n_events)\n    return df\n\ndef n_events_per_action_transformer(df):\n    xid_action_partition = Window.partitionBy('xid', 'action')\n    n_events_per_action = func.count(col('action')).over(xid_action_partition)\n    df = df.withColumn('n_events_per_action', n_events_per_action)\n    return df\n\ndef hour_transformer(df):\n    hour = func.hour(col('date'))\n    df = df.withColumn('hour', hour)\n    return df\n\ndef weekday_transformer(df):\n    weekday = func.date_format(col('date'), 'EEEE')\n    df = df.withColumn('weekday', weekday)\n    return df\n\ndef n_events_per_hour_transformer(df):\n    xid_hour_partition = Window.partitionBy('xid', 'hour')\n    n_events_per_hour = func.count(col('action')).over(xid_hour_partition)\n    df = df.withColumn('n_events_per_hour', n_events_per_hour)\n    return df\n\ndef n_events_per_weekday_transformer(df):\n    xid_weekday_partition = Window.partitionBy('xid', 'weekday')\n    n_events_per_weekday = func.count(col('action')).over(xid_weekday_partition)\n    df = df.withColumn('n_events_per_weekday', n_events_per_weekday)\n    return df\n\ndef n_days_since_last_event_transformer(df):\n    xid_partition = Window.partitionBy('xid')\n    max_date = func.max(col('date')).over(xid_partition)\n    n_days_since_last_event = func.datediff(func.current_date(), max_date)\n    df = df.withColumn('n_days_since_last_event',\n                       n_days_since_last_event + lit(0.1))\n    return df\n\ndef n_days_since_last_action_transformer(df):\n    xid_partition_action = Window.partitionBy('xid', 'action')\n    max_date = func.max(col('date')).over(xid_partition_action)\n    n_days_since_last_action = func.datediff(func.current_date(),\n                                                        max_date)\n    df = df.withColumn('n_days_since_last_action',\n                       n_days_since_last_action + lit(0.1))\n    return df\n\ndef n_unique_day_transformer(df):\n    xid_partition = Window.partitionBy('xid')\n    dayofyear = func.dayofyear(col('date'))\n    rank_day = func.dense_rank().over(xid_partition.orderBy(dayofyear))\n    n_unique_day = func.last(rank_day).over(xid_partition)\n    df = df.withColumn('n_unique_day', n_unique_day)\n    return df\n\ndef n_unique_hour_transformer(df):\n    xid_partition = Window.partitionBy('xid')\n    rank_hour = func.dense_rank().over(xid_partition.orderBy('hour'))\n    n_unique_hour = func.last(rank_hour).over(xid_partition)\n    df = df.withColumn('n_unique_hour', n_unique_hour)\n    return df\n\ndef n_events_per_device_transformer(df):\n    xid_device_partition = Window.partitionBy('xid', 'device')\n    n_events_per_device = func.count(func.col('device')) \\\n        .over(xid_device_partition)\n    df = df.withColumn('n_events_per_device', n_events_per_device)\n    return df\n\ndef n_unique_device_transformer(df):\n    xid_partition = Window.partitionBy('xid')\n    rank_device = func.dense_rank().over(xid_partition.orderBy('device'))\n    n_unique_device = func.last(rank_device).over(xid_partition)\n    df = df.withColumn('n_device', n_unique_device)\n    return df\n\ndef n_actions_per_category_id_transformer(df):\n    xid_category_id_partition = Window.partitionBy('xid', 'category_id',\n                                                   'action')\n    n_actions_per_category_id = func.count(func.col('action')) \\\n        .over(xid_category_id_partition)\n    df = df.withColumn('n_actions_per_category_id', n_actions_per_category_id)\n    return df\n\ndef n_unique_category_id_transformer(df):\n    xid_partition = Window.partitionBy('xid')\n    rank_category_id = func.dense_rank().over(xid_partition\\\n                                              .orderBy('category_id'))\n    n_unique_category_id = func.last(rank_category_id).over(xid_partition)\n    df = df.withColumn('n_unique_category_id', n_unique_category_id)\n    return df\n\ndef n_events_per_category_id_transformer(df):\n    xid_category_id_partition = Window.partitionBy('xid', 'category_id')\n    n_events_per_category_id = func.count(func.col('action')) \\\n        .over(xid_category_id_partition)\n    df = df.withColumn('n_events_per_category_id', n_events_per_category_id)\n    return df\n\ndef n_events_per_website_id_transformer(df):\n    xid_website_id_partition = Window.partitionBy('xid', 'website_id')\n    n_events_per_website_id = func.count(col('action'))\\\n        .over(xid_website_id_partition)\n    df = df.withColumn('n_events_per_website_id', n_events_per_website_id)\n    return df\n\n\n\n\nCode\ntransformers = [\n    hour_transformer,\n    weekday_transformer,\n    n_events_per_hour_transformer,\n    n_events_per_weekday_transformer,\n    n_days_since_last_event_transformer,\n    n_days_since_last_action_transformer,\n    n_unique_day_transformer,\n    n_unique_hour_transformer,\n    n_events_per_device_transformer,\n    n_unique_device_transformer,\n    n_actions_per_category_id_transformer,\n    n_events_per_category_id_transformer,\n    n_events_per_website_id_transformer,\n]\n\nfor transformer in transformers:\n    df = transformer(df)\n\ndf.head(n=1)\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[21], line 18\n      1 transformers = [\n      2     hour_transformer,\n      3     weekday_transformer,\n   (...)\n     14     n_events_per_website_id_transformer,\n     15 ]\n     17 for transformer in transformers:\n---&gt; 18     df = transformer(df)\n     20 df.head(n=1)\n\nNameError: name 'df' is not defined\n\n\n\n\n\nCode\nsorted(df.columns)\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[22], line 1\n----&gt; 1 sorted(df.columns)\n\nNameError: name 'df' is not defined"
  },
  {
    "objectID": "core/notebooks/notebook11_dive.html#load-step",
    "href": "core/notebooks/notebook11_dive.html#load-step",
    "title": "Diving deeer",
    "section": "Load step",
    "text": "Load step\nHere, we use all the previous computations (saved in the columns of the dataframe) to compute aggregated informations about each user.\n\n\nCode\ndef n_events_per_hour_loader(df):\n    csr = df\\\n        .select('xid', 'hour', 'n_events_per_hour')\\\n        .withColumnRenamed('n_events_per_hour', 'value')\\\n        .distinct()\n    feature_name = func.concat(lit('n_events_per_hour#'), col('hour'))\n    csr = csr\\\n        .withColumn('feature_name', feature_name)\\\n        .drop('hour')\n    return csr\n\ndef n_events_per_website_id_loader(df):\n    csr = df.select('xid', 'website_id', 'n_events_per_website_id')\\\n        .withColumnRenamed('n_events_per_hour', 'value')\\\n        .distinct()\n    feature_name = func.concat(lit('n_events_per_website_id#'),\n                               col('website_id'))\n    csr = csr\\\n        .withColumn('feature_name', feature_name)\\\n        .drop('website_id')\n    return csr\n\ndef n_events_per_hour_loader(df):\n    csr = df\\\n        .select('xid', 'hour', 'n_events_per_hour')\\\n        .withColumnRenamed('n_events_per_hour', 'value')\\\n        .distinct()\n    feature_name = func.concat(lit('n_events_per_hour#'), col('hour'))\n    csr = csr\\\n        .withColumn('feature_name', feature_name)\\\n        .drop('hour')\n    return csr\n\ndef n_events_per_weekday_loader(df):\n    csr = df\\\n        .select('xid', 'weekday', 'n_events_per_weekday')\\\n        .withColumnRenamed('n_events_per_weekday', 'value')\\\n        .distinct()\n    feature_name = func.concat(lit('n_events_per_weekday#'), col('weekday'))\n    csr = csr\\\n        .withColumn('feature_name', feature_name)\\\n        .drop('weekday')\n    return csr\n\ndef n_days_since_last_event_loader(df):\n    csr = df.select('xid',  'n_days_since_last_event')\\\n        .withColumnRenamed('n_days_since_last_event#', 'value')\\\n        .distinct()\n    feature_name = lit('n_days_since_last_event')\n    csr = csr\\\n        .withColumn('feature_name', feature_name)\n    return csr\n\ndef n_days_since_last_action_loader(df):\n    csr = df.select('xid', 'action', 'n_days_since_last_action')\\\n        .withColumnRenamed('n_days_since_last_action', 'value')\\\n        .distinct()\n    feature_name = func.concat(lit('n_days_since_last_action#'), col('action'))\n    csr = csr\\\n        .withColumn('feature_name', feature_name)\\\n        .drop('action')\n    return csr\n\ndef n_unique_day_loader(df):\n    csr = df.select('xid', 'n_unique_day')\\\n        .withColumnRenamed('n_unique_day', 'value')\\\n        .distinct()\n    feature_name = lit('n_unique_day')\n    csr = csr\\\n        .withColumn('feature_name', feature_name)\n    return csr\n\ndef n_unique_hour_loader(df):\n    csr = df.select('xid', 'n_unique_hour')\\\n        .withColumnRenamed('n_unique_hour', 'value')\\\n        .distinct()\n    feature_name = lit('n_unique_hour')\n    csr = csr\\\n        .withColumn('feature_name', feature_name)\n    return csr\n\ndef n_events_per_device_loader(df):\n    csr = df\\\n        .select('xid', 'device', 'n_events_per_device')\\\n        .withColumnRenamed('n_events_per_device', 'value')\\\n        .distinct()\n    feature_name = func.concat(lit('n_events_per_device#'), col('device'))\n    csr = csr\\\n        .withColumn('feature_name', feature_name)\\\n        .drop('device')\n    return csr\n\ndef n_unique_device_loader(df):\n    csr = df.select('xid', 'n_device')\\\n        .withColumnRenamed('n_device', 'value')\\\n        .distinct()\n    feature_name = lit('n_device')\n    csr = csr\\\n        .withColumn('feature_name', feature_name)\n    return csr\n\ndef n_events_per_category_id_loader(df):\n    csr = df.select('xid', 'category_id', 'n_events_per_category_id')\\\n        .withColumnRenamed('n_events_per_category_id', 'value')\\\n        .distinct()\n    feature_name = func.concat(lit('n_events_per_category_id#'),\n                               col('category_id'))\n    csr = csr\\\n        .withColumn('feature_name', feature_name)\\\n        .drop('category_id')\n    return csr\n\ndef n_actions_per_category_id_loader(df):\n    csr = df.select('xid', 'category_id', 'action', 'n_actions_per_category_id')\\\n        .withColumnRenamed('n_actions_per_category_id', 'value')\\\n        .distinct()\n    feature_name = func.concat(lit('n_actions_per_category_id#'),\n                               col('action'), lit('#'), \n                               col('category_id'))\n    csr = csr\\\n        .withColumn('feature_name', feature_name)\\\n        .drop('category_id')\\\n        .drop('action')\n    return csr\n\ndef n_events_per_website_id_loader(df):\n    csr = df.select('xid', 'website_id', 'n_events_per_website_id')\\\n        .withColumnRenamed('n_events_per_website_id', 'value')\\\n        .distinct()\n    feature_name = func.concat(lit('n_events_per_website_id#'),\n                               col('website_id'))\n    csr = csr\\\n        .withColumn('feature_name', feature_name)\\\n        .drop('website_id')\n    return csr\n\n\n\n\nCode\nfrom functools import reduce\n\nloaders = [\n    n_events_per_hour_loader,\n    n_events_per_website_id_loader,\n    n_events_per_hour_loader,\n    n_events_per_weekday_loader,\n    n_days_since_last_event_loader,\n    n_days_since_last_action_loader,\n    n_unique_day_loader,\n    n_unique_hour_loader,\n    n_events_per_device_loader,\n    n_unique_device_loader,\n    n_events_per_category_id_loader,\n    n_actions_per_category_id_loader,\n    n_events_per_website_id_loader,\n]\n\ndef union(df, other):\n    return df.union(other)\n\ncsr = reduce(\n    lambda df1, df2: df1.union(df2),\n    [loader(df) for loader in loaders]\n)\n\ncsr.head(n=3)\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[24], line 24\n     19 def union(df, other):\n     20     return df.union(other)\n     22 csr = reduce(\n     23     lambda df1, df2: df1.union(df2),\n---&gt; 24     [loader(df) for loader in loaders]\n     25 )\n     27 csr.head(n=3)\n\nNameError: name 'df' is not defined\n\n\n\n\n\nCode\ncsr.columns\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[25], line 1\n----&gt; 1 csr.columns\n\nNameError: name 'csr' is not defined\n\n\n\n\n\nCode\ncsr.count()\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[26], line 1\n----&gt; 1 csr.count()\n\nNameError: name 'csr' is not defined\n\n\n\n\n\nCode\n# Replace features names and xid by a unique number\nfeature_name_partition = Window().orderBy('feature_name')\nxid_partition = Window().orderBy('xid')\n\ncol_idx = func.dense_rank().over(feature_name_partition)\nrow_idx = func.dense_rank().over(xid_partition)\n\ncsr = csr.withColumn('col', col_idx)\\\n    .withColumn('row', row_idx)\n\ncsr = csr.na.drop('any')\n\ncsr.head(n=5)\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[27], line 8\n      5 col_idx = func.dense_rank().over(feature_name_partition)\n      6 row_idx = func.dense_rank().over(xid_partition)\n----&gt; 8 csr = csr.withColumn('col', col_idx)\\\n      9     .withColumn('row', row_idx)\n     11 csr = csr.na.drop('any')\n     13 csr.head(n=5)\n\nNameError: name 'csr' is not defined\n\n\n\n\n\nCode\n# Let's save the result of our hard work into a new parquet file\noutput_path = './'\noutput_file = os.path.join(output_path, 'csr.parquet')\ncsr.write.parquet(output_file, mode='overwrite')\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[28], line 4\n      2 output_path = './'\n      3 output_file = os.path.join(output_path, 'csr.parquet')\n----&gt; 4 csr.write.parquet(output_file, mode='overwrite')\n\nNameError: name 'csr' is not defined"
  },
  {
    "objectID": "core/notebooks/notebook11_dive.html#finally",
    "href": "core/notebooks/notebook11_dive.html#finally",
    "title": "Diving deeer",
    "section": "Finally !!",
    "text": "Finally !!\nWow ! That was a lot of work. Now we have a features matrix \\(X\\) and a vector of labels \\(y\\).\n\n\nCode\nX.indices\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[51], line 1\n----&gt; 1 X.indices\n\nNameError: name 'X' is not defined\n\n\n\n\n\nCode\nX.indptr\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[52], line 1\n----&gt; 1 X.indptr\n\nNameError: name 'X' is not defined\n\n\n\n\n\nCode\nX.shape, X.nnz\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[53], line 1\n----&gt; 1 X.shape, X.nnz\n\nNameError: name 'X' is not defined\n\n\n\n\n\nCode\ny.shape, y.sum()\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[54], line 1\n----&gt; 1 y.shape, y.sum()\n\nNameError: name 'y' is not defined"
  },
  {
    "objectID": "core/notebooks/notebook08_webdata-II.html",
    "href": "core/notebooks/notebook08_webdata-II.html",
    "title": "Using with pyspark for data preprocessing",
    "section": "",
    "text": "The data is a parquet file which contains a dataframe with 8 columns:\n\nxid: unique user id\naction: type of action. ‘C’ is a click, ‘O’ or ‘VSL’ is a web-display\ndate: date of the action\nwebsite_id: unique id of the website\nurl: url of the webpage\ncategory_id: id of the display\nzipcode: postal zipcode of the user\ndevice: type of device used by the user"
  },
  {
    "objectID": "core/notebooks/notebook08_webdata-II.html#data-description",
    "href": "core/notebooks/notebook08_webdata-II.html#data-description",
    "title": "Using with pyspark for data preprocessing",
    "section": "",
    "text": "The data is a parquet file which contains a dataframe with 8 columns:\n\nxid: unique user id\naction: type of action. ‘C’ is a click, ‘O’ or ‘VSL’ is a web-display\ndate: date of the action\nwebsite_id: unique id of the website\nurl: url of the webpage\ncategory_id: id of the display\nzipcode: postal zipcode of the user\ndevice: type of device used by the user"
  },
  {
    "objectID": "core/notebooks/notebook08_webdata-II.html#q1.-some-statistics-computations",
    "href": "core/notebooks/notebook08_webdata-II.html#q1.-some-statistics-computations",
    "title": "Using with pyspark for data preprocessing",
    "section": "Q1. Some statistics / computations",
    "text": "Q1. Some statistics / computations\nUsing pyspark.sql we want to do the following things:\n\nCompute the total number of unique users\nConstruct a column containing the total number of actions per user\nConstruct a column containing the number of days since the last action of the user\nConstruct a column containing the number of actions of each user for each modality of device"
  },
  {
    "objectID": "core/notebooks/notebook08_webdata-II.html#q2.-binary-classification",
    "href": "core/notebooks/notebook08_webdata-II.html#q2.-binary-classification",
    "title": "Using with pyspark for data preprocessing",
    "section": "Q2. Binary classification",
    "text": "Q2. Binary classification\nThen, we want to construct a classifier to predict the click on the category 1204. Here is an agenda for this:\n\nConstruction of a features matrix for which each line corresponds to the information concerning a user.\nIn this matrix, we need to keep only the users that have been exposed to the display in category 1204\nUsing this training dataset, train a binary classifier, and evaluate your classifier using a precision / recall curve computed on test data."
  },
  {
    "objectID": "core/notebooks/notebook08_webdata-II.html#compute-the-total-number-of-unique-users",
    "href": "core/notebooks/notebook08_webdata-II.html#compute-the-total-number-of-unique-users",
    "title": "Using with pyspark for data preprocessing",
    "section": "Compute the total number of unique users",
    "text": "Compute the total number of unique users\n\n\nCode\n( \n    df.select('xid')\n      .distinct()\n      .count()\n)\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[11], line 2\n      1 ( \n----&gt; 2     df.select('xid')\n      3       .distinct()\n      4       .count()\n      5 )\n\nNameError: name 'df' is not defined\n\n\n\n\n\nCode\ndef foo(x): yield len(set(x))\n\n\n\n\nCode\n( df.rdd\n    .map(lambda x : x.xid)\n    .mapPartitions(foo)\n    .collect()\n)\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[13], line 1\n----&gt; 1 ( df.rdd\n      2     .map(lambda x : x.xid)\n      3     .mapPartitions(foo)\n      4     .collect()\n      5 )\n\nNameError: name 'df' is not defined\n\n\n\nThis might pump up some computational resources\n\n\nCode\n( \n    df.select('xid')\n      .distinct() \n      .explain()\n)\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[14], line 2\n      1 ( \n----&gt; 2     df.select('xid')\n      3       .distinct() \n      4       .explain()\n      5 )\n\nNameError: name 'df' is not defined\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe distinct values of xid seem to be evenly spread among the six files making the parquet directory. Note that the last six partitions look empty."
  },
  {
    "objectID": "core/notebooks/notebook08_webdata-II.html#construct-a-column-containing-the-total-number-of-actions-per-user",
    "href": "core/notebooks/notebook08_webdata-II.html#construct-a-column-containing-the-total-number-of-actions-per-user",
    "title": "Using with pyspark for data preprocessing",
    "section": "Construct a column containing the total number of actions per user",
    "text": "Construct a column containing the total number of actions per user\n\n\nCode\nxid_partition = Window.partitionBy('xid')\n\nn_events = func.count(col('action')).over(xid_partition)\n\ndf = df.withColumn('n_events', n_events)\n\ndf.head(n=2)\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[15], line 5\n      1 xid_partition = Window.partitionBy('xid')\n      3 n_events = func.count(col('action')).over(xid_partition)\n----&gt; 5 df = df.withColumn('n_events', n_events)\n      7 df.head(n=2)\n\nNameError: name 'df' is not defined\n\n\n\n\n\nCode\n( \n  df\n    .groupBy('xid')\n    .agg(func.count('action'))\n    .head(5)\n)\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[16], line 2\n      1 ( \n----&gt; 2   df\n      3     .groupBy('xid')\n      4     .agg(func.count('action'))\n      5     .head(5)\n      6 )\n\nNameError: name 'df' is not defined"
  },
  {
    "objectID": "core/notebooks/notebook08_webdata-II.html#construct-a-column-containing-the-number-of-days-since-the-last-action-of-the-user",
    "href": "core/notebooks/notebook08_webdata-II.html#construct-a-column-containing-the-number-of-days-since-the-last-action-of-the-user",
    "title": "Using with pyspark for data preprocessing",
    "section": "Construct a column containing the number of days since the last action of the user",
    "text": "Construct a column containing the number of days since the last action of the user\n\n\nCode\nmax_date = (\n  func\n    .max(col('date'))\n    .over(xid_partition)\n)\n\nn_days_since_last_event = func.datediff(func.current_date(), max_date)\n\ndf = df.withColumn('n_days_since_last_event',\n                   n_days_since_last_event)\n\ndf.head(n=2)\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[17], line 9\n      1 max_date = (\n      2   func\n      3     .max(col('date'))\n      4     .over(xid_partition)\n      5 )\n      7 n_days_since_last_event = func.datediff(func.current_date(), max_date)\n----&gt; 9 df = df.withColumn('n_days_since_last_event',\n     10                    n_days_since_last_event)\n     12 df.head(n=2)\n\nNameError: name 'df' is not defined\n\n\n\n\n\nCode\ndf.printSchema()\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[18], line 1\n----&gt; 1 df.printSchema()\n\nNameError: name 'df' is not defined"
  },
  {
    "objectID": "core/notebooks/notebook08_webdata-II.html#construct-a-column-containing-the-number-of-actions-of-each-user-for-each-modality-of-device",
    "href": "core/notebooks/notebook08_webdata-II.html#construct-a-column-containing-the-number-of-actions-of-each-user-for-each-modality-of-device",
    "title": "Using with pyspark for data preprocessing",
    "section": "Construct a column containing the number of actions of each user for each modality of device",
    "text": "Construct a column containing the number of actions of each user for each modality of device\nDoes this partitionBy triggers shuffling?\n\n\nCode\nxid_device_partition = xid_partition.partitionBy('device')\n\nn_events_per_device = func.count(col('action')).over(xid_device_partition)\n\ndf = df.withColumn('n_events_per_device', n_events_per_device)\n\ndf.head(n=2)\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[19], line 5\n      1 xid_device_partition = xid_partition.partitionBy('device')\n      3 n_events_per_device = func.count(col('action')).over(xid_device_partition)\n----&gt; 5 df = df.withColumn('n_events_per_device', n_events_per_device)\n      7 df.head(n=2)\n\nNameError: name 'df' is not defined"
  },
  {
    "objectID": "core/notebooks/notebook08_webdata-II.html#number-of-devices-per-user",
    "href": "core/notebooks/notebook08_webdata-II.html#number-of-devices-per-user",
    "title": "Using with pyspark for data preprocessing",
    "section": "Number of devices per user ",
    "text": "Number of devices per user \n\n\nCode\n# xid_partition = Window.partitionBy('xid')\n\nrank_device = (\n  func\n    .dense_rank()\n    .over(xid_partition.orderBy('device'))\n)\n\nn_unique_device = (\n    func\n      .last(rank_device)\n      .over(xid_partition)\n)\n\ndf = df.withColumn('n_device', n_unique_device)\n\ndf.head(n=2)\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[20], line 15\n      3 rank_device = (\n      4   func\n      5     .dense_rank()\n      6     .over(xid_partition.orderBy('device'))\n      7 )\n      9 n_unique_device = (\n     10     func\n     11       .last(rank_device)\n     12       .over(xid_partition)\n     13 )\n---&gt; 15 df = df.withColumn('n_device', n_unique_device)\n     17 df.head(n=2)\n\nNameError: name 'df' is not defined\n\n\n\n\n\nCode\ndf\\\n    .where(col('n_device') &gt; 1)\\\n    .select('xid', 'device', 'n_events',  'n_device', 'n_events_per_device')\\\n    .head(n=8)\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[21], line 1\n----&gt; 1 df\\\n      2     .where(col('n_device') &gt; 1)\\\n      3     .select('xid', 'device', 'n_events',  'n_device', 'n_events_per_device')\\\n      4     .head(n=8)\n\nNameError: name 'df' is not defined\n\n\n\n\n\nCode\ndf\\\n    .where(col('n_device') &gt; 1)\\\n    .select('xid', 'device', 'n_events',  'n_device', 'n_events_per_device')\\\n    .count()\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[22], line 1\n----&gt; 1 df\\\n      2     .where(col('n_device') &gt; 1)\\\n      3     .select('xid', 'device', 'n_events',  'n_device', 'n_events_per_device')\\\n      4     .count()\n\nNameError: name 'df' is not defined"
  },
  {
    "objectID": "core/notebooks/notebook08_webdata-II.html#extraction",
    "href": "core/notebooks/notebook08_webdata-II.html#extraction",
    "title": "Using with pyspark for data preprocessing",
    "section": "Extraction",
    "text": "Extraction\nHere extraction is just about reading the data\n\n\nCode\ndf = spark.read.parquet(input_file)\ndf.head(n=3)\n\n\n\n---------------------------------------------------------------------------\nAnalysisException                         Traceback (most recent call last)\nCell In[23], line 1\n----&gt; 1 df = spark.read.parquet(input_file)\n      2 df.head(n=3)\n\nFile ~/Documents/IFEBY310/.venv/lib/python3.12/site-packages/pyspark/sql/readwriter.py:544, in DataFrameReader.parquet(self, *paths, **options)\n    533 int96RebaseMode = options.get(\"int96RebaseMode\", None)\n    534 self._set_opts(\n    535     mergeSchema=mergeSchema,\n    536     pathGlobFilter=pathGlobFilter,\n   (...)\n    541     int96RebaseMode=int96RebaseMode,\n    542 )\n--&gt; 544 return self._df(self._jreader.parquet(_to_seq(self._spark._sc, paths)))\n\nFile ~/Documents/IFEBY310/.venv/lib/python3.12/site-packages/py4j/java_gateway.py:1322, in JavaMember.__call__(self, *args)\n   1316 command = proto.CALL_COMMAND_NAME +\\\n   1317     self.command_header +\\\n   1318     args_command +\\\n   1319     proto.END_COMMAND_PART\n   1321 answer = self.gateway_client.send_command(command)\n-&gt; 1322 return_value = get_return_value(\n   1323     answer, self.gateway_client, self.target_id, self.name)\n   1325 for temp_arg in temp_args:\n   1326     if hasattr(temp_arg, \"_detach\"):\n\nFile ~/Documents/IFEBY310/.venv/lib/python3.12/site-packages/pyspark/errors/exceptions/captured.py:185, in capture_sql_exception.&lt;locals&gt;.deco(*a, **kw)\n    181 converted = convert_exception(e.java_exception)\n    182 if not isinstance(converted, UnknownException):\n    183     # Hide where the exception came from that shows a non-Pythonic\n    184     # JVM exception message.\n--&gt; 185     raise converted from None\n    186 else:\n    187     raise\n\nAnalysisException: [PATH_NOT_FOUND] Path does not exist: file:/home/boucheron/Documents/IFEBY310/core/notebooks/webdata.parquet."
  },
  {
    "objectID": "core/notebooks/notebook08_webdata-II.html#transformation-of-the-data",
    "href": "core/notebooks/notebook08_webdata-II.html#transformation-of-the-data",
    "title": "Using with pyspark for data preprocessing",
    "section": "Transformation of the data",
    "text": "Transformation of the data\nAt this step we compute a lot of extra things from the data. The aim is to build features that describe users.\n\n\nCode\ndef n_events_transformer(df):\n    xid_partition = Window.partitionBy('xid')\n    n_events = func.count(col('action')).over(xid_partition)\n    \n    df = df.withColumn('n_events', n_events)\n\n    return df\n\n\n\n\nCode\ndef n_events_per_action_transformer(df):\n    xid_action_partition = Window.partitionBy('xid', 'action')\n    n_events_per_action = func.count(col('action')).over(xid_action_partition)\n\n    df = df.withColumn('n_events_per_action', n_events_per_action)\n    \n    return df\n\n\n\n\nCode\ndef hour_transformer(df):\n    hour = func.hour(col('date'))\n    df = df.withColumn('hour', hour)\n    return df\n\ndef weekday_transformer(df):\n    weekday = func.date_format(col('date'), 'EEEE')\n    df = df.withColumn('weekday', weekday)\n    return df\n\ndef n_events_per_hour_transformer(df):\n    xid_hour_partition = Window.partitionBy('xid', 'hour')\n    n_events_per_hour = func.count(col('action')).over(xid_hour_partition)\n    df = df.withColumn('n_events_per_hour', n_events_per_hour)\n    return df\n\ndef n_events_per_weekday_transformer(df):\n    xid_weekday_partition = Window.partitionBy('xid', 'weekday')\n    n_events_per_weekday = func.count(col('action')).over(xid_weekday_partition)\n    df = df.withColumn('n_events_per_weekday', n_events_per_weekday)\n    return df\n\ndef n_days_since_last_event_transformer(df):\n    xid_partition = Window.partitionBy('xid')\n    max_date = func.max(col('date')).over(xid_partition)\n    n_days_since_last_event = func.datediff(func.current_date(), max_date)\n    df = df.withColumn('n_days_since_last_event',\n                       n_days_since_last_event + lit(0.1))\n    return df\n\ndef n_days_since_last_action_transformer(df):\n    xid_partition_action = Window.partitionBy('xid', 'action')\n    max_date = func.max(col('date')).over(xid_partition_action)\n    n_days_since_last_action = func.datediff(func.current_date(),\n                                                        max_date)\n    df = df.withColumn('n_days_since_last_action',\n                       n_days_since_last_action + lit(0.1))\n    return df\n\ndef n_unique_day_transformer(df):\n    xid_partition = Window.partitionBy('xid')\n    dayofyear = func.dayofyear(col('date'))\n    rank_day = func.dense_rank().over(xid_partition.orderBy(dayofyear))\n    n_unique_day = func.last(rank_day).over(xid_partition)\n    df = df.withColumn('n_unique_day', n_unique_day)\n    return df\n\ndef n_unique_hour_transformer(df):\n    xid_partition = Window.partitionBy('xid')\n    rank_hour = func.dense_rank().over(xid_partition.orderBy('hour'))\n    n_unique_hour = func.last(rank_hour).over(xid_partition)\n    df = df.withColumn('n_unique_hour', n_unique_hour)\n    return df\n\ndef n_events_per_device_transformer(df):\n    xid_device_partition = Window.partitionBy('xid', 'device')\n    n_events_per_device = func.count(func.col('device')) \\\n        .over(xid_device_partition)\n    df = df.withColumn('n_events_per_device', n_events_per_device)\n    return df\n\ndef n_unique_device_transformer(df):\n    xid_partition = Window.partitionBy('xid')\n    rank_device = func.dense_rank().over(xid_partition.orderBy('device'))\n    n_unique_device = func.last(rank_device).over(xid_partition)\n    df = df.withColumn('n_device', n_unique_device)\n    return df\n\ndef n_actions_per_category_id_transformer(df):\n    xid_category_id_partition = Window.partitionBy('xid', 'category_id',\n                                                   'action')\n    n_actions_per_category_id = func.count(func.col('action')) \\\n        .over(xid_category_id_partition)\n    df = df.withColumn('n_actions_per_category_id', n_actions_per_category_id)\n    return df\n\ndef n_unique_category_id_transformer(df):\n    xid_partition = Window.partitionBy('xid')\n    rank_category_id = func.dense_rank().over(xid_partition\\\n                                              .orderBy('category_id'))\n    n_unique_category_id = func.last(rank_category_id).over(xid_partition)\n    df = df.withColumn('n_unique_category_id', n_unique_category_id)\n    return df\n\ndef n_events_per_category_id_transformer(df):\n    xid_category_id_partition = Window.partitionBy('xid', 'category_id')\n    n_events_per_category_id = func.count(func.col('action')) \\\n        .over(xid_category_id_partition)\n    df = df.withColumn('n_events_per_category_id', n_events_per_category_id)\n    return df\n\ndef n_events_per_website_id_transformer(df):\n    xid_website_id_partition = Window.partitionBy('xid', 'website_id')\n    n_events_per_website_id = func.count(col('action'))\\\n        .over(xid_website_id_partition)\n    df = df.withColumn('n_events_per_website_id', n_events_per_website_id)\n    return df\n\n\n\n\nCode\ntransformers = [\n    hour_transformer,\n    weekday_transformer,\n    n_events_per_hour_transformer,\n    n_events_per_weekday_transformer,\n    n_days_since_last_event_transformer,\n    n_days_since_last_action_transformer,\n    n_unique_day_transformer,\n    n_unique_hour_transformer,\n    n_events_per_device_transformer,\n    n_unique_device_transformer,\n    n_actions_per_category_id_transformer,\n    n_events_per_category_id_transformer,\n    n_events_per_website_id_transformer,\n]\n\n\n\n\nCode\nN = 10000\n\n\n\n\nCode\nsample_df = df.sample(withReplacement=False, fraction=.05)\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[29], line 1\n----&gt; 1 sample_df = df.sample(withReplacement=False, fraction=.05)\n\nNameError: name 'df' is not defined\n\n\n\n\n\nCode\nsample_df.count()\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[30], line 1\n----&gt; 1 sample_df.count()\n\nNameError: name 'sample_df' is not defined\n\n\n\n\n\nCode\nfor transformer in transformers:\n    df = transformer(df)\n\ndf.head(n=1)\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[31], line 2\n      1 for transformer in transformers:\n----&gt; 2     df = transformer(df)\n      4 df.head(n=1)\n\nNameError: name 'df' is not defined\n\n\n\n\n\nCode\nfor transformer in transformers:\n    sample_df = transformer(sample_df)\n\nsample_df.head(n=1)\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[32], line 2\n      1 for transformer in transformers:\n----&gt; 2     sample_df = transformer(sample_df)\n      4 sample_df.head(n=1)\n\nNameError: name 'sample_df' is not defined\n\n\n\n\n\nCode\ndf = sample_df\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[33], line 1\n----&gt; 1 df = sample_df\n\nNameError: name 'sample_df' is not defined\n\n\n\n\n\nCode\nsorted(df.columns)\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[34], line 1\n----&gt; 1 sorted(df.columns)\n\nNameError: name 'df' is not defined\n\n\n\n\n\nCode\ndf.explain()\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[35], line 1\n----&gt; 1 df.explain()\n\nNameError: name 'df' is not defined\n\n\n\n\n\nCode\nspark._sc.setCheckpointDir(\".\")   \n\ndf.checkpoint()\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[36], line 3\n      1 spark._sc.setCheckpointDir(\".\")   \n----&gt; 3 df.checkpoint()\n\nNameError: name 'df' is not defined\n\n\n\n\n\nCode\ndf.explain()\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[37], line 1\n----&gt; 1 df.explain()\n\nNameError: name 'df' is not defined"
  },
  {
    "objectID": "core/notebooks/notebook08_webdata-II.html#load-step",
    "href": "core/notebooks/notebook08_webdata-II.html#load-step",
    "title": "Using with pyspark for data preprocessing",
    "section": "Load step",
    "text": "Load step\nHere, we use all the previous computations (saved in the columns of the dataframe) to compute aggregated informations about each user.\n\n\n\n\n\n\nNote\n\n\n\nThis should be DRYED\n\n\n\n\nCode\ndef n_events_per_hour_loader(df):\n    csr = df\\\n        .select('xid', 'hour', 'n_events_per_hour')\\\n        .withColumnRenamed('n_events_per_hour', 'value')\\\n        .distinct() \n            # action\n    feature_name = func.concat(lit('n_events_per_hour#'), col('hour'))\n\n    csr = csr\\\n        .withColumn('feature_name', feature_name)\\\n        .drop('hour')\n    return csr\n\ndef n_events_per_website_id_loader(df):\n    csr = df.select('xid', 'website_id', 'n_events_per_website_id')\\\n        .withColumnRenamed('n_events_per_hour', 'value')\\\n        .distinct()\n\n    feature_name = func.concat(lit('n_events_per_website_id#'),\n                               col('website_id'))\n    \n    csr = csr\\\n        .withColumn('feature_name', feature_name)\\\n        .drop('website_id')\n    return csr\n\ndef n_events_per_hour_loader(df):\n    csr = df\\\n        .select('xid', 'hour', 'n_events_per_hour')\\\n        .withColumnRenamed('n_events_per_hour', 'value')\\\n        .distinct()\n\n    feature_name = func.concat(lit('n_events_per_hour#'), col('hour'))\n    \n    csr = csr\\\n        .withColumn('feature_name', feature_name)\\\n        .drop('hour')\n    return csr\n\ndef n_events_per_weekday_loader(df):\n    csr = df\\\n        .select('xid', 'weekday', 'n_events_per_weekday')\\\n        .withColumnRenamed('n_events_per_weekday', 'value')\\\n        .distinct()\n\n    feature_name = func.concat(lit('n_events_per_weekday#'), col('weekday'))\n    \n    csr = csr\\\n        .withColumn('feature_name', feature_name)\\\n        .drop('weekday')\n\n    return csr\n\ndef n_days_since_last_event_loader(df):\n    csr = df.select('xid',  'n_days_since_last_event')\\\n        .withColumnRenamed('n_days_since_last_event', 'value')\\\n        .distinct()\n    feature_name = lit('n_days_since_last_event')\n    csr = csr\\\n        .withColumn('feature_name', feature_name)\n    return csr\n\ndef n_days_since_last_action_loader(df):\n    csr = df.select('xid', 'action', 'n_days_since_last_action')\\\n        .withColumnRenamed('n_days_since_last_action', 'value')\\\n        .distinct()\n    feature_name = func.concat(lit('n_days_since_last_action#'), col('action'))\n    csr = csr\\\n        .withColumn('feature_name', feature_name)\\\n        .drop('action')\n    return csr\n\ndef n_unique_day_loader(df):\n    csr = df.select('xid', 'n_unique_day')\\\n        .withColumnRenamed('n_unique_day', 'value')\\\n        .distinct()\n    feature_name = lit('n_unique_day')\n    csr = csr\\\n        .withColumn('feature_name', feature_name)\n    return csr\n\ndef n_unique_hour_loader(df):\n    csr = df.select('xid', 'n_unique_hour')\\\n        .withColumnRenamed('n_unique_hour', 'value')\\\n        .distinct()\n    feature_name = lit('n_unique_hour')\n    csr = csr\\\n        .withColumn('feature_name', feature_name)\n    return csr\n\ndef n_events_per_device_loader(df):\n    csr = df\\\n        .select('xid', 'device', 'n_events_per_device')\\\n        .withColumnRenamed('n_events_per_device', 'value')\\\n        .distinct()\n    feature_name = func.concat(lit('n_events_per_device#'), col('device'))\n    csr = csr\\\n        .withColumn('feature_name', feature_name)\\\n        .drop('device')\n    return csr\n\ndef n_unique_device_loader(df):\n    csr = df.select('xid', 'n_device')\\\n        .withColumnRenamed('n_device', 'value')\\\n        .distinct()\n    feature_name = lit('n_device')\n    csr = csr\\\n        .withColumn('feature_name', feature_name)\n    return csr\n\ndef n_events_per_category_id_loader(df):\n    csr = df.select('xid', 'category_id', 'n_events_per_category_id')\\\n        .withColumnRenamed('n_events_per_category_id', 'value')\\\n        .distinct()\n    feature_name = func.concat(lit('n_events_per_category_id#'),\n                               col('category_id'))\n    csr = csr\\\n        .withColumn('feature_name', feature_name)\\\n        .drop('category_id')\n    return csr\n\ndef n_actions_per_category_id_loader(df):\n    csr = df.select('xid', 'category_id', 'action', 'n_actions_per_category_id')\\\n        .withColumnRenamed('n_actions_per_category_id', 'value')\\\n        .distinct()\n    feature_name = func.concat(lit('n_actions_per_category_id#'),\n                               col('action'), lit('#'), \n                               col('category_id'))\n    csr = csr\\\n        .withColumn('feature_name', feature_name)\\\n        .drop('category_id')\\\n        .drop('action')\n    return csr\n\ndef n_events_per_website_id_loader(df):\n    csr = df.select('xid', 'website_id', 'n_events_per_website_id')\\\n        .withColumnRenamed('n_events_per_website_id', 'value')\\\n        .distinct()\n    feature_name = func.concat(lit('n_events_per_website_id#'),\n                               col('website_id'))\n    csr = csr\\\n        .withColumn('feature_name', feature_name)\\\n        .drop('website_id')\n    return csr\n\n\n\n\nCode\nfrom functools import reduce\n\n\n\n\nCode\nloaders = [\n    n_events_per_hour_loader,\n    n_events_per_website_id_loader,\n    n_events_per_hour_loader,\n    n_events_per_weekday_loader,\n    n_days_since_last_event_loader,\n    n_days_since_last_action_loader,\n    n_unique_day_loader,\n    n_unique_hour_loader,\n    n_events_per_device_loader,\n    n_unique_device_loader,\n    n_events_per_category_id_loader,\n    n_actions_per_category_id_loader,\n    n_events_per_website_id_loader,\n]\n\n\n\n\nCode\ndef union(df, other):\n    return df.union(other)\n\n\n\n\n\n\n\n\nAbout DataFrame.union()\n\n\n\nThis method performs a SQL-style set union of the rows from both DataFrame objects, with no automatic deduplication of elements.\nUse the distinct() method to perform deduplication of rows.\nThe method resolves columns by position (not by name), following the standard behavior in SQL.\n\n\n\n\nCode\nspam = [loader(df) for loader in loaders]\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[42], line 1\n----&gt; 1 spam = [loader(df) for loader in loaders]\n\nNameError: name 'df' is not defined\n\n\n\n\n\nCode\nspam[0].printSchema()\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[43], line 1\n----&gt; 1 spam[0].printSchema()\n\nNameError: name 'spam' is not defined\n\n\n\n\n\nCode\nall(spam[0].columns == it.columns for it in spam[1:])\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[44], line 1\n----&gt; 1 all(spam[0].columns == it.columns for it in spam[1:])\n\nNameError: name 'spam' is not defined\n\n\n\n\n\nCode\nlen(spam)\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[45], line 1\n----&gt; 1 len(spam)\n\nNameError: name 'spam' is not defined\n\n\n\n\n\nCode\ncsr = reduce(\n    lambda df1, df2: df1.union(df2),\n    spam\n)\n\ncsr.head(n=3)\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[46], line 3\n      1 csr = reduce(\n      2     lambda df1, df2: df1.union(df2),\n----&gt; 3     spam\n      4 )\n      6 csr.head(n=3)\n\nNameError: name 'spam' is not defined\n\n\n\n\n\nCode\ncsr.columns\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[47], line 1\n----&gt; 1 csr.columns\n\nNameError: name 'csr' is not defined\n\n\n\n\n\nCode\ncsr.show(5)\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[48], line 1\n----&gt; 1 csr.show(5)\n\nNameError: name 'csr' is not defined\n\n\n\n\n\nCode\ncsr.rdd.getNumPartitions()\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[49], line 1\n----&gt; 1 csr.rdd.getNumPartitions()\n\nNameError: name 'csr' is not defined\n\n\n\n\n\nCode\n# Replace features names and xid by a unique number\nfeature_name_partition = Window().orderBy('feature_name')\n\nxid_partition = Window().orderBy('xid')\n\ncol_idx = func.dense_rank().over(feature_name_partition)\nrow_idx = func.dense_rank().over(xid_partition)\n\n\n\n\nCode\ncsr = csr.withColumn('col', col_idx)\\\n    .withColumn('row', row_idx)\n\ncsr = csr.na.drop('any')\n\ncsr.head(n=5)\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[51], line 1\n----&gt; 1 csr = csr.withColumn('col', col_idx)\\\n      2     .withColumn('row', row_idx)\n      4 csr = csr.na.drop('any')\n      6 csr.head(n=5)\n\nNameError: name 'csr' is not defined\n\n\n\n\n\nCode\n# Let's save the result of our hard work into a new parquet file\noutput_path = './'\noutput_file = os.path.join(output_path, 'csr.parquet')\ncsr.write.parquet(output_file, mode='overwrite')\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[52], line 4\n      2 output_path = './'\n      3 output_file = os.path.join(output_path, 'csr.parquet')\n----&gt; 4 csr.write.parquet(output_file, mode='overwrite')\n\nNameError: name 'csr' is not defined"
  },
  {
    "objectID": "core/notebooks/notebook08_webdata-II.html#finally",
    "href": "core/notebooks/notebook08_webdata-II.html#finally",
    "title": "Using with pyspark for data preprocessing",
    "section": "Finally !!",
    "text": "Finally !!\nWow ! That was a lot of work. Now we have a features matrix \\(X\\) and a vector of labels \\(y\\).\n\n\nCode\nX.indices\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[76], line 1\n----&gt; 1 X.indices\n\nNameError: name 'X' is not defined\n\n\n\n\n\nCode\nX.indptr\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[77], line 1\n----&gt; 1 X.indptr\n\nNameError: name 'X' is not defined\n\n\n\n\n\nCode\nX.shape, X.nnz\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[78], line 1\n----&gt; 1 X.shape, X.nnz\n\nNameError: name 'X' is not defined\n\n\n\n\n\nCode\ny.shape, y.sum()\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[79], line 1\n----&gt; 1 y.shape, y.sum()\n\nNameError: name 'y' is not defined"
  },
  {
    "objectID": "core/notebooks/notebook06_sparksql.html",
    "href": "core/notebooks/notebook06_sparksql.html",
    "title": "DataFrame",
    "section": "",
    "text": "Code\nimport os\nimport sys\n\nos.environ['PYSPARK_PYTHON'] = sys.executable\nos.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable\nCode\nfrom pyspark import SparkConf, SparkContext\nfrom pyspark.sql import SparkSession\n\nconf = SparkConf().setAppName(\"Spark SQL Course\")\nsc = SparkContext(conf=conf)  # no need for Spark 3...\n\nspark = (SparkSession\n    .builder\n    .appName(\"Spark SQL Course\")\n    .getOrCreate()\n)\n\n\n25/02/05 21:53:06 WARN Utils: Your hostname, boucheron-Precision-5480 resolves to a loopback address: 127.0.1.1; using 192.168.10.120 instead (on interface wlp0s20f3)\n25/02/05 21:53:06 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\nSetting default log level to \"WARN\".\nTo adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n25/02/05 21:53:06 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n25/02/05 21:53:07 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\nCode\nfrom pyspark.sql import Row\n\nrow1 = Row(name=\"John\", age=21)\nrow2 = Row(name=\"James\", age=32)\nrow3 = Row(name=\"Jane\", age=18)\nrow1['name']\n\n\n'John'\nCode\ndf = spark.createDataFrame([row1, row2, row3])\nCode\ndf.printSchema()\n\n\nroot\n |-- name: string (nullable = true)\n |-- age: long (nullable = true)\nCode\ndf.show()\n\n\n[Stage 0:&gt;                                                          (0 + 1) / 1]                                                                                \n\n\n+-----+---+\n| name|age|\n+-----+---+\n| John| 21|\n|James| 32|\n| Jane| 18|\n+-----+---+\nCode\nprint(df.rdd.toDebugString().decode(\"utf-8\"))\n\n\n(20) MapPartitionsRDD[10] at javaToPython at NativeMethodAccessorImpl.java:0 []\n |   MapPartitionsRDD[9] at javaToPython at NativeMethodAccessorImpl.java:0 []\n |   SQLExecutionRDD[8] at javaToPython at NativeMethodAccessorImpl.java:0 []\n |   MapPartitionsRDD[7] at javaToPython at NativeMethodAccessorImpl.java:0 []\n |   MapPartitionsRDD[4] at applySchemaToPythonRDD at NativeMethodAccessorImpl.java:0 []\n |   MapPartitionsRDD[3] at map at SerDeUtil.scala:69 []\n |   MapPartitionsRDD[2] at mapPartitions at SerDeUtil.scala:117 []\n |   PythonRDD[1] at RDD at PythonRDD.scala:53 []\n |   ParallelCollectionRDD[0] at readRDDFromFile at PythonRDD.scala:289 []\nCode\ndf.rdd.getNumPartitions()\n\n\n20"
  },
  {
    "objectID": "core/notebooks/notebook06_sparksql.html#creating-dataframes",
    "href": "core/notebooks/notebook06_sparksql.html#creating-dataframes",
    "title": "DataFrame",
    "section": "Creating dataframes",
    "text": "Creating dataframes\n\n\nCode\nrows = [\n    Row(name=\"John\", age=21, gender=\"male\"),\n    Row(name=\"James\", age=25, gender=\"female\"),\n    Row(name=\"Albert\", age=46, gender=\"male\")\n]\n\ndf = spark.createDataFrame(rows)\n\n\n\n\nCode\ndf.show()\n\n\n+------+---+------+\n|  name|age|gender|\n+------+---+------+\n|  John| 21|  male|\n| James| 25|female|\n|Albert| 46|  male|\n+------+---+------+\n\n\n\n\n\nCode\nhelp(Row)\n\n\nHelp on class Row in module pyspark.sql.types:\n\nclass Row(builtins.tuple)\n |  Row(*args: Optional[str], **kwargs: Optional[Any]) -&gt; 'Row'\n |\n |  A row in :class:`DataFrame`.\n |  The fields in it can be accessed:\n |\n |  * like attributes (``row.key``)\n |  * like dictionary values (``row[key]``)\n |\n |  ``key in row`` will search through row keys.\n |\n |  Row can be used to create a row object by using named arguments.\n |  It is not allowed to omit a named argument to represent that the value is\n |  None or missing. This should be explicitly set to None in this case.\n |\n |  .. versionchanged:: 3.0.0\n |      Rows created from named arguments no longer have\n |      field names sorted alphabetically and will be ordered in the position as\n |      entered.\n |\n |  Examples\n |  --------\n |  &gt;&gt;&gt; from pyspark.sql import Row\n |  &gt;&gt;&gt; row = Row(name=\"Alice\", age=11)\n |  &gt;&gt;&gt; row\n |  Row(name='Alice', age=11)\n |  &gt;&gt;&gt; row['name'], row['age']\n |  ('Alice', 11)\n |  &gt;&gt;&gt; row.name, row.age\n |  ('Alice', 11)\n |  &gt;&gt;&gt; 'name' in row\n |  True\n |  &gt;&gt;&gt; 'wrong_key' in row\n |  False\n |\n |  Row also can be used to create another Row like class, then it\n |  could be used to create Row objects, such as\n |\n |  &gt;&gt;&gt; Person = Row(\"name\", \"age\")\n |  &gt;&gt;&gt; Person\n |  &lt;Row('name', 'age')&gt;\n |  &gt;&gt;&gt; 'name' in Person\n |  True\n |  &gt;&gt;&gt; 'wrong_key' in Person\n |  False\n |  &gt;&gt;&gt; Person(\"Alice\", 11)\n |  Row(name='Alice', age=11)\n |\n |  This form can also be used to create rows as tuple values, i.e. with unnamed\n |  fields.\n |\n |  &gt;&gt;&gt; row1 = Row(\"Alice\", 11)\n |  &gt;&gt;&gt; row2 = Row(name=\"Alice\", age=11)\n |  &gt;&gt;&gt; row1 == row2\n |  True\n |\n |  Method resolution order:\n |      Row\n |      builtins.tuple\n |      builtins.object\n |\n |  Methods defined here:\n |\n |  __call__(self, *args: Any) -&gt; 'Row'\n |      create new Row object\n |\n |  __contains__(self, item: Any) -&gt; bool\n |      Return bool(key in self).\n |\n |  __getattr__(self, item: str) -&gt; Any\n |\n |  __getitem__(self, item: Any) -&gt; Any\n |      Return self[key].\n |\n |  __reduce__(self) -&gt; Union[str, Tuple[Any, ...]]\n |      Returns a tuple so Python knows how to pickle Row.\n |\n |  __repr__(self) -&gt; str\n |      Printable representation of Row used in Python REPL.\n |\n |  __setattr__(self, key: Any, value: Any) -&gt; None\n |      Implement setattr(self, name, value).\n |\n |  asDict(self, recursive: bool = False) -&gt; Dict[str, Any]\n |      Return as a dict\n |\n |      Parameters\n |      ----------\n |      recursive : bool, optional\n |          turns the nested Rows to dict (default: False).\n |\n |      Notes\n |      -----\n |      If a row contains duplicate field names, e.g., the rows of a join\n |      between two :class:`DataFrame` that both have the fields of same names,\n |      one of the duplicate fields will be selected by ``asDict``. ``__getitem__``\n |      will also return one of the duplicate fields, however returned value might\n |      be different to ``asDict``.\n |\n |      Examples\n |      --------\n |      &gt;&gt;&gt; from pyspark.sql import Row\n |      &gt;&gt;&gt; Row(name=\"Alice\", age=11).asDict() == {'name': 'Alice', 'age': 11}\n |      True\n |      &gt;&gt;&gt; row = Row(key=1, value=Row(name='a', age=2))\n |      &gt;&gt;&gt; row.asDict() == {'key': 1, 'value': Row(name='a', age=2)}\n |      True\n |      &gt;&gt;&gt; row.asDict(True) == {'key': 1, 'value': {'name': 'a', 'age': 2}}\n |      True\n |\n |  ----------------------------------------------------------------------\n |  Static methods defined here:\n |\n |  __new__(cls, *args: Optional[str], **kwargs: Optional[Any]) -&gt; 'Row'\n |      Create and return a new object.  See help(type) for accurate signature.\n |\n |  ----------------------------------------------------------------------\n |  Data descriptors defined here:\n |\n |  __dict__\n |      dictionary for instance variables\n |\n |  ----------------------------------------------------------------------\n |  Methods inherited from builtins.tuple:\n |\n |  __add__(self, value, /)\n |      Return self+value.\n |\n |  __eq__(self, value, /)\n |      Return self==value.\n |\n |  __ge__(self, value, /)\n |      Return self&gt;=value.\n |\n |  __getattribute__(self, name, /)\n |      Return getattr(self, name).\n |\n |  __getnewargs__(self, /)\n |\n |  __gt__(self, value, /)\n |      Return self&gt;value.\n |\n |  __hash__(self, /)\n |      Return hash(self).\n |\n |  __iter__(self, /)\n |      Implement iter(self).\n |\n |  __le__(self, value, /)\n |      Return self&lt;=value.\n |\n |  __len__(self, /)\n |      Return len(self).\n |\n |  __lt__(self, value, /)\n |      Return self&lt;value.\n |\n |  __mul__(self, value, /)\n |      Return self*value.\n |\n |  __ne__(self, value, /)\n |      Return self!=value.\n |\n |  __rmul__(self, value, /)\n |      Return value*self.\n |\n |  count(self, value, /)\n |      Return number of occurrences of value.\n |\n |  index(self, value, start=0, stop=9223372036854775807, /)\n |      Return first index of value.\n |\n |      Raises ValueError if the value is not present.\n |\n |  ----------------------------------------------------------------------\n |  Class methods inherited from builtins.tuple:\n |\n |  __class_getitem__(...)\n |      See PEP 585\n\n\n\n\n\nCode\ncolumn_names = [\"name\", \"age\", \"gender\"]\nrows = [\n    [\"John\", 21, \"male\"],\n    [\"James\", 25, \"female\"],\n    [\"Albert\", 46, \"male\"]\n]\n\ndf = spark.createDataFrame(\n    rows, \n    column_names\n)\n\ndf.show()\n\n\n+------+---+------+\n|  name|age|gender|\n+------+---+------+\n|  John| 21|  male|\n| James| 25|female|\n|Albert| 46|  male|\n+------+---+------+\n\n\n\n\n\nCode\ndf.printSchema()\n\n\nroot\n |-- name: string (nullable = true)\n |-- age: long (nullable = true)\n |-- gender: string (nullable = true)\n\n\n\n\n\nCode\n# sc = SparkContext(conf=conf)  # no need for Spark 3...\n\ncolumn_names = [\"name\", \"age\", \"gender\"]\nrdd = sc.parallelize([\n    (\"John\", 21, \"male\"),\n    (\"James\", 25, \"female\"),\n    (\"Albert\", 46, \"male\")\n])\ndf = spark.createDataFrame(rdd, column_names)\ndf.show()\n\n\n+------+---+------+\n|  name|age|gender|\n+------+---+------+\n|  John| 21|  male|\n| James| 25|female|\n|Albert| 46|  male|\n+------+---+------+"
  },
  {
    "objectID": "core/notebooks/notebook06_sparksql.html#schema",
    "href": "core/notebooks/notebook06_sparksql.html#schema",
    "title": "DataFrame",
    "section": "Schema",
    "text": "Schema\nThere is special type schemata. A object of class StructType is made of a list of objects of type StructField.\n\n\nCode\ndf.schema\n\n\nStructType([StructField('name', StringType(), True), StructField('age', LongType(), True), StructField('gender', StringType(), True)])\n\n\n\n\nCode\ntype(df.schema)\n\n\npyspark.sql.types.StructType\n\n\nA object of type StructField has a name, a PySpark type, an d a boolean parameter.\n\n\nCode\nfrom pyspark.sql.types import *\n\nschema = StructType(\n    [\n        StructField(\"name\", StringType(), True),\n        StructField(\"age\", IntegerType(), True),\n        StructField(\"gender\", StringType(), True)\n    ]\n)\n\nrows = [(\"John\", 21, \"male\")]\ndf = spark.createDataFrame(rows, schema)\ndf.printSchema()\ndf.show()\n\n\nroot\n |-- name: string (nullable = true)\n |-- age: integer (nullable = true)\n |-- gender: string (nullable = true)\n\n+----+---+------+\n|name|age|gender|\n+----+---+------+\n|John| 21|  male|\n+----+---+------+"
  },
  {
    "objectID": "core/notebooks/notebook06_sparksql.html#select",
    "href": "core/notebooks/notebook06_sparksql.html#select",
    "title": "DataFrame",
    "section": "SELECT",
    "text": "SELECT\n\n\nCode\ndf.createOrReplaceTempView(\"table\")    \nquery = \"SELECT name, age FROM table\"\nspark.sql(query).show()\n\n\n+----+---+\n|name|age|\n+----+---+\n|John| 21|\n|Jane| 25|\n+----+---+\n\n\n\n\n\nCode\ndf.select(\"name\", \"age\").show()\n\n\n+----+---+\n|name|age|\n+----+---+\n|John| 21|\n|Jane| 25|\n+----+---+"
  },
  {
    "objectID": "core/notebooks/notebook06_sparksql.html#where",
    "href": "core/notebooks/notebook06_sparksql.html#where",
    "title": "DataFrame",
    "section": "WHERE",
    "text": "WHERE\n\n\nCode\ndf.createOrReplaceTempView(\"table\")\nquery = \"SELECT * FROM table WHERE age &gt; 21\"\nspark.sql(query).show()\n\n\n+----+---+------+\n|name|age|gender|\n+----+---+------+\n|Jane| 25|female|\n+----+---+------+\n\n\n\n\n\nCode\ndf.where(\"age &gt; 21\").show()\n\n\n+----+---+------+\n|name|age|gender|\n+----+---+------+\n|Jane| 25|female|\n+----+---+------+\n\n\n\n\n\nCode\n# Alternatively:\ndf.where(df['age'] &gt; 21).show()\n\n\n+----+---+------+\n|name|age|gender|\n+----+---+------+\n|Jane| 25|female|\n+----+---+------+\n\n\n\n\n\nCode\ndf.where(df.age &gt; 21).show()\n\n\n+----+---+------+\n|name|age|gender|\n+----+---+------+\n|Jane| 25|female|\n+----+---+------+\n\n\n\n\n\nCode\n( \n    df.where(\"age &gt; 21\")\n      .select([\"name\", \"age\"])\n      .show()\n)\n\n\n+----+---+\n|name|age|\n+----+---+\n|Jane| 25|\n+----+---+"
  },
  {
    "objectID": "core/notebooks/notebook06_sparksql.html#limit",
    "href": "core/notebooks/notebook06_sparksql.html#limit",
    "title": "DataFrame",
    "section": "LIMIT",
    "text": "LIMIT\n\n\nCode\ndf.createOrReplaceTempView(\"table\")\n\nquery = \"\"\"\n    SELECT \n        * \n    FROM \n        table \n    LIMIT 1\n\"\"\"\n\nspark.sql(query).show()\n\n\n+----+---+------+\n|name|age|gender|\n+----+---+------+\n|John| 21|  male|\n+----+---+------+\n\n\n\n\n\nCode\ndf.limit(1).show()\n\n\n+----+---+------+\n|name|age|gender|\n+----+---+------+\n|John| 21|  male|\n+----+---+------+\n\n\n\n\n\nCode\ndf.select(\"*\").limit(1).show()\n\n\n+----+---+------+\n|name|age|gender|\n+----+---+------+\n|John| 21|  male|\n+----+---+------+"
  },
  {
    "objectID": "core/notebooks/notebook06_sparksql.html#order-by",
    "href": "core/notebooks/notebook06_sparksql.html#order-by",
    "title": "DataFrame",
    "section": "ORDER BY",
    "text": "ORDER BY\n\n\nCode\ndf.createOrReplaceTempView(\"table\")\n\nquery = \"\"\"\n    SELECT \n        * \n    FROM \n        table\n    ORDER BY \n        name ASC\n\"\"\"\n\nspark.sql(query).show()\n\n\n+----+---+------+\n|name|age|gender|\n+----+---+------+\n|Jane| 25|female|\n|John| 21|  male|\n+----+---+------+\n\n\n\n\n\nCode\ndf.orderBy(df.name.asc()).show()\n\n\n+----+---+------+\n|name|age|gender|\n+----+---+------+\n|Jane| 25|female|\n|John| 21|  male|\n+----+---+------+"
  },
  {
    "objectID": "core/notebooks/notebook06_sparksql.html#alias",
    "href": "core/notebooks/notebook06_sparksql.html#alias",
    "title": "DataFrame",
    "section": "ALIAS",
    "text": "ALIAS\n\n\nCode\ndf.createOrReplaceTempView(\"table\")\nquery = \"SELECT name, age, gender AS sex FROM table\"\nspark.sql(query).show()\n\n\n+----+---+------+\n|name|age|   sex|\n+----+---+------+\n|John| 21|  male|\n|Jane| 25|female|\n+----+---+------+\n\n\n\n\n\nCode\ntype(df.age)\n\n\npyspark.sql.column.Column\n\n\n\n\nCode\ndf.select(df.name, df.age, df.gender.alias('sex')).show()\n\n\n+----+---+------+\n|name|age|   sex|\n+----+---+------+\n|John| 21|  male|\n|Jane| 25|female|\n+----+---+------+"
  },
  {
    "objectID": "core/notebooks/notebook06_sparksql.html#cast",
    "href": "core/notebooks/notebook06_sparksql.html#cast",
    "title": "DataFrame",
    "section": "CAST",
    "text": "CAST\n\n\nCode\ndf.createOrReplaceTempView(\"table\")\nquery = \"SELECT name, cast(age AS float) AS age_f FROM table\"\nspark.sql(query).show()\n\n\n+----+-----+\n|name|age_f|\n+----+-----+\n|John| 21.0|\n|Jane| 25.0|\n+----+-----+\n\n\n\n\n\nCode\ndf.select(df.name, df.age.cast(\"float\").alias(\"age_f\")).show()\n\n\n+----+-----+\n|name|age_f|\n+----+-----+\n|John| 21.0|\n|Jane| 25.0|\n+----+-----+\n\n\n\n\n\nCode\nnew_age_col = df.age.cast(\"float\").alias(\"age_f\")\ntype(new_age_col), type(df.age)\n\n\n(pyspark.sql.column.Column, pyspark.sql.column.Column)\n\n\n\n\nCode\ndf.select(df.name, new_age_col).show()\n\n\n+----+-----+\n|name|age_f|\n+----+-----+\n|John| 21.0|\n|Jane| 25.0|\n+----+-----+"
  },
  {
    "objectID": "core/notebooks/notebook06_sparksql.html#adding-new-columns",
    "href": "core/notebooks/notebook06_sparksql.html#adding-new-columns",
    "title": "DataFrame",
    "section": "Adding new columns",
    "text": "Adding new columns\n\n\nCode\ndf.createOrReplaceTempView(\"table\")\n\nquery = \"\"\"\n    SELECT \n        *, \n        12*age AS age_months \n    FROM \n        table\n\"\"\"\n\nspark.sql(query).show()\n\n\n+----+---+------+----------+\n|name|age|gender|age_months|\n+----+---+------+----------+\n|John| 21|  male|       252|\n|Jane| 25|female|       300|\n+----+---+------+----------+\n\n\n\n\n\nCode\n( \n    df\n        .withColumn(\"age_months\", df.age * 12)\n        .show()\n)\n\n\n+----+---+------+----------+\n|name|age|gender|age_months|\n+----+---+------+----------+\n|John| 21|  male|       252|\n|Jane| 25|female|       300|\n+----+---+------+----------+\n\n\n\n\n\nCode\n(\n    df\n        .select(\"*\", \n                (df.age * 12).alias(\"age_months\"))\n        .show()\n)\n\n\n+----+---+------+----------+\n|name|age|gender|age_months|\n+----+---+------+----------+\n|John| 21|  male|       252|\n|Jane| 25|female|       300|\n+----+---+------+----------+\n\n\n\n\n\nCode\nimport datetime\n\nhui = datetime.date.today()\n\nhui = hui.replace(year=hui.year-21)\n\nstr(hui)\n\n\n'2004-02-05'\n\n\n\n\nCode\n# df.select(\"*\", hui.replace(year=hui.year - df.age ).alias(\"yob\")).show()"
  },
  {
    "objectID": "core/notebooks/notebook06_sparksql.html#numeric-functions-examples",
    "href": "core/notebooks/notebook06_sparksql.html#numeric-functions-examples",
    "title": "DataFrame",
    "section": "Numeric functions examples",
    "text": "Numeric functions examples\n\n\nCode\nfrom pyspark.sql import functions as fn\n\ncolumns = [\"brand\", \"cost\"]\ndf = spark.createDataFrame([\n    (\"garnier\", 3.49),\n    (\"elseve\", 2.71)\n], columns)\n\nround_cost = fn.round(df.cost, 1)\nfloor_cost = fn.floor(df.cost)\nceil_cost = fn.ceil(df.cost)\n\ndf.withColumn('round', round_cost)\\\n    .withColumn('floor', floor_cost)\\\n    .withColumn('ceil', ceil_cost)\\\n    .show()\n\n\n+-------+----+-----+-----+----+\n|  brand|cost|round|floor|ceil|\n+-------+----+-----+-----+----+\n|garnier|3.49|  3.5|    3|   4|\n| elseve|2.71|  2.7|    2|   3|\n+-------+----+-----+-----+----+"
  },
  {
    "objectID": "core/notebooks/notebook06_sparksql.html#string-functions-examples",
    "href": "core/notebooks/notebook06_sparksql.html#string-functions-examples",
    "title": "DataFrame",
    "section": "String functions examples",
    "text": "String functions examples\n\n\nCode\nfrom pyspark.sql import functions as fn\n\ncolumns = [\"first_name\", \"last_name\"]\n\ndf = spark.createDataFrame([\n    (\"John\", \"Doe\"),\n    (\"Mary\", \"Jane\")\n], columns)\n\nlast_name_initial = fn.substring(df.last_name, 0, 1)\n# last_name_initial_dotted = fn.concat(last_name_initial, \".\")\n\nname = fn.concat_ws(\" \", df.first_name, last_name_initial)\ndf.withColumn(\"name\", name).show()\n\n\n+----------+---------+------+\n|first_name|last_name|  name|\n+----------+---------+------+\n|      John|      Doe|John D|\n|      Mary|     Jane|Mary J|\n+----------+---------+------+\n\n\n\n\n\nCode\n( \n    df.selectExpr(\"*\", \"substring(last_name, 0, 1) as lni\")\n      .selectExpr(\"first_name\", \"last_name\", \"concat(first_name, ' ', lni, '.') as nname\")\n      .show()\n)\n\n\n+----------+---------+-------+\n|first_name|last_name|  nname|\n+----------+---------+-------+\n|      John|      Doe|John D.|\n|      Mary|     Jane|Mary J.|\n+----------+---------+-------+"
  },
  {
    "objectID": "core/notebooks/notebook06_sparksql.html#date-functions-examples",
    "href": "core/notebooks/notebook06_sparksql.html#date-functions-examples",
    "title": "DataFrame",
    "section": "Date functions examples",
    "text": "Date functions examples\n\n\nCode\nfrom datetime import date\nfrom pyspark.sql import functions as fn\n\ndf = spark.createDataFrame([\n    (date(2015, 1, 1), date(2015, 1, 15)),\n    (date(2015, 2, 21), date(2015, 3, 8)),\n], [\"start_date\", \"end_date\"])\n\ndays_between = fn.datediff(df.end_date, df.start_date)\nstart_month = fn.month(df.start_date)\n\ndf.withColumn('days_between', days_between)\\\n    .withColumn('start_month', start_month)\\\n    .show()\n\n\n+----------+----------+------------+-----------+\n|start_date|  end_date|days_between|start_month|\n+----------+----------+------------+-----------+\n|2015-01-01|2015-01-15|          14|          1|\n|2015-02-21|2015-03-08|          15|          2|\n+----------+----------+------------+-----------+\n\n\n\n\n\nCode\nstr(date(2015, 1, 1) - date(2015, 1, 15))\n\n\n'-14 days, 0:00:00'\n\n\n\n\nCode\nfrom datetime import timedelta\n\ndate(2023, 2 , 14) + timedelta(days=3)\n\n\ndatetime.date(2023, 2, 17)"
  },
  {
    "objectID": "core/notebooks/notebook06_sparksql.html#conditional-transformations",
    "href": "core/notebooks/notebook06_sparksql.html#conditional-transformations",
    "title": "DataFrame",
    "section": "Conditional transformations",
    "text": "Conditional transformations\n\n\nCode\ndf = spark.createDataFrame([\n    (\"John\", 21, \"male\"),\n    (\"Jane\", 25, \"female\"),\n    (\"Albert\", 46, \"male\"),\n    (\"Brad\", 49, \"super-hero\")\n], [\"name\", \"age\", \"gender\"])\n\n\n\n\nCode\nsupervisor = ( \n    fn.when(df.gender == 'male', 'Mr. Smith')\n      .when(df.gender == 'female', 'Miss Jones')\n      .otherwise('NA')\n)\n\ntype(supervisor), type(fn.when)\n\n\n(pyspark.sql.column.Column, function)\n\n\n\n\nCode\ndf.withColumn(\"supervisor\", supervisor).show()\n\n\n+------+---+----------+----------+\n|  name|age|    gender|supervisor|\n+------+---+----------+----------+\n|  John| 21|      male| Mr. Smith|\n|  Jane| 25|    female|Miss Jones|\n|Albert| 46|      male| Mr. Smith|\n|  Brad| 49|super-hero|        NA|\n+------+---+----------+----------+"
  },
  {
    "objectID": "core/notebooks/notebook06_sparksql.html#user-defined-functions",
    "href": "core/notebooks/notebook06_sparksql.html#user-defined-functions",
    "title": "DataFrame",
    "section": "User-defined functions",
    "text": "User-defined functions\n\n\nCode\nfrom pyspark.sql import functions as fn\nfrom pyspark.sql.types import StringType\n\ndf = spark.createDataFrame([(1, 3), (4, 2)], [\"first\", \"second\"])\n\ndef my_func(col_1, col_2):\n    if (col_1 &gt; col_2):\n        return \"{} is bigger than {}\".format(col_1, col_2)\n    else:\n        return \"{} is bigger than {}\".format(col_2, col_1)\n\nmy_udf = fn.udf(my_func, StringType())\n\ndf.withColumn(\"udf\", my_udf(df['first'], df['second'])).show()\n\n\n[Stage 92:&gt;                                                         (0 + 1) / 1]                                                                                \n\n\n+-----+------+------------------+\n|first|second|               udf|\n+-----+------+------------------+\n|    1|     3|3 is bigger than 1|\n|    4|     2|4 is bigger than 2|\n+-----+------+------------------+"
  },
  {
    "objectID": "core/notebooks/notebook06_sparksql.html#using-the-spark.sql-api",
    "href": "core/notebooks/notebook06_sparksql.html#using-the-spark.sql-api",
    "title": "DataFrame",
    "section": "Using the spark.sql API",
    "text": "Using the spark.sql API\n\n\nCode\nfrom datetime import date\n\nproducts = spark.createDataFrame([\n    ('1', 'mouse', 'microsoft', 39.99),\n    ('2', 'keyboard', 'logitech', 59.99),\n], ['prod_id', 'prod_cat', 'prod_brand', 'prod_value'])\n\npurchases = spark.createDataFrame([\n    (date(2017, 11, 1), 2, '1'),\n    (date(2017, 11, 2), 1, '1'),\n    (date(2017, 11, 5), 1, '2'),\n], ['date', 'quantity', 'prod_id'])\n\n# The default join type is the \"INNER\" join\npurchases.join(products, 'prod_id').show()\n\n\n+-------+----------+--------+--------+----------+----------+\n|prod_id|      date|quantity|prod_cat|prod_brand|prod_value|\n+-------+----------+--------+--------+----------+----------+\n|      1|2017-11-01|       2|   mouse| microsoft|     39.99|\n|      1|2017-11-02|       1|   mouse| microsoft|     39.99|\n|      2|2017-11-05|       1|keyboard|  logitech|     59.99|\n+-------+----------+--------+--------+----------+----------+\n\n\n\n\n\nCode\npurchases.join(products, 'prod_id').explain()\n\n\n== Physical Plan ==\nAdaptiveSparkPlan isFinalPlan=false\n+- Project [prod_id#577, date#575, quantity#576L, prod_cat#568, prod_brand#569, prod_value#570]\n   +- SortMergeJoin [prod_id#577], [prod_id#567], Inner\n      :- Sort [prod_id#577 ASC NULLS FIRST], false, 0\n      :  +- Exchange hashpartitioning(prod_id#577, 200), ENSURE_REQUIREMENTS, [plan_id=575]\n      :     +- Filter isnotnull(prod_id#577)\n      :        +- Scan ExistingRDD[date#575,quantity#576L,prod_id#577]\n      +- Sort [prod_id#567 ASC NULLS FIRST], false, 0\n         +- Exchange hashpartitioning(prod_id#567, 200), ENSURE_REQUIREMENTS, [plan_id=576]\n            +- Filter isnotnull(prod_id#567)\n               +- Scan ExistingRDD[prod_id#567,prod_cat#568,prod_brand#569,prod_value#570]"
  },
  {
    "objectID": "core/notebooks/notebook06_sparksql.html#using-a-sql-query",
    "href": "core/notebooks/notebook06_sparksql.html#using-a-sql-query",
    "title": "DataFrame",
    "section": "Using a SQL query",
    "text": "Using a SQL query\n\n\nCode\nproducts.createOrReplaceTempView(\"products\")\npurchases.createOrReplaceTempView(\"purchases\")\n\nquery = \"\"\"\n    SELECT * \n    FROM purchases AS prc INNER JOIN \n        products AS prd \n    ON prc.prod_id = prd.prod_id\n\"\"\"\nspark.sql(query).show()\n\n\n+----------+--------+-------+-------+--------+----------+----------+\n|      date|quantity|prod_id|prod_id|prod_cat|prod_brand|prod_value|\n+----------+--------+-------+-------+--------+----------+----------+\n|2017-11-01|       2|      1|      1|   mouse| microsoft|     39.99|\n|2017-11-02|       1|      1|      1|   mouse| microsoft|     39.99|\n|2017-11-05|       1|      2|      2|keyboard|  logitech|     59.99|\n+----------+--------+-------+-------+--------+----------+----------+\n\n\n\n\n\nCode\nspark.sql(query).explain()\n\n\n== Physical Plan ==\nAdaptiveSparkPlan isFinalPlan=false\n+- SortMergeJoin [prod_id#577], [prod_id#567], Inner\n   :- Sort [prod_id#577 ASC NULLS FIRST], false, 0\n   :  +- Exchange hashpartitioning(prod_id#577, 200), ENSURE_REQUIREMENTS, [plan_id=718]\n   :     +- Filter isnotnull(prod_id#577)\n   :        +- Scan ExistingRDD[date#575,quantity#576L,prod_id#577]\n   +- Sort [prod_id#567 ASC NULLS FIRST], false, 0\n      +- Exchange hashpartitioning(prod_id#567, 200), ENSURE_REQUIREMENTS, [plan_id=719]\n         +- Filter isnotnull(prod_id#567)\n            +- Scan ExistingRDD[prod_id#567,prod_cat#568,prod_brand#569,prod_value#570]\n\n\n\n\n\n\nCode\nnew_purchases = spark.createDataFrame([\n    (date(2017, 11, 1), 2, '1'),\n    (date(2017, 11, 2), 1, '3'),\n], ['date', 'quantity', 'prod_id_x'])\n\n# The default join type is the \"INNER\" join\njoin_rule = new_purchases.prod_id_x == products.prod_id\n\nprint(type(join_rule))\n\nnew_purchases.join(products, join_rule, 'left').show()\n\n\n&lt;class 'pyspark.sql.column.Column'&gt;\n+----------+--------+---------+-------+--------+----------+----------+\n|      date|quantity|prod_id_x|prod_id|prod_cat|prod_brand|prod_value|\n+----------+--------+---------+-------+--------+----------+----------+\n|2017-11-01|       2|        1|      1|   mouse| microsoft|     39.99|\n|2017-11-02|       1|        3|   NULL|    NULL|      NULL|      NULL|\n+----------+--------+---------+-------+--------+----------+----------+\n\n\n\n\n\nCode\njoin_rule.info\n\n\nColumn&lt;'(prod_id_x = prod_id)[info]'&gt;\n\n\n\n\nCode\nnew_purchases = spark.createDataFrame([\n    (date(2017, 11, 1), 2, '1'),\n    (date(2017, 11, 2), 1, '3'),\n], ['date', 'quantity', 'prod_id_x'])\n\n# The default join type is the \"INNER\" join\njoin_rule = new_purchases.prod_id_x == products.prod_id\n\nnew_purchases.join(products, join_rule, 'left').show()\n\n\n+----------+--------+---------+-------+--------+----------+----------+\n|      date|quantity|prod_id_x|prod_id|prod_cat|prod_brand|prod_value|\n+----------+--------+---------+-------+--------+----------+----------+\n|2017-11-01|       2|        1|      1|   mouse| microsoft|     39.99|\n|2017-11-02|       1|        3|   NULL|    NULL|      NULL|      NULL|\n+----------+--------+---------+-------+--------+----------+----------+"
  },
  {
    "objectID": "core/notebooks/notebook06_sparksql.html#various-types-of-joins",
    "href": "core/notebooks/notebook06_sparksql.html#various-types-of-joins",
    "title": "DataFrame",
    "section": "Various types of joins",
    "text": "Various types of joins\n\n\nCode\nleft = spark.createDataFrame([\n    (1, \"A1\"), (2, \"A2\"), (3, \"A3\"), (4, \"A4\")], \n    [\"id\", \"value\"])\n\nright = spark.createDataFrame([\n    (3, \"A3\"), (4, \"A4\"), (4, \"A4_1\"), (5, \"A5\"), (6, \"A6\")], \n    [\"id\", \"value\"])\n\njoin_types = [\n    \"inner\", \"outer\", \"left\", \"right\",\n    \"leftsemi\", \"leftanti\"\n]\n\n\n\n\nCode\nfor join_type in join_types:\n    print(join_type)\n    left.join(right, on=\"id\", how=join_type)\\\n        .orderBy(\"id\")\\\n        .show()\n\n\ninner\n+---+-----+-----+\n| id|value|value|\n+---+-----+-----+\n|  3|   A3|   A3|\n|  4|   A4|   A4|\n|  4|   A4| A4_1|\n+---+-----+-----+\n\nouter\n+---+-----+-----+\n| id|value|value|\n+---+-----+-----+\n|  1|   A1| NULL|\n|  2|   A2| NULL|\n|  3|   A3|   A3|\n|  4|   A4|   A4|\n|  4|   A4| A4_1|\n|  5| NULL|   A5|\n|  6| NULL|   A6|\n+---+-----+-----+\n\nleft\n+---+-----+-----+\n| id|value|value|\n+---+-----+-----+\n|  1|   A1| NULL|\n|  2|   A2| NULL|\n|  3|   A3|   A3|\n|  4|   A4|   A4|\n|  4|   A4| A4_1|\n+---+-----+-----+\n\nright\n+---+-----+-----+\n| id|value|value|\n+---+-----+-----+\n|  3|   A3|   A3|\n|  4|   A4|   A4|\n|  4|   A4| A4_1|\n|  5| NULL|   A5|\n|  6| NULL|   A6|\n+---+-----+-----+\n\nleftsemi\n+---+-----+\n| id|value|\n+---+-----+\n|  3|   A3|\n|  4|   A4|\n+---+-----+\n\nleftanti\n+---+-----+\n| id|value|\n+---+-----+\n|  1|   A1|\n|  2|   A2|\n+---+-----+"
  },
  {
    "objectID": "core/notebooks/notebook06_sparksql.html#examples-using-the-api",
    "href": "core/notebooks/notebook06_sparksql.html#examples-using-the-api",
    "title": "DataFrame",
    "section": "Examples using the API",
    "text": "Examples using the API\n\n\nCode\nfrom pyspark.sql import functions as fn\n\nproducts = spark.createDataFrame([\n    ('1', 'mouse', 'microsoft', 39.99),\n    ('2', 'mouse', 'microsoft', 59.99),\n    ('3', 'keyboard', 'microsoft', 59.99),\n    ('4', 'keyboard', 'logitech', 59.99),\n    ('5', 'mouse', 'logitech', 29.99),\n], ['prod_id', 'prod_cat', 'prod_brand', 'prod_value'])\n\n( \n    products\n        .groupBy('prod_cat')\n        .avg('prod_value')\n        .show()\n)\n\n\n+--------+-----------------+\n|prod_cat|  avg(prod_value)|\n+--------+-----------------+\n|   mouse|43.32333333333333|\n|keyboard|            59.99|\n+--------+-----------------+\n\n\n\n\n\nCode\n(\n    products\n        .groupBy('prod_cat')\n        .agg(fn.avg('prod_value'))\n        .show()\n)\n\n\n+--------+-----------------+\n|prod_cat|  avg(prod_value)|\n+--------+-----------------+\n|   mouse|43.32333333333333|\n|keyboard|            59.99|\n+--------+-----------------+\n\n\n\n\n\nCode\n(\n    products\n        .groupBy('prod_cat')\n        .agg(\n            fn.mean('prod_value'), \n            fn.stddev('prod_value')\n        )\n        .show()\n)\n\n\n+--------+-----------------+------------------+\n|prod_cat|  avg(prod_value)|stddev(prod_value)|\n+--------+-----------------+------------------+\n|   mouse|43.32333333333333|15.275252316519468|\n|keyboard|            59.99|               0.0|\n+--------+-----------------+------------------+\n\n\n\n\n\nCode\nfrom pyspark.sql import functions as fn\n\n(\n    products\n        .groupBy('prod_brand', 'prod_cat')\\\n        .agg(\n            fn.avg('prod_value')\n        )\n        .show()\n)\n\n\n+----------+--------+---------------+\n|prod_brand|prod_cat|avg(prod_value)|\n+----------+--------+---------------+\n| microsoft|   mouse|          49.99|\n| microsoft|keyboard|          59.99|\n|  logitech|keyboard|          59.99|\n|  logitech|   mouse|          29.99|\n+----------+--------+---------------+\n\n\n\n\n\nCode\nfrom pyspark.sql import functions as fn\n\n(\n    products\n        .groupBy('prod_brand')\n        .agg(\n            fn.round(\n                fn.avg('prod_value'), 1)\n                .alias('average'),\n            fn.ceil(\n                fn.sum('prod_value'))\n                .alias('sum'),\n            fn.min('prod_value')\n                .alias('min')\n        )\n        .show()\n)\n\n\n+----------+-------+---+-----+\n|prod_brand|average|sum|  min|\n+----------+-------+---+-----+\n| microsoft|   53.3|160|39.99|\n|  logitech|   45.0| 90|29.99|\n+----------+-------+---+-----+"
  },
  {
    "objectID": "core/notebooks/notebook06_sparksql.html#example-using-a-query",
    "href": "core/notebooks/notebook06_sparksql.html#example-using-a-query",
    "title": "DataFrame",
    "section": "Example using a query",
    "text": "Example using a query\n\n\nCode\nproducts.createOrReplaceTempView(\"products\")\n\n\n\n\nCode\nquery = \"\"\"\nSELECT\n    prod_brand,\n    round(avg(prod_value), 1) AS average,\n    min(prod_value) AS min\nFROM \n    products\nGROUP BY \n    prod_brand\n\"\"\"\n\nspark.sql(query).show()\n\n\n+----------+-------+-----+\n|prod_brand|average|  min|\n+----------+-------+-----+\n| microsoft|   53.3|39.99|\n|  logitech|   45.0|29.99|\n+----------+-------+-----+"
  },
  {
    "objectID": "core/notebooks/notebook06_sparksql.html#numerical-window-functions",
    "href": "core/notebooks/notebook06_sparksql.html#numerical-window-functions",
    "title": "DataFrame",
    "section": "Numerical window functions",
    "text": "Numerical window functions\n\n\nCode\nfrom pyspark.sql import Window\nfrom pyspark.sql import functions as fn\n\n# First, we create the Window definition\nwindow = Window.partitionBy('prod_brand')\n\nprint(type(window))\n\n\n&lt;class 'pyspark.sql.window.WindowSpec'&gt;\n\n\nThen, we can use over to aggregate on this window\n\n\nCode\navg = fn.avg('prod_value').over(window)\n\n# Finally, we can it as a classical column\n(\n    products\n        .withColumn('avg_brand_value', fn.round(avg, 2))\n        .show()\n)\n\n\n+-------+--------+----------+----------+---------------+\n|prod_id|prod_cat|prod_brand|prod_value|avg_brand_value|\n+-------+--------+----------+----------+---------------+\n|      4|keyboard|  logitech|     59.99|          44.99|\n|      5|   mouse|  logitech|     29.99|          44.99|\n|      1|   mouse| microsoft|     39.99|          53.32|\n|      2|   mouse| microsoft|     59.99|          53.32|\n|      3|keyboard| microsoft|     59.99|          53.32|\n+-------+--------+----------+----------+---------------+\n\n\n\nWith SQL queries, using multiple windows is not a problem\n\n\nCode\nquery = \"\"\"\n    SELECT \n        *, \n        ROUND(AVG(prod_value) OVER w1, 2)  AS avg_brand_value,\n        ROUND(AVG(prod_value) OVER w2, 1)  AS avg_prod_value\n    FROM \n        products\n    WINDOW \n        w1 AS (PARTITION BY prod_brand),\n        w2 AS (PARTITION BY prod_cat)\n\"\"\"\n\nspark.sql(query).show()\n\n\n+-------+--------+----------+----------+---------------+--------------+\n|prod_id|prod_cat|prod_brand|prod_value|avg_brand_value|avg_prod_value|\n+-------+--------+----------+----------+---------------+--------------+\n|      4|keyboard|  logitech|     59.99|          44.99|          60.0|\n|      3|keyboard| microsoft|     59.99|          53.32|          60.0|\n|      5|   mouse|  logitech|     29.99|          44.99|          43.3|\n|      1|   mouse| microsoft|     39.99|          53.32|          43.3|\n|      2|   mouse| microsoft|     59.99|          53.32|          43.3|\n+-------+--------+----------+----------+---------------+--------------+\n\n\n\n\n\nCode\nwindow2 = Window.partitionBy('prod_cat')\n\navg2 = fn.avg('prod_value').over(window2)\n\n# Finally, we can it as a classical column\n( \n    products\n        .withColumn('avg_brand_value', fn.round(avg, 2))\n        .withColumn('avg_prod_value', fn.round(avg2, 1))\n        .show()\n)\n\n\n+-------+--------+----------+----------+---------------+--------------+\n|prod_id|prod_cat|prod_brand|prod_value|avg_brand_value|avg_prod_value|\n+-------+--------+----------+----------+---------------+--------------+\n|      4|keyboard|  logitech|     59.99|          44.99|          60.0|\n|      3|keyboard| microsoft|     59.99|          53.32|          60.0|\n|      5|   mouse|  logitech|     29.99|          44.99|          43.3|\n|      1|   mouse| microsoft|     39.99|          53.32|          43.3|\n|      2|   mouse| microsoft|     59.99|          53.32|          43.3|\n+-------+--------+----------+----------+---------------+--------------+\n\n\n\nNow we can compare the physical plans associated with the two jobs.\n\n\nCode\n( \n    products\n        .withColumn('avg_brand_value', fn.round(avg, 2))\n        .withColumn('avg_prod_value', fn.round(avg2, 1))\n        .explain()\n)\n\n\n== Physical Plan ==\nAdaptiveSparkPlan isFinalPlan=false\n+- Project [prod_id#854, prod_cat#855, prod_brand#856, prod_value#857, avg_brand_value#1195, round(_we0#1203, 1) AS avg_prod_value#1202]\n   +- Window [avg(prod_value#857) windowspecdefinition(prod_cat#855, specifiedwindowframe(RowFrame, unboundedpreceding$(), unboundedfollowing$())) AS _we0#1203], [prod_cat#855]\n      +- Sort [prod_cat#855 ASC NULLS FIRST], false, 0\n         +- Exchange hashpartitioning(prod_cat#855, 200), ENSURE_REQUIREMENTS, [plan_id=2249]\n            +- Project [prod_id#854, prod_cat#855, prod_brand#856, prod_value#857, round(_we0#1196, 2) AS avg_brand_value#1195]\n               +- Window [avg(prod_value#857) windowspecdefinition(prod_brand#856, specifiedwindowframe(RowFrame, unboundedpreceding$(), unboundedfollowing$())) AS _we0#1196], [prod_brand#856]\n                  +- Sort [prod_brand#856 ASC NULLS FIRST], false, 0\n                     +- Exchange hashpartitioning(prod_brand#856, 200), ENSURE_REQUIREMENTS, [plan_id=2244]\n                        +- Scan ExistingRDD[prod_id#854,prod_cat#855,prod_brand#856,prod_value#857]\n\n\n\n\n\n\nCode\nspark.sql(query).explain()\n\n\n== Physical Plan ==\nAdaptiveSparkPlan isFinalPlan=false\n+- Project [prod_id#854, prod_cat#855, prod_brand#856, prod_value#857, round(_we0#1214, 2) AS avg_brand_value#1210, round(_we1#1215, 1) AS avg_prod_value#1211]\n   +- Window [avg(prod_value#857) windowspecdefinition(prod_cat#855, specifiedwindowframe(RowFrame, unboundedpreceding$(), unboundedfollowing$())) AS _we1#1215], [prod_cat#855]\n      +- Sort [prod_cat#855 ASC NULLS FIRST], false, 0\n         +- Exchange hashpartitioning(prod_cat#855, 200), ENSURE_REQUIREMENTS, [plan_id=2273]\n            +- Window [avg(prod_value#857) windowspecdefinition(prod_brand#856, specifiedwindowframe(RowFrame, unboundedpreceding$(), unboundedfollowing$())) AS _we0#1214], [prod_brand#856]\n               +- Sort [prod_brand#856 ASC NULLS FIRST], false, 0\n                  +- Exchange hashpartitioning(prod_brand#856, 200), ENSURE_REQUIREMENTS, [plan_id=2269]\n                     +- Scan ExistingRDD[prod_id#854,prod_cat#855,prod_brand#856,prod_value#857]"
  },
  {
    "objectID": "core/notebooks/notebook06_sparksql.html#lag-and-lead",
    "href": "core/notebooks/notebook06_sparksql.html#lag-and-lead",
    "title": "DataFrame",
    "section": "Lag and Lead",
    "text": "Lag and Lead\n\n\nCode\npurchases = spark.createDataFrame(\n    [\n        (date(2017, 11, 1), 'mouse'),\n        (date(2017, 11, 2), 'mouse'),\n        (date(2017, 11, 4), 'keyboard'),\n        (date(2017, 11, 6), 'keyboard'),\n        (date(2017, 11, 9), 'keyboard'),\n        (date(2017, 11, 12), 'mouse'),\n        (date(2017, 11, 18), 'keyboard')\n    ], \n    ['date', 'prod_cat']\n)\n\npurchases.show()\n\nwindow = Window.partitionBy('prod_cat').orderBy('date')\n\nprev_purch = fn.lag('date', 1).over(window)\nnext_purch = fn.lead('date', 1).over(window)\n\npurchases\\\n    .withColumn('prev', prev_purch)\\\n    .withColumn('next', next_purch)\\\n    .orderBy('prod_cat', 'date')\\\n    .show()\n\n\n+----------+--------+\n|      date|prod_cat|\n+----------+--------+\n|2017-11-01|   mouse|\n|2017-11-02|   mouse|\n|2017-11-04|keyboard|\n|2017-11-06|keyboard|\n|2017-11-09|keyboard|\n|2017-11-12|   mouse|\n|2017-11-18|keyboard|\n+----------+--------+\n\n+----------+--------+----------+----------+\n|      date|prod_cat|      prev|      next|\n+----------+--------+----------+----------+\n|2017-11-04|keyboard|      NULL|2017-11-06|\n|2017-11-06|keyboard|2017-11-04|2017-11-09|\n|2017-11-09|keyboard|2017-11-06|2017-11-18|\n|2017-11-18|keyboard|2017-11-09|      NULL|\n|2017-11-01|   mouse|      NULL|2017-11-02|\n|2017-11-02|   mouse|2017-11-01|2017-11-12|\n|2017-11-12|   mouse|2017-11-02|      NULL|\n+----------+--------+----------+----------+"
  },
  {
    "objectID": "core/notebooks/notebook06_sparksql.html#rank-denserank-and-rownumber",
    "href": "core/notebooks/notebook06_sparksql.html#rank-denserank-and-rownumber",
    "title": "DataFrame",
    "section": "Rank, DenseRank and RowNumber",
    "text": "Rank, DenseRank and RowNumber\n\n\nCode\ncontestants = spark.createDataFrame(\n    [   \n        ('veterans', 'John', 3000),\n        ('veterans', 'Bob', 3200),\n        ('veterans', 'Mary', 4000),\n        ('young', 'Jane', 4000),\n        ('young', 'April', 3100),\n        ('young', 'Alice', 3700),\n        ('young', 'Micheal', 4000),\n    ], \n    ['category', 'name', 'points']\n)\n\ncontestants.show()\n\n\n+--------+-------+------+\n|category|   name|points|\n+--------+-------+------+\n|veterans|   John|  3000|\n|veterans|    Bob|  3200|\n|veterans|   Mary|  4000|\n|   young|   Jane|  4000|\n|   young|  April|  3100|\n|   young|  Alice|  3700|\n|   young|Micheal|  4000|\n+--------+-------+------+\n\n\n\n\n\nCode\nwindow = (\n    Window\n        .partitionBy('category')\n        .orderBy(contestants.points.desc())\n)\n\nrank = fn.rank().over(window)\ndense_rank = fn.dense_rank().over(window)\nrow_number = fn.row_number().over(window)\n\ncontestants\\\n    .withColumn('rank', rank)\\\n    .withColumn('dense_rank', dense_rank)\\\n    .withColumn('row_number', row_number)\\\n    .orderBy('category', fn.col('points').desc())\\\n    .show()\n\n\n+--------+-------+------+----+----------+----------+\n|category|   name|points|rank|dense_rank|row_number|\n+--------+-------+------+----+----------+----------+\n|veterans|   Mary|  4000|   1|         1|         1|\n|veterans|    Bob|  3200|   2|         2|         2|\n|veterans|   John|  3000|   3|         3|         3|\n|   young|   Jane|  4000|   1|         1|         1|\n|   young|Micheal|  4000|   1|         1|         2|\n|   young|  Alice|  3700|   3|         2|         3|\n|   young|  April|  3100|   4|         3|         4|\n+--------+-------+------+----+----------+----------+"
  },
  {
    "objectID": "core/notebooks/notebook04_pandas_spark.html",
    "href": "core/notebooks/notebook04_pandas_spark.html",
    "title": "Data preprocessing and visualisation of a credit scoring dataset",
    "section": "",
    "text": "We’ll work on a dataset gro.csv for credit scoring that was proposed some years ago as a data challenge on some data challenge website. It is a realistic and somewhat messy dataset that contains a lot of missing values, several types of features (dates, categories, continuous features), so that serious data cleaning and formating is required. This dataset contains the following columns:"
  },
  {
    "objectID": "core/notebooks/notebook04_pandas_spark.html#lets-assess-what-we-did",
    "href": "core/notebooks/notebook04_pandas_spark.html#lets-assess-what-we-did",
    "title": "Data preprocessing and visualisation of a credit scoring dataset",
    "section": "Let’s assess what we did",
    "text": "Let’s assess what we did\nIt appears that we have to work a little bit more for a correct import of the data. Here is a list of the problems we face. - The last three columns are empty - Dates are actually str (python’s string type) - There is a lot of missing values - Categorial features are str - The Net_Annual_Income is imported as a string\nBy looking at the column names, the descriptions of the columns and using some basic, we infer the type of features that we have. There are dates features, continuous features, categorical features, and some features that could be either treated as categorical or continuous.\n\nThere are many missing values, that need to be handled.\nThe annual net income is imported as a string, we need to understand why.\nWe really need to treat dates as dates and not strings (because we want to compute the age of a client based on its birth year for instance).\n\nHere is a tentative structure of the features\nContinuous features\n\nYears_At_Residence\nNet_Annual_Income\nYears_At_Business\n\nFeatures to be decided\n\nNumber_Of_Dependant\nNb_Of_Products\n\nCategorical features\n\nCustomer_Type\nP_Client\nEducational_Level\nMarital_Status\nProd_Sub_Category\nSource\nType_Of_Residence\nProd_Category\n\nDate features\n\nBirthDate\nCustomer_Open_Date\nProd_Decision_Date\nProd_Closed_Date"
  },
  {
    "objectID": "core/notebooks/notebook04_pandas_spark.html#the-last-three-columns-are-weird-and-empty",
    "href": "core/notebooks/notebook04_pandas_spark.html#the-last-three-columns-are-weird-and-empty",
    "title": "Data preprocessing and visualisation of a credit scoring dataset",
    "section": "The last three columns are weird and empty",
    "text": "The last three columns are weird and empty\nIt seems to come from the fact that the data always ends with several ';' characters. We can remove them simply using the usecols option from read_csv."
  },
  {
    "objectID": "core/notebooks/notebook04_pandas_spark.html#dates-are-actually-str",
    "href": "core/notebooks/notebook04_pandas_spark.html#dates-are-actually-str",
    "title": "Data preprocessing and visualisation of a credit scoring dataset",
    "section": "Dates are actually str",
    "text": "Dates are actually str\nWe need to specify which columns must be encoded as dates using the parse_dates option from read_csv. Fortunately enough, pandas is clever enough to interpret the date format.\n\n\nCode\ntype(psdf.loc[0, 'BirthDate'])\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[15], line 1\n----&gt; 1 type(psdf.loc[0, 'BirthDate'])\n\nNameError: name 'psdf' is not defined"
  },
  {
    "objectID": "core/notebooks/notebook04_pandas_spark.html#there-is-a-lot-of-missing-values",
    "href": "core/notebooks/notebook04_pandas_spark.html#there-is-a-lot-of-missing-values",
    "title": "Data preprocessing and visualisation of a credit scoring dataset",
    "section": "There is a lot of missing values",
    "text": "There is a lot of missing values\nWe’ll see below that actually a single column mostly contain missing values.\n\n\nCode\npsdf.isnull().sum()\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[16], line 1\n----&gt; 1 psdf.isnull().sum()\n\nNameError: name 'psdf' is not defined\n\n\n\nThe column Prod_Closed_Date contains mostly missing values !\n\n\nCode\npsdf[['Prod_Closed_Date']].head(5)\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[17], line 1\n----&gt; 1 psdf[['Prod_Closed_Date']].head(5)\n\nNameError: name 'psdf' is not defined\n\n\n\nLet’s remove the useless columns and check the remaining missing values\nAgain there are variations. Keyword inplace is not legal in Pandas API on Spark\n\n\nCode\n# df.drop(['Prod_Closed_Date', 'Unnamed: 19', \n#          'Unnamed: 20', 'Unnamed: 21'], axis=\"columns\", inplace=True)\n\npsdf = psdf.drop(['Prod_Closed_Date', \n        'Unnamed: 19', \n        'Unnamed: 20', \n        'Unnamed: 21'], \n        axis=\"columns\")\n        \npsdf.head()         \n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[18], line 4\n      1 # df.drop(['Prod_Closed_Date', 'Unnamed: 19', \n      2 #          'Unnamed: 20', 'Unnamed: 21'], axis=\"columns\", inplace=True)\n----&gt; 4 psdf = psdf.drop(['Prod_Closed_Date', \n      5         'Unnamed: 19', \n      6         'Unnamed: 20', \n      7         'Unnamed: 21'], \n      8         axis=\"columns\")\n     10 psdf.head()         \n\nNameError: name 'psdf' is not defined\n\n\n\nLet’s display the rows with missing values and let’s highlight them\n\n\nCode\n# psdf[psdf.isnull().any(axis=\"columns\")].style.highlight_null()"
  },
  {
    "objectID": "core/notebooks/notebook04_pandas_spark.html#categorial-features-are-str",
    "href": "core/notebooks/notebook04_pandas_spark.html#categorial-features-are-str",
    "title": "Data preprocessing and visualisation of a credit scoring dataset",
    "section": "Categorial features are str",
    "text": "Categorial features are str\nWe need to say the dtype we want to use for some columns using the dtype option of read_csv.\n\n\nCode\ntype(psdf.loc[0, 'Prod_Sub_Category'])\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[20], line 1\n----&gt; 1 type(psdf.loc[0, 'Prod_Sub_Category'])\n\nNameError: name 'psdf' is not defined\n\n\n\n\n\nCode\npsdf['Prod_Sub_Category'].unique()\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[21], line 1\n----&gt; 1 psdf['Prod_Sub_Category'].unique()\n\nNameError: name 'psdf' is not defined"
  },
  {
    "objectID": "core/notebooks/notebook04_pandas_spark.html#the-annual-net-income-is-imported-as-a-string",
    "href": "core/notebooks/notebook04_pandas_spark.html#the-annual-net-income-is-imported-as-a-string",
    "title": "Data preprocessing and visualisation of a credit scoring dataset",
    "section": "The annual net income is imported as a string",
    "text": "The annual net income is imported as a string\nThis problem comes from the fact that the decimal separator is in European notation: it’s a ',' and not a '.', so we need to specify it using the decimal option to read_csv. (Data is French, pardon my French…)\n\n\nCode\ntype(psdf.loc[0, 'Net_Annual_Income'])\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[22], line 1\n----&gt; 1 type(psdf.loc[0, 'Net_Annual_Income'])\n\nNameError: name 'psdf' is not defined\n\n\n\n\n\nCode\npsdf['Net_Annual_Income'].head(n=10)\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[23], line 1\n----&gt; 1 psdf['Net_Annual_Income'].head(n=10)\n\nNameError: name 'psdf' is not defined"
  },
  {
    "objectID": "core/notebooks/notebook04_pandas_spark.html#comment-on-file-formats",
    "href": "core/notebooks/notebook04_pandas_spark.html#comment-on-file-formats",
    "title": "Data preprocessing and visualisation of a credit scoring dataset",
    "section": "Comment on file formats",
    "text": "Comment on file formats\nYou can use other methods starting with .to_XX to save in another format. Here are some main examples\n\nOK to use csv for “small” datasets (several MB)\nUse pickle for more compressed and faster format (limited to 4GB). It’s the standard binary serialization format of Python\nfeather is another fast and lightweight file format for storing data frames. A very popular exchange format.\nparquet is a format for big distributed data (works nicely with Spark)\n\namong several others…\n\n\nCode\n#df.to_pickle(\"gro_cleaned.pkl\")\npssdf.to_parquet(\"gro_cleaned.parquet\")\n# pssdf.reset_index().to_feather(\"gro_cleaned.feather\")\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[37], line 2\n      1 #df.to_pickle(\"gro_cleaned.pkl\")\n----&gt; 2 pssdf.to_parquet(\"gro_cleaned.parquet\")\n      3 # pssdf.reset_index().to_feather(\"gro_cleaned.feather\")\n\nNameError: name 'pssdf' is not defined\n\n\n\n\n\nCode\npssdf.index\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[38], line 1\n----&gt; 1 pssdf.index\n\nNameError: name 'pssdf' is not defined\n\n\n\nAnd you can read again using the corresponding read_XX function\n\n\nCode\npssdf = ps.read_parquet(\"gro_cleaned.parquet\")\npssdf.head()\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[39], line 1\n----&gt; 1 pssdf = ps.read_parquet(\"gro_cleaned.parquet\")\n      2 pssdf.head()\n\nNameError: name 'ps' is not defined\n\n\n\n\n\nCode\n!ls -alh gro_cleaned*\n\n\nls: cannot access 'gro_cleaned*': No such file or directory"
  },
  {
    "objectID": "core/notebooks/notebook04_pandas_spark.html#the-net-income-columns-is-very-weird",
    "href": "core/notebooks/notebook04_pandas_spark.html#the-net-income-columns-is-very-weird",
    "title": "Data preprocessing and visualisation of a credit scoring dataset",
    "section": "The net income columns is very weird",
    "text": "The net income columns is very weird\n\n\nCode\nincome = pssdf['Net_Annual_Income']\nincome.describe()\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[41], line 1\n----&gt; 1 income = pssdf['Net_Annual_Income']\n      2 income.describe()\n\nNameError: name 'pssdf' is not defined\n\n\n\n\n\nCode\n(income &lt;= 100).sum(), (income &gt; 100).sum()\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[42], line 1\n----&gt; 1 (income &lt;= 100).sum(), (income &gt; 100).sum()\n\nNameError: name 'income' is not defined\n\n\n\nMost values are smaller than 100, while some are much much larger…\n\n\nCode\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n%matplotlib inline\n\nsns.set_context(\"notebook\", font_scale=1.2)\n\n\n\n\nCode\nsns.displot(x='Net_Annual_Income', \n            data=pssdf, \n            bins=20,\n            height=4, \n            aspect=1.5)\n\n\n\n\nCode\npssdf[\"Net_Annual_Income\"].plot.hist(bins=40, \nhitsnorm='density', log_x=True)\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[43], line 1\n----&gt; 1 pssdf[\"Net_Annual_Income\"].plot.hist(bins=40, \n      2 hitsnorm='density', log_x=True)\n\nNameError: name 'pssdf' is not defined\n\n\n\n\n\nCode\npssdf[\"Net_Annual_Income\"].plot.kde(bw_method=10, log_x=True)\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[44], line 1\n----&gt; 1 pssdf[\"Net_Annual_Income\"].plot.kde(bw_method=10, log_x=True)\n\nNameError: name 'pssdf' is not defined\n\n\n\nThis is annoying, we don’t really see much…\n\n\nCode\nsns.displot(x='Net_Annual_Income', \n            data=pssdf, \n            bins=20, \n            height=4, \n            aspect=1.5, \n            log_scale=(False, True))\n\n\nDistribution for less than 100K revenue\n\n\nCode\nsns.displot(x='Net_Annual_Income', \n            data=pssdf[pssdf['Net_Annual_Income'] &lt; 100], \n            bins=15, \n            height=4, \n            aspect=1.5)\n\n\nDistribution for less than 400K revenue\n\n\nCode\nsns.displot(x='Net_Annual_Income', data=pssdf[pssdf['Net_Annual_Income'] &lt; 400], \n            bins=15, height=4, aspect=1.5)\n\n\n\n\nCode\n(pssdf['Net_Annual_Income'] == 36.0).sum()\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[45], line 1\n----&gt; 1 (pssdf['Net_Annual_Income'] == 36.0).sum()\n\nNameError: name 'pssdf' is not defined\n\n\n\n\n\nCode\nincome_counts = (\n    ps.DataFrame({\n        \"income_category\": pssdf['Net_Annual_Income'].astype(\"category\"),\n        \"income\": pssdf['Net_Annual_Income']\n    })\n    .groupby(\"income_category\")\n    .count()\n    .reset_index()\n    .rename(columns={\"income\": \"#customers\"})\n    .sort_values(by=\"#customers\", axis=\"index\", ascending=False)\n)\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[46], line 2\n      1 income_counts = (\n----&gt; 2     ps.DataFrame({\n      3         \"income_category\": pssdf['Net_Annual_Income'].astype(\"category\"),\n      4         \"income\": pssdf['Net_Annual_Income']\n      5     })\n      6     .groupby(\"income_category\")\n      7     .count()\n      8     .reset_index()\n      9     .rename(columns={\"income\": \"#customers\"})\n     10     .sort_values(by=\"#customers\", axis=\"index\", ascending=False)\n     11 )\n\nNameError: name 'ps' is not defined\n\n\n\n\n\nCode\nincome_counts[\"%cummulative clients\"] \\\n    = income_counts[\"#customers\"].cumsum() / income_counts[\"#customers\"].sum()\n\nincome_counts.iloc[:20].style.bar(subset=[\"%cummulative clients\"], vmin=0, vmax=1)\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[47], line 2\n      1 income_counts[\"%cummulative clients\"] \\\n----&gt; 2     = income_counts[\"#customers\"].cumsum() / income_counts[\"#customers\"].sum()\n      4 income_counts.iloc[:20].style.bar(subset=[\"%cummulative clients\"], vmin=0, vmax=1)\n\nNameError: name 'income_counts' is not defined\n\n\n\n\nWe have some overrepresented values (many possible explanations for this)\nTo clean the data, we can, for instance, keep only the revenues between [10, 200], or leave it as such\n\n\n\nCode\ndf = df[(df['Net_Annual_Income'] &gt;= 10) & (df['Net_Annual_Income'] &lt;= 200)]\n\nsns.displot(x='Net_Annual_Income', data=df, bins=15, height=4, aspect=1.5)"
  },
  {
    "objectID": "core/notebooks/notebook04_pandas_spark.html#creation-of-the-features-matrix",
    "href": "core/notebooks/notebook04_pandas_spark.html#creation-of-the-features-matrix",
    "title": "Data preprocessing and visualisation of a credit scoring dataset",
    "section": "Creation of the features matrix",
    "text": "Creation of the features matrix\n\n\nCode\ndf[cnt_featnames].head()\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[54], line 1\n----&gt; 1 df[cnt_featnames].head()\n\nNameError: name 'df' is not defined\n\n\n\n\n\nCode\nbin_features = pd.get_dummies(df[cat_featnames],\n                              prefix_sep='#', drop_first=True)\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[55], line 1\n----&gt; 1 bin_features = pd.get_dummies(df[cat_featnames],\n      2                               prefix_sep='#', drop_first=True)\n\nNameError: name 'df' is not defined\n\n\n\n\n\nCode\nbin_features.head()\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[56], line 1\n----&gt; 1 bin_features.head()\n\nNameError: name 'bin_features' is not defined\n\n\n\n\n\nCode\ncnt_features = df[cnt_featnames]\ncnt_features.head()\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[57], line 1\n----&gt; 1 cnt_features = df[cnt_featnames]\n      2 cnt_features.head()\n\nNameError: name 'df' is not defined\n\n\n\n\n\nCode\nfrom pandas import Timestamp\n\ndef age(x):\n    today = Timestamp.today()\n    return (today - x).dt.days\n\ndate_features = df[date_featnames].apply(age, axis=\"index\")\ndate_features.head()\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[58], line 7\n      4     today = Timestamp.today()\n      5     return (today - x).dt.days\n----&gt; 7 date_features = df[date_featnames].apply(age, axis=\"index\")\n      8 date_features.head()\n\nNameError: name 'df' is not defined\n\n\n\n\n\nCode\ntoday = Timestamp.today()\ntoday\n\n\nTimestamp('2025-02-03 23:20:11.628606')\n\n\n\n\nCode\ntt = (today - df[\"BirthDate\"]).loc[0]\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[60], line 1\n----&gt; 1 tt = (today - df[\"BirthDate\"]).loc[0]\n\nNameError: name 'df' is not defined\n\n\n\n\n\nCode\n(today - df[\"BirthDate\"]).dt.days\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[61], line 1\n----&gt; 1 (today - df[\"BirthDate\"]).dt.days\n\nNameError: name 'df' is not defined\n\n\n\n\n\nCode\ntt\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[62], line 1\n----&gt; 1 tt\n\nNameError: name 'tt' is not defined"
  },
  {
    "objectID": "core/notebooks/notebook04_pandas_spark.html#final-features-matrix",
    "href": "core/notebooks/notebook04_pandas_spark.html#final-features-matrix",
    "title": "Data preprocessing and visualisation of a credit scoring dataset",
    "section": "Final features matrix",
    "text": "Final features matrix\n\n\nCode\nall_features = pd.concat([bin_features, cnt_features, date_features], axis=1)\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[63], line 1\n----&gt; 1 all_features = pd.concat([bin_features, cnt_features, date_features], axis=1)\n\nNameError: name 'bin_features' is not defined\n\n\n\n\n\nCode\nall_features.columns\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[64], line 1\n----&gt; 1 all_features.columns\n\nNameError: name 'all_features' is not defined\n\n\n\n\n\nCode\nall_features.head()\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[65], line 1\n----&gt; 1 all_features.head()\n\nNameError: name 'all_features' is not defined\n\n\n\n\n\nCode\ndf_debile = pd.DataFrame({\"nom etudiant\": [\"yiyang\", \"jaouad\", \"mokhtar\", \"massil\", \"simon\"], \n              \"portable\": [True, True, None, True, False]})\n\n\n\n\nCode\ndf_debile\n\n\n\n\n\n\n\n\n\nnom etudiant\nportable\n\n\n\n\n0\nyiyang\nTrue\n\n\n1\njaouad\nTrue\n\n\n2\nmokhtar\nNone\n\n\n3\nmassil\nTrue\n\n\n4\nsimon\nFalse\n\n\n\n\n\n\n\n\n\nCode\ndf_debile.index\n\n\nRangeIndex(start=0, stop=5, step=1)\n\n\n\n\nCode\ndf_debile.dropna().index\n\n\nIndex([0, 1, 3, 4], dtype='int64')\n\n\n\n\nCode\ndf_debile.info()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 5 entries, 0 to 4\nData columns (total 2 columns):\n #   Column        Non-Null Count  Dtype \n---  ------        --------------  ----- \n 0   nom etudiant  5 non-null      object\n 1   portable      4 non-null      object\ndtypes: object(2)\nmemory usage: 212.0+ bytes\n\n\nVERY IMPORTANT: we removed lines of data that contained missing values. The index of the dataframe is therefore not contiguous anymore\n\n\nCode\nall_features.index.max()\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[71], line 1\n----&gt; 1 all_features.index.max()\n\nNameError: name 'all_features' is not defined\n\n\n\nThis could be a problem for later. So let’s reset the index to get a contiguous one\n\n\nCode\nall_features.shape\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[72], line 1\n----&gt; 1 all_features.shape\n\nNameError: name 'all_features' is not defined\n\n\n\n\n\nCode\nall_features.reset_index(inplace=True, drop=True)\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[73], line 1\n----&gt; 1 all_features.reset_index(inplace=True, drop=True)\n\nNameError: name 'all_features' is not defined\n\n\n\n\n\nCode\nall_features.head()\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[74], line 1\n----&gt; 1 all_features.head()\n\nNameError: name 'all_features' is not defined"
  },
  {
    "objectID": "core/notebooks/notebook04_pandas_spark.html#lets-save-the-data-using-pickle",
    "href": "core/notebooks/notebook04_pandas_spark.html#lets-save-the-data-using-pickle",
    "title": "Data preprocessing and visualisation of a credit scoring dataset",
    "section": "Let’s save the data using pickle",
    "text": "Let’s save the data using pickle\n\n\nCode\nimport pickle as pkl\n\nX = all_features\ny = df['Y']\n\n# Let's put eveything in a dictionary\ndf_pkl = {}\n# The features and the labels\ndf_pkl['features'] = X\ndf_pkl['labels'] = y\n# And also the list of columns we built above\ndf_pkl['cnt_featnames'] = cnt_featnames\ndf_pkl['cat_featnames'] = cat_featnames\ndf_pkl['date_featnames'] = date_featnames\n\nwith open(\"gro_training.pkl\", 'wb') as f:\n    pkl.dump(df_pkl, f)\n\n\n\n\nCode\nls -al gro*\n\n\n-rw-rw-r-- 1 boucheron boucheron 9115 janv. 14 22:37 gro.csv.gz\n\n\nThe preprocessed data is saved in a pickle file called gro_training.pkdfl.\nDatabricks blog about Koalas, SPIP, Zen\n\npandas users will be able scale their workloads with one simple line change in the upcoming Spark 3.2 release:\n\n&lt;s&gt;from pandas import read_csv&lt;/s&gt;\nfrom pyspark.pandas import read_csv\npdf = read_csv(\"data.csv\")"
  },
  {
    "objectID": "core/notebooks/notebook02_numpy.html",
    "href": "core/notebooks/notebook02_numpy.html",
    "title": "Introduction to numpy",
    "section": "",
    "text": "NumPy is the fundamental package for scientific computing with Python. It contains among other things:\nBesides its obvious scientific uses, NumPy can also be used as an efficient multi-dimensional container for general data. Arbitrary data-types can be defined. This allows NumPy to seamlessly and speedily integrate with a wide variety of databases.\nLibrary documentation: http://numpy.org/"
  },
  {
    "objectID": "core/notebooks/notebook02_numpy.html#the-base-numpy.array-object",
    "href": "core/notebooks/notebook02_numpy.html#the-base-numpy.array-object",
    "title": "Introduction to numpy",
    "section": "The base numpy.array object",
    "text": "The base numpy.array object\n\n\nCode\nimport numpy as np\n\n# declare a vector using a list as the argument\nv = np.array([1, 2.0, 3, 4])\nv\n\n\narray([1., 2., 3., 4.])\n\n\n\n\nCode\nlist([1, 2.0, 3, 4])\n\n\n[1, 2.0, 3, 4]\n\n\n\n\nCode\ntype(v)\n\n\nnumpy.ndarray\n\n\n\n\nCode\nv.shape\n\n\n(4,)\n\n\n\n\nCode\nv.ndim\n\n\n1\n\n\n\n\nCode\nv.dtype is float\n\n\nFalse\n\n\n\n\nCode\nv.dtype \n\n\ndtype('float64')\n\n\n\n\nCode\nnp.uint8 is int\n\n\nFalse\n\n\n\n\n\n\n\n\nTip\n\n\n\n\n\nUse copilot explain to understand the chunks:\n\nThe np.uint8 is a data type in NumPy, representing an unsigned 8-bit integer, which can store values from 0 to 255. The int type is the built-in integer type in Python, which can represent any integer value without a fixed size limit.\n\n\n\n\n\n\nCode\nnp.array([2**120, 2**40], dtype=np.int64)\n\n\n\n---------------------------------------------------------------------------\nOverflowError                             Traceback (most recent call last)\nCell In[9], line 1\n----&gt; 1 np.array([2**120, 2**40], dtype=np.int64)\n\nOverflowError: Python int too large to convert to C long\n\n\n\n\n\nCode\nnp.uint16 is int \n\n\nFalse\n\n\n\n\nCode\nnp.uint32  is int\n\n\nFalse\n\n\n\n\nCode\nw = np.array([1.3, 2, 3, 4], dtype=np.int64)\nw\n\n\narray([1, 2, 3, 4])\n\n\n\n\nCode\nw.dtype\n\n\ndtype('int64')\n\n\n\n\nCode\na = np.arange(100)\n\n\n\n\nCode\ntype(a)\n\n\nnumpy.ndarray\n\n\n\n\nCode\nnp.array(range(100))\n\n\narray([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n       34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50,\n       51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67,\n       68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84,\n       85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99])\n\n\n\n\nCode\na\n\n\narray([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n       34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50,\n       51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67,\n       68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84,\n       85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99])\n\n\n\n\nCode\na.dtype\n\n\ndtype('int64')\n\n\n\n\nCode\n-3 * a ** 2\n\n\narray([     0,     -3,    -12,    -27,    -48,    -75,   -108,   -147,\n         -192,   -243,   -300,   -363,   -432,   -507,   -588,   -675,\n         -768,   -867,   -972,  -1083,  -1200,  -1323,  -1452,  -1587,\n        -1728,  -1875,  -2028,  -2187,  -2352,  -2523,  -2700,  -2883,\n        -3072,  -3267,  -3468,  -3675,  -3888,  -4107,  -4332,  -4563,\n        -4800,  -5043,  -5292,  -5547,  -5808,  -6075,  -6348,  -6627,\n        -6912,  -7203,  -7500,  -7803,  -8112,  -8427,  -8748,  -9075,\n        -9408,  -9747, -10092, -10443, -10800, -11163, -11532, -11907,\n       -12288, -12675, -13068, -13467, -13872, -14283, -14700, -15123,\n       -15552, -15987, -16428, -16875, -17328, -17787, -18252, -18723,\n       -19200, -19683, -20172, -20667, -21168, -21675, -22188, -22707,\n       -23232, -23763, -24300, -24843, -25392, -25947, -26508, -27075,\n       -27648, -28227, -28812, -29403])\n\n\n\n\nCode\na[42] = 13\n\n\n\n\nCode\na[42] = 1025\n\n\n\n\nCode\nnp.info(np.int16)\n\n\n int16()\n\nSigned integer type, compatible with C ``short``.\n\n:Character code: ``'h'``\n:Canonical name: `numpy.short`\n:Alias on this platform (Linux x86_64): `numpy.int16`: 16-bit signed integer (``-32_768`` to ``32_767``).\n\n\nMethods:\n\n  all  --  Scalar method identical to the corresponding array attribute.\n  any  --  Scalar method identical to the corresponding array attribute.\n  argmax  --  Scalar method identical to the corresponding array attribute.\n  argmin  --  Scalar method identical to the corresponding array attribute.\n  argsort  --  Scalar method identical to the corresponding array attribute.\n  astype  --  Scalar method identical to the corresponding array attribute.\n  bit_count  --  int16.bit_count() -&gt; int\n  byteswap  --  Scalar method identical to the corresponding array attribute.\n  choose  --  Scalar method identical to the corresponding array attribute.\n  clip  --  Scalar method identical to the corresponding array attribute.\n  compress  --  Scalar method identical to the corresponding array attribute.\n  conj  --  None\n  conjugate  --  Scalar method identical to the corresponding array attribute.\n  copy  --  Scalar method identical to the corresponding array attribute.\n  cumprod  --  Scalar method identical to the corresponding array attribute.\n  cumsum  --  Scalar method identical to the corresponding array attribute.\n  diagonal  --  Scalar method identical to the corresponding array attribute.\n  dump  --  Scalar method identical to the corresponding array attribute.\n  dumps  --  Scalar method identical to the corresponding array attribute.\n  fill  --  Scalar method identical to the corresponding array attribute.\n  flatten  --  Scalar method identical to the corresponding array attribute.\n  getfield  --  Scalar method identical to the corresponding array attribute.\n  is_integer  --  integer.is_integer() -&gt; bool\n  item  --  Scalar method identical to the corresponding array attribute.\n  max  --  Scalar method identical to the corresponding array attribute.\n  mean  --  Scalar method identical to the corresponding array attribute.\n  min  --  Scalar method identical to the corresponding array attribute.\n  nonzero  --  Scalar method identical to the corresponding array attribute.\n  prod  --  Scalar method identical to the corresponding array attribute.\n  put  --  Scalar method identical to the corresponding array attribute.\n  ravel  --  Scalar method identical to the corresponding array attribute.\n  repeat  --  Scalar method identical to the corresponding array attribute.\n  reshape  --  Scalar method identical to the corresponding array attribute.\n  resize  --  Scalar method identical to the corresponding array attribute.\n  round  --  Scalar method identical to the corresponding array attribute.\n  searchsorted  --  Scalar method identical to the corresponding array attribute.\n  setfield  --  Scalar method identical to the corresponding array attribute.\n  setflags  --  Scalar method identical to the corresponding array attribute.\n  sort  --  Scalar method identical to the corresponding array attribute.\n  squeeze  --  Scalar method identical to the corresponding array attribute.\n  std  --  Scalar method identical to the corresponding array attribute.\n  sum  --  Scalar method identical to the corresponding array attribute.\n  swapaxes  --  Scalar method identical to the corresponding array attribute.\n  take  --  Scalar method identical to the corresponding array attribute.\n  to_device  --  None\n  tobytes  --  None\n  tofile  --  Scalar method identical to the corresponding array attribute.\n  tolist  --  Scalar method identical to the corresponding array attribute.\n  tostring  --  Scalar method identical to the corresponding array attribute.\n  trace  --  Scalar method identical to the corresponding array attribute.\n  transpose  --  Scalar method identical to the corresponding array attribute.\n  var  --  Scalar method identical to the corresponding array attribute.\n  view  --  Scalar method identical to the corresponding array attribute.\n\n\n\n\nCode\nnp.int16\n\n\nnumpy.int16\n\n\n\n\nCode\ndict(enumerate(a))\n\n\n{0: np.int64(0),\n 1: np.int64(1),\n 2: np.int64(2),\n 3: np.int64(3),\n 4: np.int64(4),\n 5: np.int64(5),\n 6: np.int64(6),\n 7: np.int64(7),\n 8: np.int64(8),\n 9: np.int64(9),\n 10: np.int64(10),\n 11: np.int64(11),\n 12: np.int64(12),\n 13: np.int64(13),\n 14: np.int64(14),\n 15: np.int64(15),\n 16: np.int64(16),\n 17: np.int64(17),\n 18: np.int64(18),\n 19: np.int64(19),\n 20: np.int64(20),\n 21: np.int64(21),\n 22: np.int64(22),\n 23: np.int64(23),\n 24: np.int64(24),\n 25: np.int64(25),\n 26: np.int64(26),\n 27: np.int64(27),\n 28: np.int64(28),\n 29: np.int64(29),\n 30: np.int64(30),\n 31: np.int64(31),\n 32: np.int64(32),\n 33: np.int64(33),\n 34: np.int64(34),\n 35: np.int64(35),\n 36: np.int64(36),\n 37: np.int64(37),\n 38: np.int64(38),\n 39: np.int64(39),\n 40: np.int64(40),\n 41: np.int64(41),\n 42: np.int64(1025),\n 43: np.int64(43),\n 44: np.int64(44),\n 45: np.int64(45),\n 46: np.int64(46),\n 47: np.int64(47),\n 48: np.int64(48),\n 49: np.int64(49),\n 50: np.int64(50),\n 51: np.int64(51),\n 52: np.int64(52),\n 53: np.int64(53),\n 54: np.int64(54),\n 55: np.int64(55),\n 56: np.int64(56),\n 57: np.int64(57),\n 58: np.int64(58),\n 59: np.int64(59),\n 60: np.int64(60),\n 61: np.int64(61),\n 62: np.int64(62),\n 63: np.int64(63),\n 64: np.int64(64),\n 65: np.int64(65),\n 66: np.int64(66),\n 67: np.int64(67),\n 68: np.int64(68),\n 69: np.int64(69),\n 70: np.int64(70),\n 71: np.int64(71),\n 72: np.int64(72),\n 73: np.int64(73),\n 74: np.int64(74),\n 75: np.int64(75),\n 76: np.int64(76),\n 77: np.int64(77),\n 78: np.int64(78),\n 79: np.int64(79),\n 80: np.int64(80),\n 81: np.int64(81),\n 82: np.int64(82),\n 83: np.int64(83),\n 84: np.int64(84),\n 85: np.int64(85),\n 86: np.int64(86),\n 87: np.int64(87),\n 88: np.int64(88),\n 89: np.int64(89),\n 90: np.int64(90),\n 91: np.int64(91),\n 92: np.int64(92),\n 93: np.int64(93),\n 94: np.int64(94),\n 95: np.int64(95),\n 96: np.int64(96),\n 97: np.int64(97),\n 98: np.int64(98),\n 99: np.int64(99)}\n\n\n\n\nCode\na + 1\n\n\narray([   1,    2,    3,    4,    5,    6,    7,    8,    9,   10,   11,\n         12,   13,   14,   15,   16,   17,   18,   19,   20,   21,   22,\n         23,   24,   25,   26,   27,   28,   29,   30,   31,   32,   33,\n         34,   35,   36,   37,   38,   39,   40,   41,   42, 1026,   44,\n         45,   46,   47,   48,   49,   50,   51,   52,   53,   54,   55,\n         56,   57,   58,   59,   60,   61,   62,   63,   64,   65,   66,\n         67,   68,   69,   70,   71,   72,   73,   74,   75,   76,   77,\n         78,   79,   80,   81,   82,   83,   84,   85,   86,   87,   88,\n         89,   90,   91,   92,   93,   94,   95,   96,   97,   98,   99,\n        100])\n\n\n\n\nCode\nb = a + 1\nb\n\n\narray([   1,    2,    3,    4,    5,    6,    7,    8,    9,   10,   11,\n         12,   13,   14,   15,   16,   17,   18,   19,   20,   21,   22,\n         23,   24,   25,   26,   27,   28,   29,   30,   31,   32,   33,\n         34,   35,   36,   37,   38,   39,   40,   41,   42, 1026,   44,\n         45,   46,   47,   48,   49,   50,   51,   52,   53,   54,   55,\n         56,   57,   58,   59,   60,   61,   62,   63,   64,   65,   66,\n         67,   68,   69,   70,   71,   72,   73,   74,   75,   76,   77,\n         78,   79,   80,   81,   82,   83,   84,   85,   86,   87,   88,\n         89,   90,   91,   92,   93,   94,   95,   96,   97,   98,   99,\n        100])\n\n\n\n\nCode\na is b\n\n\nFalse\n\n\n\n\nCode\nf = id(a)\na += 1\nf, id(a)\n\n\n(136305970830608, 136305970830608)\n\n\n\n\nCode\na\n\n\narray([   1,    2,    3,    4,    5,    6,    7,    8,    9,   10,   11,\n         12,   13,   14,   15,   16,   17,   18,   19,   20,   21,   22,\n         23,   24,   25,   26,   27,   28,   29,   30,   31,   32,   33,\n         34,   35,   36,   37,   38,   39,   40,   41,   42, 1026,   44,\n         45,   46,   47,   48,   49,   50,   51,   52,   53,   54,   55,\n         56,   57,   58,   59,   60,   61,   62,   63,   64,   65,   66,\n         67,   68,   69,   70,   71,   72,   73,   74,   75,   76,   77,\n         78,   79,   80,   81,   82,   83,   84,   85,   86,   87,   88,\n         89,   90,   91,   92,   93,   94,   95,   96,   97,   98,   99,\n        100])\n\n\n\n\nCode\nb\n\n\narray([   1,    2,    3,    4,    5,    6,    7,    8,    9,   10,   11,\n         12,   13,   14,   15,   16,   17,   18,   19,   20,   21,   22,\n         23,   24,   25,   26,   27,   28,   29,   30,   31,   32,   33,\n         34,   35,   36,   37,   38,   39,   40,   41,   42, 1026,   44,\n         45,   46,   47,   48,   49,   50,   51,   52,   53,   54,   55,\n         56,   57,   58,   59,   60,   61,   62,   63,   64,   65,   66,\n         67,   68,   69,   70,   71,   72,   73,   74,   75,   76,   77,\n         78,   79,   80,   81,   82,   83,   84,   85,   86,   87,   88,\n         89,   90,   91,   92,   93,   94,   95,   96,   97,   98,   99,\n        100])\n\n\n\n\n\n\n\n\nWarning\n\n\n\nBeware of the dimensions: a 1D array is not the same as a 2D array with 1 column\n\n\n\n\nCode\na1 = np.array([1, 2, 3])\nprint(a1, a1.shape, a1.ndim)\n\n\n[1 2 3] (3,) 1\n\n\n\n\nCode\na2 = np.array([1, 2, 3])\nprint(a2, a2.shape, a2.ndim)\n\n\n[1 2 3] (3,) 1\n\n\nMore on NumPy quickstart\n\n\n\n\n\n\nNote\n\n\n\nList the attributes and methods of class numpy.ndarray. You may use function dir() and filter the result using methods for objects of class string."
  },
  {
    "objectID": "core/notebooks/notebook02_numpy.html#matrix-multiplication",
    "href": "core/notebooks/notebook02_numpy.html#matrix-multiplication",
    "title": "Introduction to numpy",
    "section": "Matrix multiplication",
    "text": "Matrix multiplication\n\n\nCode\na2.dot(a1) # inner product \n\n\nnp.int64(14)\n\n\n\n\nCode\n( \n    np.array([a2])\n        .transpose() # column vector\n        .dot(np.array([a1]))\n) # column vector multiplied by row vector\n\n\narray([[1, 2, 3],\n       [2, 4, 6],\n       [3, 6, 9]])\n\n\n\n\nCode\n(\n    np.array([a2])\n    .transpose()#.shape\n)\n\n\narray([[1],\n       [2],\n       [3]])\n\n\n\n\nCode\n(\n    a2.reshape(3,1)  # all explicit\n      .dot(a1.reshape(1, 3))\n)\n\n\narray([[1, 2, 3],\n       [2, 4, 6],\n       [3, 6, 9]])\n\n\n\n\nCode\n# Declare a 2D array using a nested list as the constructor argument\nM = np.array([[1,2], \n              [3,4], \n              [3.14, -9.17]])\nM\n\n\narray([[ 1.  ,  2.  ],\n       [ 3.  ,  4.  ],\n       [ 3.14, -9.17]])\n\n\n\n\nCode\nM.shape, M.size\n\n\n((3, 2), 6)\n\n\n\n\nCode\nM.ravel(), M.ndim, M.ravel().shape\n\n\n(array([ 1.  ,  2.  ,  3.  ,  4.  ,  3.14, -9.17]), 2, (6,))\n\n\n\n\nCode\n# arguments: start, stop, step\nx = (\n     np.arange(12)\n       .reshape(4, 3)\n)\nx\n\n\narray([[ 0,  1,  2],\n       [ 3,  4,  5],\n       [ 6,  7,  8],\n       [ 9, 10, 11]])\n\n\n\n\nCode\ny = np.arange(3).reshape(3,1)\n\ny\n\n\narray([[0],\n       [1],\n       [2]])\n\n\n\n\nCode\nx @ y, x.dot(y)\n\n\n(array([[ 5],\n        [14],\n        [23],\n        [32]]),\n array([[ 5],\n        [14],\n        [23],\n        [32]]))\n\n\n\n\n\n\n\n\nNote"
  },
  {
    "objectID": "core/notebooks/notebook02_numpy.html#generating-arrays",
    "href": "core/notebooks/notebook02_numpy.html#generating-arrays",
    "title": "Introduction to numpy",
    "section": "Generating arrays",
    "text": "Generating arrays\n\n\nCode\nnp.linspace(0, 10, 51)  # meaning of the 3 positional parameters ? \n\n\narray([ 0. ,  0.2,  0.4,  0.6,  0.8,  1. ,  1.2,  1.4,  1.6,  1.8,  2. ,\n        2.2,  2.4,  2.6,  2.8,  3. ,  3.2,  3.4,  3.6,  3.8,  4. ,  4.2,\n        4.4,  4.6,  4.8,  5. ,  5.2,  5.4,  5.6,  5.8,  6. ,  6.2,  6.4,\n        6.6,  6.8,  7. ,  7.2,  7.4,  7.6,  7.8,  8. ,  8.2,  8.4,  8.6,\n        8.8,  9. ,  9.2,  9.4,  9.6,  9.8, 10. ])\n\n\n\n\nCode\nnp.logspace(0, 10, 11, base=np.e), np.e**(np.arange(11))\n\n\n(array([1.00000000e+00, 2.71828183e+00, 7.38905610e+00, 2.00855369e+01,\n        5.45981500e+01, 1.48413159e+02, 4.03428793e+02, 1.09663316e+03,\n        2.98095799e+03, 8.10308393e+03, 2.20264658e+04]),\n array([1.00000000e+00, 2.71828183e+00, 7.38905610e+00, 2.00855369e+01,\n        5.45981500e+01, 1.48413159e+02, 4.03428793e+02, 1.09663316e+03,\n        2.98095799e+03, 8.10308393e+03, 2.20264658e+04]))\n\n\n\n\nCode\nimport matplotlib.pyplot as plt\n\n# Random standard Gaussian numbers\nfig = plt.figure(figsize=(8, 4))\nwn = np.random.randn(1000)\nbm = wn.cumsum()\n\nplt.plot(bm, lw=3)\n\n\n\n\n\n\n\n\n\n\n\nCode\nnp.diag(np.arange(10))\n\n\narray([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 2, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 3, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 4, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 5, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 6, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 7, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 8, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 0, 9]])\n\n\n\n\nCode\nzozo = np.zeros((10, 10), dtype=np.float32)\nzozo\n\n\narray([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]], dtype=float32)\n\n\n\n\nCode\nzozo.shape\n\n\n(10, 10)\n\n\n\n\nCode\nprint(M)\n\n\n[[ 1.    2.  ]\n [ 3.    4.  ]\n [ 3.14 -9.17]]\n\n\n\n\nCode\nM[1, 1]\n\n\nnp.float64(4.0)\n\n\n\n\nCode\n# assign new value\nM[0, 0] = 7\nM[:, 0] = 42\nM\n\n\narray([[42.  ,  2.  ],\n       [42.  ,  4.  ],\n       [42.  , -9.17]])\n\n\n\n\nCode\nM\n\n\narray([[42.  ,  2.  ],\n       [42.  ,  4.  ],\n       [42.  , -9.17]])\n\n\n\n\nCode\n# Warning: the next m is a **view** on M. \n# One again, no copies unless you ask for one!\nm = M[0, :]\nm\n\n\narray([42.,  2.])\n\n\n\n\nCode\nm[:] = 3.14\nM\n\n\narray([[ 3.14,  3.14],\n       [42.  ,  4.  ],\n       [42.  , -9.17]])\n\n\n\n\nCode\nm[:] = 7\nM\n\n\narray([[ 7.  ,  7.  ],\n       [42.  ,  4.  ],\n       [42.  , -9.17]])"
  },
  {
    "objectID": "core/notebooks/notebook02_numpy.html#copies",
    "href": "core/notebooks/notebook02_numpy.html#copies",
    "title": "Introduction to numpy",
    "section": "Copies",
    "text": "Copies\nDon’t forget that python does not make copies unless told to do so (same as with any mutable type)\nIf you are not careful enough, this typically leads to a lot of errors and to being fired !!\n\n\nCode\ny = x = np.arange(6)\nx[2] = 123\ny\n\n\narray([  0,   1, 123,   3,   4,   5])\n\n\n\n\nCode\nx is y\n\n\nTrue\n\n\n\n\nCode\n# A real copy\ny = x.copy()\nx is y \n\n\nFalse\n\n\n\n\nCode\n# Or equivalently (but the one above is better...)\ny = np.copy(x)\n\n\n\n\nCode\nx[0] = -12\nprint(x, y, x is y)\n\n\n[-12   1 123   3   4   5] [  0   1 123   3   4   5] False\n\n\nTo put values of x in y (copy values into an existing array) use\n\n\nCode\nx = np.random.randn(10)\nx, id(x)\n\n\n(array([ 0.34002269,  1.01538901, -0.1228508 , -0.56881556,  0.16581012,\n        -1.3008782 , -0.11687439,  1.60499027, -1.15791809,  0.37011871]),\n 136305888671184)\n\n\n\n\nCode\nx.fill(2.78)   # in place. \nx, id(x)\n\n\n(array([2.78, 2.78, 2.78, 2.78, 2.78, 2.78, 2.78, 2.78, 2.78, 2.78]),\n 136305888671184)\n\n\n\n\nCode\nx[:] = 3.14  # x.fill(3.14)  can. be chained ...\nx, id(x)\n\n\n(array([3.14, 3.14, 3.14, 3.14, 3.14, 3.14, 3.14, 3.14, 3.14, 3.14]),\n 136305888671184)\n\n\n\n\nCode\nx[:] = np.random.randn(x.shape[0])\nx, id(x)\n\n\n(array([-0.6169953 , -1.91385024,  0.85656348,  0.39530284, -0.48132056,\n        -0.66261283, -1.4573383 , -0.76337779, -0.79105644, -2.14953431]),\n 136305888671184)\n\n\n\n\nCode\ny = np.empty(x.shape)  # how does empty() work ?\ny, id(y)\n\n\n(array([0.6169953 , 1.91385024, 0.85656348, 0.39530284, 0.48132056,\n        0.66261283, 1.4573383 , 0.76337779, 0.79105644, 2.14953431]),\n 136305888671376)\n\n\n\n\nCode\ny = x\ny, id(y), id(x), y is x\n\n\n(array([-0.6169953 , -1.91385024,  0.85656348,  0.39530284, -0.48132056,\n        -0.66261283, -1.4573383 , -0.76337779, -0.79105644, -2.14953431]),\n 136305888671184,\n 136305888671184,\n True)\n\n\n\n\n\n\n\n\nFinal warning\n\n\n\n\n\n\nIn the next line you copy the values of x into an existing array y (of same size…)\n\n\nCode\ny = np.zeros(x.shape)\ny[:] = x\ny, y is x, np.all(y==x)\n\n\n(array([-0.6169953 , -1.91385024,  0.85656348,  0.39530284, -0.48132056,\n        -0.66261283, -1.4573383 , -0.76337779, -0.79105644, -2.14953431]),\n False,\n np.True_)\n\n\nWhile in the next line, you are aliasing, you are giving a new name y to the object named x (you should never, ever write something like this)\n\n\nCode\ny = x\ny is x\n\n\nTrue"
  },
  {
    "objectID": "core/notebooks/notebook02_numpy.html#miscellanea",
    "href": "core/notebooks/notebook02_numpy.html#miscellanea",
    "title": "Introduction to numpy",
    "section": "Miscellanea",
    "text": "Miscellanea\n\nNon-numerical values\nA numpy array can contain other things than numeric types\n\n\nCode\narr = np.array(['Labore', 'neque', 'ipsum', 'ut', 'non', 'quiquia', 'dolore.'])\narr, arr.shape, arr.dtype\n\n\n(array(['Labore', 'neque', 'ipsum', 'ut', 'non', 'quiquia', 'dolore.'],\n       dtype='&lt;U7'),\n (7,),\n dtype('&lt;U7'))\n\n\n\n\nCode\n# arr.sum()\n\n\n\n\nCode\n\"_\".join(arr)\n\n\n'Labore_neque_ipsum_ut_non_quiquia_dolore.'\n\n\n\n\nCode\narr.dtype\n\n\ndtype('&lt;U7')"
  },
  {
    "objectID": "core/notebooks/notebook02_numpy.html#a-matrix-is-no-2d-array-in-numpy",
    "href": "core/notebooks/notebook02_numpy.html#a-matrix-is-no-2d-array-in-numpy",
    "title": "Introduction to numpy",
    "section": "A matrix is no 2D array in numpy",
    "text": "A matrix is no 2D array in numpy\nSo far, we have only used array or ndarray objects\nThe is another type: the matrix type\nIn words: don’t use it (IMhO) and stick with arrays\n\n\nCode\n# Matrix VS array objects in numpy\nm1 = np.matrix(np.arange(3))\nm2 = np.matrix(np.arange(3))\nm1, m2\n\n\n(matrix([[0, 1, 2]]), matrix([[0, 1, 2]]))\n\n\n\n\nCode\nm1.transpose() @ m2, m1.shape, m1.transpose() * m2\n\n\n(matrix([[0, 0, 0],\n         [0, 1, 2],\n         [0, 2, 4]]),\n (1, 3),\n matrix([[0, 0, 0],\n         [0, 1, 2],\n         [0, 2, 4]]))\n\n\n\n\nCode\na1 = np.arange(3)\na2 = np.arange(3)\na1, a2\n\n\n(array([0, 1, 2]), array([0, 1, 2]))\n\n\n\n\nCode\nm1 * m2.T, m1.dot(m2.T)\n\n\n(matrix([[5]]), matrix([[5]]))\n\n\n\n\nCode\na1 * a2\n\n\narray([0, 1, 4])\n\n\n\n\nCode\na1.dot(a2)\n\n\nnp.int64(5)\n\n\n\n\nCode\nnp.outer(a1, a2)\n\n\narray([[0, 0, 0],\n       [0, 1, 2],\n       [0, 2, 4]])\n\n\n\n\n\n\n\n\nNote\n\n\n\nVisit https://numpy.org/doc/stable/reference/arrays.ndarray.html#arrays-ndarray"
  },
  {
    "objectID": "core/notebooks/notebook02_numpy.html#sparse-matrices",
    "href": "core/notebooks/notebook02_numpy.html#sparse-matrices",
    "title": "Introduction to numpy",
    "section": "Sparse matrices",
    "text": "Sparse matrices\n\n\nCode\nfrom scipy.sparse import csc_matrix, csr_matrix, coo_matrix\n\n\n\n\nCode\nprobs = np.full(fill_value=1/4, shape=(4,))\nprobs\n\n\narray([0.25, 0.25, 0.25, 0.25])\n\n\n\n\nCode\nX = np.random.multinomial(n=2, pvals=probs, size=4)   # check you understand what is going on \nX\n\n\narray([[1, 0, 0, 1],\n       [0, 0, 1, 1],\n       [1, 0, 0, 1],\n       [1, 1, 0, 0]])\n\n\n\n\nCode\nprobs\n\n\narray([0.25, 0.25, 0.25, 0.25])\n\n\n\n\nCode\nX_coo = coo_matrix(X)  ## coordinate format\n\n\n\n\nCode\nprint(X_coo)\nX_coo\n\n\n&lt;COOrdinate sparse matrix of dtype 'int64'\n    with 8 stored elements and shape (4, 4)&gt;\n  Coords    Values\n  (0, 0)    1\n  (0, 3)    1\n  (1, 2)    1\n  (1, 3)    1\n  (2, 0)    1\n  (2, 3)    1\n  (3, 0)    1\n  (3, 1)    1\n\n\n&lt;COOrdinate sparse matrix of dtype 'int64'\n    with 8 stored elements and shape (4, 4)&gt;\n\n\n\n\nCode\nX_coo.nnz    # number pf non-zero coordinates \n\n\n8\n\n\n\n\nCode\nprint(X, end='\\n----\\n')\nprint(X_coo.data, end='\\n----\\n')\nprint(X_coo.row, end='\\n----\\n')\nprint(X_coo.col, end='\\n----\\n')\n\n\n[[1 0 0 1]\n [0 0 1 1]\n [1 0 0 1]\n [1 1 0 0]]\n----\n[1 1 1 1 1 1 1 1]\n----\n[0 0 1 1 2 2 3 3]\n----\n[0 3 2 3 0 3 0 1]\n----\n\n\nThere is also\n\ncsr_matrix: sparse rows format\ncsc_matrix: sparse columns format\n\nSparse rows is often used for machine learning: sparse features vectors\nBut sparse column format useful as well (e.g. coordinate gradient descent)"
  },
  {
    "objectID": "core/notebooks/notebook02_numpy.html#bored-with-decimals",
    "href": "core/notebooks/notebook02_numpy.html#bored-with-decimals",
    "title": "Introduction to numpy",
    "section": "Bored with decimals?",
    "text": "Bored with decimals?\n\n\nCode\nX = np.random.randn(5, 5)\nX\n\n\narray([[-0.99797502, -0.94290771,  2.19768283,  0.71887881,  0.39349881],\n       [-1.0226244 , -1.1181209 ,  0.78497548,  0.91808323,  0.96270816],\n       [ 0.83876462,  0.22891149, -0.45344217,  1.05224938,  2.19486222],\n       [ 0.71779239,  0.44567938,  0.3233876 ,  0.78038022,  0.55369577],\n       [ 0.0356274 , -0.99344526,  1.65423239, -1.4034462 ,  1.30612322]])\n\n\n\n\nCode\n# All number displayed by numpy (in the current kernel) are with 3 decimals max\nnp.set_printoptions(precision=3)\nprint(X)\nnp.set_printoptions(precision=8)\n\n\n[[-0.998 -0.943  2.198  0.719  0.393]\n [-1.023 -1.118  0.785  0.918  0.963]\n [ 0.839  0.229 -0.453  1.052  2.195]\n [ 0.718  0.446  0.323  0.78   0.554]\n [ 0.036 -0.993  1.654 -1.403  1.306]]"
  },
  {
    "objectID": "core/notebooks/notebook02_numpy.html#not-limited-to-2d",
    "href": "core/notebooks/notebook02_numpy.html#not-limited-to-2d",
    "title": "Introduction to numpy",
    "section": "Not limited to 2D!",
    "text": "Not limited to 2D!\nnumpy arrays can have any number of dimension (hence the name ndarray)\n\n\nCode\nX = np.arange(18).reshape(3, 2, 3)\nX\n\n\narray([[[ 0,  1,  2],\n        [ 3,  4,  5]],\n\n       [[ 6,  7,  8],\n        [ 9, 10, 11]],\n\n       [[12, 13, 14],\n        [15, 16, 17]]])\n\n\n\n\nCode\nX.shape\n\n\n(3, 2, 3)\n\n\n\n\nCode\nX.ndim\n\n\n3\n\n\nVisit https://numpy.org/doc/stable/reference/arrays.ndarray.html#arrays-ndarray"
  },
  {
    "objectID": "core/notebooks/notebook02_numpy.html#inner-products",
    "href": "core/notebooks/notebook02_numpy.html#inner-products",
    "title": "Introduction to numpy",
    "section": "Inner products",
    "text": "Inner products\n\n\nCode\n# Inner product between vectors\nprint(v1.dot(v2))\n\n# You can use also (but first solution is better)\nprint(np.dot(v1, v2))\n\n\n80\n80\n\n\n\n\nCode\nA, v1\n\n\n(array([[ 0,  1,  2,  3,  4],\n        [ 5,  6,  7,  8,  9],\n        [10, 11, 12, 13, 14],\n        [15, 16, 17, 18, 19],\n        [20, 21, 22, 23, 24],\n        [25, 26, 27, 28, 29]]),\n array([0, 1, 2, 3, 4]))\n\n\n\n\nCode\nA.shape, v1.shape\n\n\n((6, 5), (5,))\n\n\n\n\nCode\n# Matrix-vector inner product\nA.dot(v1)\n\n\narray([ 30,  80, 130, 180, 230, 280])\n\n\n\n\nCode\n# Transpose\nA.T\n\n\narray([[ 0,  5, 10, 15, 20, 25],\n       [ 1,  6, 11, 16, 21, 26],\n       [ 2,  7, 12, 17, 22, 27],\n       [ 3,  8, 13, 18, 23, 28],\n       [ 4,  9, 14, 19, 24, 29]])\n\n\n\n\nCode\nprint(v1)\n# Inline operations (same for *=, /=, -=)\nv1 += 2\n\n\n[0 1 2 3 4]"
  },
  {
    "objectID": "core/notebooks/notebook02_numpy.html#linear-systems",
    "href": "core/notebooks/notebook02_numpy.html#linear-systems",
    "title": "Introduction to numpy",
    "section": "Linear systems",
    "text": "Linear systems\n\n\nCode\nA = np.array([[42,2,3], [4,5,6], [7,8,9]])\nb = np.array([1,2,3])\nprint(A, b, sep=2 * '\\n')\n\n\n[[42  2  3]\n [ 4  5  6]\n [ 7  8  9]]\n\n[1 2 3]\n\n\n\n\nCode\n# solve a system of linear equations\nx = np.linalg.solve(A, b)\nx\n\n\narray([2.18366847e-18, 2.31698718e-16, 3.33333333e-01])\n\n\n\n\nCode\nA.dot(x)\n\n\narray([1., 2., 3.])"
  },
  {
    "objectID": "core/notebooks/notebook02_numpy.html#eigenvalues-and-eigenvectors",
    "href": "core/notebooks/notebook02_numpy.html#eigenvalues-and-eigenvectors",
    "title": "Introduction to numpy",
    "section": "Eigenvalues and eigenvectors",
    "text": "Eigenvalues and eigenvectors\n\n\nCode\nA = np.random.rand(3,3)\nB = np.random.rand(3,3)\n\nevals, evecs = np.linalg.eig(A)\nevals\n\n\narray([ 1.32414267,  0.27580133, -0.33560889])\n\n\n\n\nCode\nevecs\n\n\narray([[-0.52723476, -0.73385244, -0.22493719],\n       [-0.63706383,  0.0548087 , -0.70095111],\n       [-0.56229279,  0.67709423,  0.67680928]])"
  },
  {
    "objectID": "core/notebooks/notebook02_numpy.html#singular-value-decomposition-svd",
    "href": "core/notebooks/notebook02_numpy.html#singular-value-decomposition-svd",
    "title": "Introduction to numpy",
    "section": "Singular value decomposition (SVD)",
    "text": "Singular value decomposition (SVD)\nDecomposes any matrix \\(A \\in \\mathbb R^{m \\times n}\\) as follows: \\[\nA = U \\times S \\times V^\\top\n\\] where - \\(U\\) and \\(V\\) are orthonormal matrices (meaning that \\(U^\\top \\times U = I\\) and \\(V^\\top \\times V = I\\)) - \\(S\\) is a diagonal matrix that contains the singular values in non-increasing order\n\n\nCode\nprint(A)\nU, S, V = np.linalg.svd(A)\n\n\n[[0.70607527 0.10759194 0.45763291]\n [0.65372608 0.14866791 0.7188165 ]\n [0.17303511 0.6640075  0.40959193]]\n\n\n\n\nCode\nU.dot(np.diag(S)).dot(V)\n\n\narray([[0.70607527, 0.10759194, 0.45763291],\n       [0.65372608, 0.14866791, 0.7188165 ],\n       [0.17303511, 0.6640075 , 0.40959193]])\n\n\n\n\nCode\nA - U @ np.diag(S) @ V\n\n\narray([[-2.22044605e-16,  1.80411242e-16, -3.33066907e-16],\n       [ 0.00000000e+00,  5.55111512e-17, -1.11022302e-16],\n       [ 0.00000000e+00,  3.33066907e-16, -5.55111512e-17]])\n\n\n\n\nCode\n# U and V are indeed orthonormal\nnp.set_printoptions(precision=2)\nprint(U.T.dot(U), V.T.dot(V), sep=2 * '\\n')\nnp.set_printoptions(precision=8)\n\n\n[[ 1.00e+00 -1.56e-16 -1.21e-16]\n [-1.56e-16  1.00e+00  4.99e-16]\n [-1.21e-16  4.99e-16  1.00e+00]]\n\n[[ 1.00e+00  1.15e-17 -2.64e-16]\n [ 1.15e-17  1.00e+00  9.52e-18]\n [-2.64e-16  9.52e-18  1.00e+00]]"
  },
  {
    "objectID": "core/notebooks/notebook02_numpy.html#exercice-the-racoon-svd",
    "href": "core/notebooks/notebook02_numpy.html#exercice-the-racoon-svd",
    "title": "Introduction to numpy",
    "section": "Exercice: the racoon SVD",
    "text": "Exercice: the racoon SVD\n\nLoad the racoon face picture using scipy.misc.face()\nVisualize the picture\nWrite a function which reshapes the picture into a 2D array, and computes the best rank-r approximation of it (the prototype of the function is compute_approx(X, r)\nDisplay the different approximations for r between 5 and 100\n\n\n\nCode\n!pip3 install pooch\n\n\nRequirement already satisfied: pooch in /home/boucheron/Documents/IFEBY310/.venv/lib/python3.12/site-packages (1.8.2)\nRequirement already satisfied: platformdirs&gt;=2.5.0 in /home/boucheron/Documents/IFEBY310/.venv/lib/python3.12/site-packages (from pooch) (4.3.6)\nRequirement already satisfied: packaging&gt;=20.0 in /home/boucheron/Documents/IFEBY310/.venv/lib/python3.12/site-packages (from pooch) (24.2)\nRequirement already satisfied: requests&gt;=2.19.0 in /home/boucheron/Documents/IFEBY310/.venv/lib/python3.12/site-packages (from pooch) (2.32.3)\nRequirement already satisfied: charset-normalizer&lt;4,&gt;=2 in /home/boucheron/Documents/IFEBY310/.venv/lib/python3.12/site-packages (from requests&gt;=2.19.0-&gt;pooch) (3.4.1)\nRequirement already satisfied: idna&lt;4,&gt;=2.5 in /home/boucheron/Documents/IFEBY310/.venv/lib/python3.12/site-packages (from requests&gt;=2.19.0-&gt;pooch) (3.10)\nRequirement already satisfied: urllib3&lt;3,&gt;=1.21.1 in /home/boucheron/Documents/IFEBY310/.venv/lib/python3.12/site-packages (from requests&gt;=2.19.0-&gt;pooch) (2.3.0)\nRequirement already satisfied: certifi&gt;=2017.4.17 in /home/boucheron/Documents/IFEBY310/.venv/lib/python3.12/site-packages (from requests&gt;=2.19.0-&gt;pooch) (2024.12.14)\n\n\n\n\nCode\nimport numpy as np\nfrom scipy.datasets import face\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nX = face()\n\n\n\n\nCode\ntype(X)\n\n\nnumpy.ndarray\n\n\n\n\nCode\nplt.imshow(X)\n_ = plt.axis('off')\n\n\n\n\n\n\n\n\n\n\n\nCode\nn_rows, n_cols, n_channels = X.shape\nX_reshaped = X.reshape(n_rows, n_cols * n_channels)\nU, S, V = np.linalg.svd(X_reshaped, full_matrices=False)\n\n\n\n\nCode\nX_reshaped.shape\n\n\n(768, 3072)\n\n\n\n\nCode\nX.shape\n\n\n(768, 1024, 3)\n\n\n\n\nCode\nplt.plot(S**2)  ## a kind of screeplot\nplt.yscale(\"log\")\n\n\n\n\n\n\n\n\n\n\n\nCode\ndef compute_approx(X: np.ndarray, r: int):\n    \"\"\"Computes the best rank-r approximation of X using SVD.\n    We expect X to the 3D array corresponding to a color image, that we \n    reduce to a 2D one to apply SVD (no broadcasting).\n    \n    Parameters\n    ----------\n    X : `np.ndarray`, shape=(n_rows, n_cols, 3)\n        The input 3D ndarray\n    \n    r : `int`\n        The desired rank\n        \n    Return\n    ------\n    output : `np.ndarray`, shape=(n_rows, n_cols, 3)\n        The best rank-r approximation of X\n    \"\"\"\n    n_rows, n_cols, n_channels = X.shape\n    # Reshape X to a 2D array\n    X_reshape = X.reshape(n_rows, n_cols * n_channels)\n    # Compute SVD\n    U, S, V = np.linalg.svd(X_reshape, full_matrices=False)\n    # Keep only the top r first singular values\n    S[r:] = 0\n    # Compute the approximation\n    X_reshape_r = U.dot(np.diag(S)).dot(V)\n    # Put it between 0 and 255 again and cast to integer type\n    return X_reshape_r.clip(min=0, max=255).astype('int')\\\n        .reshape(n_rows, n_cols, n_channels)\n\n\n\n\nCode\nranks = [100, 70, 50, 30, 10, 5]\nn_ranks = len(ranks)\nfor i, r in enumerate(ranks):\n    X_r = compute_approx(X, r)\n    # plt.subplot(n_ranks, 1, i + 1)\n    plt.figure(figsize=(5, 5))\n    plt.imshow(X_r)\n    _ = plt.axis('off')\n    # plt.title(f'Rank {r} approximation of the racoon' % r, fontsize=16)\n    plt.tight_layout()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVariations\n\n\n\nIn the code above, we recompute the SVD of X for every element in list rank.\nIn the next chunk, we compute the SVD once, and define a generator to generate the low rank approximations of matrix X. We take advantage of the fact that the SVD defines an orthonormal basis for the space of matrices. In this adapted orthonormal basis the optimal low rank approximations of \\(X\\) have a sparse expansion.\n\n\n\n\nCode\ndef gen_rank_k_approx(X):\n    \"\"\"Generator for low rank \n    approximation of a matrix X using truncated SVD.\n\n    Args:\n        X (numpy.ndarray): a numerical matrix\n\n    Yields:\n        (int,numpy.ndarray): rank k and best rank-k approximation of X using truncated SVD(according to Eckart-Young theorem).\n    \"\"\"  \n    U, S, V = np.linalg.svd(X, full_matrices=False)\n    r = 0\n    Y = np.zeros_like(X, dtype='float64')\n    while (r&lt;len(S)):\n      Y = Y + S[r] * (U[:,r,np.newaxis] @ V[r,:, np.newaxis].T)\n      r += 1\n      yield r, Y\n\n\n\n\nCode\ng = gen_rank_k_approx(X_reshaped) \n\n\n\n\nCode\nfor i in range(100):\n    _, Xr = next(g)\n    if i % 10 ==0:  \n      plt.figure(figsize=(5, 5))\n      plt.imshow(\n          Xr\n          .clip(min=0, max=255)\n          .astype('int')\n          .reshape(n_rows, n_cols, n_channels)\n      )\n      _ = plt.axis('off')\n      plt.tight_layout()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVisit https://numpy.org/numpy-tutorials/content/tutorial-svd.html"
  },
  {
    "objectID": "core/notebooks/notebook-0.html",
    "href": "core/notebooks/notebook-0.html",
    "title": "Notebook 0",
    "section": "",
    "text": "Iterators\n\n\nGenerators\n\n\nCoroutines"
  },
  {
    "objectID": "computing-pipelines.html",
    "href": "computing-pipelines.html",
    "title": "Pipelines",
    "section": "",
    "text": "Interroger une base de données avec R (via ODBC)\n\n\nInterroger une base de données avec Python (via ODBC)"
  },
  {
    "objectID": "computing-access.html",
    "href": "computing-access.html",
    "title": "Moyens de calcul",
    "section": "",
    "text": "Serveur\nLe cours et les TP\n\nServeur PostGreSQL\nMachine dédiée : etu-pgsql.math.univ-paris-diderot.fr\n\n\n\nClients\nEn salle TP, vous pourrez choisir entre trois clients\n\npsql\npgcli\ndbeaver\n\npsql et pgcli sont très proches. Ce sont des applications qui fonctionnent en mode ligne de commande. pgcli est un peu plus conviviale que psql avec un système de complétion plus performant. L’ensemble des commandes spéciales proposées par pgcli est un peu moins vaste que celui proposé par psql\ndbeaver est un client graphique qui ne tombe pas dans le cliquodrome. dbeaver permet d’attaquer une grande famille de SGBDs.\nTous ces clients doivent utiliser des connexions sécurisées ssh.\n\n\nConnexions ssh (Linux/MacOS)\nPour accéder au serveur Postgres (SGBD), il faut d’abord se connecter à la machine qui héberge ce serveur etu-pgsql.math.univ-paris-diderot.fr.\nPour se connecter à etu-pgsql.math.univ-paris-diderot.fr, on utilise le protocole ssh, avec son identifiant et son mot de passe ENT.\n$ ssh username@etu-pgsql.math.univ-paris-diderot.fr\n(username@etu-pgsql.math.univ-paris-diderot.fr) Password for username@etu-pgsql.math.univ-paris-diderot.fr:\nLast login: ..................\n....\n....\nThis fortune brought to you by:\n$FreeBSD$\n[username@etu-pgsql ~]$ ...\nAttention : pas d’écho lorsque vous saisissez votre mot de passe.\nOn peut maintenant se connecter au serveur Postgres, toujours avec son identifiant et son mot de passe ENT\nIci, nous proposons d’utiliser le client pgcli exécuté sur etu-pgsql.math.univ-paris-diderot.fr :\n[username@etu-pgsql ~]$ pgcli -d bd_2023-24 \nPassword for username: \nServer: PostgreSQL 13.8\nVersion: 3.5.0\nHome: http://pgcli.com\nuserame@/tmp:bd_2023-24&gt; \n\n\n\n\n\n\nPourquoi préciser -d bd_2023-24 ?\n\n\n\nbd_2023-24 est un des catalogues hébergés par le serveur PostGres. bd_2023-24 contient les schemas sur lesquels nous travaillerons.\n\n\nNous sommes maintenant dans une session du serveur\nusername@/tmp:bd_2023-24&gt; \\conninfo\nYou are connected to database \"bd_2023-24\" as user \"username\" on socket \"/tmp\" at port \"None\".\nTime: 0.000s\nPour obtenir de l’aide, utiliser la commande \\d? et laissez vous guider.\nNous nous intéresserons en général à un ensemble de tables formant un schéma. Ici, nous choisissons comme schéma par défaut world et nous affichons les tables du schéma.\nusername@/tmp:bd_2023-24&gt; set search_path to world ;\nSET\nTime: 0.001s\nusername@/tmp:bd_2023-24&gt; \\dt\n+--------+-----------------+-------+-----------+\n| Schema | Name            | Type  | Owner     |\n|--------+-----------------+-------+-----------|\n| world  | city            | table | boucheron |\n| world  | country         | table | boucheron |\n| world  | countrylanguage | table | boucheron |\n+--------+-----------------+-------+-----------+\nSELECT 3\nTime: 0.011s\nusername@/tmp:bd_2023-24&gt; ...\n\n\n\n\n\n\nPlus pratique\n\n\n\nPour pgcli et psql, il est plus pratique d’exécuter psql et/ou pgcli sur votre machine et de communiquer avec le serveur Postgres via un tunnel ssh. Voir détails pour pgcli et détails pour psql.\n\n\n\n\nConnexions ssh sous windows\nPour accéder au serveur Postgres (SGBD), il faut d’abord se connecter à la machine qui héberge ce serveur etu-pgsql.math.univ-paris-diderot.fr.\nPour se connecter à etu-pgsql.math.univ-paris-diderot.fr, on utilise le protocole ssh, avec son identifiant et son mot de passe ENT.\nOn lance d’abord une fenêtre Powershell.\n$ ssh username@etu-pgsql.math.univ-paris-diderot.fr\n(username@etu-pgsql.math.univ-paris-diderot.fr) Password for username@etu-pgsql.math.univ-paris-diderot.fr:\nLast login: ..................\n....\n....\nThis fortune brought to you by:\n$FreeBSD$\n[username@etu-pgsql ~]$ ...\n Attention : pas d’écho lorsque vous saisissez votre mot de passe.\nOn peut maintenant se connecter au serveur Postgres, toujours avec son identifiant et son mot de passe ENT\nIci, nous proposons d’utiliser le client pgcli :\n[username@etu-pgsql ~]$ pgcli -d bd_2023-24 \nPassword for username: \nServer: PostgreSQL 13.8\nVersion: 3.5.0\nHome: http://pgcli.com\nuserame@/tmp:bd_2023-24&gt; \n\n\n\n\n\n\nPourquoi préciser -d bd_2023-24 ?\n\n\n\nbd_2023-24 est un des catalogues hébergés par le serveur PostGres. bd_2023-24 contient les schemas sur lesquels nous travaillerons.\n\n\nNous sommes maintenant dans une session du serveur\nusername@/tmp:bd_2023-24&gt; \\conninfo\nYou are connected to database \"bd_2023-24\" as user \"username\" on socket \"/tmp\" at port \"None\".\nTime: 0.000s\nPour obtenir de l’aide, utiliser la commande \\d? et laissez vous guider.\nNous nous intéresserons en général à un ensemble de tables formant un schéma. Ici, nous choisissons comme schéma par défaut world et nous affichons les tables du schéma.\nusername@/tmp:bd_2023-24&gt; set search_path to world ;\nSET\nTime: 0.001s\nusername@/tmp:bd_2023-24&gt; \\dt\n+--------+-----------------+-------+-----------+\n| Schema | Name            | Type  | Owner     |\n|--------+-----------------+-------+-----------|\n| world  | city            | table | boucheron |\n| world  | country         | table | boucheron |\n| world  | countrylanguage | table | boucheron |\n+--------+-----------------+-------+-----------+\nSELECT 3\nTime: 0.011s\nusername@/tmp:bd_2023-24&gt; ...\n\n\nConnexions ssh avec tunnel\nVous avez pu constater que les connexions ssh sous MacOS, Linux et Windows sont presque identiques.\nMais utiliser une connexion ssh et un client base de données qui s’exécute sur etu-pgsql.math.univ-paris-diderot.fr n’est pas la manière la plus confortable de travailler.\nIl est plus agréable d’utiliser un client base de données qui s’exécute sur sa propre machine (en local) et qui interagit avec le serveur PostGres au travers d’un tunnel ssh.\nLa commande suivante établit un tunnel en tâche de fond (background job) grâce à l’option -f\n$ ssh -f username@etu-pgsql.math.univ-paris-diderot.fr -L 5436:localhost:5432 -N\n(username@etu-pgsql.math.univ-paris-diderot.fr) Password for username@etu-pgsql.math.univ-paris-diderot.fr:\n$ \nUne fois le tunnel établi, on peut continuer d’utiliser la fenêtre terminal, par exemple pour lancer pgcli ou psql.\nLa commande suivante établit aussi un tunnel mais en tâche de premier plan.\n$ ssh username@etu-pgsql.math.univ-paris-diderot.fr -L 5436:localhost:5432 -N\n(username@etu-pgsql.math.univ-paris-diderot.fr) Password for username@etu-pgsql.math.univ-paris-diderot.fr:\n$ \nUne fois le tunnel établi, la fenêtre terminal est bloquée. Si on veut lancer pgcli ou psql, il faut disposer d’une autre fenêtre.\n\n\n\n\n\n\nTunnel en tâche de fond ou pas ?\n\n\n\nLe lancement du tunnel en tâche de premier plan peut paraître moins pratique que le lancement du tunnel en tâche de fond.\nIl présente un avantage : lorsque le tunnel cesse de fonctionner (en général parce qu’on ne s’en est pas servi depuis quelques minutes), il faut termniner (tuer) le processus qui contrôle le tunnel, pour pouvoir récupérer l’usage du port local ; si le tunnel est contrôlé par une tache de premier plan, c’est trivial (^C sous Unix), si le tunnel est contrôlé par une tâche de fond, il faut déterminer le processus contrôleur, puis le terminer explicitement ($ kill -9 pid).\n\n\n\n\n\n\n\n\nRenvoi de port -L 5436:localhost:5432\n\n\n\nUn serveur PostGres écoute (attend) d’éventuels clients sur le port officiel 5432. Le serveur que nous utiliserons attend effectivement ses clients sur le port 5432 de la machine qui l’héberge. Notre client local ne va pas s’adresser directement au port 5432 de etu-pgsql.math.univ-paris-diderot.fr (c’est interdit). Notre client local s’adressera au port 5436 de la machine qui héberge le client et qui est lui-même renvoyé via le tunnel ssh vers le port 5432 de la machine qui héberge le serveur.\n\n\nOn peut maintenant lancer un client sur sa propre machine (localhost) en précisant qu’on s’adresse au port local 5436 (ou le port que vous choisissez), la requête de conexion au serveur PostGres distant sera transmise par le tunnel : elle sera envoyée sur le port officiel 5432 de la machine distante. Une fois la session établie, tout se passsera comme précédemment (ou presque).\n$ pgcli -d bd_2023-24 -h localhost -p 5436 -u username -W\nServer: PostgreSQL 13.8\nVersion: 3.5.0\nHome: http://pgcli.com\nbd_2023-24&gt; \\dn\n+----------------+--------------+\n| Name           | Owner        |\n|----------------+--------------|\n...\n...\n\n\nClient dbeaver\nLe mécanisme du tunnel ssh peut être utilisé pour connecter un client plus ambitieux au serveur. Le client dbeaver est particulièrement facile à utiliser.\n\n\nClient VS Code + extensions SQLTools\nSi vous êtes déjà habitué à l’éditeur Visual Studio Code (VS Code), vous pouvez utiliser l’extension SQLToos et son pilote ‘PostgreSQL/Cockroach’.\nVotre configuration de connexion devrait ressembler à :\n{\n  \"label\": \"etu-pgsql\",\n  \"host\": \"localhost\",\n  \"user\": \"&lt;identifiant ENT&gt;\",\n  \"port\": 5436,\n  \"ssl\": false,\n  \"database\": \"bd_2023-24\",\n  \"schema\": \"world\",\n  \"password\": \"Ask on connect\"\n}\nIl faut par ailleurs ouvrir un tunnel SSH dans un terminal\n$ ssh  username@etu-pgsql.math.univ-paris-diderot.fr -L 5436:localhost:5432 -N\n(username@etu-pgsql.math.univ-paris-diderot.fr) Password for username@etu-pgsql.math.univ-paris-diderot.fr:\n$ \nen remplaçant username par votre identifiant ENT.",
    "crumbs": [
      "Support",
      "Computing resources"
    ]
  },
  {
    "objectID": "bck_course-syllabus.html",
    "href": "bck_course-syllabus.html",
    "title": "IFEBY310 Syllabus",
    "section": "",
    "text": "Schedule\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDay\nTime\nLocation\nStart\n\n\n\n\nLab session\nFriday\n9:00-10:30\nSophie Germain 2012\n2024-01-17\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOrganization \n\n\n\nWe will have one weeky lecture. Each session is organized around Slides and Notebooks. We will switch from blackboard to laptop and back. You are invited to bring your laptop to the lectures.\n We will not attempt to complete the notebooks during the sessions. You are expected to complete the noteboks on your own time. Solutions (at least partial solutions) are available on the course website.\nYou can fork the course repository and post issues, comments, and corrections.\n\n\n\n\n\n\n\n\nObjectives\n\n\n\nDuring this course, you shall learn to:\n\nHandle middlesize data using Python Data Stack: Numpy/Scipy/Pandas\nScale up and down with Dask\nHandle Big Data with Spark (PySpark)\nManage and store data using dedicated columnar formats (Parquet, ORC, Avro, Arrow)\n\n\n\n\n\n\n\n\n\nCommunication \n\n\n\nMaterial is available from s-v-b.github.io/IFEBY310\nMoodle\nSubscribe to Moodle portal\n\n\n\n\n\n\n\n\nSoftware \n\n\n\n\nR\nPosit\nrstudio\nquarto\nvs code\ngit\ndocker\npostgresql\n\n\n\n\n\n\n\n\n\nReferences \n\n\n\n\nBin Yu and Rebecca Barter, Veridical Data Science\nHadley Wickham, ggplot2: Elegant Graphics for Data Analysis\nHadley Wickham, Mine Çetinkaya-Rundel, and Garrett Grolemund, R for Data Science\nHadley Wickham, Advanced R\nHadley Wickham and Jennifer Bryan., R packages\nHadley Wickham, Mastering Shiny\nkaggle\n\n\n\n\n\n\n\n\n\nCourse material \n\n\n\nSlides\nLabs are available (html and pdf)\nLabs with corrections are available\n\n\n\n\n\n\n\n\n\n\n\n\n\n3 projects:\n\n\\(\\textsf{P}_1\\) Visualization\n\\(\\textsf{P}_2\\) Regression\n\\(\\textsf{P}_3\\) Packaging\n\nGrading\n\n\\[.2 \\textsf{P}_1 + .3 \\textsf{P}_2 + .5 \\textsf{P}_3\\]\n\n\n\n\n\n\n\n\nCode of conduct\n\n\n\nTL;DR: No cheating!\n\n\n\n\n\n\n\n\nSave the dates ! \n\n\n\nClick here for U Paris Cite Calendar.\nClick here for M1 MIDS Calendar\n\n\n\n\n\n\n\n\nUniversité Paris Cité\n\n\n\nUseful links:\n\nCharte Université Paris Cité\nDémarches et accessibilité\nCentre de contact\nRelais handicap"
  },
  {
    "objectID": "computing-docker.html",
    "href": "computing-docker.html",
    "title": ": Docker",
    "section": "",
    "text": "Download, install and launch docker on your laptop\n\nFollow instructions at https://www.docker.com/get-started\n\n\n\n\n\n\nNote\n\n\n\nIt is enough to install the CLI tool.\n\n\n\nCreate an account on docker hub (if you don’t have one already) at https://hub.docker.com\n\n\nDocker Hub is a service provided by Docker for finding and sharing container images with your team. Learn more and find images at https://hub.docker.com\n\n\nOpen a terminal (powershell on windows) and type\n\ndocker login\nand provide the username and password you use at https://hub.docker.com",
    "crumbs": [
      "Support",
      "Docker"
    ]
  },
  {
    "objectID": "computing-docker.html#get-a-docker-account-and-connect",
    "href": "computing-docker.html#get-a-docker-account-and-connect",
    "title": ": Docker",
    "section": "",
    "text": "Download, install and launch docker on your laptop\n\nFollow instructions at https://www.docker.com/get-started\n\n\n\n\n\n\nNote\n\n\n\nIt is enough to install the CLI tool.\n\n\n\nCreate an account on docker hub (if you don’t have one already) at https://hub.docker.com\n\n\nDocker Hub is a service provided by Docker for finding and sharing container images with your team. Learn more and find images at https://hub.docker.com\n\n\nOpen a terminal (powershell on windows) and type\n\ndocker login\nand provide the username and password you use at https://hub.docker.com",
    "crumbs": [
      "Support",
      "Docker"
    ]
  },
  {
    "objectID": "computing-docker.html#docker-desktop-optional",
    "href": "computing-docker.html#docker-desktop-optional",
    "title": ": Docker",
    "section": "Docker desktop (optional)",
    "text": "Docker desktop (optional)",
    "crumbs": [
      "Support",
      "Docker"
    ]
  },
  {
    "objectID": "computing-docker.html#pull-docker-iamges",
    "href": "computing-docker.html#pull-docker-iamges",
    "title": ": Docker",
    "section": "Pull docker iamges",
    "text": "Pull docker iamges\ndocker pull svbo/ifeby310\ndocker image ls",
    "crumbs": [
      "Support",
      "Docker"
    ]
  },
  {
    "objectID": "computing-docker.html#run-containers",
    "href": "computing-docker.html#run-containers",
    "title": ": Docker",
    "section": "Run containers",
    "text": "Run containers\n\nA container is a runtime instance of a docker image. A container will always run the same, regardless of the infrastructure. Containers isolate software from its environment and ensure that it works uniformly despite differences for instance between development and staging.\n\n\nConfigure docker-compose.yml\ndocker-compose.yml\nversion: \"3.7\"\nservices:\n  big_data_course:\n    container_name: ifeby310  \n    image: svbo/ifeby310\n    ports:\n      - \"8192:8192\"\n      - \"8888:8888\"\n      - \"4040:4040\"\n    restart: always\n    volumes:\n      - \"PATH_GROSSES_DATA:/opt/polynote/notebooks/\"\n    restart: always\n    environment:\n      - PYSPARK_ALLOW_INSECURE_GATEWAY=1\n\n\n\n\n\n\nImportant\n\n\n\nPATH_GROSSES_DATA denotes the path on your hard drive where you will work during this course. It denotes a local volume that is mapped on container path /opt/polynote/notebooks\n\n\n\n\nCompose the container\ndocker-compose up\ndocker container ls",
    "crumbs": [
      "Support",
      "Docker"
    ]
  },
  {
    "objectID": "computing-docker.html#what-happens",
    "href": "computing-docker.html#what-happens",
    "title": ": Docker",
    "section": "What happens?",
    "text": "What happens?",
    "crumbs": [
      "Support",
      "Docker"
    ]
  },
  {
    "objectID": "computing-docker.html#use-jupyter-notebooks",
    "href": "computing-docker.html#use-jupyter-notebooks",
    "title": ": Docker",
    "section": "Use jupyter notebooks",
    "text": "Use jupyter notebooks",
    "crumbs": [
      "Support",
      "Docker"
    ]
  },
  {
    "objectID": "computing-docker.html#use-pyspark-and-spark-submit",
    "href": "computing-docker.html#use-pyspark-and-spark-submit",
    "title": ": Docker",
    "section": "Use pyspark and spark-submit",
    "text": "Use pyspark and spark-submit",
    "crumbs": [
      "Support",
      "Docker"
    ]
  },
  {
    "objectID": "computing-docker.html#use-polynote",
    "href": "computing-docker.html#use-polynote",
    "title": ": Docker",
    "section": "Use polynote",
    "text": "Use polynote",
    "crumbs": [
      "Support",
      "Docker"
    ]
  },
  {
    "objectID": "computing-docker.html#docker-cheatsheet",
    "href": "computing-docker.html#docker-cheatsheet",
    "title": ": Docker",
    "section": "Docker cheatsheet",
    "text": "Docker cheatsheet\nFrom docker.com",
    "crumbs": [
      "Support",
      "Docker"
    ]
  },
  {
    "objectID": "computing-psql.html",
    "href": "computing-psql.html",
    "title": "Client psql",
    "section": "",
    "text": "Note\n\n\n\nQuelques possibilités si vous disposez d’une machine sur laquelle on peut installer psql et sur laquelle on peut établir des tunnels ssh\n\n\n\nInstaller\n\nGénéra\nWindows\nMacOS\nUbuntu\n\nDocumentation\n\n\nUtiliser\n\n\n\n\n\n\nÉtablissement d’un tunel SSH sur votre machine (ici sous Linux)\n\n\n\nRemplacer id_ent par votre identifiant ENT dans la suite.\nSaisissez votre mot de passe (attention : pas d’écho)\n$ ssh id_ent@etu-pgsql.math.univ-paris-diderot.fr -L 5436:localhost:5432 -N\n(id_ent@etu-pgsql.math.univ-paris-diderot.fr) Password for id_ent@etu-pgsql.math.univ-paris-diderot.fr:\n$\n\n\n\n\n\n\n\n\nConnexion au serveur PostGres, demander la liste des commandes disponibles\n\n\n\nUtilisez votre tunnel SSH pour accéder au serveur PostGres. Dans une autre fenêtre terminal, lancer psql, saisissez à nouveau votre mot de passe.\n$ psql -p 5436 -U id_ent -W -h localhost -d bd_2023-24\nPassword for id_ent: \n\nbd_2023-24=# \\?  \nVous êtes maintenant dans une session sur le serveur PostGres. Vous êtes connecté au catalogue bd_2023-24\nVous pouvez utiliser une grande partie des commandes magiques de psql\n\n\n\n\n\n\n\n\nChoisir un schéma par défaut (ici world)\n\n\n\nbd_2023-24=# SET search_path TO world ;\nSET\n\n\n\n\n\n\n\n\nLister les tables du schéma par défaut\n\n\n\nbd_2023-24=# \\d\n+--------+-----------------+-------+-----------+\n| Schema | Name            | Type  | Owner     |\n|--------+-----------------+-------+-----------|\n| world  | city            | table | boucheron |\n| world  | country         | table | boucheron |\n| world  | countrylanguage | table | boucheron |\n+--------+-----------------+-------+-----------+\n(3 rows)\n\n\n\n\n\n\n\n\nSchéma d’une table\n\n\n\nbd_2023-24=# \\d city\n \n+-------------+--------------+-----------+\n| Column      | Type         | Modifiers |\n|-------------+--------------+-----------|\n| id          | integer      |  not null |\n| name        | text         |  not null |\n| countrycode | character(3) |  not null |\n| district    | text         |  not null |\n| population  | integer      |  not null |\n+-------------+--------------+-----------+\nIndexes:\n    \"city_pkey\" PRIMARY KEY, btree (id)\nForeign-key constraints:\n    \"city_country_fk\" FOREIGN KEY (countrycode) REFERENCES country(countrycode) ON UPDATE CASCADE ON DELETE SET NULL DEFE&gt;\nReferenced by:\n    TABLE \"country\" CONSTRAINT \"country_capital_fkey\" FOREIGN KEY (capital) REFERENCES city(id)\n\n\n\n\n\n\n\n\nInformations de connexion\n\n\n\nbd_2023-24=# \\conninfo\nYou are connected to database \"bd_2023-24\" as user \"id_end\" on host \"localhost\"  (address \"127.0.0.1\") at port \"5436\".\n\n\n\n\n\n\n\n\nEditer, sauvegarder et exécuter des requêtes\n\n\n\nbd_2023-24=# \\e \n\nSelect an editor.  To change later, run 'select-editor'.\n  1. /bin/nano        &lt;---- easiest\n  2. /usr/bin/vim.basic\n  3. /usr/bin/nvim\n  4. /usr/bin/vim.tiny\n  5. /usr/bin/emacs\n  6. /usr/bin/code\n  7. /bin/ed\nChoose 1-7 [1]: 6\nSous mon éditeur préféré (vs code ici), j’edite une requête\nSELECT ci.name, co.name_country\nFROM \n  world.city ci JOIN \n  world.country co ON (\n    ci.countrycode=co.countrycode AND \n    ci.id = co.capital\n  ) \nORDER BY co.name_country;\nsauvegardée dans un fichier de chemin d’accès /tmp/psql.edit.23866.sql (construit automatiquement)\nDans ma session sur bd_2023-24, je peux maintenant inclure et exécuter cette requête.\nbd_2023-24=# \\i /tmp/psql.edit.23866.sql\n               name                |             name_country              \n-----------------------------------+---------------------------------------\n Kabul                             | Afghanistan\n Tirana                            | Albania\n Alger                             | Algeria\n Fagatogo                          | American Samoa\n Andorra la Vella                  | Andorra\n Luanda                            | Angola\n:\n...\nEntrez q pour sortir du pager\n\n\n\n\n\n\n\n\nUn fichier par TP ?\n\n\n\nIl est commode d’archiver le travail d’une séance de TP dans un fichier *.sql. On peut créer les fichiers avant la session ou en cours de session (ici dans un dialecte d’Unix)\nbd_2023-24=# \\! touch tp-x.sql\nbd_2023-24=# -- editer tp-x.sql\nbd_2023-24=# \\e tp-x.sql \nbd_2023-24=# -- charger/exécuter tp-x.sql\nbd_2023-24=# \\i tp-x.sql\n\n\n\n\nRenseignements utiles\nDocumentation psql)"
  },
  {
    "objectID": "core/notebooks/checking_parquet_citibike.html",
    "href": "core/notebooks/checking_parquet_citibike.html",
    "title": "Check consistency of parquet files",
    "section": "",
    "text": "Code\nimport glob\n\nimport os\nimport sys\nimport re \nimport shutils\n\nimport pandas as pd\nimport numpy as np\n\nimport datetime\n\nimport itertools\n\nimport zipfile\nfrom zipfile import ZipFile\nfrom tqdm import tqdm\n\nimport pyarrow as pa\nimport comet    as co\nimport pyarrow.parquet as pq\nimport pyarrow.dataset as ds\n\nos.environ['PYSPARK_PYTHON'] = sys.executable\nos.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable\n\n\n\n---------------------------------------------------------------------------\nModuleNotFoundError                       Traceback (most recent call last)\nCell In[1], line 6\n      4 import sys\n      5 import re \n----&gt; 6 import shutils\n      8 import pandas as pd\n      9 import numpy as np\n\nModuleNotFoundError: No module named 'shutils'"
  },
  {
    "objectID": "core/notebooks/checking_parquet_citibike.html#paths",
    "href": "core/notebooks/checking_parquet_citibike.html#paths",
    "title": "Check consistency of parquet files",
    "section": "Paths",
    "text": "Paths\n\n\nCode\ndata_dir = \"../data\"\nos.path.exists(data_dir)\n\nextract_dir = os.path.join(data_dir, \"xcitibike\")\nif not os.path.exists(extract_dir):\n    os.mkdir(extract_dir)\n\nparquet_dir = os.path.join(data_dir, \"pq_citibike\")\nif not os.path.exists(parquet_dir):\n    os.mkdir(parquet_dir)\n\n\n\n---------------------------------------------------------------------------\nFileNotFoundError                         Traceback (most recent call last)\nCell In[2], line 6\n      4 extract_dir = os.path.join(data_dir, \"xcitibike\")\n      5 if not os.path.exists(extract_dir):\n----&gt; 6     os.mkdir(extract_dir)\n      8 parquet_dir = os.path.join(data_dir, \"pq_citibike\")\n      9 if not os.path.exists(parquet_dir):\n\nFileNotFoundError: [Errno 2] No such file or directory: '../data/xcitibike'"
  },
  {
    "objectID": "core/notebooks/checking_parquet_citibike.html#spark-session",
    "href": "core/notebooks/checking_parquet_citibike.html#spark-session",
    "title": "Check consistency of parquet files",
    "section": "Spark session",
    "text": "Spark session\n\n\nCode\nfrom pyspark import SparkConf, SparkContext\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql import functions as fn\nfrom pyspark.sql.types import *\n\n\n\n\nCode\nspark = (SparkSession\n    .builder\n    .appName(\"Spark checking citibike parquet file\")\n    .getOrCreate()\n)\n\n\n25/01/15 06:07:57 WARN Utils: Your hostname, boucheron-Precision-5480 resolves to a loopback address: 127.0.1.1; using 192.168.10.120 instead (on interface wlp0s20f3)\n25/01/15 06:07:57 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\nSetting default log level to \"WARN\".\nTo adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n25/01/15 06:07:58 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable"
  },
  {
    "objectID": "core/notebooks/checking_parquet_citibike.html#try-to-load-parquet-file",
    "href": "core/notebooks/checking_parquet_citibike.html#try-to-load-parquet-file",
    "title": "Check consistency of parquet files",
    "section": "Try to load parquet file",
    "text": "Try to load parquet file\n\n\nCode\nsch_1 = StructType([\n    StructField('trip_duration', StringType(), True), \n    StructField('started_at', TimestampType(), True), \n    StructField('ended_at', TimestampType(), True), \n    StructField('start_station_id', StringType(), True), \n    StructField('start_station_name', StringType(), True), \n    StructField('start_lat', StringType(), True), \n    StructField('start_lng', StringType(), True), \n    StructField('end_station_id', StringType(), True), \n    StructField('end_station_name', StringType(), True), \n    StructField('end_lat', StringType(), True), \n    StructField('end_lng', StringType(), True), \n    StructField('bike_id', StringType(), True), \n    StructField('user_type', StringType(), True), \n    StructField('birth_year', StringType(), True), \n    StructField('gender', StringType(), True), \n    StructField('start_year', IntegerType(), True), \n    StructField('start_month', IntegerType(), True)\n    ]\n)\n\n\n\n\nCode\ninput_file = os.path.join(parquet_dir, 'start_year=2022')\n\ndf = ( \n    spark.read\n        .option(\"mergeSchema\", \"true\")\n        .parquet(parquet_dir)\n)\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[6], line 1\n----&gt; 1 input_file = os.path.join(parquet_dir, 'start_year=2022')\n      3 df = ( \n      4     spark.read\n      5         .option(\"mergeSchema\", \"true\")\n      6         .parquet(parquet_dir)\n      7 )\n\nNameError: name 'parquet_dir' is not defined\n\n\n\n\n\nCode\ndf.printSchema()\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[7], line 1\n----&gt; 1 df.printSchema()\n\nNameError: name 'df' is not defined\n\n\n\nroot\n |-- trip_duration: string (nullable = true)\n |-- started_at: timestamp (nullable = true)\n |-- ended_at: timestamp (nullable = true)\n |-- start_station_id: string (nullable = true)\n |-- start_station_name: string (nullable = true)\n |-- start_lat: string (nullable = true)\n |-- start_lng: string (nullable = true)\n |-- end_station_id: string (nullable = true)\n |-- end_station_name: string (nullable = true)\n |-- end_lat: string (nullable = true)\n |-- end_lng: string (nullable = true)\n |-- bike_id: string (nullable = true)\n |-- user_type: string (nullable = true)\n |-- birth_year: string (nullable = true)\n |-- gender: string (nullable = true)\n |-- ride_id: string (nullable = true)\n |-- rideable_type: string (nullable = true)\n |-- member_casual: string (nullable = true)\n |-- start_year: integer (nullable = true)\n |-- start_month: integer (nullable = true)\n\n\nCode\ndf.select([\"started_at\", \"ended_at\"]).show(5)\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[8], line 1\n----&gt; 1 df.select([\"started_at\", \"ended_at\"]).show(5)\n\nNameError: name 'df' is not defined\n\n\n\n\n\nCode\nprint(df.rdd.toDebugString().decode(\"utf-8\"))\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[9], line 1\n----&gt; 1 print(df.rdd.toDebugString().decode(\"utf-8\"))\n\nNameError: name 'df' is not defined\n\n\n\n\n\nCode\ndf.rdd.getNumPartitions()\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[10], line 1\n----&gt; 1 df.rdd.getNumPartitions()\n\nNameError: name 'df' is not defined\n\n\n\n\n\nCode\ndf_pd = df.groupBy(['start_year', 'start_month']).count().toPandas()\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[11], line 1\n----&gt; 1 df_pd = df.groupBy(['start_year', 'start_month']).count().toPandas()\n\nNameError: name 'df' is not defined\n\n\n\n\n\nCode\ndf_pd.sort_values(by=['start_year', 'start_month'])\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[12], line 1\n----&gt; 1 df_pd.sort_values(by=['start_year', 'start_month'])\n\nNameError: name 'df_pd' is not defined\n\n\n\n\n\nCode\n(\n    df\n        .groupBy('rideable_type')\n        .agg(fn.count('started_at'))\n        .show()\n)\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[13], line 2\n      1 (\n----&gt; 2     df\n      3         .groupBy('rideable_type')\n      4         .agg(fn.count('started_at'))\n      5         .show()\n      6 )\n\nNameError: name 'df' is not defined\n\n\n\n\n\nCode\nspark.stop()\n\n\n\n\nCode\nfrom pyspark.sql.types import *\n\n\n\n\nCode\nprint(sch_1)\n\n\nStructType([StructField('trip_duration', StringType(), True), StructField('started_at', TimestampType(), True), StructField('ended_at', TimestampType(), True), StructField('start_station_id', StringType(), True), StructField('start_station_name', StringType(), True), StructField('start_lat', StringType(), True), StructField('start_lng', StringType(), True), StructField('end_station_id', StringType(), True), StructField('end_station_name', StringType(), True), StructField('end_lat', StringType(), True), StructField('end_lng', StringType(), True), StructField('bike_id', StringType(), True), StructField('user_type', StringType(), True), StructField('birth_year', StringType(), True), StructField('gender', StringType(), True), StructField('start_year', IntegerType(), True), StructField('start_month', IntegerType(), True)])"
  },
  {
    "objectID": "core/notebooks/notebook01_python.html",
    "href": "core/notebooks/notebook01_python.html",
    "title": "Introduction to Python",
    "section": "",
    "text": "We introduce here the python language. Only the bare minimum necessary for getting started with the data-science stack (a bunch of libraries for data science). Python is a programming language, as are C++, java, fortran, javascript, etc."
  },
  {
    "objectID": "core/notebooks/notebook01_python.html#specific-features-of-python",
    "href": "core/notebooks/notebook01_python.html#specific-features-of-python",
    "title": "Introduction to Python",
    "section": "Specific features of Python",
    "text": "Specific features of Python\n\nan interpreted (as opposed to compiled) language. Contrary to e.g. C++ or fortran, one does not compile Python code before executing it.\nUsed as a scripting language, by python python script.py in a terminal\nBut can be used also interactively: the jupyter notebook, iPython, etc.\nA free software released under an open-source license: Python can be used and distributed free of charge, even for building commercial software.\nmulti-platform: Python is available for all major operating systems, Windows, Linux/Unix, MacOS X, most likely your mobile phone OS, etc.\nA very readable language with clear non-verbose syntax\nA language for which a large amount of high-quality packages are available for various applications, including web-frameworks and scientific computing\nIt has been one of the top languages for data science and machine learning for several years, because it is expressive and and easy to deploy\nAn object-oriented language\n\nSee https://www.python.org/about/ for more information about distinguishing features of Python.\n\n\n\n\n\n\nPython 2 or Python 3?\n\n\n\n\nSimple answer: don’t use Python 2, use Python 3\nPython 2 is mostly deprecated and has not been maintained for years\nYou’ll end up hanged if you use Python 2\nIf Python 2 is mandatory at your workplace, find another work\n\n\n\n\n\n\n\n\n\nJupyter or Quarto notebooks?\n\n\n\n\nquarto is more git friendly than jupyter\nEnjoy authentic editors\nGo for quarto"
  },
  {
    "objectID": "core/notebooks/notebook01_python.html#integers",
    "href": "core/notebooks/notebook01_python.html#integers",
    "title": "Introduction to Python",
    "section": "Integers",
    "text": "Integers\n\n\nCode\n1 + 42\n\n\n43\n\n\n\n\nCode\ntype(1+1)\n\n\nint\n\n\nWe can assign values to variables with =\n\n\nCode\na = (3 + 5 ** 2) % 4\na\n\n\n0"
  },
  {
    "objectID": "core/notebooks/notebook01_python.html#remark",
    "href": "core/notebooks/notebook01_python.html#remark",
    "title": "Introduction to Python",
    "section": "Remark",
    "text": "Remark\nWe don’t declare the type of a variable before assigning its value. In C, conversely, one should write\nint a = 4;"
  },
  {
    "objectID": "core/notebooks/notebook01_python.html#something-cool",
    "href": "core/notebooks/notebook01_python.html#something-cool",
    "title": "Introduction to Python",
    "section": "Something cool",
    "text": "Something cool\n\nArbitrary large integer arithmetics\n\n\n\nCode\n17 ** 542\n\n\n8004153099680695240677662228684856314409365427758266999205063931175132640587226837141154215226851187899067565063096026317140186260836873939218139105634817684999348008544433671366043519135008200013865245747791955240844192282274023825424476387832943666754140847806277355805648624376507618604963106833797989037967001806494232055319953368448928268857747779203073913941756270620192860844700087001827697624308861431399538404552468712313829522630577767817531374612262253499813723569981496051353450351968993644643291035336065584116155321928452618573467361004489993801594806505273806498684433633838323916674207622468268867047187858269410016150838175127772100983052010703525089"
  },
  {
    "objectID": "core/notebooks/notebook01_python.html#floats",
    "href": "core/notebooks/notebook01_python.html#floats",
    "title": "Introduction to Python",
    "section": "Floats",
    "text": "Floats\nThere exists a floating point type that is created when the variable has decimal values\n\n\nCode\nc = 2.\n\n\n\n\nCode\ntype(c)\n\n\nfloat\n\n\n\n\nCode\nc = 2\ntype(c)\n\n\nint\n\n\n\n\nCode\ntruc = 1 / 2\ntruc\n\n\n0.5\n\n\n\n\nCode\n1 // 2 + 1 % 2\n\n\n1\n\n\n\n\nCode\ntype(truc)\n\n\nfloat"
  },
  {
    "objectID": "core/notebooks/notebook01_python.html#boolean",
    "href": "core/notebooks/notebook01_python.html#boolean",
    "title": "Introduction to Python",
    "section": "Boolean",
    "text": "Boolean\nSimilarly, boolean types are created from a comparison\n\n\nCode\ntest = 3 &gt; 4\ntest\n\n\nFalse\n\n\n\n\nCode\ntype(test)\n\n\nbool\n\n\n\n\nCode\nFalse == (not True)\n\n\nTrue\n\n\n\n\nCode\n1.41 &lt; 2.71 and 2.71 &lt; 3.14\n\n\nTrue\n\n\n\n\nCode\n# It's equivalent to\n1.41 &lt; 2.71 &lt; 3.14\n\n\nTrue"
  },
  {
    "objectID": "core/notebooks/notebook01_python.html#type-conversion-casting",
    "href": "core/notebooks/notebook01_python.html#type-conversion-casting",
    "title": "Introduction to Python",
    "section": "Type conversion (casting)",
    "text": "Type conversion (casting)\n\n\nCode\na = 1\ntype(a)\n\n\nint\n\n\n\n\nCode\nb = float(a)\ntype(b)\n\n\nfloat\n\n\n\n\nCode\nstr(b)\n\n\n'1.0'\n\n\n\n\nCode\nbool(b)\n# All non-zero, non empty objects are casted to boolean as True (more later)\n\n\nTrue\n\n\n\n\nCode\nbool(1-1)\n\n\nFalse"
  },
  {
    "objectID": "core/notebooks/notebook01_python.html#tuples",
    "href": "core/notebooks/notebook01_python.html#tuples",
    "title": "Introduction to Python",
    "section": "Tuples",
    "text": "Tuples\n\n\nCode\ntt = 'truc', 3.14, \"truc\"\ntt\n\n\n('truc', 3.14, 'truc')\n\n\n\n\nCode\ntt[0]\n\n\n'truc'\n\n\nYou can’t change a tuple, we say that it’s immutable\n\n\nCode\ntry:\n    tt[0] = 1\nexcept TypeError:\n    print(f\"TypeError: 'tuple' object does not support item assignment\")\n\n\nTypeError: 'tuple' object does not support item assignment\n\n\nThree ways of doing the same thing\n\n\nCode\n# Method 1\ntuple([1, 2, 3])\n\n\n(1, 2, 3)\n\n\n\n\nCode\n# Method 2\n1, 2, 3\n\n\n(1, 2, 3)\n\n\n\n\nCode\n# Method 3\n(1, 2, 3)\n\n\n(1, 2, 3)\n\n\nSimpler is better in Python, so usually you want to use Method 2.\n\n\nCode\ntoto = 1, 2, 3\ntoto\n\n\n(1, 2, 3)\n\n\n\nThis is serious !"
  },
  {
    "objectID": "core/notebooks/notebook01_python.html#the-zen-of-python-easters-egg",
    "href": "core/notebooks/notebook01_python.html#the-zen-of-python-easters-egg",
    "title": "Introduction to Python",
    "section": "The Zen of Python easter’s egg",
    "text": "The Zen of Python easter’s egg\n\n\nCode\nimport this\n\n\nThe Zen of Python, by Tim Peters\n\nBeautiful is better than ugly.\nExplicit is better than implicit.\nSimple is better than complex.\nComplex is better than complicated.\nFlat is better than nested.\nSparse is better than dense.\nReadability counts.\nSpecial cases aren't special enough to break the rules.\nAlthough practicality beats purity.\nErrors should never pass silently.\nUnless explicitly silenced.\nIn the face of ambiguity, refuse the temptation to guess.\nThere should be one-- and preferably only one --obvious way to do it.\nAlthough that way may not be obvious at first unless you're Dutch.\nNow is better than never.\nAlthough never is often better than *right* now.\nIf the implementation is hard to explain, it's a bad idea.\nIf the implementation is easy to explain, it may be a good idea.\nNamespaces are one honking great idea -- let's do more of those!"
  },
  {
    "objectID": "core/notebooks/notebook01_python.html#lists",
    "href": "core/notebooks/notebook01_python.html#lists",
    "title": "Introduction to Python",
    "section": "Lists",
    "text": "Lists\nA list is an ordered collection of objects. These objects may have different types. For example:\n\n\nCode\ncolors = ['red', 'blue', 'green', 'black', 'white']\n\n\n\n\nCode\ncolors[0]\n\n\n'red'\n\n\n\n\nCode\ntype(colors)\n\n\nlist\n\n\nIndexing: accessing individual objects contained in the list by their position\n\n\nCode\ncolors[2]\n\n\n'green'\n\n\n\n\nCode\ncolors[2] = 3.14\ncolors\n\n\n['red', 'blue', 3.14, 'black', 'white']\n\n\n\n\n\n\n\n\nWarning\n\n\n\nFor any iterable object in Python, indexing starts at 0 (as in C), not at 1 (as in Fortran, R, or Matlab).\n\n\nCounting from the end with negative indices:\n\n\nCode\ncolors[-1]\n\n\n'white'\n\n\nIndex must remain in the range of the list\n\n\nCode\ntry:\n    colors[10]\nexcept IndexError:\n    print(f\"IndexError: 10 &gt;= {len(colors)} ==len(colors), index out of range \")\n\n\n\n\nCode\ncolors\n\n\n['red', 'blue', 3.14, 'black', 'white']\n\n\n\n\nCode\ntt\n\n\n('truc', 3.14, 'truc')\n\n\n\n\nCode\ncolors.append(tt)\ncolors\n\n\n['red', 'blue', 3.14, 'black', 'white', ('truc', 3.14, 'truc')]\n\n\n\n\nCode\nlen(colors)\n\n\n6\n\n\n\n\nCode\nlen(tt)\n\n\n3"
  },
  {
    "objectID": "core/notebooks/notebook01_python.html#slicing-obtaining-sublists-of-regularly-spaced-elements",
    "href": "core/notebooks/notebook01_python.html#slicing-obtaining-sublists-of-regularly-spaced-elements",
    "title": "Introduction to Python",
    "section": "Slicing: obtaining sublists of regularly-spaced elements",
    "text": "Slicing: obtaining sublists of regularly-spaced elements\nThis work with anything iterable whenever it makes sense (list, str, tuple, etc.)\n\n\nCode\ncolors\n\n\n['red', 'blue', 3.14, 'black', 'white', ('truc', 3.14, 'truc')]\n\n\n\n\nCode\nlist(reversed(colors))\n\n\n[('truc', 3.14, 'truc'), 'white', 'black', 3.14, 'blue', 'red']\n\n\n\n\nCode\ncolors[::-1]\n\n\n[('truc', 3.14, 'truc'), 'white', 'black', 3.14, 'blue', 'red']\n\n\n\n\n\n\n\n\nSlicing syntax:\n\n\n\ncolors[start:stop:stride]\nstart, stop, stride are optional, with default values 0, len(sequence), 1\n\n\nl\n\n\nCode\nprint(slice(4))\nprint(slice(1,5))\nprint(slice(None,13,3))\n\n\nslice(None, 4, None)\nslice(1, 5, None)\nslice(None, 13, 3)\n\n\n\n\nCode\nsl = slice(1,5,2)\ncolors[sl]\n\n\n['blue', 'black']\n\n\n\n\nCode\ncolors\n\n\n['red', 'blue', 3.14, 'black', 'white', ('truc', 3.14, 'truc')]\n\n\n\n\nCode\ncolors[3:]\n\n\n['black', 'white', ('truc', 3.14, 'truc')]\n\n\n\n\nCode\ncolors[:3]\n\n\n['red', 'blue', 3.14]\n\n\n\n\nCode\ncolors[1::2]\n\n\n['blue', 'black', ('truc', 3.14, 'truc')]\n\n\n\n\nCode\ncolors[::-1]\n\n\n[('truc', 3.14, 'truc'), 'white', 'black', 3.14, 'blue', 'red']"
  },
  {
    "objectID": "core/notebooks/notebook01_python.html#strings",
    "href": "core/notebooks/notebook01_python.html#strings",
    "title": "Introduction to Python",
    "section": "Strings",
    "text": "Strings\nDifferent string syntaxes (simple, double or triple quotes):\n\n\nCode\ns = 'tintin'\ntype(s)\n\n\nstr\n\n\n\n\nCode\ns\n\n\n'tintin'\n\n\n\n\nCode\ns = \"\"\"         Bonjour,\nJe m'appelle Stephane.\nJe vous souhaite une bonne journée.\nSalut.       \n\"\"\"\ns\n\n\n\"         Bonjour,\\nJe m'appelle Stephane.\\nJe vous souhaite une bonne journée.\\nSalut.       \\n\"\n\n\n\n\nCode\ns.strip()\n\n\n\"Bonjour,\\nJe m'appelle Stephane.\\nJe vous souhaite une bonne journée.\\nSalut.\"\n\n\n\n\nCode\nprint(s.strip())\n\n\nBonjour,\nJe m'appelle Stephane.\nJe vous souhaite une bonne journée.\nSalut.\n\n\n\n\nCode\nlen(s)\n\n\n91\n\n\n\n\nCode\n# Casting to a list\nlist(s.strip()[:15])\n\n\n['B', 'o', 'n', 'j', 'o', 'u', 'r', ',', '\\n', 'J', 'e', ' ', 'm', \"'\", 'a']\n\n\n\n\nCode\n# Arithmetics\nprint('Bonjour' * 2)\nprint('Hello' + ' all')\n\n\nBonjourBonjour\nHello all\n\n\n\n\nCode\nsss = 'A'\nsss += 'bc'\nsss += 'dE'\nsss.lower()\n\n\n'abcde'\n\n\n\n\nCode\nss = s.strip()\nprint(ss[:10] + ss[24:28])\n\n\nBonjour,\nJepha\n\n\n\n\nCode\ns.strip()\n\n\n\"Bonjour,\\nJe m'appelle Stephane.\\nJe vous souhaite une bonne journée.\\nSalut.\"\n\n\n\n\nCode\ns.strip().split('\\n')\n\n\n['Bonjour,',\n \"Je m'appelle Stephane.\",\n 'Je vous souhaite une bonne journée.',\n 'Salut.']\n\n\n\n\nCode\ns[::3]\n\n\n'   BjrJmpl ea.eo ui eoeon.at  \\n'\n\n\n\n\nCode\ns[3:10]\n\n\n'      B'\n\n\n\n\nCode\n\" \".join(['Il', 'fait', 'super', 'beau', \"aujourd'hui\"])\n\n\n\"Il fait super beau aujourd'hui\"\n\n\nChaining method calls is the basic of pipeline building.\n\n\nCode\n( \n    \" \".join(['Il', 'fait', 'super', 'beau', \"aujourd'hui\"])\n       .title()\n       .replace(' ', '')\n       .replace(\"'\",\"\")\n)\n\n\n'IlFaitSuperBeauAujourdHui'\n\n\n\nImportant\nA string is immutable !!\n\n\nCode\ns = 'I am an immutable guy'\n\n\n\n\nCode\ntry:  \n    s[2] = 's'\nexcept TypeError:\n    print(f\"Strings are immutable! s is still '{s}'\")\n\n\nStrings are immutable! s is still 'I am an immutable guy'\n\n\n\n\nCode\nid(s)\n\n\n132790602760240\n\n\n\n\nCode\nprint(s + ', for sure')\nid(s), id(s + ' for sure')\n\n\nI am an immutable guy, for sure\n\n\n(132790602760240, 132790076549968)\n\n\n\n\nExtra stuff with strings\n\n\nCode\n'square of 2 is ' + str(2 ** 2)\n\n\n'square of 2 is 4'\n\n\n\n\nCode\n'square of 2 is %d' % 2 ** 2\n\n\n'square of 2 is 4'\n\n\n\n\nCode\n'square of 2 is {}'.format(2 ** 2)\n\n\n'square of 2 is 4'\n\n\n\n\nCode\n'square of 2 is {square}'.format(square=2 ** 2)\n\n\n'square of 2 is 4'\n\n\n\n\nCode\n# And since Python 3.6 you can use an `f-string`\nnumber = 2\nsquare = number ** 2\n\nf'square of {number} is {square}'\n\n\n'square of 2 is 4'\n\n\n\n\nThe in keyword\nYou can use the in keyword with any container, whenever it makes sense\n\n\nCode\nprint(s)\nprint('Salut' in s)\n\n\nI am an immutable guy\nFalse\n\n\n\n\nCode\nprint(tt)\nprint('truc' in tt)\n\n\n('truc', 3.14, 'truc')\nTrue\n\n\n\n\nCode\nprint(colors)\nprint('truc' in colors)\n\n\n['red', 'blue', 3.14, 'black', 'white', ('truc', 3.14, 'truc')]\nFalse\n\n\n\n\nCode\n('truc', 3.14, 'truc') in colors\n\n\nTrue\n\n\n\n\n\n\n\n\nWarning\n\n\n\nStrings are not bytes. Have a look at chapter 4 Unicode Text versus Bytes in Fluent Python\n\n\n\n\nBrain-teasing\nExplain this weird behaviour:\n\n\nCode\n5 in [1, 2, 3, 4] == False\n\n\nFalse\n\n\n\n\nCode\n[1, 2, 3, 4] == False\n\n\nFalse\n\n\n\n\nCode\n5 not in [1, 2, 3, 4]\n\n\nTrue\n\n\n\n\nCode\n(5 in [1, 2, 3, 4]) == False\n\n\nTrue\n\n\n\n\nCode\n# ANSWER.\n# This is a chained comparison. We have seen that \n1 &lt; 2 &lt; 3\n# is equivalent to\n(1 &lt; 2) and (2 &lt; 3)\n# so that\n5 in [1, 2, 3, 4] == False\n# is equivalent to\n(5 in [1, 2, 3, 4]) and ([1, 2, 3, 4] == False)\n\n\nFalse\n\n\n\n\nCode\n(5 in [1, 2, 3, 4])\n\n\nFalse\n\n\n\n\nCode\n([1, 2, 3, 4] == False)\n\n\nFalse"
  },
  {
    "objectID": "core/notebooks/notebook01_python.html#dictionaries",
    "href": "core/notebooks/notebook01_python.html#dictionaries",
    "title": "Introduction to Python",
    "section": "Dictionaries",
    "text": "Dictionaries\n\nA dictionary is basically an efficient table that maps keys to values.\nThe MOST important container in Python.\nMany things are actually a dict under the hood in Python\n\n\n\nCode\ntel = {'emmanuelle': 5752, 'sebastian': 5578}\nprint(tel)\nprint(type(tel))\n\n\n{'emmanuelle': 5752, 'sebastian': 5578}\n&lt;class 'dict'&gt;\n\n\n\n\nCode\ntel['emmanuelle'], tel['sebastian']\n\n\n(5752, 5578)\n\n\n\n\nCode\ntel['francis'] = '5919'\ntel\n\n\n{'emmanuelle': 5752, 'sebastian': 5578, 'francis': '5919'}\n\n\n\n\nCode\nlen(tel)\n\n\n3\n\n\n\nImportant remarks\n\nKeys can be of different types\nA key must be of immutable type\n\n\n\nCode\ntel[7162453] = [1, 3, 2]\ntel[3.14] = 'bidule'\ntel[('jaouad', 2)] = 1234\ntel\n\n\n{'emmanuelle': 5752,\n 'sebastian': 5578,\n 'francis': '5919',\n 7162453: [1, 3, 2],\n 3.14: 'bidule',\n ('jaouad', 2): 1234}\n\n\n\n\nCode\ntry:\n    sorted(tel)\nexcept TypeError:\n    print(\"TypeError: '&lt;' not supported between instances of 'int' and 'str'\")    \n\n\nTypeError: '&lt;' not supported between instances of 'int' and 'str'\n\n\n\n\nCode\n# A list is mutable and not hashable\ntry:\n    tel[['jaouad']] = '5678'\nexcept TypeError:\n    print(\"TypeError: unhashable type: 'list'\")\n\n\nTypeError: unhashable type: 'list'\n\n\n\n\nCode\ntry:\n    tel[2]\nexcept KeyError:\n    print(\"KeyError: 2\")\n\n\nKeyError: 2\n\n\n\n\nCode\ntel = {'emmanuelle': 5752, 'sebastian' : 5578, 'jaouad' : 1234}\nprint(tel.keys())\nprint(tel.values())\nprint(tel.items())\n\n\ndict_keys(['emmanuelle', 'sebastian', 'jaouad'])\ndict_values([5752, 5578, 1234])\ndict_items([('emmanuelle', 5752), ('sebastian', 5578), ('jaouad', 1234)])\n\n\n\n\nCode\nlist(tel.keys())[2]\n\n\n'jaouad'\n\n\n\n\nCode\ntel.values().mapping\n\n\nmappingproxy({'emmanuelle': 5752, 'sebastian': 5578, 'jaouad': 1234})\n\n\n\n\nCode\ntype(tel.keys())\n\n\ndict_keys\n\n\n\n\nCode\n'rémi' in tel\n\n\nFalse\n\n\n\n\nCode\nlist(tel)\n\n\n['emmanuelle', 'sebastian', 'jaouad']\n\n\n\n\nCode\n'rémi' in tel.keys()\n\n\nFalse\n\n\nYou can swap values like this\n\n\nCode\nprint(tel)\ntel['emmanuelle'], tel['sebastian'] = tel['sebastian'], tel['emmanuelle']\nprint(tel)\n\n\n{'emmanuelle': 5752, 'sebastian': 5578, 'jaouad': 1234}\n{'emmanuelle': 5578, 'sebastian': 5752, 'jaouad': 1234}\n\n\n\n\nCode\n# It works, since\na, b = 2.71, 3.14\na, b = b, a\na, b\n\n\n(3.14, 2.71)\n\n\n\n\nExercise 1\nGet keys of tel sorted by decreasing order\n\n\nCode\ntel = {'emmanuelle': 5752, 'sebastian' : 5578, 'jaouad' : 1234}\n\n\n\n\nExercise 2\nGet keys of tel sorted by increasing values\n\n\nCode\ntel = {'emmanuelle': 5752, 'sebastian' : 5578, 'jaouad' : 1234}\n\n\n\n\nExercise 3\nObtain a sorted-by-key version of tel\n\n\nCode\ntel = {'emmanuelle': 5752, 'sebastian' : 5578, 'jaouad' : 1234}"
  },
  {
    "objectID": "core/notebooks/notebook01_python.html#sets",
    "href": "core/notebooks/notebook01_python.html#sets",
    "title": "Introduction to Python",
    "section": "Sets",
    "text": "Sets\nA set is an unordered container, containing unique elements\n\n\nCode\nss = {1, 2, 2, 2, 3, 3, 'tintin', 'tintin', 'toto'}\nss\n\n\n{1, 2, 3, 'tintin', 'toto'}\n\n\n\n\nCode\ns = 'truc truc bidule truc'\nset(s)\n\n\n{' ', 'b', 'c', 'd', 'e', 'i', 'l', 'r', 't', 'u'}\n\n\n\n\nCode\nset(list(s))\n\n\n{' ', 'b', 'c', 'd', 'e', 'i', 'l', 'r', 't', 'u'}\n\n\n\n\nCode\n{1, 5, 2, 1, 1}.union({1, 2, 3})\n\n\n{1, 2, 3, 5}\n\n\n\n\nCode\nset((1, 5, 3, 2))\n\n\n{1, 2, 3, 5}\n\n\n\n\nCode\nset([1, 5, 2, 1, 1]).intersection(set([1, 2, 3]))\n\n\n{1, 2}\n\n\n\n\nCode\nss.add('tintin')\nss\n\n\n{1, 2, 3, 'tintin', 'toto'}\n\n\n\n\nCode\nss.difference(range(6))\n\n\n{'tintin', 'toto'}\n\n\nYou can combine all containers together\n\n\nCode\ndd = {\n    'truc': [1, 2, 3], \n    5: (1, 4, 2),\n    (1, 3): {'hello', 'world'}\n}\ndd\n\n\n{'truc': [1, 2, 3], 5: (1, 4, 2), (1, 3): {'hello', 'world'}}"
  },
  {
    "objectID": "core/notebooks/notebook01_python.html#everything-is-either-mutable-or-immutable",
    "href": "core/notebooks/notebook01_python.html#everything-is-either-mutable-or-immutable",
    "title": "Introduction to Python",
    "section": "Everything is either mutable or immutable",
    "text": "Everything is either mutable or immutable\n\n\nCode\nss = {1, 2, 3}\nsss = ss\nsss, ss\n\n\n({1, 2, 3}, {1, 2, 3})\n\n\n\n\nCode\nid(ss), id(sss)\n\n\n(132789088710304, 132789088710304)\n\n\n\n\nCode\nsss.add(\"Truc\")\n\n\nQuestion. What is in ss ?\n\n\nCode\nss, sss\n\n\n({1, 2, 3, 'Truc'}, {1, 2, 3, 'Truc'})\n\n\nss and sss are names for the same object\n\n\nCode\nid(ss), id(sss)\n\n\n(132789088710304, 132789088710304)\n\n\n\n\nCode\nss is sss\n\n\nTrue\n\n\n\n\nCode\nhelp('is')\n\n\nComparisons\n***********\n\nUnlike C, all comparison operations in Python have the same priority,\nwhich is lower than that of any arithmetic, shifting or bitwise\noperation.  Also unlike C, expressions like \"a &lt; b &lt; c\" have the\ninterpretation that is conventional in mathematics:\n\n   comparison    ::= or_expr (comp_operator or_expr)*\n   comp_operator ::= \"&lt;\" | \"&gt;\" | \"==\" | \"&gt;=\" | \"&lt;=\" | \"!=\"\n                     | \"is\" [\"not\"] | [\"not\"] \"in\"\n\nComparisons yield boolean values: \"True\" or \"False\". Custom *rich\ncomparison methods* may return non-boolean values. In this case Python\nwill call \"bool()\" on such value in boolean contexts.\n\nComparisons can be chained arbitrarily, e.g., \"x &lt; y &lt;= z\" is\nequivalent to \"x &lt; y and y &lt;= z\", except that \"y\" is evaluated only\nonce (but in both cases \"z\" is not evaluated at all when \"x &lt; y\" is\nfound to be false).\n\nFormally, if *a*, *b*, *c*, …, *y*, *z* are expressions and *op1*,\n*op2*, …, *opN* are comparison operators, then \"a op1 b op2 c ... y\nopN z\" is equivalent to \"a op1 b and b op2 c and ... y opN z\", except\nthat each expression is evaluated at most once.\n\nNote that \"a op1 b op2 c\" doesn’t imply any kind of comparison between\n*a* and *c*, so that, e.g., \"x &lt; y &gt; z\" is perfectly legal (though\nperhaps not pretty).\n\n\nValue comparisons\n=================\n\nThe operators \"&lt;\", \"&gt;\", \"==\", \"&gt;=\", \"&lt;=\", and \"!=\" compare the values\nof two objects.  The objects do not need to have the same type.\n\nChapter Objects, values and types states that objects have a value (in\naddition to type and identity).  The value of an object is a rather\nabstract notion in Python: For example, there is no canonical access\nmethod for an object’s value.  Also, there is no requirement that the\nvalue of an object should be constructed in a particular way, e.g.\ncomprised of all its data attributes. Comparison operators implement a\nparticular notion of what the value of an object is.  One can think of\nthem as defining the value of an object indirectly, by means of their\ncomparison implementation.\n\nBecause all types are (direct or indirect) subtypes of \"object\", they\ninherit the default comparison behavior from \"object\".  Types can\ncustomize their comparison behavior by implementing *rich comparison\nmethods* like \"__lt__()\", described in Basic customization.\n\nThe default behavior for equality comparison (\"==\" and \"!=\") is based\non the identity of the objects.  Hence, equality comparison of\ninstances with the same identity results in equality, and equality\ncomparison of instances with different identities results in\ninequality.  A motivation for this default behavior is the desire that\nall objects should be reflexive (i.e. \"x is y\" implies \"x == y\").\n\nA default order comparison (\"&lt;\", \"&gt;\", \"&lt;=\", and \"&gt;=\") is not provided;\nan attempt raises \"TypeError\".  A motivation for this default behavior\nis the lack of a similar invariant as for equality.\n\nThe behavior of the default equality comparison, that instances with\ndifferent identities are always unequal, may be in contrast to what\ntypes will need that have a sensible definition of object value and\nvalue-based equality.  Such types will need to customize their\ncomparison behavior, and in fact, a number of built-in types have done\nthat.\n\nThe following list describes the comparison behavior of the most\nimportant built-in types.\n\n* Numbers of built-in numeric types (Numeric Types — int, float,\n  complex) and of the standard library types \"fractions.Fraction\" and\n  \"decimal.Decimal\" can be compared within and across their types,\n  with the restriction that complex numbers do not support order\n  comparison.  Within the limits of the types involved, they compare\n  mathematically (algorithmically) correct without loss of precision.\n\n  The not-a-number values \"float('NaN')\" and \"decimal.Decimal('NaN')\"\n  are special.  Any ordered comparison of a number to a not-a-number\n  value is false. A counter-intuitive implication is that not-a-number\n  values are not equal to themselves.  For example, if \"x =\n  float('NaN')\", \"3 &lt; x\", \"x &lt; 3\" and \"x == x\" are all false, while \"x\n  != x\" is true.  This behavior is compliant with IEEE 754.\n\n* \"None\" and \"NotImplemented\" are singletons.  **PEP 8** advises that\n  comparisons for singletons should always be done with \"is\" or \"is\n  not\", never the equality operators.\n\n* Binary sequences (instances of \"bytes\" or \"bytearray\") can be\n  compared within and across their types.  They compare\n  lexicographically using the numeric values of their elements.\n\n* Strings (instances of \"str\") compare lexicographically using the\n  numerical Unicode code points (the result of the built-in function\n  \"ord()\") of their characters. [3]\n\n  Strings and binary sequences cannot be directly compared.\n\n* Sequences (instances of \"tuple\", \"list\", or \"range\") can be compared\n  only within each of their types, with the restriction that ranges do\n  not support order comparison.  Equality comparison across these\n  types results in inequality, and ordering comparison across these\n  types raises \"TypeError\".\n\n  Sequences compare lexicographically using comparison of\n  corresponding elements.  The built-in containers typically assume\n  identical objects are equal to themselves.  That lets them bypass\n  equality tests for identical objects to improve performance and to\n  maintain their internal invariants.\n\n  Lexicographical comparison between built-in collections works as\n  follows:\n\n  * For two collections to compare equal, they must be of the same\n    type, have the same length, and each pair of corresponding\n    elements must compare equal (for example, \"[1,2] == (1,2)\" is\n    false because the type is not the same).\n\n  * Collections that support order comparison are ordered the same as\n    their first unequal elements (for example, \"[1,2,x] &lt;= [1,2,y]\"\n    has the same value as \"x &lt;= y\").  If a corresponding element does\n    not exist, the shorter collection is ordered first (for example,\n    \"[1,2] &lt; [1,2,3]\" is true).\n\n* Mappings (instances of \"dict\") compare equal if and only if they\n  have equal \"(key, value)\" pairs. Equality comparison of the keys and\n  values enforces reflexivity.\n\n  Order comparisons (\"&lt;\", \"&gt;\", \"&lt;=\", and \"&gt;=\") raise \"TypeError\".\n\n* Sets (instances of \"set\" or \"frozenset\") can be compared within and\n  across their types.\n\n  They define order comparison operators to mean subset and superset\n  tests.  Those relations do not define total orderings (for example,\n  the two sets \"{1,2}\" and \"{2,3}\" are not equal, nor subsets of one\n  another, nor supersets of one another).  Accordingly, sets are not\n  appropriate arguments for functions which depend on total ordering\n  (for example, \"min()\", \"max()\", and \"sorted()\" produce undefined\n  results given a list of sets as inputs).\n\n  Comparison of sets enforces reflexivity of its elements.\n\n* Most other built-in types have no comparison methods implemented, so\n  they inherit the default comparison behavior.\n\nUser-defined classes that customize their comparison behavior should\nfollow some consistency rules, if possible:\n\n* Equality comparison should be reflexive. In other words, identical\n  objects should compare equal:\n\n     \"x is y\" implies \"x == y\"\n\n* Comparison should be symmetric. In other words, the following\n  expressions should have the same result:\n\n     \"x == y\" and \"y == x\"\n\n     \"x != y\" and \"y != x\"\n\n     \"x &lt; y\" and \"y &gt; x\"\n\n     \"x &lt;= y\" and \"y &gt;= x\"\n\n* Comparison should be transitive. The following (non-exhaustive)\n  examples illustrate that:\n\n     \"x &gt; y and y &gt; z\" implies \"x &gt; z\"\n\n     \"x &lt; y and y &lt;= z\" implies \"x &lt; z\"\n\n* Inverse comparison should result in the boolean negation. In other\n  words, the following expressions should have the same result:\n\n     \"x == y\" and \"not x != y\"\n\n     \"x &lt; y\" and \"not x &gt;= y\" (for total ordering)\n\n     \"x &gt; y\" and \"not x &lt;= y\" (for total ordering)\n\n  The last two expressions apply to totally ordered collections (e.g.\n  to sequences, but not to sets or mappings). See also the\n  \"total_ordering()\" decorator.\n\n* The \"hash()\" result should be consistent with equality. Objects that\n  are equal should either have the same hash value, or be marked as\n  unhashable.\n\nPython does not enforce these consistency rules. In fact, the\nnot-a-number values are an example for not following these rules.\n\n\nMembership test operations\n==========================\n\nThe operators \"in\" and \"not in\" test for membership.  \"x in s\"\nevaluates to \"True\" if *x* is a member of *s*, and \"False\" otherwise.\n\"x not in s\" returns the negation of \"x in s\".  All built-in sequences\nand set types support this as well as dictionary, for which \"in\" tests\nwhether the dictionary has a given key. For container types such as\nlist, tuple, set, frozenset, dict, or collections.deque, the\nexpression \"x in y\" is equivalent to \"any(x is e or x == e for e in\ny)\".\n\nFor the string and bytes types, \"x in y\" is \"True\" if and only if *x*\nis a substring of *y*.  An equivalent test is \"y.find(x) != -1\".\nEmpty strings are always considered to be a substring of any other\nstring, so \"\"\" in \"abc\"\" will return \"True\".\n\nFor user-defined classes which define the \"__contains__()\" method, \"x\nin y\" returns \"True\" if \"y.__contains__(x)\" returns a true value, and\n\"False\" otherwise.\n\nFor user-defined classes which do not define \"__contains__()\" but do\ndefine \"__iter__()\", \"x in y\" is \"True\" if some value \"z\", for which\nthe expression \"x is z or x == z\" is true, is produced while iterating\nover \"y\". If an exception is raised during the iteration, it is as if\n\"in\" raised that exception.\n\nLastly, the old-style iteration protocol is tried: if a class defines\n\"__getitem__()\", \"x in y\" is \"True\" if and only if there is a non-\nnegative integer index *i* such that \"x is y[i] or x == y[i]\", and no\nlower integer index raises the \"IndexError\" exception.  (If any other\nexception is raised, it is as if \"in\" raised that exception).\n\nThe operator \"not in\" is defined to have the inverse truth value of\n\"in\".\n\n\nIdentity comparisons\n====================\n\nThe operators \"is\" and \"is not\" test for an object’s identity: \"x is\ny\" is true if and only if *x* and *y* are the same object.  An\nObject’s identity is determined using the \"id()\" function.  \"x is not\ny\" yields the inverse truth value. [4]\n\nRelated help topics: EXPRESSIONS, BASICMETHODS"
  },
  {
    "objectID": "core/notebooks/notebook01_python.html#about-assigments",
    "href": "core/notebooks/notebook01_python.html#about-assigments",
    "title": "Introduction to Python",
    "section": "About assigments",
    "text": "About assigments\n\nPython never copies an object\nUnless you ask him to\n\nWhen you code\nx = [1, 2, 3]\ny = x\nyou just - bind the variable name x to a list [1, 2, 3] - give another name y to the same object\nImportant remarks\n\nEverything is an object in Python\nEither immutable or mutable\n\n\n\nCode\nid(1), id(1+1), id(2)\n\n\n(11757992, 11758024, 11758024)\n\n\nA list is mutable\n\n\nCode\nx = [1, 2, 3]\nprint(id(x), x)\nx[0] += 42; x.append(3.14)\nprint(id(x), x)\n\n\n132790603034624 [1, 2, 3]\n132790603034624 [43, 2, 3, 3.14]\n\n\nA str is immutable\nIn order to “change” an immutable object, Python creates a new one\n\n\nCode\ns = 'to'\nprint(id(s), s)\ns += 'to'\nprint(id(s), s)\n\n\n132790696648864 to\n132790602609984 toto\n\n\nOnce again, a list is mutable\n\n\nCode\nsuper_list = [3.14, (1, 2, 3), 'tintin']\nother_list = super_list\nid(other_list), id(super_list)\n\n\n(132790603134080, 132790603134080)\n\n\n\nother_list and super_list are the same list\nIf you change one, you change the other.\nid returns the identity of an object. Two objects with the same idendity are the same (not only the same type, but the same instance)\n\n\n\nCode\nother_list[1] = 'youps'\nother_list, super_list\n\n\n([3.14, 'youps', 'tintin'], [3.14, 'youps', 'tintin'])\n\n\n\n\nCode\nid(super_list), id(other_list)\n\n\n(132790603134080, 132790603134080)"
  },
  {
    "objectID": "core/notebooks/notebook01_python.html#if-you-want-a-copy-to-need-to-ask-for-one",
    "href": "core/notebooks/notebook01_python.html#if-you-want-a-copy-to-need-to-ask-for-one",
    "title": "Introduction to Python",
    "section": "If you want a copy, to need to ask for one",
    "text": "If you want a copy, to need to ask for one\n\n\nCode\nother_list = super_list.copy()\nid(other_list), id(super_list)\n\n\n(132790603064256, 132790603134080)\n\n\n\n\nCode\nother_list[1] = 'copy'\nother_list, super_list\n\n\n([3.14, 'copy', 'tintin'], [3.14, 'youps', 'tintin'])\n\n\nOnly other_list is modified.\nBut… what if you have a list of list ? (or a mutable object containing mutable objects)\n\n\nCode\nl1, l2 = [1, 2, 3], [4, 5, 6]\nlist_list = [l1, l2]\nlist_list\n\n\n[[1, 2, 3], [4, 5, 6]]\n\n\n\n\nCode\nid(list_list), id(list_list[0]), id(l1), list_list[0] is l1\n\n\n(132790602967360, 132790602760896, 132790602760896, True)\n\n\nLet’s make a copy of list_list\n\n\nCode\ncopy_list = list_list.copy()\ncopy_list.append('super')\nlist_list, copy_list\n\n\n([[1, 2, 3], [4, 5, 6]], [[1, 2, 3], [4, 5, 6], 'super'])\n\n\n\n\nCode\nid(list_list[0]), id(copy_list[0])\n\n\n(132790602760896, 132790602760896)\n\n\nOK, only copy_list is modified, as expected\nBut now…\n\n\nCode\ncopy_list[0][1] = 'oups'\ncopy_list, list_list\n\n\n([[1, 'oups', 3], [4, 5, 6], 'super'], [[1, 'oups', 3], [4, 5, 6]])\n\n\nQuestion. What happened ?!?\n\nThe list_list object is copied\nBut NOT what it’s containing !\nBy default copy does a shallow copy, not a deep copy\nIt does not build copies of what is contained\nIf you want to copy an object and all that is contained in it, you need to use deepcopy.\n\n\n\nCode\nfrom copy import deepcopy\n\ncopy_list = deepcopy(list_list)\ncopy_list[0][1] = 'incredible !'\nlist_list, copy_list\n\n\n([[1, 'oups', 3], [4, 5, 6]], [[1, 'incredible !', 3], [4, 5, 6]])"
  },
  {
    "objectID": "core/notebooks/notebook01_python.html#final-remarks",
    "href": "core/notebooks/notebook01_python.html#final-remarks",
    "title": "Introduction to Python",
    "section": "Final remarks",
    "text": "Final remarks\n\n\nCode\ntt = ([1, 2, 3], [4, 5, 6])\nprint(id(tt), tt)\nprint(list(map(id, tt)))\n\n\n132790602767488 ([1, 2, 3], [4, 5, 6])\n[132790602956544, 132790603026496]\n\n\n\n\nCode\ntt[0][1] = '42'\nprint(id(tt), tt)\nprint(list(map(id, tt)))\n\n\n132790602767488 ([1, '42', 3], [4, 5, 6])\n[132790602956544, 132790603026496]\n\n\n\n\nCode\ns = [1, 2, 3]\n\n\n\n\nCode\ns2 = s\n\n\n\n\nCode\ns2 is s\n\n\nTrue\n\n\n\n\nCode\nid(s2), id(s)\n\n\n(132790603025600, 132790603025600)"
  },
  {
    "objectID": "core/notebooks/notebook01_python.html#blocks-are-delimited-by-indentation",
    "href": "core/notebooks/notebook01_python.html#blocks-are-delimited-by-indentation",
    "title": "Introduction to Python",
    "section": "Blocks are delimited by indentation!",
    "text": "Blocks are delimited by indentation!\n\n\nCode\na = 3\nif a &gt; 0:\n    if a == 1:\n        print(1)\n    elif a == 2:\n        print(2)\nelif a == 2:\n    print(2)\nelif a == 3:\n    print(3)\nelse:\n    print(a)"
  },
  {
    "objectID": "core/notebooks/notebook01_python.html#anything-can-be-understood-as-a-boolean",
    "href": "core/notebooks/notebook01_python.html#anything-can-be-understood-as-a-boolean",
    "title": "Introduction to Python",
    "section": "Anything can be understood as a boolean",
    "text": "Anything can be understood as a boolean\nFor example, don’t do this to test if a list is empty\n\n\nCode\nl2 = ['hello', 'everybody']\n\nif len(l2) &gt; 0:\n    print(l2[0])\n\n\nhello\n\n\nbut this\n\n\nCode\nif l2:\n    print(l2[0])\n\n\nhello\n\n\nSome poetry\n\nAn empty dict is False\nAn empty string is False\nAn empty list is False\nAn empty tuple is False\nAn empty set is False\n0 is False\n.0 is False\netc…\neverything else is True"
  },
  {
    "objectID": "core/notebooks/notebook01_python.html#while-loops",
    "href": "core/notebooks/notebook01_python.html#while-loops",
    "title": "Introduction to Python",
    "section": "While loops",
    "text": "While loops\n\n\nCode\na = 10\nb = 1\nwhile b &lt; a:\n    b = b + 1\n    print(b)\n\n\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\nCompute the decimals of Pi using the Wallis formula\n\\[\n\\pi = 2 \\prod_{i=1}^{100} \\frac{4i^2}{4i^2 - 1}\n\\]\n\n\nCode\npi = 2\neps = 1e-10\ndif = 2 * eps\ni = 1\nwhile dif &gt; eps:\n    pi, i, old_pi = pi * 4 * i ** 2 / (4 * i ** 2 - 1), i + 1, pi\n    dif = pi - old_pi\n\n\n\n\nCode\npi\n\n\n3.1415837914138556\n\n\n\n\nCode\nfrom math import pi\n\npi\n\n\n3.141592653589793"
  },
  {
    "objectID": "core/notebooks/notebook01_python.html#for-loop-with-range",
    "href": "core/notebooks/notebook01_python.html#for-loop-with-range",
    "title": "Introduction to Python",
    "section": "for loop with range",
    "text": "for loop with range\n\nIteration with an index, with a list, with many things !\nrange has the same parameters as with slicing start:end:stride, all parameters being optional\n\n\n\nCode\nfor i in range(10):\n    print(i)\n\n\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n\n\nCode\nfor i in range(4):\n    print(i + 1)\nprint('-')\n\nfor i in range(1, 5):\n    print(i)\nprint('-')\n\nfor i in range(1, 10, 3):\n    print(i)\n\n\n1\n2\n3\n4\n-\n1\n2\n3\n4\n-\n1\n4\n7\n\n\nSomething for nerds. You can use else in a for loop\n\n\nCode\nnames = ['stephane', 'mokhtar', 'jaouad', 'simon', 'yiyang']\n\nfor name in names:\n    if name.startswith('u'):\n        print(name)\n        break\nelse:\n    print('Not found.')\n\n\nNot found.\n\n\n\n\nCode\nnames = ['stephane', 'mokhtar', 'jaouad', 'ulysse', 'simon', 'yiyang']\n\nfor name in names:\n    if name.startswith('u'):\n        print(name)\n        break\nelse:\n    print('Not found.')\n\n\nulysse"
  },
  {
    "objectID": "core/notebooks/notebook01_python.html#for-loops-over-iterable-objects",
    "href": "core/notebooks/notebook01_python.html#for-loops-over-iterable-objects",
    "title": "Introduction to Python",
    "section": "For loops over iterable objects",
    "text": "For loops over iterable objects\nYou can iterate using for over any container: list, tuple, dict, str, set among others…\n\n\nCode\ncolors = ['red', 'blue', 'black', 'white']\npeoples = ['stephane', 'jaouad', 'mokhtar', 'yiyang', 'rémi']\n\n\n\n\nCode\n# This is stupid\nfor i in range(len(colors)):\n    print(colors[i])\n    \n# This is better\nfor color in colors:\n    print(color)\n\n\nred\nblue\nblack\nwhite\nred\nblue\nblack\nwhite\n\n\nTo iterate over several sequences at the same time, use zip\n\n\nCode\nfor color, people in zip(colors, peoples):\n    print(color, people)\n\n\nred stephane\nblue jaouad\nblack mokhtar\nwhite yiyang\n\n\n\n\nCode\nl = [\"Bonjour\", {'francis': 5214, 'stephane': 5123}, ('truc', 3)]\nfor e in l:\n    print(e, len(e))\n\n\nBonjour 7\n{'francis': 5214, 'stephane': 5123} 2\n('truc', 3) 2\n\n\nLoop over a str\n\n\nCode\ns = 'Bonjour'\nfor c in s:\n    print(c)\n\n\nB\no\nn\nj\no\nu\nr\n\n\nLoop over a dict\n\n\nCode\ndd = {(1, 3): {'hello', 'world'}, 'truc': [1, 2, 3], 5: (1, 4, 2)}\n\n# Default is to loop over keys\nfor key in dd:\n    print(key)\n\n\n(1, 3)\ntruc\n5\n\n\n\n\nCode\n# Loop over values\nfor e in dd.values():\n    print(e)\n\n\n{'world', 'hello'}\n[1, 2, 3]\n(1, 4, 2)\n\n\n\n\nCode\n# Loop over items (key, value) pairs\nfor key, val in dd.items():\n    print(key, val)\n\n\n(1, 3) {'world', 'hello'}\ntruc [1, 2, 3]\n5 (1, 4, 2)\n\n\n\n\nCode\nfor t in dd.items():\n    print(t)\n\n\n((1, 3), {'world', 'hello'})\n('truc', [1, 2, 3])\n(5, (1, 4, 2))"
  },
  {
    "objectID": "core/notebooks/notebook01_python.html#comprehensions",
    "href": "core/notebooks/notebook01_python.html#comprehensions",
    "title": "Introduction to Python",
    "section": "Comprehensions",
    "text": "Comprehensions\nYou can construct a list, dict, set and others using the comprehension syntax\nlist comprehension\n\n\nCode\nprint(colors)\nprint(peoples)\n\n\n['red', 'blue', 'black', 'white']\n['stephane', 'jaouad', 'mokhtar', 'yiyang', 'rémi']\n\n\n\n\nCode\nl = []\nfor p, c in zip(peoples, colors):\n    if len(c)&lt;=4 :\n        l.append(p)\nprint(l)\n\n\n['stephane', 'jaouad']\n\n\n\n\nCode\n# The list of people with favorite color that has no more than 4 characters\n\n[people for color, people in zip(colors, peoples) if len(color) &lt;= 4]\n\n\n['stephane', 'jaouad']\n\n\ndict comprehension\n\n\nCode\n{people: color for color, people in zip(colors, peoples) if len(color) &lt;= 4}\n\n\n{'stephane': 'red', 'jaouad': 'blue'}\n\n\n\n\nCode\n# Allows to build a dict from two lists (for keys and values)\n{key: value for (key, value) in zip(peoples, colors)}\n\n\n{'stephane': 'red', 'jaouad': 'blue', 'mokhtar': 'black', 'yiyang': 'white'}\n\n\n\n\nCode\n# But it's simpler (so better) to use\ndict(zip(peoples, colors))\n\n\n{'stephane': 'red', 'jaouad': 'blue', 'mokhtar': 'black', 'yiyang': 'white'}\n\n\nSomething very convenient is enumerate\n\n\nCode\nfor i, color in enumerate(colors):\n    print(i, color)\n\n\n0 red\n1 blue\n2 black\n3 white\n\n\n\n\nCode\nlist(enumerate(colors))\n\n\n[(0, 'red'), (1, 'blue'), (2, 'black'), (3, 'white')]\n\n\n\n\nCode\ndict(enumerate(s))\n\n\n{0: 'B', 1: 'o', 2: 'n', 3: 'j', 4: 'o', 5: 'u', 6: 'r'}\n\n\n\n\nCode\nprint(dict(enumerate(s)))\n\n\n{0: 'B', 1: 'o', 2: 'n', 3: 'j', 4: 'o', 5: 'u', 6: 'r'}\n\n\n\n\nCode\ns = 'Hey everyone'\n{c: i for i, c in enumerate(s)}\n\n\n{'H': 0, 'e': 11, 'y': 8, ' ': 3, 'v': 5, 'r': 7, 'o': 9, 'n': 10}"
  },
  {
    "objectID": "core/notebooks/notebook01_python.html#about-functional-programming",
    "href": "core/notebooks/notebook01_python.html#about-functional-programming",
    "title": "Introduction to Python",
    "section": "About functional programming",
    "text": "About functional programming\nWe can use lambda to define anonymous functions, and use them in the map and reduce functions\n\n\nCode\nsquare = lambda x: x ** 2\nsquare(2)\n\n\n4\n\n\n\n\nCode\ntype(square)\n\n\nfunction\n\n\n\n\nCode\ndir(square)\n\n\n['__annotations__',\n '__builtins__',\n '__call__',\n '__class__',\n '__closure__',\n '__code__',\n '__defaults__',\n '__delattr__',\n '__dict__',\n '__dir__',\n '__doc__',\n '__eq__',\n '__format__',\n '__ge__',\n '__get__',\n '__getattribute__',\n '__getstate__',\n '__globals__',\n '__gt__',\n '__hash__',\n '__init__',\n '__init_subclass__',\n '__kwdefaults__',\n '__le__',\n '__lt__',\n '__module__',\n '__name__',\n '__ne__',\n '__new__',\n '__qualname__',\n '__reduce__',\n '__reduce_ex__',\n '__repr__',\n '__setattr__',\n '__sizeof__',\n '__str__',\n '__subclasshook__',\n '__type_params__']\n\n\n\n\nCode\ns = \"a\"\n\n\n\n\nCode\ntry:\n    square(\"a\")\nexcept TypeError:\n    print(\"TypeError: unsupported operand type(s) for ** or pow(): 'str' and 'int'\")\n\n\nTypeError: unsupported operand type(s) for ** or pow(): 'str' and 'int'\n\n\n\n\nCode\nsum2 = lambda a, b: a + b\nprint(sum2('Hello', ' world'))\nprint(sum2(1, 2))\n\n\nHello world\n3\n\n\nIntended for short and one-line function.\nMore complex functions use def (see below)"
  },
  {
    "objectID": "core/notebooks/notebook01_python.html#exercise",
    "href": "core/notebooks/notebook01_python.html#exercise",
    "title": "Introduction to Python",
    "section": "Exercise",
    "text": "Exercise\nPrint the squares of even numbers between 0 et 15\n\nUsing a list comprehension as before\nUsing map"
  },
  {
    "objectID": "core/notebooks/notebook01_python.html#brain-teasing-1",
    "href": "core/notebooks/notebook01_python.html#brain-teasing-1",
    "title": "Introduction to Python",
    "section": "Brain-teasing",
    "text": "Brain-teasing\nWhat is the output of\n\n\nCode\nreduce(lambda a, b: a + b[0] * b[1], enumerate('abcde'), 'A')"
  },
  {
    "objectID": "core/notebooks/notebook01_python.html#why-generators",
    "href": "core/notebooks/notebook01_python.html#why-generators",
    "title": "Introduction to Python",
    "section": "Why generators ?",
    "text": "Why generators ?\nThe memory used by range(i) does not scale linearly with i\nWhat is happening ?\n\nrange(n) does not allocate a list of n elements !\nIt generates on the fly the list of required integers\nWe say that such an object behaves like a generator in Python\nMany things in the Python standard library behaves like this\n\nWarning. Getting the real memory footprint of a Python object is difficult. Note that sizeof calls the __sizeof__ method of r, which does not give in general the actual memory used by an object. But nevermind here.\nThe following computation has no memory footprint:\n\n\nCode\nsum(range(10**8))\n\n\n4999999950000000\n\n\n\n\nCode\nmap(lambda x: x**2, range(10**7))\n\n\n&lt;map at 0x78c55e7eff40&gt;\n\n\nmap does not return a list for the same reason\n\n\nCode\nsum(map(lambda x: x**2, range(10**6)))\n\n\n333332833333500000"
  },
  {
    "objectID": "core/notebooks/notebook01_python.html#generator-expression",
    "href": "core/notebooks/notebook01_python.html#generator-expression",
    "title": "Introduction to Python",
    "section": "Generator expression",
    "text": "Generator expression\nNamely generators defined through comprehensions. Just replace [] by () in the comprehension.\nA generator can be iterated on only once\n\n\nCode\nrange(10)\n\n\nrange(0, 10)\n\n\n\n\nCode\ncarres = (i**2 for i in range(10))\n\n\n\n\nCode\ncarres\n\n\n&lt;generator object &lt;genexpr&gt; at 0x78c5b8b3a9b0&gt;\n\n\n\n\nCode\nfor c in carres:\n    print(c)\n\n\n0\n1\n4\n9\n16\n25\n36\n49\n64\n81\n\n\n\n\nCode\nfor i in range(4):\n    for j in range(3):\n        print(i, j)\n\n\n0 0\n0 1\n0 2\n1 0\n1 1\n1 2\n2 0\n2 1\n2 2\n3 0\n3 1\n3 2\n\n\n\n\nCode\nfrom itertools import product\n\nfor t in product(range(4), range(3)):\n    print(t)\n\n\n(0, 0)\n(0, 1)\n(0, 2)\n(1, 0)\n(1, 1)\n(1, 2)\n(2, 0)\n(2, 1)\n(2, 2)\n(3, 0)\n(3, 1)\n(3, 2)\n\n\n\n\nCode\nfrom itertools import product\n\ngene = (i + j for i, j in product(range(3), range(3)))\ngene\n\n\n&lt;generator object &lt;genexpr&gt; at 0x78c5b8b3ae90&gt;\n\n\n\n\nCode\nprint(list(gene))\nprint(list(gene))\n\n\n[0, 1, 2, 1, 2, 3, 2, 3, 4]\n[]"
  },
  {
    "objectID": "core/notebooks/notebook01_python.html#yield",
    "href": "core/notebooks/notebook01_python.html#yield",
    "title": "Introduction to Python",
    "section": "yield",
    "text": "yield\nSomething very powerful\n\n\nCode\ndef startswith(words, letter):\n    for word in words:\n        if word.startswith(letter):\n            yield word\n\n\n\n\nCode\nwords = [\n    'Python', \"is\", 'awesome', 'in', 'particular', 'generators', \n    'are', 'really', 'cool'\n]\n\n\n\n\nCode\nlist(word for word in words if word.startswith(\"a\"))\n\n\n['awesome', 'are']\n\n\n\n\nCode\na = 2\n\n\n\n\nCode\nfloat(a)\n\n\n2.0\n\n\nBut also with a for loop\n\n\nCode\nfor word in startswith(words, letter='a'):\n    print(word)\n\n\nawesome\nare\n\n\n\n\nCode\nit = startswith(words, letter='a')\n\n\n\n\nCode\ntype(it)\n\n\ngenerator\n\n\n\n\nCode\nnext(it)\n\n\n'awesome'\n\n\n\n\nCode\nnext(it)\n\n\n'are'\n\n\n\n\nCode\ntry:\n    next(it)\nexcept StopIteration:\n    print(\"StopIteration exception!\")\n\n\nStopIteration exception!"
  },
  {
    "objectID": "core/notebooks/notebook01_python.html#exercise-4",
    "href": "core/notebooks/notebook01_python.html#exercise-4",
    "title": "Introduction to Python",
    "section": "Exercise",
    "text": "Exercise\nCount the number of occurences of all the words in words.\nOutput must be a dictionary containg word: count\n\n\nCode\nprint(words)\n\n\n['Bonjour', 'Python', 'c', 'est', 'super', 'Python', 'ca', 'a', 'l', 'air', 'quand', 'même', 'un', 'peu', 'compliqué', 'Mais', 'bon', 'ca', 'a', 'l', 'air', 'pratique', 'Peut-être', 'que', 'je', 'pourrais', 'm', 'en', 'servir', 'pour', 'faire', 'des', 'trucs', 'super']"
  },
  {
    "objectID": "core/notebooks/notebook01_python.html#exercise-5",
    "href": "core/notebooks/notebook01_python.html#exercise-5",
    "title": "Introduction to Python",
    "section": "Exercise",
    "text": "Exercise\nCompute the number of occurences AND the length of each word in words.\nOutput must be a dictionary containing word: (count, length)"
  },
  {
    "objectID": "core/notebooks/notebook01_python.html#exercise-6",
    "href": "core/notebooks/notebook01_python.html#exercise-6",
    "title": "Introduction to Python",
    "section": "Exercise",
    "text": "Exercise\nCount the number of occurences of each word in the text file miserables.txt. We use a open context and the Counter from before."
  },
  {
    "objectID": "core/notebooks/notebook01_python.html#contexts",
    "href": "core/notebooks/notebook01_python.html#contexts",
    "title": "Introduction to Python",
    "section": "Contexts",
    "text": "Contexts\n\nA context in Python is something that we use with the with keyword.\nIt allows to deal automatically with the opening and the closing of the file.\n\nNote the for loop:\nfor line in f:\n    ...\nYou loop directly over the lines of the open file from within the open context"
  },
  {
    "objectID": "core/notebooks/notebook01_python.html#about-pickle",
    "href": "core/notebooks/notebook01_python.html#about-pickle",
    "title": "Introduction to Python",
    "section": "About pickle",
    "text": "About pickle\nYou can save your computation with pickle.\n\npickle is a way of saving almost anything with Python.\nIt serializes the object in a binary format, and is usually the simplest and fastest way to go.\n\n\n\nCode\nimport pickle as pkl\n\n# Let's save it\nwith open('miserable_word_counts.pkl', 'wb') as f:\n    pkl.dump(counter, f)\n\n# And read it again\nwith open('miserable_word_counts.pkl', 'rb') as f:\n    counter = pkl.load(f)\n\n\n\n\nCode\ncounter.most_common(10)\n\n\n[('{', 15),\n ('}', 15),\n ('0', 8),\n ('img', 6),\n ('margin:', 6),\n ('font', 6),\n ('logo', 6),\n ('only', 6),\n ('screen', 6),\n ('and', 6)]"
  },
  {
    "objectID": "core/notebooks/notebook01_python.html#function-definition",
    "href": "core/notebooks/notebook01_python.html#function-definition",
    "title": "Introduction to Python",
    "section": "Function definition",
    "text": "Function definition\nFunction blocks must be indented as other control-flow blocks.\n\n\nCode\ndef test():\n    return 'in test function'\n\ntest()\n\n\n'in test function'"
  },
  {
    "objectID": "core/notebooks/notebook01_python.html#return-statement",
    "href": "core/notebooks/notebook01_python.html#return-statement",
    "title": "Introduction to Python",
    "section": "Return statement",
    "text": "Return statement\nFunctions can optionally return values. By default, functions return None.\nThe syntax to define a function:\n\nthe def keyword;\nis followed by the function’s name, then\nthe arguments of the function are given between parentheses followed by a colon\nthe function body;\nand return object for optionally returning values.\n\n\n\nCode\nNone is None\n\n\nTrue\n\n\n\n\nCode\ndef f(x):\n    return x + 10\nf(20)\n\n\n30\n\n\nA function that returns several elements returns a tuple\n\n\nCode\ndef f(x):\n    return x + 1, x + 4\n\nf(5)\n\n\n(6, 9)\n\n\n\n\nCode\ntype(f)\n\n\nfunction\n\n\n\n\nCode\nf.truc = \"bonjour\"\n\n\n\n\nCode\ntype(f(5))\n\n\ntuple"
  },
  {
    "objectID": "core/notebooks/notebook01_python.html#parameters",
    "href": "core/notebooks/notebook01_python.html#parameters",
    "title": "Introduction to Python",
    "section": "Parameters",
    "text": "Parameters\nMandatory parameters (positional arguments)\n\n\nCode\ndef double_it(x):\n    return x * 2\n\ndouble_it(2)\n\n\n4\n\n\n\n\nCode\ntry:\n    double_it()\nexcept TypeError:\n    print(\"TypeError: double_it() missing 1 required positional argument: 'x'\")\n\n\nTypeError: double_it() missing 1 required positional argument: 'x'\n\n\nOptimal parameters\n\n\nCode\ndef double_it(x=2):\n    return x * 2\n\ndouble_it()\n\n\n4\n\n\n\n\nCode\ndouble_it(3)\n\n\n6\n\n\n\n\nCode\ndef f(x, y=2, z=10):\n    print(x, '+', y, '+', z, '=', x + y + z)\n\n\n\n\nCode\nf(5)\n\n\n5 + 2 + 10 = 17\n\n\n\n\nCode\nf(5, -2)\n\n\n5 + -2 + 10 = 13\n\n\n\n\nCode\nf(5, -2, 8)\n\n\n5 + -2 + 8 = 11\n\n\n\n\nCode\nf(z=5, x=-2, y=8)\n\n\n-2 + 8 + 5 = 11"
  },
  {
    "objectID": "core/notebooks/notebook01_python.html#argument-unpacking-and-keyword-argument-unpacking",
    "href": "core/notebooks/notebook01_python.html#argument-unpacking-and-keyword-argument-unpacking",
    "title": "Introduction to Python",
    "section": "Argument unpacking and keyword argument unpacking",
    "text": "Argument unpacking and keyword argument unpacking\nYou can do stuff like this, using unpacking * notation\n\n\nCode\na, *b, c = 1, 2, 3, 4, 5\na, b, c\n\n\n(1, [2, 3, 4], 5)\n\n\nBack to function f you can unpack a tuple as positional arguments\n\n\nCode\ntt = (1, 2, 3)\nf(*tt)\n\n\n1 + 2 + 3 = 6\n\n\n\n\nCode\ndd = {'y': 10, 'z': -5}\n\n\n\n\nCode\nf(3, **dd)\n\n\n3 + 10 + -5 = 8\n\n\n\n\nCode\ndef g(x, z, y, t=1, u=2):\n    print(x, '+', y, '+', z, '+', t, '+', \n          u, '=', x + y + z + t + u)\n\n\n\n\nCode\ntt = (1, -4, 2)\ndd = {'t': 10, 'u': -5}\ng(*tt, **dd)\n\n\n1 + 2 + -4 + 10 + -5 = 4"
  },
  {
    "objectID": "core/notebooks/notebook01_python.html#the-prototype-of-all-functions-in-python",
    "href": "core/notebooks/notebook01_python.html#the-prototype-of-all-functions-in-python",
    "title": "Introduction to Python",
    "section": "The prototype of all functions in Python",
    "text": "The prototype of all functions in Python\n\n\nCode\ndef f(*args, **kwargs):\n    print('args=', args)\n    print('kwargs=', kwargs)\n\nf(1, 2, 'truc', lastname='gaiffas', firstname='stephane')\n\n\nargs= (1, 2, 'truc')\nkwargs= {'lastname': 'gaiffas', 'firstname': 'stephane'}\n\n\n\nUses * for argument unpacking and ** for keyword argument unpacking\nThe names args and kwargs are a convention, not mandatory\n(but you are fired if you name these arguments otherwise)\n\n\n\nCode\n# How to get fired\ndef f(*aaa, **bbb):\n    print('args=', aaa)\n    print('kwargs=', bbb)\nf(1, 2, 'truc', lastname='gaiffas', firstname='stephane')    \n\n\nargs= (1, 2, 'truc')\nkwargs= {'lastname': 'gaiffas', 'firstname': 'stephane'}\n\n\nRemark. A function is a regular an object… you can add attributes on it !\n\n\nCode\nf.truc = 4\n\n\n\n\nCode\nf(1, 3)\n\n\nargs= (1, 3)\nkwargs= {}\n\n\n\n\nCode\nf(3, -2, y='truc')\n\n\nargs= (3, -2)\nkwargs= {'y': 'truc'}"
  },
  {
    "objectID": "core/notebooks/notebook01_python.html#exercise-7",
    "href": "core/notebooks/notebook01_python.html#exercise-7",
    "title": "Introduction to Python",
    "section": "Exercise",
    "text": "Exercise\nAdd a age method to the Student class that computes the age of the student. - You can (and should) use the datetime module. - Since we only know about the birth year, let’s assume that the day of the birth is January, 1st."
  },
  {
    "objectID": "core/notebooks/notebook01_python.html#properties",
    "href": "core/notebooks/notebook01_python.html#properties",
    "title": "Introduction to Python",
    "section": "Properties",
    "text": "Properties\nWe can make methods look like attributes using properties, as shown below\n\n\nCode\nclass Student(object):\n\n    def __init__(self, name, birthyear, major='computer science'):\n        self.name = name\n        self.birthyear = birthyear\n        self.major = major\n\n    def __repr__(self):\n        return \"Student(name='{name}', birthyear={birthyear}, major='{major}')\"\\\n                .format(name=self.name, birthyear=self.birthyear, major=self.major)\n\n    @property\n    def age(self):\n        return datetime.now().year - self.birthyear\n        \nanna = Student('anna', 1987)\nanna.age\n\n\n38"
  },
  {
    "objectID": "core/notebooks/notebook01_python.html#inheritance",
    "href": "core/notebooks/notebook01_python.html#inheritance",
    "title": "Introduction to Python",
    "section": "Inheritance",
    "text": "Inheritance\nA MasterStudent is a Student with a new extra mandatory internship attribute\n\n\nCode\n\"%d\" % 2\n\n\n'2'\n\n\n\n\nCode\nx = 2\n\nf\"truc {x}\"\n\n\n'truc 2'\n\n\n\n\nCode\nclass MasterStudent(Student):\n    \n    def __init__(self, name, age, internship, major='computer science'):\n        # Student.__init__(self, name, age, major)\n        Student.__init__(self, name, age, major)\n        self.internship = internship\n\n    def __repr__(self):\n        return f\"MasterStudent(name='{self.name}', internship={self.internship}, birthyear={self.birthyear}, major={self.major})\"\n    \nMasterStudent('djalil', 22, 'pwc')\n\n\nMasterStudent(name='djalil', internship=pwc, birthyear=22, major=computer science)\n\n\n\n\nCode\nclass MasterStudent(Student):\n    \n    def __init__(self, name, age, internship, major='computer science'):\n        # Student.__init__(self, name, age, major)\n        Student.__init__(self, name, age, major)\n        self.internship = internship\n\n    def __repr__(self):\n        return \"MasterStudent(name='{name}', internship='{internship}'\" \\\n               \", birthyear={birthyear}, major='{major}')\"\\\n                .format(name=self.name, internship=self.internship,\n                        birthyear=self.birthyear, major=self.major)\n    \ndjalil = MasterStudent('djalil', 1996, 'pwc')\n\n\n\n\nCode\ndjalil.__dict__\n\n\n{'name': 'djalil',\n 'birthyear': 1996,\n 'major': 'computer science',\n 'internship': 'pwc'}\n\n\n\n\nCode\ndjalil.birthyear\n\n\n1996\n\n\n\n\nCode\ndjalil.__dict__[\"birthyear\"]\n\n\n1996"
  },
  {
    "objectID": "core/notebooks/notebook01_python.html#monkey-patching",
    "href": "core/notebooks/notebook01_python.html#monkey-patching",
    "title": "Introduction to Python",
    "section": "Monkey patching",
    "text": "Monkey patching\n\nClasses in Python are objects and actually dicts under the hood…\nTherefore classes are objects that can be changed on the fly\n\n\n\nCode\nclass Monkey(object):\n    \n    def __init__(self, name):\n        self.name = name\n\n    def describe(self):\n        print(\"Old monkey %s\" % self.name)\n\ndef patch(self):\n    print(\"New monkey %s\" % self.name)\n\nmonkey = Monkey(\"Baloo\")\nmonkey.describe()\n\nMonkey.describe = patch\nmonkey.describe()\n\n\nOld monkey Baloo\nNew monkey Baloo\n\n\n\n\nCode\nmonkeys = [Monkey(\"Baloo\"), Monkey(\"Super singe\")]\n\n\nmonkey_name = monkey.name\n\nfor i in range(1000):    \n    monkey_name"
  },
  {
    "objectID": "core/notebooks/notebook01_python.html#data-classes",
    "href": "core/notebooks/notebook01_python.html#data-classes",
    "title": "Introduction to Python",
    "section": "Data classes",
    "text": "Data classes\nSince Python 3.7 you can use a dataclass for this\nDoes a lot of work for you (produces the __repr__ among many other things for you)\n\n\nCode\nfrom dataclasses import dataclass\nfrom datetime import datetime \n\n@dataclass\nclass Student(object):\n    name: str\n    birthyear: int\n    major: str = 'computer science'\n\n    @property\n    def age(self):\n        return datetime.now().year - self.birthyear\n        \nanna = Student(name=\"anna\", birthyear=1987)\nanna\n\n\nStudent(name='anna', birthyear=1987, major='computer science')\n\n\n\n\nCode\nprint(anna.age)\n\n\n38"
  },
  {
    "objectID": "core/notebooks/notebook01_python.html#using-a-mutable-value-as-a-default-value",
    "href": "core/notebooks/notebook01_python.html#using-a-mutable-value-as-a-default-value",
    "title": "Introduction to Python",
    "section": "Using a mutable value as a default value",
    "text": "Using a mutable value as a default value\n\n\nCode\ndef foo(bar=[]):\n    bar.append('oops')\n    return bar\n\nprint(foo())\nprint(foo())\nprint(foo())\n\nprint('-' * 8)\nprint(foo(['Ah ah']))\nprint(foo([]))\n\n\n['oops']\n['oops', 'oops']\n['oops', 'oops', 'oops']\n--------\n['Ah ah', 'oops']\n['oops']\n\n\n\n\nCode\nprint(foo.__defaults__)\nfoo()\nprint(foo.__defaults__)\n\n\n(['oops', 'oops', 'oops'],)\n(['oops', 'oops', 'oops', 'oops'],)\n\n\n\nThe default value for a function argument is evaluated once, when the function is defined\nthe bar argument is initialized to its default (i.e., an empty list) only when foo() is first defined\nsuccessive calls to foo() (with no a bar argument specified) use the same list!\n\nOne should use instead\n\n\nCode\ndef foo(bar=None):\n    if bar is None:\n        bar = []\n    bar.append('oops')\n    return bar\n\nprint(foo())\nprint(foo())\nprint(foo())\nprint(foo(['OK']))\n\n\n['oops']\n['oops']\n['oops']\n['OK', 'oops']\n\n\n\n\nCode\nprint(foo.__defaults__)\nfoo()\nprint(foo.__defaults__)\n\n\n(None,)\n(None,)\n\n\nNo problem with immutable types\n\n\nCode\ndef foo(bar=()):\n    bar += ('oops',)\n    return bar\n\nprint(foo())\nprint(foo())\nprint(foo())\n\n\n('oops',)\n('oops',)\n('oops',)\n\n\n\n\nCode\nprint(foo.__defaults__)\n\n\n((),)"
  },
  {
    "objectID": "core/notebooks/notebook01_python.html#class-attributes-vs-object-attributes",
    "href": "core/notebooks/notebook01_python.html#class-attributes-vs-object-attributes",
    "title": "Introduction to Python",
    "section": "Class attributes VS object attributes",
    "text": "Class attributes VS object attributes\n\n\nCode\nclass A(object):\n    x = 1\n\n    def __init__(self):\n        self.y = 2\n\nclass B(A):\n    def __init__(self):\n        super().__init__()\n\nclass C(A):\n    def __init__(self):\n        super().__init__()\n\na, b, c = A(), B(), C()\n\n\n\n\nCode\nprint(a.x, b.x, c.x)\nprint(a.y, b.y, c.y)\n\n\n1 1 1\n2 2 2\n\n\n\n\nCode\na.y = 3\nprint(a.y, b.y, c.y)\n\n\n3 2 2\n\n\n\n\nCode\na.x = 3  # Adds a new attribute named x in object a\nprint(a.x, b.x, c.x)\n\n\n3 1 1\n\n\n\n\nCode\nA.x = 4 # Changes the class attribute x of class A\nprint(a.x, b.x, c.x)\n\n\n3 4 4\n\n\n\nAttribute x is not an attribute of b nor c\nIt is also not a class attribute of classes B and C\nSo, it is is looked up in the base class A, which contains a class attribute x\n\nClasses and objects contain a hidden dict to store their attributes, and are accessed following a method resolution order (MRO)\n\n\nCode\na.__dict__, b.__dict__, c.__dict__\n\n\n({'y': 3, 'x': 3}, {'y': 2}, {'y': 2})\n\n\n\n\nCode\nA.__dict__, B.__dict__, C.__dict__\n\n\n(mappingproxy({'__module__': '__main__',\n               'x': 4,\n               '__init__': &lt;function __main__.A.__init__(self)&gt;,\n               '__dict__': &lt;attribute '__dict__' of 'A' objects&gt;,\n               '__weakref__': &lt;attribute '__weakref__' of 'A' objects&gt;,\n               '__doc__': None}),\n mappingproxy({'__module__': '__main__',\n               '__init__': &lt;function __main__.B.__init__(self)&gt;,\n               '__doc__': None}),\n mappingproxy({'__module__': '__main__',\n               '__init__': &lt;function __main__.C.__init__(self)&gt;,\n               '__doc__': None}))\n\n\nThis can lead to nasty errors when using class attributes: learn more about this"
  },
  {
    "objectID": "core/notebooks/notebook01_python.html#python-scope-rules",
    "href": "core/notebooks/notebook01_python.html#python-scope-rules",
    "title": "Introduction to Python",
    "section": "Python scope rules",
    "text": "Python scope rules\n\n\nCode\ntry:\n    ints += [4]\nexcept NameError:\n    print(\"NameError: name 'ints' is not defined\")\n\n\nNameError: name 'ints' is not defined\n\n\n\n\nCode\nints = [1]\n\ndef foo1():\n    ints.append(2)\n    return ints\n\ndef foo2():\n    ints += [2]\n    return ints\n\n\n\n\nCode\nfoo1()\n\n\n[1, 2]\n\n\n\n\nCode\ntry:    \n    foo2()\nexcept UnboundLocalError as inst:\n    print(inst)\n\n\ncannot access local variable 'ints' where it is not associated with a value\n\n\n\nWhat the hell ?\n\nAn assignment to a variable in a scope assumes that the variable is local to that scope\nand shadows any similarly named variable in any outer scope\n\nints += [2]\nmeans\nints = ints + [2]\nwhich is an assigment: ints must be defined in the local scope, but it is not, while\nints.append(2)\nis not an assignemnt"
  },
  {
    "objectID": "core/notebooks/notebook01_python.html#modify-a-list-while-iterating-over-it",
    "href": "core/notebooks/notebook01_python.html#modify-a-list-while-iterating-over-it",
    "title": "Introduction to Python",
    "section": "Modify a list while iterating over it",
    "text": "Modify a list while iterating over it\n\n\nCode\nodd = lambda x: bool(x % 2)\nnumbers = list(range(10))\n\ntry:\n  for i in range(len(numbers)):\n      if odd(numbers[i]):\n          del numbers[i]\nexcept IndexError as inst:\n    print(inst)\n\n\nlist index out of range\n\n\nTypically an example where one should use a list comprehension\n\n\nCode\n[number for number in numbers if not odd(number)]\n\n\n[0, 2, 4, 6, 8]"
  },
  {
    "objectID": "core/notebooks/notebook01_python.html#no-docstrings",
    "href": "core/notebooks/notebook01_python.html#no-docstrings",
    "title": "Introduction to Python",
    "section": "No docstrings",
    "text": "No docstrings\nAccept to spend time to write clean docstrings (look at numpydoc style)\n\n\nCode\ndef create_student(name, age, address, major='computer science'):\n    \"\"\"Add a student in the database\n    \n    Parameters\n    ----------\n    name: `str`\n        Name of the student\n    \n    age: `int`\n        Age of the student\n    \n    address: `str`\n        Address of the student\n    \n    major: `str`, default='computer science'\n        The major chosen by the student\n    \n    Returns\n    -------\n    output: `Student`\n        A fresh student\n    \"\"\"\n    pass\n\n\n\n\nCode\ncreate_student('Duduche', 28, 'Chalons')"
  },
  {
    "objectID": "core/notebooks/notebook01_python.html#not-using-available-methods-andor-the-simplest-solution",
    "href": "core/notebooks/notebook01_python.html#not-using-available-methods-andor-the-simplest-solution",
    "title": "Introduction to Python",
    "section": "Not using available methods and/or the simplest solution",
    "text": "Not using available methods and/or the simplest solution\n\n\nCode\ndd = {'stephane': 1234, 'gael': 4567, 'gontran': 891011}\n\n# Bad\nfor key in dd.keys():\n    print(key, dd[key])\n\nprint('-' * 8)\n\n# Good\nfor key, value in dd.items():\n    print(key, value)\n\n\nstephane 1234\ngael 4567\ngontran 891011\n--------\nstephane 1234\ngael 4567\ngontran 891011\n\n\n\n\nCode\ncolors = ['black', 'yellow', 'brown', 'red', 'pink']\n\n# Bad\nfor i in range(len(colors)):\n    print(i, colors[i])\n\nprint('-' * 8)\n\n# Good\nfor i, color in enumerate(colors):\n    print(i, color)\n\n\n0 black\n1 yellow\n2 brown\n3 red\n4 pink\n--------\n0 black\n1 yellow\n2 brown\n3 red\n4 pink"
  },
  {
    "objectID": "core/notebooks/notebook01_python.html#not-using-the-standard-library",
    "href": "core/notebooks/notebook01_python.html#not-using-the-standard-library",
    "title": "Introduction to Python",
    "section": "Not using the standard library",
    "text": "Not using the standard library\nWhile it’s always better than a hand-made solution\n\n\nCode\nlist1 = [1, 2]\nlist2 = [3, 4]\nlist3 = [5, 6, 7]\n\nfor a in list1:\n    for b in list2:\n        for c in list3:\n            print(a, b, c)\n\n\n1 3 5\n1 3 6\n1 3 7\n1 4 5\n1 4 6\n1 4 7\n2 3 5\n2 3 6\n2 3 7\n2 4 5\n2 4 6\n2 4 7\n\n\n\n\nCode\nfrom itertools import product\n\nfor a, b, c in product(list1, list2, list3):\n    print(a, b, c)\n\n\n1 3 5\n1 3 6\n1 3 7\n1 4 5\n1 4 6\n1 4 7\n2 3 5\n2 3 6\n2 3 7\n2 4 5\n2 4 6\n2 4 7"
  },
  {
    "objectID": "core/notebooks/notebook03_pandas.html",
    "href": "core/notebooks/notebook03_pandas.html",
    "title": "Introduction to pandas",
    "section": "",
    "text": "The pandas library (https://pandas.pydata.org) is one of the most used tool at the disposal of people working with data in python today."
  },
  {
    "objectID": "core/notebooks/notebook03_pandas.html#why",
    "href": "core/notebooks/notebook03_pandas.html#why",
    "title": "Introduction to pandas",
    "section": "Why ?",
    "text": "Why ?\nThrough pandas, you get acquainted with your data by analyzing it\n\nWhat’s the average, median, max, or min of each column?\nDoes column A correlate with column B?\nWhat does the distribution of data in column C look like?"
  },
  {
    "objectID": "core/notebooks/notebook03_pandas.html#why-cont",
    "href": "core/notebooks/notebook03_pandas.html#why-cont",
    "title": "Introduction to pandas",
    "section": "Why (con’t) ?",
    "text": "Why (con’t) ?\nyou get acquainted with your data by cleaning and transforming it\n\nRemoving missing values, filter rows or columns using some criteria\nStore the cleaned, transformed data back into virtually any format or database\nData visualization (when combined matplotlib or seaborn or others)"
  },
  {
    "objectID": "core/notebooks/notebook03_pandas.html#where",
    "href": "core/notebooks/notebook03_pandas.html#where",
    "title": "Introduction to pandas",
    "section": "Where ?",
    "text": "Where ?\npandas is a central component of the python “stack” for data science\n\npandas is built on top of numpy\noften used in conjunction with other libraries\na DataFrame is often fed to plotting functions or machine learning algorithms (such as scikit-learn)\nWell-interfaced with jupyter, leading to a nice interactive environment for data exploration and modeling"
  },
  {
    "objectID": "core/notebooks/notebook03_pandas.html#core-components-of-pandas",
    "href": "core/notebooks/notebook03_pandas.html#core-components-of-pandas",
    "title": "Introduction to pandas",
    "section": "Core components of pandas",
    "text": "Core components of pandas\nThe two primary components of pandas are the Series and DataFrame.\n\nA Series is essentially a column\nA DataFrame is a multi-dimensional table made up of a collection of Series with equal length"
  },
  {
    "objectID": "core/notebooks/notebook03_pandas.html#creating-a-dataframe-from-scratch",
    "href": "core/notebooks/notebook03_pandas.html#creating-a-dataframe-from-scratch",
    "title": "Introduction to pandas",
    "section": "Creating a DataFrame from scratch",
    "text": "Creating a DataFrame from scratch\n\n\nCode\nimport pandas as pd\n\nfruits = {\n    \"apples\": [3, 2, 0, 1],\n    \"oranges\": [0, 3, 7, 2]\n}\n\ndf_fruits = pd.DataFrame(fruits)\ndf_fruits\n\n\n\n\n\n\n\n\n\napples\noranges\n\n\n\n\n0\n3\n0\n\n\n1\n2\n3\n\n\n2\n0\n7\n\n\n3\n1\n2\n\n\n\n\n\n\n\n\n\nCode\ntype(df_fruits)\n\n\npandas.core.frame.DataFrame\n\n\n\n\nCode\ndf_fruits[\"apples\"]\n\n\n0    3\n1    2\n2    0\n3    1\nName: apples, dtype: int64\n\n\n\n\nCode\ntype(df_fruits[\"apples\"])\n\n\npandas.core.series.Series"
  },
  {
    "objectID": "core/notebooks/notebook03_pandas.html#indexing",
    "href": "core/notebooks/notebook03_pandas.html#indexing",
    "title": "Introduction to pandas",
    "section": "Indexing",
    "text": "Indexing\n\nBy default, a DataFrame uses a contiguous index\nBut what if we want to say who buys the fruits ?\n\n\n\nCode\ndf_fruits = pd.DataFrame(fruits, index=[\"Daniel\", \"Sean\", \"Pierce\", \"Roger\"])\ndf_fruits\n\n\n\n\n\n\n\n\n\napples\noranges\n\n\n\n\nDaniel\n3\n0\n\n\nSean\n2\n3\n\n\nPierce\n0\n7\n\n\nRoger\n1\n2"
  },
  {
    "objectID": "core/notebooks/notebook03_pandas.html#loc-versus-.iloc",
    "href": "core/notebooks/notebook03_pandas.html#loc-versus-.iloc",
    "title": "Introduction to pandas",
    "section": ".loc versus .iloc",
    "text": ".loc versus .iloc\n\n.loc locates by name\n.iloc locates by numerical index\n\n\n\nCode\ndf_fruits\n\n\n\n\n\n\n\n\n\napples\noranges\n\n\n\n\nDaniel\n3\n0\n\n\nSean\n2\n3\n\n\nPierce\n0\n7\n\n\nRoger\n1\n2\n\n\n\n\n\n\n\n\n\nCode\n# What's in Sean's basket ?\ndf_fruits.loc['Sean']\n\n\napples     2\noranges    3\nName: Sean, dtype: int64\n\n\n\n\nCode\n# Who has oranges ?\ndf_fruits.loc[:, 'oranges']\n\n\nDaniel    0\nSean      3\nPierce    7\nRoger     2\nName: oranges, dtype: int64\n\n\n\n\nCode\n# How many apples in Pierce's basket ?\ndf_fruits.loc['Pierce', 'apples']\n\n\nnp.int64(0)\n\n\n\n\nCode\ndf_fruits\n\n\n\n\n\n\n\n\n\napples\noranges\n\n\n\n\nDaniel\n3\n0\n\n\nSean\n2\n3\n\n\nPierce\n0\n7\n\n\nRoger\n1\n2\n\n\n\n\n\n\n\n\n\nCode\ndf_fruits.iloc[2, 1]\n\n\nnp.int64(7)"
  },
  {
    "objectID": "core/notebooks/notebook03_pandas.html#main-attributes-and-methods-of-a-dataframe",
    "href": "core/notebooks/notebook03_pandas.html#main-attributes-and-methods-of-a-dataframe",
    "title": "Introduction to pandas",
    "section": "Main attributes and methods of a DataFrame",
    "text": "Main attributes and methods of a DataFrame\nA DataFrame has many attributes\n\n\nCode\ndf_fruits.columns\n\n\nIndex(['apples', 'oranges'], dtype='object')\n\n\n\n\nCode\ndf_fruits.index\n\n\nIndex(['Daniel', 'Sean', 'Pierce', 'Roger'], dtype='object')\n\n\n\n\nCode\ndf_fruits.dtypes\n\n\napples     int64\noranges    int64\ndtype: object\n\n\nA DataFrame has many methods\n\n\nCode\ndf_fruits.info()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 4 entries, Daniel to Roger\nData columns (total 2 columns):\n #   Column   Non-Null Count  Dtype\n---  ------   --------------  -----\n 0   apples   4 non-null      int64\n 1   oranges  4 non-null      int64\ndtypes: int64(2)\nmemory usage: 268.0+ bytes\n\n\n\n\nCode\ndf_fruits.describe()\n\n\n\n\n\n\n\n\n\napples\noranges\n\n\n\n\ncount\n4.000000\n4.00000\n\n\nmean\n1.500000\n3.00000\n\n\nstd\n1.290994\n2.94392\n\n\nmin\n0.000000\n0.00000\n\n\n25%\n0.750000\n1.50000\n\n\n50%\n1.500000\n2.50000\n\n\n75%\n2.250000\n4.00000\n\n\nmax\n3.000000\n7.00000"
  },
  {
    "objectID": "core/notebooks/notebook03_pandas.html#missing-values",
    "href": "core/notebooks/notebook03_pandas.html#missing-values",
    "title": "Introduction to pandas",
    "section": "Missing values",
    "text": "Missing values\nWhat if we don’t know how many apples are in Sean’s basket ?\n\n\nCode\ndf_fruits.loc['Sean', 'apples'] = None\ndf_fruits\n\n\n\n\n\n\n\n\n\napples\noranges\n\n\n\n\nDaniel\n3.0\n0\n\n\nSean\nNaN\n3\n\n\nPierce\n0.0\n7\n\n\nRoger\n1.0\n2\n\n\n\n\n\n\n\n\n\nCode\ndf_fruits.describe()\n\n\n\n\n\n\n\n\n\napples\noranges\n\n\n\n\ncount\n3.000000\n4.00000\n\n\nmean\n1.333333\n3.00000\n\n\nstd\n1.527525\n2.94392\n\n\nmin\n0.000000\n0.00000\n\n\n25%\n0.500000\n1.50000\n\n\n50%\n1.000000\n2.50000\n\n\n75%\n2.000000\n4.00000\n\n\nmax\n3.000000\n7.00000\n\n\n\n\n\n\n\nNote that count is 3 for apples now, since we have 1 missing value among the 4\n\n\n\n\n\n\nNote\n\n\n\nTo review the members of objects of class pandas.DataFrame, dir() and module inspect are convenient.\n\n\n\n\nCode\n[x for x in dir(df_fruits) if not x.startswith('_') and not callable(x)]\n\n\n\n\nCode\nimport inspect\n\n# Get a list of methods\nmembres = inspect.getmembers(df_fruits)\n\nmethod_names = [m[0] for m in membres \n    if callable(m[1]) and not m[0].startswith('_')]\n\nprint(method_names)\n\n\n['abs', 'add', 'add_prefix', 'add_suffix', 'agg', 'aggregate', 'align', 'all', 'any', 'apply', 'applymap', 'asfreq', 'asof', 'assign', 'astype', 'at_time', 'backfill', 'between_time', 'bfill', 'bool', 'boxplot', 'clip', 'combine', 'combine_first', 'compare', 'convert_dtypes', 'copy', 'corr', 'corrwith', 'count', 'cov', 'cummax', 'cummin', 'cumprod', 'cumsum', 'describe', 'diff', 'div', 'divide', 'dot', 'drop', 'drop_duplicates', 'droplevel', 'dropna', 'duplicated', 'eq', 'equals', 'eval', 'ewm', 'expanding', 'explode', 'ffill', 'fillna', 'filter', 'first', 'first_valid_index', 'floordiv', 'from_dict', 'from_records', 'ge', 'get', 'groupby', 'gt', 'head', 'hist', 'idxmax', 'idxmin', 'iloc', 'infer_objects', 'info', 'insert', 'interpolate', 'isetitem', 'isin', 'isna', 'isnull', 'items', 'iterrows', 'itertuples', 'join', 'keys', 'kurt', 'kurtosis', 'last', 'last_valid_index', 'le', 'loc', 'lt', 'map', 'mask', 'max', 'mean', 'median', 'melt', 'memory_usage', 'merge', 'min', 'mod', 'mode', 'mul', 'multiply', 'ne', 'nlargest', 'notna', 'notnull', 'nsmallest', 'nunique', 'pad', 'pct_change', 'pipe', 'pivot', 'pivot_table', 'plot', 'pop', 'pow', 'prod', 'product', 'quantile', 'query', 'radd', 'rank', 'rdiv', 'reindex', 'reindex_like', 'rename', 'rename_axis', 'reorder_levels', 'replace', 'resample', 'reset_index', 'rfloordiv', 'rmod', 'rmul', 'rolling', 'round', 'rpow', 'rsub', 'rtruediv', 'sample', 'select_dtypes', 'sem', 'set_axis', 'set_flags', 'set_index', 'shift', 'skew', 'sort_index', 'sort_values', 'squeeze', 'stack', 'std', 'sub', 'subtract', 'sum', 'swapaxes', 'swaplevel', 'tail', 'take', 'to_clipboard', 'to_csv', 'to_dict', 'to_excel', 'to_feather', 'to_gbq', 'to_hdf', 'to_html', 'to_json', 'to_latex', 'to_markdown', 'to_numpy', 'to_orc', 'to_parquet', 'to_period', 'to_pickle', 'to_records', 'to_sql', 'to_stata', 'to_string', 'to_timestamp', 'to_xarray', 'to_xml', 'transform', 'transpose', 'truediv', 'truncate', 'tz_convert', 'tz_localize', 'unstack', 'update', 'value_counts', 'var', 'where', 'xs']\n\n\n\n\nCode\nothers = [x for x in membres\n    if not callable(x[1])]\n\n[x[0] for x in others if not x[0].startswith('_')]\n\n\n['T',\n 'apples',\n 'at',\n 'attrs',\n 'axes',\n 'columns',\n 'dtypes',\n 'empty',\n 'flags',\n 'iat',\n 'index',\n 'ndim',\n 'oranges',\n 'shape',\n 'size',\n 'style',\n 'values']"
  },
  {
    "objectID": "core/notebooks/notebook03_pandas.html#adding-a-column",
    "href": "core/notebooks/notebook03_pandas.html#adding-a-column",
    "title": "Introduction to pandas",
    "section": "Adding a column",
    "text": "Adding a column\nOoooops, we forgot about the bananas !\n\n\nCode\ndf_fruits[\"bananas\"] = [0, 2, 1, 6]\ndf_fruits\n\n\n\n\n\n\n\n\n\napples\noranges\nbananas\n\n\n\n\nDaniel\n3.0\n0\n0\n\n\nSean\nNaN\n3\n2\n\n\nPierce\n0.0\n7\n1\n\n\nRoger\n1.0\n2\n6"
  },
  {
    "objectID": "core/notebooks/notebook03_pandas.html#adding-a-column-with-the-date",
    "href": "core/notebooks/notebook03_pandas.html#adding-a-column-with-the-date",
    "title": "Introduction to pandas",
    "section": "Adding a column with the date",
    "text": "Adding a column with the date\nAnd we forgot the dates !\n\n\nCode\ndf_fruits['time'] = [\n    \"2020/10/08 12:13\", \"2020/10/07 11:37\", \n    \"2020/10/10 14:07\", \"2020/10/09 10:51\"\n]\ndf_fruits\n\n\n\n\n\n\n\n\n\napples\noranges\nbananas\ntime\n\n\n\n\nDaniel\n3.0\n0\n0\n2020/10/08 12:13\n\n\nSean\nNaN\n3\n2\n2020/10/07 11:37\n\n\nPierce\n0.0\n7\n1\n2020/10/10 14:07\n\n\nRoger\n1.0\n2\n6\n2020/10/09 10:51\n\n\n\n\n\n\n\n\n\nCode\ndf_fruits.dtypes\n\n\napples     float64\noranges      int64\nbananas      int64\ntime        object\ndtype: object\n\n\n\n\nCode\ntype(df_fruits.loc[\"Roger\", \"time\"])\n\n\nstr\n\n\nIt is not a date but a string (str) ! So we convert this column to something called datetime\n\n\nCode\ndf_fruits[\"time\"] = pd.to_datetime(df_fruits[\"time\"])\ndf_fruits\n\n\n\n\n\n\n\n\n\napples\noranges\nbananas\ntime\n\n\n\n\nDaniel\n3.0\n0\n0\n2020-10-08 12:13:00\n\n\nSean\nNaN\n3\n2\n2020-10-07 11:37:00\n\n\nPierce\n0.0\n7\n1\n2020-10-10 14:07:00\n\n\nRoger\n1.0\n2\n6\n2020-10-09 10:51:00\n\n\n\n\n\n\n\n\n\nCode\ndf_fruits.dtypes\n\n\napples            float64\noranges             int64\nbananas             int64\ntime       datetime64[ns]\ndtype: object\n\n\n\n\n\n\n\n\nNote\n\n\n\nEvery data science framework implements some datetime handling scheme. For Python see Python official documentation on datetime module\n\n\nWhat if we want to keep only the baskets after (including) October, 9th ?\n\n\nCode\ndf_fruits.loc[df_fruits[\"time\"] &gt;= pd.Timestamp(\"2020/10/09\")]\n\n\n\n\n\n\n\n\n\napples\noranges\nbananas\ntime\n\n\n\n\nPierce\n0.0\n7\n1\n2020-10-10 14:07:00\n\n\nRoger\n1.0\n2\n6\n2020-10-09 10:51:00"
  },
  {
    "objectID": "core/notebooks/notebook03_pandas.html#slices-and-subsets-of-rows-or-columns",
    "href": "core/notebooks/notebook03_pandas.html#slices-and-subsets-of-rows-or-columns",
    "title": "Introduction to pandas",
    "section": "Slices and subsets of rows or columns",
    "text": "Slices and subsets of rows or columns\n\n\nCode\ndf_fruits\n\n\n\n\n\n\n\n\n\napples\noranges\nbananas\ntime\n\n\n\n\nDaniel\n3.0\n0\n0\n2020-10-08 12:13:00\n\n\nSean\nNaN\n3\n2\n2020-10-07 11:37:00\n\n\nPierce\n0.0\n7\n1\n2020-10-10 14:07:00\n\n\nRoger\n1.0\n2\n6\n2020-10-09 10:51:00\n\n\n\n\n\n\n\n\n\nCode\ndf_fruits.loc[:, \"oranges\":\"time\"]\n\n\n\n\n\n\n\n\n\noranges\nbananas\ntime\n\n\n\n\nDaniel\n0\n0\n2020-10-08 12:13:00\n\n\nSean\n3\n2\n2020-10-07 11:37:00\n\n\nPierce\n7\n1\n2020-10-10 14:07:00\n\n\nRoger\n2\n6\n2020-10-09 10:51:00\n\n\n\n\n\n\n\n\n\nCode\ndf_fruits.loc[\"Daniel\":\"Sean\", \"apples\":\"bananas\"]\n\n\n\n\n\n\n\n\n\napples\noranges\nbananas\n\n\n\n\nDaniel\n3.0\n0\n0\n\n\nSean\nNaN\n3\n2\n\n\n\n\n\n\n\n\n\nCode\ndf_fruits[[\"apples\", \"time\"]]\n\n\n\n\n\n\n\n\n\napples\ntime\n\n\n\n\nDaniel\n3.0\n2020-10-08 12:13:00\n\n\nSean\nNaN\n2020-10-07 11:37:00\n\n\nPierce\n0.0\n2020-10-10 14:07:00\n\n\nRoger\n1.0\n2020-10-09 10:51:00"
  },
  {
    "objectID": "core/notebooks/notebook03_pandas.html#write-our-data-to-a-csv-file",
    "href": "core/notebooks/notebook03_pandas.html#write-our-data-to-a-csv-file",
    "title": "Introduction to pandas",
    "section": "Write our data to a CSV file",
    "text": "Write our data to a CSV file\nWhat if we want to write the file ?\n\n\nCode\ndf_fruits\n\n\n\n\n\n\n\n\n\napples\noranges\nbananas\ntime\n\n\n\n\nDaniel\n3.0\n0\n0\n2020-10-08 12:13:00\n\n\nSean\nNaN\n3\n2\n2020-10-07 11:37:00\n\n\nPierce\n0.0\n7\n1\n2020-10-10 14:07:00\n\n\nRoger\n1.0\n2\n6\n2020-10-09 10:51:00\n\n\n\n\n\n\n\n\n\nCode\ndf_fruits.to_csv(\"fruits.csv\")\n\n\n\n\nCode\n# Use !dir on windows\n!ls -alh | grep fru\n\n\n-rw-rw-r--  1 boucheron boucheron  163 févr.  3 23:20 fruits.csv\n\n\n\n\nCode\n!head -n 5 fruits.csv\n\n\n,apples,oranges,bananas,time\nDaniel,3.0,0,0,2020-10-08 12:13:00\nSean,,3,2,2020-10-07 11:37:00\nPierce,0.0,7,1,2020-10-10 14:07:00\nRoger,1.0,2,6,2020-10-09 10:51:00"
  },
  {
    "objectID": "core/notebooks/notebook03_pandas.html#reading-data-and-working-with-it",
    "href": "core/notebooks/notebook03_pandas.html#reading-data-and-working-with-it",
    "title": "Introduction to pandas",
    "section": "Reading data and working with it",
    "text": "Reading data and working with it\n\n\n\n\n\n\nNote\n\n\n\nThe tips dataset comes through Kaggle\n\nThis dataset is a treasure trove of information from a collection of case studies for business statistics. Special thanks to Bryant and Smith for their diligent work:\n\n\nBryant, P. G. and Smith, M (1995) Practical Data Analysis: Case Studies in Business Statistics. Homewood, IL: Richard D. Irwin Publishing.\n\n\nYou can also access this dataset now through the Python package Seaborn.\n\n\n\nIt contains data about a restaurant: the bill, tip and some informations about the customers.\n\n\n\n\n\n\nA toy extraction pattern\n\n\n\nA data pipeline usually starts with Extraction, that is gathering data from some source, possibly in a galaxy far, far awy. Here follows a toy extraction pattern\n\nobtain the data from some URL using package requests\nsave the data on the hard drive\nload the data using Pandas\n\n\n\nCode\nimport requests\nimport os\n\n# The path containing your notebook\npath_data = './'\n# The name of the file\nfilename = 'tips.csv'\n\nif os.path.exists(os.path.join(path_data, filename)):\n    print('The file %s already exists.' % os.path.join(path_data, filename))\nelse:\n    url = 'https://raw.githubusercontent.com/mwaskom/seaborn-data/refs/heads/master/tips.csv'\n    r = requests.get(url)\n    with open(os.path.join(path_data, filename), 'wb') as f:\n        f.write(r.content)\n    print('Downloaded file %s.' % os.path.join(path_data, filename))\n\n\n\n\nCode\ndf = pd.read_csv(\n    \"tips.csv\", \n    delimiter=\",\"\n)\n\n\n\n\nThe data can be obtained from package seaborn.\n\n\nCode\nimport seaborn as sns\n\nsns_ds = sns.get_dataset_names()\n\n'tips' in sns_ds\n\ndf = sns.load_dataset('tips')\n\n\n\n\nCode\n# `.head()` shows the first rows of the dataframe\ndf.head(n=10)\n\n\n\n\n\n\n\n\n\ntotal_bill\ntip\nsex\nsmoker\nday\ntime\nsize\n\n\n\n\n0\n16.99\n1.01\nFemale\nNo\nSun\nDinner\n2\n\n\n1\n10.34\n1.66\nMale\nNo\nSun\nDinner\n3\n\n\n2\n21.01\n3.50\nMale\nNo\nSun\nDinner\n3\n\n\n3\n23.68\n3.31\nMale\nNo\nSun\nDinner\n2\n\n\n4\n24.59\n3.61\nFemale\nNo\nSun\nDinner\n4\n\n\n5\n25.29\n4.71\nMale\nNo\nSun\nDinner\n4\n\n\n6\n8.77\n2.00\nMale\nNo\nSun\nDinner\n2\n\n\n7\n26.88\n3.12\nMale\nNo\nSun\nDinner\n4\n\n\n8\n15.04\n1.96\nMale\nNo\nSun\nDinner\n2\n\n\n9\n14.78\n3.23\nMale\nNo\nSun\nDinner\n2\n\n\n\n\n\n\n\n\n\nCode\ndf.info()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 244 entries, 0 to 243\nData columns (total 7 columns):\n #   Column      Non-Null Count  Dtype   \n---  ------      --------------  -----   \n 0   total_bill  244 non-null    float64 \n 1   tip         244 non-null    float64 \n 2   sex         244 non-null    category\n 3   smoker      244 non-null    category\n 4   day         244 non-null    category\n 5   time        244 non-null    category\n 6   size        244 non-null    int64   \ndtypes: category(4), float64(2), int64(1)\nmemory usage: 7.4 KB\n\n\n\n\nCode\ndf.loc[42, \"day\"]\n\n\n'Sun'\n\n\n\n\nCode\ntype(df.loc[42, \"day\"])\n\n\nstr\n\n\nBy default, columns that are non-numerical contain strings (str type)"
  },
  {
    "objectID": "core/notebooks/notebook03_pandas.html#the-category-type",
    "href": "core/notebooks/notebook03_pandas.html#the-category-type",
    "title": "Introduction to pandas",
    "section": "The category type",
    "text": "The category type\nAn important type in pandas is category for variables that are non-numerical\nPro tip. It’s always a good idea to tell pandas which columns should be imported as categorical\nSo, let’s read again the file specifying some dtypes to the read_csv function\n\n\nCode\ndtypes = {\n    \"sex\": \"category\",\n    \"smoker\": \"category\",\n    \"day\": \"category\",\n    \"time\": \"category\"\n} \n\ndf = pd.read_csv(\"tips.csv\", dtype=dtypes)\n\n\n\n\nCode\ndf.dtypes\n\n\ntotal_bill     float64\ntip            float64\nsex           category\nsmoker        category\nday           category\ntime          category\nsize             int64\ndtype: object"
  },
  {
    "objectID": "core/notebooks/notebook03_pandas.html#computing-statistics",
    "href": "core/notebooks/notebook03_pandas.html#computing-statistics",
    "title": "Introduction to pandas",
    "section": "Computing statistics",
    "text": "Computing statistics\n\n\nCode\n# The describe method only shows statistics for the numerical columns by default\ndf.describe()\n\n\n\n\n\n\n\n\n\ntotal_bill\ntip\nsize\n\n\n\n\ncount\n244.000000\n244.000000\n244.000000\n\n\nmean\n19.785943\n2.998279\n2.569672\n\n\nstd\n8.902412\n1.383638\n0.951100\n\n\nmin\n3.070000\n1.000000\n1.000000\n\n\n25%\n13.347500\n2.000000\n2.000000\n\n\n50%\n17.795000\n2.900000\n2.000000\n\n\n75%\n24.127500\n3.562500\n3.000000\n\n\nmax\n50.810000\n10.000000\n6.000000\n\n\n\n\n\n\n\n\n\nCode\n# We use the include=\"all\" option to see everything\ndf.describe(include=\"all\")\n\n\n\n\n\n\n\n\n\ntotal_bill\ntip\nsex\nsmoker\nday\ntime\nsize\n\n\n\n\ncount\n244.000000\n244.000000\n244\n244\n244\n244\n244.000000\n\n\nunique\nNaN\nNaN\n2\n2\n4\n2\nNaN\n\n\ntop\nNaN\nNaN\nMale\nNo\nSat\nDinner\nNaN\n\n\nfreq\nNaN\nNaN\n157\n151\n87\n176\nNaN\n\n\nmean\n19.785943\n2.998279\nNaN\nNaN\nNaN\nNaN\n2.569672\n\n\nstd\n8.902412\n1.383638\nNaN\nNaN\nNaN\nNaN\n0.951100\n\n\nmin\n3.070000\n1.000000\nNaN\nNaN\nNaN\nNaN\n1.000000\n\n\n25%\n13.347500\n2.000000\nNaN\nNaN\nNaN\nNaN\n2.000000\n\n\n50%\n17.795000\n2.900000\nNaN\nNaN\nNaN\nNaN\n2.000000\n\n\n75%\n24.127500\n3.562500\nNaN\nNaN\nNaN\nNaN\n3.000000\n\n\nmax\n50.810000\n10.000000\nNaN\nNaN\nNaN\nNaN\n6.000000\n\n\n\n\n\n\n\n\n\nCode\n# Correlation between the numerical columns\ndf.corr(numeric_only = True)\n\n\n\n\n\n\n\n\n\ntotal_bill\ntip\nsize\n\n\n\n\ntotal_bill\n1.000000\n0.675734\n0.598315\n\n\ntip\n0.675734\n1.000000\n0.489299\n\n\nsize\n0.598315\n0.489299\n1.000000\n\n\n\n\n\n\n\n\n\nCode\n?df.corr"
  },
  {
    "objectID": "core/notebooks/notebook03_pandas.html#how-do-the-tip-depends-on-the-total-bill",
    "href": "core/notebooks/notebook03_pandas.html#how-do-the-tip-depends-on-the-total-bill",
    "title": "Introduction to pandas",
    "section": "How do the tip depends on the total bill ?",
    "text": "How do the tip depends on the total bill ?\n\n\nCode\nsns.jointplot(x=\"total_bill\", y=\"tip\", data=df)"
  },
  {
    "objectID": "core/notebooks/notebook03_pandas.html#when-do-customers-go-to-this-restaurant",
    "href": "core/notebooks/notebook03_pandas.html#when-do-customers-go-to-this-restaurant",
    "title": "Introduction to pandas",
    "section": "When do customers go to this restaurant ?",
    "text": "When do customers go to this restaurant ?\n\n\nCode\nsns.countplot(x='day', hue=\"time\", data=df)"
  },
  {
    "objectID": "core/notebooks/notebook03_pandas.html#when-do-customers-spend-the-most",
    "href": "core/notebooks/notebook03_pandas.html#when-do-customers-spend-the-most",
    "title": "Introduction to pandas",
    "section": "When do customers spend the most ?",
    "text": "When do customers spend the most ?\n\n\nCode\nplt.figure(figsize=(7, 5))\nsns.boxplot(x='day', y='total_bill', hue='time', data=df)\nplt.legend(loc=\"upper left\")\n\n\n\n\n\n\n\n\n\n\n\nCode\nplt.figure(figsize=(7, 5))\nsns.violinplot(x='day', y='total_bill', hue='time', split=True, data=df)\nplt.legend(loc=\"upper left\")"
  },
  {
    "objectID": "core/notebooks/notebook03_pandas.html#who-spends-the-most",
    "href": "core/notebooks/notebook03_pandas.html#who-spends-the-most",
    "title": "Introduction to pandas",
    "section": "Who spends the most ?",
    "text": "Who spends the most ?\n\n\nCode\nsns.boxplot(x='sex', y='total_bill', hue='smoker', data=df)"
  },
  {
    "objectID": "core/notebooks/notebook03_pandas.html#when-should-waiters-want-to-work",
    "href": "core/notebooks/notebook03_pandas.html#when-should-waiters-want-to-work",
    "title": "Introduction to pandas",
    "section": "When should waiters want to work ?",
    "text": "When should waiters want to work ?\n\n\nCode\nsns.boxplot(x='day', y='tip', hue='time', data=df)\n\n\n\n\n\n\n\n\n\n\n\nCode\nsns.violinplot(x='day', y='tip', hue='time', data=df)"
  },
  {
    "objectID": "core/notebooks/notebook03_pandas.html#computations-using-pandas-broadcasting",
    "href": "core/notebooks/notebook03_pandas.html#computations-using-pandas-broadcasting",
    "title": "Introduction to pandas",
    "section": "Computations using pandas : broadcasting",
    "text": "Computations using pandas : broadcasting\nLet’s add a column that contains the tip percentage\n\n\nCode\ndf[\"tip_percentage\"] = df[\"tip\"] / df[\"total_bill\"]\ndf.head()\n\n\n\n\n\n\n\n\n\ntotal_bill\ntip\nsex\nsmoker\nday\ntime\nsize\ntip_percentage\n\n\n\n\n0\n16.99\n1.01\nFemale\nNo\nSun\nDinner\n2\n0.059447\n\n\n1\n10.34\n1.66\nMale\nNo\nSun\nDinner\n3\n0.160542\n\n\n2\n21.01\n3.50\nMale\nNo\nSun\nDinner\n3\n0.166587\n\n\n3\n23.68\n3.31\nMale\nNo\nSun\nDinner\n2\n0.139780\n\n\n4\n24.59\n3.61\nFemale\nNo\nSun\nDinner\n4\n0.146808\n\n\n\n\n\n\n\nThe computation\n```{python}\ndf[\"tip\"] / df[\"total_bill\"]\n```\nuses a broadcast rule.\n\nWe can multiply, add, subtract, etc. together numpy arrays, Series or pandas dataframes when the computation makes sense in view of their respective shape\n\nThis principle is called broadcast or broadcasting.\n\n\n\n\n\n\nNote\n\n\n\nBroadcasting is a key feature of numpy ndarray, see\n\nNumpy User’s guide\nPandas book\n\n\n\n\n\nCode\ndf[\"tip\"].shape, df[\"total_bill\"].shape\n\n\n((244,), (244,))\n\n\nThe tip and total_billcolumns have the same shape, so broadcasting performs pairwise division.\nThis corresponds to the following “hand-crafted” approach with a for loop:\n\n\nCode\nfor i in range(df.shape[0]):\n    df.loc[i, \"tip_percentage\"] = df.loc[i, \"tip\"] / df.loc[i, \"total_bill\"]\n\n\nBut using such a loop is:\n\nlonger to write\nless readable\nprone to mistakes\nand slower :(\n\nNEVER use Python for-loops unless you need to !\n\n\nCode\n%%timeit -n 10\nfor i in range(df.shape[0]):\n    df.loc[i, \"tip_percentage\"] = df.loc[i, \"tip\"] / df.loc[i, \"total_bill\"]\n\n\n22.9 ms ± 45.7 μs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n\n\n\n\nCode\n%%timeit -n 10\ndf[\"tip_percentage\"] = df[\"tip\"] / df[\"total_bill\"]\n\n\n69.2 μs ± 11.8 μs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n\n\nThe for loop is \\(\\approx\\) 100 times slower ! (even worse on larger data)\n\nPitfall. Changing values in a DataFrame\nWhen you want to change a value in a DataFrame, never use\ndf[\"tip_percentage\"].loc[i] = 42\nbut use\ndf.loc[i, \"tip_percentage\"] = 42\n\n\n\n\n\n\nCaution\n\n\n\nUse a single loc or iloc statement. The first version might not work: it might modify a copy of the column and not the dataframe itself !\n\n\nAnother example of broadcasting is:\n\n\nCode\n(100 * df[[\"tip_percentage\"]]).head()\n\n\n\n\n\n\n\n\n\ntip_percentage\n\n\n\n\n0\n5.944673\n\n\n1\n16.054159\n\n\n2\n16.658734\n\n\n3\n13.978041\n\n\n4\n14.680765\n\n\n\n\n\n\n\nwhere we multiplied each entry of the tip_percentage column by 100.\n\n\n\n\n\n\nRemark\n\n\n\nNote the difference between\ndf[['tip_percentage']]\nwhich returns a DataFrame containing only the tip_percentage column and\ndf['tip_percentage']\nwhich returns a Series containing the data of the tip_percentage column"
  },
  {
    "objectID": "core/notebooks/notebook03_pandas.html#some-more-plots",
    "href": "core/notebooks/notebook03_pandas.html#some-more-plots",
    "title": "Introduction to pandas",
    "section": "Some more plots",
    "text": "Some more plots\n\nHow do the tip percentages relates to the total bill ?\n\n\nCode\nsns.jointplot(\n    x=\"total_bill\", \n    y=\"tip_percentage\", \n    data=df\n)\n\n\n\n\n\n\n\n\n\n\n\nWho tips best ?\n\n\nCode\nsns.boxplot(\n    x='sex', \n    y='tip_percentage', \n    hue='smoker', \n    data=df\n)\n\n\n\n\n\n\n\n\n\n\n\nWho tips best without the tip_percentage outliers ?\n\n\nCode\nsns.boxplot(\n    x='sex', \n    y='tip_percentage', \n    hue='smoker', \n    data=df.loc[df[\"tip_percentage\"] &lt;= 0.3]\n)\n\n\n\n\n\n\n\n\n\nObject identity\n\n\nCode\nid(df)\n\n\n124919428343584"
  },
  {
    "objectID": "core/notebooks/notebook03_pandas.html#the-all-mighty-groupby-and-aggregate",
    "href": "core/notebooks/notebook03_pandas.html#the-all-mighty-groupby-and-aggregate",
    "title": "Introduction to pandas",
    "section": "The all-mighty groupby and aggregate",
    "text": "The all-mighty groupby and aggregate\nMany computations can be formulated as a groupby followed by and aggregation.\n\nWhat is the mean tip and tip percentage each day ?\n\n\nCode\ndf.head()\n\n\n\n\n\n\n\n\n\ntotal_bill\ntip\nsex\nsmoker\nday\ntime\nsize\ntip_percentage\n\n\n\n\n0\n16.99\n1.01\nFemale\nNo\nSun\nDinner\n2\n0.059447\n\n\n1\n10.34\n1.66\nMale\nNo\nSun\nDinner\n3\n0.160542\n\n\n2\n21.01\n3.50\nMale\nNo\nSun\nDinner\n3\n0.166587\n\n\n3\n23.68\n3.31\nMale\nNo\nSun\nDinner\n2\n0.139780\n\n\n4\n24.59\n3.61\nFemale\nNo\nSun\nDinner\n4\n0.146808\n\n\n\n\n\n\n\n\n\nCode\ntry:\n\n    df.groupby(\"day\", observed=True).mean()\nexcept TypeError:\n    print('TypeError: category dtype does not support aggregation \"mean\"')\n\n\nTypeError: category dtype does not support aggregation \"mean\"\n\n\nBut we do not care about the size column here, so we can use instead\n\n\nCode\n(\n    df[[\"total_bill\", \"tip\", \"tip_percentage\", \"day\"]]\n        .groupby(\"day\")\n        .mean()\n)\n\n\n/tmp/ipykernel_93138/1740663163.py:3: FutureWarning:\n\nThe default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n\n\n\n\n\n\n\n\n\n\ntotal_bill\ntip\ntip_percentage\n\n\nday\n\n\n\n\n\n\n\nFri\n17.151579\n2.734737\n0.169913\n\n\nSat\n20.441379\n2.993103\n0.153152\n\n\nSun\n21.410000\n3.255132\n0.166897\n\n\nThur\n17.682742\n2.771452\n0.161276\n\n\n\n\n\n\n\nIf we want to be more precise, we can groupby using several columns\n\n\nCode\n(\n    df[[\"total_bill\", \"tip\", \"tip_percentage\", \"day\", \"time\"]]   # selection\n        .groupby([\"day\",\"time\"])                                # partition\n        .mean()                                                  # aggregation\n)\n\n\n/tmp/ipykernel_93138/391063870.py:3: FutureWarning:\n\nThe default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n\n\n\n\n\n\n\n\n\n\n\ntotal_bill\ntip\ntip_percentage\n\n\nday\ntime\n\n\n\n\n\n\n\nFri\nDinner\n19.663333\n2.940000\n0.158916\n\n\nLunch\n12.845714\n2.382857\n0.188765\n\n\nSat\nDinner\n20.441379\n2.993103\n0.153152\n\n\nLunch\nNaN\nNaN\nNaN\n\n\nSun\nDinner\n21.410000\n3.255132\n0.166897\n\n\nLunch\nNaN\nNaN\nNaN\n\n\nThur\nDinner\n18.780000\n3.000000\n0.159744\n\n\nLunch\n17.664754\n2.767705\n0.161301\n\n\n\n\n\n\n\n\n\n\n\n\n\nRemarks\n\n\n\n\nWe obtain a DataFrame with a two-level indexing: on the day and the time\nGroups must be homogeneous: we have NaN values for empty groups (e.g. Sat, Lunch)\n\n\n\n\n\nPro tip\nSometimes, it is more convenient to get the groups as columns instead of a multi-level index.\nFor this, use reset_index:\n\n\nCode\n(\n    df[[\"total_bill\", \"tip\", \"tip_percentage\", \"day\", \"time\"]]   # selection\n        .groupby([\"day\", \"time\"])                                # partition\n        .mean() # aggregation\n        .reset_index()   # ako ungroup\n)\n\n\n/tmp/ipykernel_93138/835267922.py:3: FutureWarning:\n\nThe default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n\n\n\n\n\n\n\n\n\n\nday\ntime\ntotal_bill\ntip\ntip_percentage\n\n\n\n\n0\nFri\nDinner\n19.663333\n2.940000\n0.158916\n\n\n1\nFri\nLunch\n12.845714\n2.382857\n0.188765\n\n\n2\nSat\nDinner\n20.441379\n2.993103\n0.153152\n\n\n3\nSat\nLunch\nNaN\nNaN\nNaN\n\n\n4\nSun\nDinner\n21.410000\n3.255132\n0.166897\n\n\n5\nSun\nLunch\nNaN\nNaN\nNaN\n\n\n6\nThur\nDinner\n18.780000\n3.000000\n0.159744\n\n\n7\nThur\nLunch\n17.664754\n2.767705\n0.161301\n\n\n\n\n\n\n\n\n\nAnother pro tip: care about code readers\nComputations with pandas can include many operations that are pipelined until the final computation.\nPipelining many operations is good practice and perfectly normal, but in order to make the code readable you can put it between parenthesis (python expression) as follows:\n\n\nCode\n(\n    df[[\"total_bill\", \"tip\", \"tip_percentage\", \"day\", \"time\"]]\n    .groupby([\"day\", \"time\"])\n    .mean()\n    .reset_index()\n    # and on top of all this we sort the dataframe with respect \n    # to the tip_percentage\n    .sort_values(\"tip_percentage\")\n)\n\n\n/tmp/ipykernel_93138/45053252.py:3: FutureWarning:\n\nThe default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n\n\n\n\n\n\n\n\n\n\nday\ntime\ntotal_bill\ntip\ntip_percentage\n\n\n\n\n2\nSat\nDinner\n20.441379\n2.993103\n0.153152\n\n\n0\nFri\nDinner\n19.663333\n2.940000\n0.158916\n\n\n6\nThur\nDinner\n18.780000\n3.000000\n0.159744\n\n\n7\nThur\nLunch\n17.664754\n2.767705\n0.161301\n\n\n4\nSun\nDinner\n21.410000\n3.255132\n0.166897\n\n\n1\nFri\nLunch\n12.845714\n2.382857\n0.188765\n\n\n3\nSat\nLunch\nNaN\nNaN\nNaN\n\n\n5\nSun\nLunch\nNaN\nNaN\nNaN"
  },
  {
    "objectID": "core/notebooks/notebook03_pandas.html#displaying-a-dataframe-with-style",
    "href": "core/notebooks/notebook03_pandas.html#displaying-a-dataframe-with-style",
    "title": "Introduction to pandas",
    "section": "Displaying a DataFrame with style",
    "text": "Displaying a DataFrame with style\nNow, we can answer, with style, to the question: what are the average tip percentages along the week ?\n\n\nCode\n(\n    df[[\"tip_percentage\", \"day\", \"time\"]]\n    .groupby([\"day\", \"time\"])\n    .mean()\n    # At the end of the pipeline you can use .style\n    .style\n    # Print numerical values as percentages \n    .format(\"{:.2%}\")\n    .background_gradient()\n)\n\n\n/tmp/ipykernel_93138/838795167.py:3: FutureWarning:\n\nThe default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n\n\n\n\n\n\n\n\n \n \ntip_percentage\n\n\nday\ntime\n \n\n\n\n\nFri\nDinner\n15.89%\n\n\nLunch\n18.88%\n\n\nSat\nDinner\n15.32%\n\n\nLunch\nnan%\n\n\nSun\nDinner\n16.69%\n\n\nLunch\nnan%\n\n\nThur\nDinner\n15.97%\n\n\nLunch\n16.13%"
  },
  {
    "objectID": "core/notebooks/notebook03_pandas.html#removing-the-nan-values",
    "href": "core/notebooks/notebook03_pandas.html#removing-the-nan-values",
    "title": "Introduction to pandas",
    "section": "Removing the NaN values",
    "text": "Removing the NaN values\nBut the NaN values are somewhat annoying. Let’s remove them\n\n\nCode\n(\n    df[[\"tip_percentage\", \"day\", \"time\"]]\n    .groupby([\"day\", \"time\"])\n    .mean()\n    # We just add this from the previous pipeline\n    .dropna()\n    .style\n    .format(\"{:.2%}\")\n    .background_gradient()\n)\n\n\n/tmp/ipykernel_93138/2662169510.py:3: FutureWarning:\n\nThe default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n\n\n\n\n\n\n\n\n \n \ntip_percentage\n\n\nday\ntime\n \n\n\n\n\nFri\nDinner\n15.89%\n\n\nLunch\n18.88%\n\n\nSat\nDinner\n15.32%\n\n\nSun\nDinner\n16.69%\n\n\nThur\nDinner\n15.97%\n\n\nLunch\n16.13%\n\n\n\n\n\nNow, we see when tip_percentage is maximal. But what about the standard deviation?\n\nWe used only .mean() for now, but we can use several aggregating function using .agg()\n\n\n\nCode\n(\n    df[[\"tip_percentage\", \"day\", \"time\"]]\n    .groupby([\"day\", \"time\"])\n    .agg([\"mean\", \"std\"])   # we feed `agg`  with a list of names of callables \n    .dropna()\n    .style\n    .format(\"{:.2%}\")\n    .background_gradient()\n)\n\n\n/tmp/ipykernel_93138/3957220442.py:3: FutureWarning:\n\nThe default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n\n\n\n\n\n\n\n\n \n \ntip_percentage\n\n\n \n \nmean\nstd\n\n\nday\ntime\n \n \n\n\n\n\nFri\nDinner\n15.89%\n4.70%\n\n\nLunch\n18.88%\n4.59%\n\n\nSat\nDinner\n15.32%\n5.13%\n\n\nSun\nDinner\n16.69%\n8.47%\n\n\nThur\nLunch\n16.13%\n3.90%\n\n\n\n\n\nAnd we can use also .describe() as aggregation function. Moreover we - use the subset option to specify which column we want to style - we use (\"tip_percentage\", \"count\") to access multi-level index\n\n\nCode\n(\n    df[[\"tip_percentage\", \"day\", \"time\"]]\n    .groupby([\"day\", \"time\"])\n    .describe()    # all-purpose summarising function\n)\n\n\n/tmp/ipykernel_93138/3924876303.py:3: FutureWarning:\n\nThe default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n\n\n\n\n\n\n\n\n\n\n\ntip_percentage\n\n\n\n\ncount\nmean\nstd\nmin\n25%\n50%\n75%\nmax\n\n\nday\ntime\n\n\n\n\n\n\n\n\n\n\n\n\nFri\nDinner\n12.0\n0.158916\n0.047024\n0.103555\n0.123613\n0.144742\n0.179199\n0.263480\n\n\nLunch\n7.0\n0.188765\n0.045885\n0.117735\n0.167289\n0.187735\n0.210996\n0.259314\n\n\nSat\nDinner\n87.0\n0.153152\n0.051293\n0.035638\n0.123863\n0.151832\n0.188271\n0.325733\n\n\nSun\nDinner\n76.0\n0.166897\n0.084739\n0.059447\n0.119982\n0.161103\n0.187889\n0.710345\n\n\nThur\nDinner\n1.0\n0.159744\nNaN\n0.159744\n0.159744\n0.159744\n0.159744\n0.159744\n\n\nLunch\n61.0\n0.161301\n0.038972\n0.072961\n0.137741\n0.153846\n0.193424\n0.266312\n\n\n\n\n\n\n\n\n\nCode\n(\n    df[[\"tip_percentage\", \"day\", \"time\"]]\n    .groupby([\"day\", \"time\"])\n    .describe()\n    .dropna()\n    .style\n    .bar(subset=[(\"tip_percentage\", \"count\")])\n    .background_gradient(subset=[(\"tip_percentage\", \"50%\")])\n)\n\n\n/tmp/ipykernel_93138/673231177.py:3: FutureWarning:\n\nThe default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n\n\n\n\n\n\n\n\n \n \ntip_percentage\n\n\n \n \ncount\nmean\nstd\nmin\n25%\n50%\n75%\nmax\n\n\nday\ntime\n \n \n \n \n \n \n \n \n\n\n\n\nFri\nDinner\n12.000000\n0.158916\n0.047024\n0.103555\n0.123613\n0.144742\n0.179199\n0.263480\n\n\nLunch\n7.000000\n0.188765\n0.045885\n0.117735\n0.167289\n0.187735\n0.210996\n0.259314\n\n\nSat\nDinner\n87.000000\n0.153152\n0.051293\n0.035638\n0.123863\n0.151832\n0.188271\n0.325733\n\n\nSun\nDinner\n76.000000\n0.166897\n0.084739\n0.059447\n0.119982\n0.161103\n0.187889\n0.710345\n\n\nThur\nLunch\n61.000000\n0.161301\n0.038972\n0.072961\n0.137741\n0.153846\n0.193424\n0.266312"
  },
  {
    "objectID": "core/notebooks/notebook03_pandas.html#supervised-learning-of-tip-based-on-the-total_bill",
    "href": "core/notebooks/notebook03_pandas.html#supervised-learning-of-tip-based-on-the-total_bill",
    "title": "Introduction to pandas",
    "section": "Supervised learning of tip based on the total_bill",
    "text": "Supervised learning of tip based on the total_bill\nAs an example of very simple machine-learning problem, let us try to understand how we can predict tip based on total_bill.\n\n\nCode\nimport numpy as np\n\nplt.scatter(df[\"total_bill\"], df[\"tip\"])\nplt.xlabel(\"total_bill\", fontsize=12)\nplt.ylabel(\"tip\", fontsize=12)\n\n\nText(0, 0.5, 'tip')\n\n\n\n\n\n\n\n\n\nThere’s a rough linear dependence between the two. Let us try to find it by hand! Namely, we look for numbers \\(b\\) and \\(w\\) such that\ntip ≈ b + w × total_bill\nfor all the examples of pairs of (tip, total_bill) we observe in the data.\nIn machine learning, we say that this is a very simple example of a supervised learning problem (here it is a regression problem), where tip is the label and where total_bill is the (only) feature, for which we intend to use a linear predictor.\n\n\nCode\nplt.scatter(df[\"total_bill\"], df[\"tip\"])\nplt.xlabel(\"total_bill\", fontsize=12)\nplt.ylabel(\"tip\", fontsize=12)\n\nslope = 1.0\nintercept = 0.0\n\nx = np.linspace(0, 50, 1000)\nplt.plot(x, intercept + slope * x, color=\"red\")\n\n\n\n\n\n\n\n\n\n\nA more interactive way\nThis might require\n\n\nCode\n# !pip install ipympl\n\n\n\n\nCode\nimport ipywidgets as widgets\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n%matplotlib widget\n%matplotlib inline\n\nx = np.linspace(0, 50, 1000)\n\n@widgets.interact(intercept=(-5, 5, 1.), slope=(0, 1, .05))\ndef update(intercept=0.0, slope=0.5):\n    plt.scatter(df[\"total_bill\"], df[\"tip\"])\n    plt.plot(x, intercept + slope * x, color=\"red\")\n    plt.xlim((0, 50))\n    plt.ylim((0, 10))\n    plt.xlabel(\"total_bill\", fontsize=12)\n    plt.ylabel(\"tip\", fontsize=12)\n\n\n\n\n\n\n\n\n\n\n\n\nThis is kind of tedious to do this by hand… it would be nice to come up with an automated way of doing this. Moreover:\n\nWe are using a linear function, while something more complicated (such as a polynomial) might be better\nMore importantly, we use only the total_bill column to predict the tip, while we know about many other things\n\n\n\nCode\ndf.head()\n\n\n\n\n\n\n\n\n\ntotal_bill\ntip\nsex\nsmoker\nday\ntime\nsize\ntip_percentage\n\n\n\n\n0\n16.99\n1.01\nFemale\nNo\nSun\nDinner\n2\n0.059447\n\n\n1\n10.34\n1.66\nMale\nNo\nSun\nDinner\n3\n0.160542\n\n\n2\n21.01\n3.50\nMale\nNo\nSun\nDinner\n3\n0.166587\n\n\n3\n23.68\n3.31\nMale\nNo\nSun\nDinner\n2\n0.139780\n\n\n4\n24.59\n3.61\nFemale\nNo\nSun\nDinner\n4\n0.146808"
  },
  {
    "objectID": "core/notebooks/notebook03_pandas.html#one-hot-encoding-of-categorical-variables",
    "href": "core/notebooks/notebook03_pandas.html#one-hot-encoding-of-categorical-variables",
    "title": "Introduction to pandas",
    "section": "One-hot encoding of categorical variables",
    "text": "One-hot encoding of categorical variables\nWe can’t perform computations (products and sums) with columns containing categorical variables. So, we can’t use them like this to predict the tip. We need to convert them to numbers somehow.\nThe most classical approach for this is one-hot encoding (or “create dummies” or “binarize”) of the categorical variables, which can be easily achieved with pandas.get_dummies\nWhy one-hot ? See wikipedia for a plausible explanation\n\n\nCode\ndf_one_hot = pd.get_dummies(df, prefix_sep='#')\ndf_one_hot.head(5)\n\n\n\n\n\n\n\n\n\ntotal_bill\ntip\nsize\ntip_percentage\nsex#Female\nsex#Male\nsmoker#No\nsmoker#Yes\nday#Fri\nday#Sat\nday#Sun\nday#Thur\ntime#Dinner\ntime#Lunch\n\n\n\n\n0\n16.99\n1.01\n2\n0.059447\nTrue\nFalse\nTrue\nFalse\nFalse\nFalse\nTrue\nFalse\nTrue\nFalse\n\n\n1\n10.34\n1.66\n3\n0.160542\nFalse\nTrue\nTrue\nFalse\nFalse\nFalse\nTrue\nFalse\nTrue\nFalse\n\n\n2\n21.01\n3.50\n3\n0.166587\nFalse\nTrue\nTrue\nFalse\nFalse\nFalse\nTrue\nFalse\nTrue\nFalse\n\n\n3\n23.68\n3.31\n2\n0.139780\nFalse\nTrue\nTrue\nFalse\nFalse\nFalse\nTrue\nFalse\nTrue\nFalse\n\n\n4\n24.59\n3.61\n4\n0.146808\nTrue\nFalse\nTrue\nFalse\nFalse\nFalse\nTrue\nFalse\nTrue\nFalse\n\n\n\n\n\n\n\nOnly the categorical columns have been one-hot encoded. For instance, the \"day\" column is replaced by 4 columns named \"day#Thur\", \"day#Fri\", \"day#Sat\", \"day#Sun\", since \"day\" has 4 modalities (see next line).\n\n\nCode\ndf['day'].unique()\n\n\n['Sun', 'Sat', 'Thur', 'Fri']\nCategories (4, object): ['Fri', 'Sat', 'Sun', 'Thur']\n\n\n\n\nCode\ndf_one_hot.dtypes\n\n\ntotal_bill        float64\ntip               float64\nsize                int64\ntip_percentage    float64\nsex#Female           bool\nsex#Male             bool\nsmoker#No            bool\nsmoker#Yes           bool\nday#Fri              bool\nday#Sat              bool\nday#Sun              bool\nday#Thur             bool\ntime#Dinner          bool\ntime#Lunch           bool\ndtype: object"
  },
  {
    "objectID": "core/notebooks/notebook03_pandas.html#pitfall.-colinearities-with-one-hot-encoding",
    "href": "core/notebooks/notebook03_pandas.html#pitfall.-colinearities-with-one-hot-encoding",
    "title": "Introduction to pandas",
    "section": "Pitfall. Colinearities with one-hot encoding",
    "text": "Pitfall. Colinearities with one-hot encoding\nSums over dummies for sex, smoker, day, time and size are all equal to one (by constrution of the one-hot encoded vectors).\n\nLeads to colinearities in the matrix of features\nIt is much harder to train a linear regressor when the columns of the features matrix has colinearities\n\n\n\nCode\nday_cols = [col for col in df_one_hot.columns if col.startswith(\"day\")]\ndf_one_hot[day_cols].head()\ndf_one_hot[day_cols].sum(axis=1)\n\n\n0      1\n1      1\n2      1\n3      1\n4      1\n      ..\n239    1\n240    1\n241    1\n242    1\n243    1\nLength: 244, dtype: int64\n\n\n\n\nCode\nall(df_one_hot[day_cols].sum(axis=1) == 1)\n\n\nTrue\n\n\nThe most standard solution is to remove a modality (i.e. remove a one-hot encoding vector). Simply achieved by specifying drop_first=True in the get_dummies function.\n\n\nCode\ndf[\"day\"].unique()\n\n\n['Sun', 'Sat', 'Thur', 'Fri']\nCategories (4, object): ['Fri', 'Sat', 'Sun', 'Thur']\n\n\n\n\nCode\npd.get_dummies(df, prefix_sep='#', drop_first=True).head()\n\n\n\n\n\n\n\n\n\ntotal_bill\ntip\nsize\ntip_percentage\nsex#Male\nsmoker#Yes\nday#Sat\nday#Sun\nday#Thur\ntime#Lunch\n\n\n\n\n0\n16.99\n1.01\n2\n0.059447\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\n\n\n1\n10.34\n1.66\n3\n0.160542\nTrue\nFalse\nFalse\nTrue\nFalse\nFalse\n\n\n2\n21.01\n3.50\n3\n0.166587\nTrue\nFalse\nFalse\nTrue\nFalse\nFalse\n\n\n3\n23.68\n3.31\n2\n0.139780\nTrue\nFalse\nFalse\nTrue\nFalse\nFalse\n\n\n4\n24.59\n3.61\n4\n0.146808\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\n\n\n\n\n\n\n\nNow, if a categorical feature has \\(K\\) modalities, we use only \\(K-1\\) dummies. For instance, there is no more sex#Female binary column.\nQuestion. So, a linear regression won’t fit a weight for sex#Female. But, where do the model weights of the dropped binary columns go ?\nAnswer. They just “go” to the intercept: interpretation of the population bias depends on the “dropped” one-hot encodings.\nSo, we actually fit: \\[\\begin{array}{rl} \\texttt{tip} \\approx b & + w_1 \\times \\texttt{total_bill} + w_2 \\times \\texttt{size} \\\\ & + w_3 \\times \\texttt{sex#Male} + w_4 \\times \\texttt{smoker#Yes} \\\\ & + w_5 \\times \\texttt{day#Sat} + w_6 \\times \\texttt{day#Sun} + w_7 \\times \\texttt{day#Thur} \\\\ & + w_8 \\times \\texttt{time#Lunch} \\end{array}\\]"
  },
  {
    "objectID": "core/notebooks/notebook05_sparkrdd.html",
    "href": "core/notebooks/notebook05_sparkrdd.html",
    "title": "Introduction to Spark RDD",
    "section": "",
    "text": "Code\nimport numpy as np\nCode\nimport os\nimport sys\nimport inspect\n\nos.environ['PYSPARK_PYTHON'] = sys.executable\nos.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable\nCode\nfrom pyspark import SparkConf, SparkContext\n\nconf = SparkConf().setAppName(\"Spark RDD Course\")\nsc = SparkContext(conf=conf)\n\n\n25/02/03 23:20:16 WARN Utils: Your hostname, boucheron-Precision-5480 resolves to a loopback address: 127.0.1.1; using 192.168.10.120 instead (on interface wlp0s20f3)\n25/02/03 23:20:16 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\nSetting default log level to \"WARN\".\nTo adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n25/02/03 23:20:17 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n25/02/03 23:20:17 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\nCode\nrdd = sc.parallelize(range(64))\nNote that parallelize takes an optional argument to choose the number of partitions\nCode\nrdd.getNumPartitions()\n\n\n20\nCode\nrdd = sc.parallelize(range(1000), 10)\nrdd.getNumPartitions()\n\n\n10"
  },
  {
    "objectID": "core/notebooks/notebook05_sparkrdd.html#transformations",
    "href": "core/notebooks/notebook05_sparkrdd.html#transformations",
    "title": "Introduction to Spark RDD",
    "section": "Transformations",
    "text": "Transformations\n\nmap\n\n\nCode\nrdd = sc.parallelize([2, 3, 4])\nrdd = rdd.map(lambda x: list(range(1, x)))\n\n\n\n\nCode\nrdd\n\n\nPythonRDD[3] at RDD at PythonRDD.scala:53\n\n\n\n\nCode\n(\n    sc.parallelize([2, 3, 4])\n      .map(lambda x: list(range(1, x)))\n)\n\n\nPythonRDD[5] at RDD at PythonRDD.scala:53\n\n\nmap is a transformation. It is lazily evaluated. Hence execution is delayed until an action is met in the DAG).\n\n\nCode\nrdd.collect()  # collect is an action \n\n\n[Stage 0:&gt;                                                        (0 + 20) / 20]                                                                                \n\n\n[[1], [1, 2], [1, 2, 3]]\n\n\n\n\nCode\n(\n    sc.parallelize([2, 3, 4])\n      .map(lambda x: list(range(1, x)))\n      .collect()\n)\n\n\n[[1], [1, 2], [1, 2, 3]]\n\n\n\n\nExercice: map with a method\nWarning. This example is a bad practice !!! Don’t do this at home\n\n\nCode\ndbtel = {'arthur': 1234, 'riad': 4567, 'anatole': 3615}\n\n\n\n\nCode\nclass TelephoneDB(object):\n    \n    def __init__(self):\n        self.tel = {'arthur': 1234, 'riad': 4567, 'anatole': 3615}\n   \n    def add_tel(self, name):\n        return name, self.tel.get(name)\n\n\n\n\nCode\ntel_db = TelephoneDB()\nnames = ['arthur', 'riad']\n\n\n\n\nCode\nrdd = (\n    sc\n        .parallelize(names)\n        .map(tel_db.add_tel)\n        .collect()\n)\n\nrdd\n\n\n[('arthur', 1234), ('riad', 4567)]\n\n\n\nReplace the tel dictionary by a defaultdict with default number 999\nUse it on a rdd containing names as above including an unknown one, and try it\n\n\n\nCode\nfrom collections import defaultdict\n\nclass TelephoneDefaultDB(object):\n    \n    def __init__(self):\n        self.tel = defaultdict(lambda: 999, {'arthur': 1234, 'riad': 4567, 'anatole': 3615})\n    \n    def add_tel(self, name):\n        return name, self.tel[name]\n    \n    def add_tel_rdd(self, rdd):  \n        return rdd.map(self.add_tel)\n\n\n\n\nCode\ntel_db = TelephoneDefaultDB()\nnames = ['riad', 'anatole', 'yiyang']\nrdd = (\n    sc\n        .parallelize(names)\n        .map(tel_db.add_tel)\n        .collect()\n)\nrdd\n\n\n[('riad', 4567), ('anatole', 3615), ('yiyang', 999)]\n\n\n\n\n\n\n\n\nCaution\n\n\n\nIt is a bad idea to pass methods to spark’s map. Since add_tel needs self, the whole object is serialized so that spark can use it.\nThis breaks if the tel is large, or if it is not serializable.\n\n\n\n\nflatMap\n\n\nCode\nrdd = sc.parallelize([2, 3, 4, 5])\n( \n    rdd\n        .flatMap(lambda x: range(1, x))\n        .collect()\n)\n\n\n[1, 1, 2, 1, 2, 3, 1, 2, 3, 4]\n\n\n\n\nfilter\n\n\nCode\nrdd = sc.parallelize(range(10))\n\nrdd\\\n    .filter(lambda x: x % 2 == 0)\\\n    .collect()\n\n\n[0, 2, 4, 6, 8]\n\n\n\n\ndistinct\n\n\nCode\nrdd = sc.parallelize([1, 1, 4, 2, 1, 3, 3])\nrdd.distinct().collect()\n\n\n[1, 2, 3, 4]\n\n\n\n\n“Pseudo-set” operations\n\n\nCode\nrdd1 = sc.parallelize(range(5))\nrdd2 = sc.parallelize(range(3, 9))\nrdd3 = rdd1.union(rdd2)\nrdd3.collect()\n\n\n[0, 1, 2, 3, 4, 3, 4, 5, 6, 7, 8]\n\n\n\n\nCode\nrdd3.distinct().collect()\n\n\n[0, 1, 2, 3, 4, 5, 6, 7, 8]\n\n\n\n\nCode\nrdd1 = sc.parallelize([1, 2])\nrdd2 = sc.parallelize([\"a\", \"b\"])\nrdd1.cartesian(rdd2).collect()\n\n\n[(1, 'a'), (1, 'b'), (2, 'a'), (2, 'b')]"
  },
  {
    "objectID": "core/notebooks/notebook05_sparkrdd.html#actions",
    "href": "core/notebooks/notebook05_sparkrdd.html#actions",
    "title": "Introduction to Spark RDD",
    "section": "Actions",
    "text": "Actions\ncollect is obviously an action…\n\ncount, countByValue\n\n\nCode\nrdd = sc.parallelize([1, 3, 1, 2, 2, 2])\nrdd.count()\n\n\n6\n\n\n\n\nCode\nrdd.countByValue()\n\n\ndefaultdict(int, {1: 2, 3: 1, 2: 3})\n\n\nWhy does countByValue() returns a dictionary?\nAre count() and countByValue() actions or transformations?\n\n\nCode\nu = np.int32((np.random.sample(100000) * 100000))  # 100000 random integers uniformly distributed on 0, ..., 100000\n\np = (\n    sc.parallelize(u)\n    .countByValue()\n)\n\nq = sorted(\n    p.items(), \n    key = lambda x : x[1], \n    reverse=True\n)\n\nq[0:10]\n\nq[0], 1 + np.log(len(u))/ np.log(np.log(len(u))), len(q)\n\n\n((np.int32(63157), 8), np.float64(5.711710714547694), 63165)\n\n\n\nHow many distinct values do you expect in u ?\nHow large is the largest value in \\(q\\) ?\n\n\n\nCode\nfrom scipy.stats import poisson \n\n( \n    len(q), \n    (1-np.exp(-1)) * len(u),\n    poisson.ppf(1.-1./len(u), 1)\n)\n\n\n(63165, np.float64(63212.05588285577), np.float64(8.0))\n\n\n\n\ntake, takeOrdered\n\n\nCode\nrdd = sc.parallelize([(3, 'a'), (1, 'b'), (2, 'd')])\n\n\n\n\nCode\n(1, 'b') &lt;=  (2, 'd') &lt;= (3, 'a')\n\n\nTrue\n\n\n\n\nCode\nrdd.takeOrdered(2)\n\n\n[(1, 'b'), (2, 'd')]\n\n\n\n\nCode\nrdd.takeOrdered(2, key=lambda x: x[1])\n\n\n[(3, 'a'), (1, 'b')]\n\n\n\n\nreduce, fold\n\n\nCode\nrdd = sc.range(1, 4)\nrdd.reduce(lambda a, b: a + b)\n\n\n6\n\n\n\n\nCode\nrdd = sc.range(1, 4, numSlices=7)\nrdd.reduce(lambda a, b: a + b)\n\n\n6\n\n\n\n\nCode\nrdd = sc.parallelize(range(1,4), 3)\nrdd.reduce(lambda a, b: a + b)\n\n\n6\n\n\n\n\nCode\n( \n    sc.parallelize(range(1, 4), 2)\n      .fold(0, lambda a, b: a + b)\n)\n\n\n6\n\n\n\n\nCode\n( \n    sc.parallelize(range(1, 4), 1)\n      .fold(3, lambda a, b: a + b)\n),( \n    sc.parallelize(range(1, 4), 2)\n      .fold(2, lambda a, b: a + b)\n)\n\n\n(12, 12)\n\n\n\n\nCode\nrdd =  sc.parallelize(range(1, 4),3)\n( \n    rdd.fold(1, lambda a, b: a + b), \n    rdd.getNumPartitions()\n)\n\n\n(10, 3)\n\n\n\n\nCode\nrdd =  sc.parallelize(range(1, 4),4)\n\n(\n    rdd.fold(1, lambda a, b: a + b), \n    rdd.getNumPartitions()\n)\n\n\n(11, 4)\n\n\n\n\nCode\nrdd = sc.parallelize([1, 2, 4], 2)\nrdd.fold(2, lambda a, b: a + b)\n\n\n13\n\n\n\n\nCode\nrdd = sc.parallelize([1, 2, 4], 3)\nrdd.fold(2, lambda a, b: a + b)\n\n\n15\n\n\n\n\nCode\nrdd.getNumPartitions()\n\n\n3\n\n\n\n\naggregate\n\n\nCode\nseqOp = lambda x, y: (x[0] + y, x[1] + 1)\ncombOp = lambda x, y: (x[0] + y[0], x[1] + y[1])\n\nrdd = sc.parallelize([1, 2, 3, 4], 8)\n(\n    rdd.aggregate((0, 0), seqOp, combOp), rdd.getNumPartitions()\n)\n\n\n((10, 4), 8)\n\n\n\n\nCode\nop = lambda x, y: x+y\nrdd = sc.parallelize([1, 2, 3, 4], 4)\n(\n    rdd.aggregate(0, op, op),\n    rdd.getNumPartitions()\n)\n\n\n(10, 4)\n\n\n\n\nExercice: sum of powers with aggregate\n\nUsing aggregate, compute the sum, the sum of squares \\(x^2\\) and the sum of cubes \\(x^3\\) for \\(x \\in \\{1, \\ldots, 10 \\}\\).\nCheck your computations using numpy\n\n\n\nCode\nseqOp = lambda x, y: (x[0] + y, x[1] + y ** 2, x[2] + y ** 3)\n\n\n\n\nCode\ncombOp = lambda x, y: (x[0] + y[0], x[1] + y[1], x[2] + y[2])\n\n\n\n\nCode\nsc.range(5)\n\n\nPythonRDD[68] at RDD at PythonRDD.scala:53\n\n\n\n\nCode\n( \n    sc\n        .range(1, 11)\n        .aggregate((0, 0, 0), seqOp, combOp)\n)\n\n\n(55, 385, 3025)\n\n\n\n\nCode\nimport numpy as np\n\nx = np.arange(1, 11)\nx\n\n\narray([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10])\n\n\n\n\nCode\nx.sum(), (x**2).sum(), (x**3).sum(), x.cumsum()\n\n\n(np.int64(55),\n np.int64(385),\n np.int64(3025),\n array([ 1,  3,  6, 10, 15, 21, 28, 36, 45, 55]))\n\n\n\n\nComputing an empirical variance with aggregate\nAssume a sample is stored as a RDD. Using aggregate, compute the sample variance \\(\\frac{1}{n}\\sum_{i=1}^n (x_i - \\overline{X}_n)^2\\) where \\(\\overline{X}_n = \\frac{1}{n} \\sum_{i=1}^n x_i\\)"
  },
  {
    "objectID": "core/notebooks/notebook05_sparkrdd.html#transformations-1",
    "href": "core/notebooks/notebook05_sparkrdd.html#transformations-1",
    "title": "Introduction to Spark RDD",
    "section": "Transformations",
    "text": "Transformations\n\nkeys, values\n\n\nCode\nrdd.keys().collect()\n\n\n[1, 2, 2]\n\n\n\n\nCode\nrdd.values().collect()\n\n\n[['a', 7], ['b', 13], ['c', 17]]\n\n\n\n\n\n\n\n\nWarning\n\n\n\nAll elements must be tuples with two elements (key and value)\n\n\n\n\nCode\nrdd = sc.parallelize([[1, \"a\", 7], [2, \"b\", 13], [2, \"c\", 17]])\nrdd.keys().collect()\n\n\n[1, 2, 2]\n\n\n\n\nCode\nrdd.values().collect()\n\n\n['a', 'b', 'c']\n\n\nThe values are not what we expected wrong… so we must do\n\n\nCode\nrdd = ( sc.parallelize([[1, \"a\", 7], [2, \"b\", 13], [2, \"c\", 17]])\n          .map(lambda x: (x[0], x[1:]))\n      )\nrdd.keys().collect()\n\n\n[1, 2, 2]\n\n\n\n\nCode\nrdd.values().collect()\n\n\n[['a', 7], ['b', 13], ['c', 17]]\n\n\nNow, the values are correct.\n\n\nmapValues, flatMapValues\n\n\nCode\nrdd = sc.parallelize([(\"a\", \"x y z\"), (\"b\", \"p r\")])\n\nrdd.mapValues(lambda v: v.split(' ')).collect(), rdd.collect()\n\n\n([('a', ['x', 'y', 'z']), ('b', ['p', 'r'])], [('a', 'x y z'), ('b', 'p r')])\n\n\n\n\nCode\nrdd.flatMapValues(lambda v: v.split(' ')).collect()\n\n\n[('a', 'x'), ('a', 'y'), ('a', 'z'), ('b', 'p'), ('b', 'r')]\n\n\n\n\ngroupByKey\n\n\nCode\nrdd = sc.parallelize([(\"a\", 1), (\"b\", 1), (\"a\", 1), (\"b\", 3), (\"c\", 42)])\n( \n    rdd.groupByKey()\n       .mapValues(list)\n       .collect()\n)\n\n\n[('b', [1, 3]), ('c', [42]), ('a', [1, 1])]\n\n\n\n\nCode\nrdd.groupByKey().collect()\n\n\n[('b', &lt;pyspark.resultiterable.ResultIterable at 0x764e53f362d0&gt;),\n ('c', &lt;pyspark.resultiterable.ResultIterable at 0x764e53f36900&gt;),\n ('a', &lt;pyspark.resultiterable.ResultIterable at 0x764e53f480b0&gt;)]\n\n\n\n\nreduceByKey\n\n\nCode\nrdd = sc.parallelize([(\"a\", 1), (\"b\", 1), (\"a\", 1)])\nrdd.reduceByKey(lambda a, b: a + b).collect()\n\n\n[('b', 1), ('a', 2)]\n\n\n\n\ncombineByKey\n\n\nCode\nrdd = sc.parallelize([('a', 1), ('b', 2), ('a', 13)])\n\ndef add(a, b): \n    return a + str(b)\n\nrdd.combineByKey(str, add, add).collect()\n\n\n[('b', '2'), ('a', '113')]\n\n\n\n\njoin, rightOuterJoin, leftOuterJoin\n\n\nCode\nemployees = sc.parallelize([\n    (31, \"Rafferty\"),\n    (33, \"Jones\"),\n    (33, \"Heisenberg\"),\n    (34, \"Robinson\"),\n    (34, \"Smith\"),\n    (None, \"Williams\")\n])\n\n\n\n\nCode\ndepartments = sc.parallelize([\n    (31, \"Sales\"),\n    (33, \"Engineering\"),\n    (34, \"Clerical\"),\n    (35, \"Marketing\")\n])\n\n\n\n\nCode\n( \n    employees\n        .join(departments)\n        .sortByKey()\n        .collect()\n)\n\n\n[(31, ('Rafferty', 'Sales')),\n (33, ('Jones', 'Engineering')),\n (33, ('Heisenberg', 'Engineering')),\n (34, ('Robinson', 'Clerical')),\n (34, ('Smith', 'Clerical'))]\n\n\n\n\nCode\n( \n    employees\n        .rightOuterJoin(departments)\n        .sortByKey()\n        .collect()\n)\n\n\n[(31, ('Rafferty', 'Sales')),\n (33, ('Jones', 'Engineering')),\n (33, ('Heisenberg', 'Engineering')),\n (34, ('Robinson', 'Clerical')),\n (34, ('Smith', 'Clerical')),\n (35, (None, 'Marketing'))]\n\n\n\n\nCode\n(\n    employees\n        .leftOuterJoin(departments)\n        .collect()\n)\n\n\n[(None, ('Williams', None)),\n (31, ('Rafferty', 'Sales')),\n (33, ('Jones', 'Engineering')),\n (33, ('Heisenberg', 'Engineering')),\n (34, ('Robinson', 'Clerical')),\n (34, ('Smith', 'Clerical'))]"
  },
  {
    "objectID": "core/notebooks/notebook05_sparkrdd.html#actions-1",
    "href": "core/notebooks/notebook05_sparkrdd.html#actions-1",
    "title": "Introduction to Spark RDD",
    "section": "Actions",
    "text": "Actions\n\n\nCode\nemployees.countByKey()\n\n\ndefaultdict(int, {31: 1, 33: 2, 34: 2, None: 1})\n\n\n\n\nCode\nemployees.lookup(33)\n\n\n['Jones', 'Heisenberg']\n\n\n\n\nCode\nemployees.lookup(None)\n\n\n['Williams']\n\n\n\n\nCode\nemployees.collectAsMap()\n\n\n{31: 'Rafferty', 33: 'Heisenberg', 34: 'Smith', None: 'Williams'}"
  },
  {
    "objectID": "core/notebooks/notebook05_sparkrdd.html#references",
    "href": "core/notebooks/notebook05_sparkrdd.html#references",
    "title": "Introduction to Spark RDD",
    "section": "References",
    "text": "References\nSpark Core reference"
  },
  {
    "objectID": "core/notebooks/notebook07_json-format.html",
    "href": "core/notebooks/notebook07_json-format.html",
    "title": "Using JSON data with Python",
    "section": "",
    "text": "This notebook is concerned with JSON a format that serves many purposes. Just as csv files, json files are important sources and sinks for Spark. As a exchange format, JSON is also a serialization tool for Python and many other languages. JSON provides a way to accomodate semi-structured data in otherwise tabular environments (dataframes and databases tables).\nThe notebook is organized in the following way:"
  },
  {
    "objectID": "core/notebooks/notebook07_json-format.html#serialization-and-deserialization-of-built-in-types",
    "href": "core/notebooks/notebook07_json-format.html#serialization-and-deserialization-of-built-in-types",
    "title": "Using JSON data with Python",
    "section": "Serialization and deserialization of built-in types",
    "text": "Serialization and deserialization of built-in types\n\n\nCode\nimport json\n\nobj = {\n    \"name\": \"Foo Bar\",\n    \"age\": 78,\n    \"friends\": [\"Jane\",\"John\"],\n    \"balance\": 345.80,\n    \"other_names\":(\"Doe\",\"Joe\"),\n    \"active\": True,\n    \"spouse\": None\n}\n\nprint(json.dumps(obj, sort_keys=True, indent=4))\n\n\n\n\n\n\n\n\nNote\n\n\n\njson.dumps() outputs a JSON formatted string.\nNot every type of object can be fed to json.dumps(). Licit types are:\n\n\n\n\n\n\n\nCode\nwith open('user.json','w') as file:\n    json.dump(obj, file, sort_keys=True, indent=4)\n\n\n\n\nCode\n!cat user.json\n\n\n\n\nCode\njson.loads('{\"active\": true, \"age\": 78, \"balance\": 345.8, \"friends\": [\"Jane\",\"John\"], \"name\": \"Foo Bar\", \"other_names\": [\"Doe\",\"Joe\"],\"spouse\":null}')\n\n\n\n\nCode\nwith open('user.json', 'r') as file:\n    user_data = json.load(file)\n\nprint(user_data)\n\n\n\n\n\n\n\n\nWhat happens if we feed json.dumps() with a numpy array?"
  },
  {
    "objectID": "core/notebooks/notebook07_json-format.html#serialization-and-deserialization-of-custom-objects",
    "href": "core/notebooks/notebook07_json-format.html#serialization-and-deserialization-of-custom-objects",
    "title": "Using JSON data with Python",
    "section": "Serialization and deserialization of custom objects",
    "text": "Serialization and deserialization of custom objects\n\n\nCode\nclass User(object):\n    \"\"\"Custom User Class\n    \"\"\"\n    def __init__(self, name, age, active, balance, \n                 other_names, friends, spouse):\n        self.name = name\n        self.age = age\n        self.active = active\n        self.balance = balance\n        self.other_names = other_names\n        self.friends = friends\n        self.spouse = spouse\n            \n    def __repr__(self):\n        s = \"User(\"\n        s += \"name=\" + repr(self.name)\n        s += \", age=\" + repr(self.age)\n        s += \", active=\" + repr(self.active)\n        s += \", other_names=\" + repr(self.other_names)\n        s += \", friends=\" + repr(self.friends)\n        s += \", spouse=\" + repr(self.spouse) + \")\"\n        return s\n\n\n\n\nCode\nnew_user = User(\n    name = \"Foo Bar\",\n    age = 78,\n    friends = [\"Jane\", \"John\"],\n    balance = 345.80,\n    other_names = (\"Doe\", \"Joe\"),\n    active = True,\n    spouse = None\n)\n\nnew_user\n\n\n\n\nCode\n# This will raise a TypeError\n# json.dumps(new_user)\n\n\nAs expected, the custom object new_user is not JSON serializable. So let’s build a method that does that for us.\n\nThis comes as no surprise to us, since earlier on we observed that the json module only handles the built-in types, and User is not one.\nWe need to send our user data to a client over a network, so how do we get ourselves out of this error state?\nA simple solution would be to convert our custom type into a serializable type that is a built-in type. We can conveniently define a method convert_to_dict() that returns a dictionary representation of our object. json.dumps() takes in a optional argument, default, which specifies a function to be called if the object is not serializable. This function returns a JSON encodable version of the object.\n\nRecall that class obj has a dunder method __dict__ that provides a basis for obtaining a dictionary with the attributes of any object:\n\n\nCode\nnew_user.__dict__\n\n\n\n\nCode\ndef obj_to_dict(obj):\n    \"\"\"Converts an object to a dictionary representation of the object including \n    meta-data information about the object's module and class name.\n\n    Parameters\n    ----------\n    obj : `object`\n        A python object to be converted into a dictionary representation\n\n    Returns\n    -------\n    output : `dict`\n        A dictionary representation of the object\n    \"\"\"\n    # Add object meta data \n    obj_dict = {\n        \"__class__\": obj.__class__.__name__,\n        \"__module__\": obj.__module__\n    }\n    # Add the object properties\n    return obj_dict | obj.__dict__\n\n\n\n\nCode\nobj_to_dict(new_user)\n\n\nThe function convert_to_dict does the following:\n\ncreate a dictionary named obj_dict to act as the dict representation of our object.\nmagic methods __class__.__name__ and __module__ provide crucial metadata on the object: the class name and the module name\nadd the instance attributes of the object using obj.__dict__ (Python stores instance attributes in a dictionary under the hood)\nThe resulting obj_dict is now serializable (provided all attributes of our object are).\n\nNow we can comfortably call json.dumps() on the object and pass default=convert_to_dict\n\n\n\n\n\n\nNote\n\n\n\nObviously this fails if one of the attributes is not JSON serializable\n\n\n\n\nCode\nprint(json.dumps(new_user, default=obj_to_dict, indent=4, sort_keys=True))\n\n\nNow, if we want to decode (deserialiaze) a custom object, and create the correct object type, we need a function that does the opposite of convert_to_dict, since json.loads simply returns a dict:\n\n\nCode\nuser_data = json.loads(json.dumps(new_user, default=obj_to_dict))\nprint(user_data)\n\n\nWe need json.loads() to reconstruct a User object from this dictionary: json.loads() takes an optional argument object_hook which specifies a function that returns the desired custom object, given the decoded output (which in this case is a dict).\n\n\nCode\ndef dict_to_obj(input_dict):\n    \"\"\"Converts a dictionary representation of an object to an instance of the object.\n\n    Parameters\n    ----------\n    input_dict : `dict`\n        A dictionary representation of the object, containing \"__module__\" \n        and \"__class__\" metadata\n\n    Returns\n    -------    \n    obj : `object`\n        A python object constructed from the dictionary representation    \n    \"\"\"\n    assert \"__class__\" in input_dict and \"__module__\" in input_dict\n    class_name = input_dict.pop(\"__class__\")\n    module_name = input_dict.pop(\"__module__\")\n    module = __import__(module_name)\n    class_ = getattr(module, class_name)\n    obj = class_(**input_dict)\n    return obj\n\n\nThis function does the following:\n\nExtract the class name from the dictionary under the key __class__\nExtract the module name from the dictionary under the key __module__\nImports the module and get the class\nInstantiate the class by giving to the class constructor all the instance arguments through dictionary unpacking\n\n\n\nCode\nobj_data = json.dumps(new_user, default=obj_to_dict)\nnew_object = json.loads(obj_data, object_hook=dict_to_obj)\nnew_object\n\n\n\n\nCode\ntype(new_object)\n\n\n\n\nCode\nnew_object.age\n\n\n\n\n\n\n\n\nNote\n\n\n\nFunctions obj_to_dict() and dict_to_obj() are showcases for special/magic/dunder methods.\nIn the definition of class User, two special methods were explicitly defined: __init__ and __repr__. But many more are available, including __dir__().\n\n\n\n\nCode\n[dude for dude in dir(new_object) if dude.startswith('__')]\n\n\n\n\nCode\nnew_object.__getattribute__('age')\n\n\n\n\n\n\n\n\nNote\n\n\n\nClass User could have been implemented as a dataclass\n\n\n\n\nCode\nfrom dataclasses import dataclass\n\n@dataclass\nclass UserBis(object):\n    \"\"\"Custom User Class\n    \"\"\"\n    name: str \n    age: int\n    active: bool\n    balance: float\n    other_names: list[str]\n    friends: list[str]\n    spouse: str\n\n\n\n\nCode\nother_user = UserBis(**(new_user.__dict__))\n\n\n\n\nCode\nrepr(other_user)\n\n\n\n\nCode\n{dude for dude in dir(other_user) if dude.startswith('__')} -  {dude for dude in dir(new_user) if dude.startswith('__')}"
  },
  {
    "objectID": "core/notebooks/notebook07_json-format.html#reading-a-json-dataset-with-spark",
    "href": "core/notebooks/notebook07_json-format.html#reading-a-json-dataset-with-spark",
    "title": "Using JSON data with Python",
    "section": "Reading a JSON dataset with Spark",
    "text": "Reading a JSON dataset with Spark\n\n\nCode\nfrom pyspark import SparkConf, SparkContext\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql import functions as fn\nfrom pyspark.sql.functions import col\n\nspark = (SparkSession\n    .builder\n    .appName(\"Spark JSON\")\n    .getOrCreate()\n)\n\nsc = spark._sc\n\n\n\n\nCode\nfilename = \"drug-enforcement.json\"\n\n\nFirst, lets look at the data. It’s a large set of JSON records about drugs enforcement.\n\n\nCode\n!head -n 1000 drug-enforcement.json\n\n\nWe need to tell spark that rows span on several lines with the multLine option\n\n\nCode\ndf = spark.read.json(filename, multiLine=True)\n\n\n\n\nCode\ndf.printSchema()\n\n\n\n\nCode\ndf.schema\n\n\n\n\n\n\n\n\nNote\n\n\n\nThis dataset is a little bit of a mess!\nThis should not be surprising. The data used to populate the Spark dataframe are not classically tabular but what people call semi-structured. Json is well-suited to store, represent, and exchange such data.\nIn the classical age of tabular data (according to Codd’s principles), a table cell could only hold a scalar value (numeric, logical, text, date, timestamp, …), nowadays Relational Database Management Systems handle Arrays, Composite Types, Range Types, …, and Json (see PostgreSQL).\nSpark, R, and Pandas also allow us to work with complex types.\n\n\n\nFirst, there is a nested opendfa dictionary. Each element of the dictionary is an array\nA first good idea is to “flatten” the schema of the DataFrame, so that there is no nested types"
  },
  {
    "objectID": "core/notebooks/notebook07_json-format.html#flattening-of-the-schema",
    "href": "core/notebooks/notebook07_json-format.html#flattening-of-the-schema",
    "title": "Using JSON data with Python",
    "section": "Flattening of the schema",
    "text": "Flattening of the schema\nAll the columns in the nested structure openfda are put up in the schema. These columns nested in the openfda are as follows:\n\n\nCode\ndf.select('openfda.*').columns\n\n\n\n\nCode\ndf.select(\"openfda.*\").head(2)\n\n\n\n\nCode\nfor c in df.select(\"openfda.*\").columns:\n    df = df.withColumn(\"openfda_\" + c, col(\"openfda.\" + c))\n\n\n\n\nCode\ndf = df.select([c for c in df.columns if c != \"openfda\"])\n\n\n\n\nCode\ndf.printSchema()\n\n\n\n\nCode\ndf.head(2)\n\n\nNote that the display of the DataFrame is not as usual… it displays the dataframe like a list of Row, since the columns “openfda*” contain arrays of varying length"
  },
  {
    "objectID": "core/notebooks/notebook07_json-format.html#missing-data",
    "href": "core/notebooks/notebook07_json-format.html#missing-data",
    "title": "Using JSON data with Python",
    "section": "Missing data",
    "text": "Missing data\nA strategy can be to remove rows with missing data. dropna has several options, explained below.\n\n\nCode\ndf.dropna().count()\n\n\nIf we remove all lines with at least one missing value, we end up with an empty dataframe !\n\n\nCode\ndf.dropna(how='all').count()\n\n\ndropna accepts the following arguments\n\nhow: can be 'any' or 'all'. If 'any', rows containing any null values will be dropped entirely (this is the default). If 'all', only rows which are entirely empty will be dropped.\nthresh: accepts an integer representing the “threshold” for how many empty cells a row must have before being dropped. tresh is a middle ground between how='any' and how='all'. As a result, the presence of thresh will override how\nsubset: accepts a list of column names. When a subset is present, N/A values will only be checked against the columns whose names are provided.\n\n\n\nCode\nn_columns = len(df.columns)\n\n\n\n\nCode\ndf.dropna(thresh=n_columns).count()\n\n\n\n\nCode\ndf.dropna(thresh=n_columns-1).count()\n\n\n\n\nCode\ndf.dropna(thresh=n_columns-10).count()\n\n\n\n\nCode\ndf = df.dropna(subset=['postal_code', 'city', 'country', 'address_1'])\ndf.count()\n\n\nBut before this, let’s count the number of missing value for each column\n\n\nCode\n# For each column we create a new column containing 1 if the value is null and 0 otherwise.\n# We need to bast Boolean to Int so that we can use fn.sum after\nfor c in df.columns:\n    # Do not do this for _isnull columns (ince case you run this cell twice...)\n    if not c.endswith(\"_isnull\"):\n        df = df.withColumn(c + \"_isnull\", fn.isnull(col(c)).cast('int'))\n\n\n\n\nCode\ndf.head()\n\n\n\n\nCode\n# Get the list of _isnull columns\nisnull_columns = [c for c in df.columns if c.endswith(\"_isnull\")]\n\n# On the _isnull columns :\n#  - we compute the sum to have the number of null values and rename the column\n#  - convert to pandas for better readability\n#  - transpose the pandas dataframe for better readability\nmissing_values = df.select(isnull_columns)\\\n    .agg(*[fn.sum(c).alias(c.replace(\"_isnull\", \"\")) for c in isnull_columns])\\\n    .toPandas()\n\nmissing_values.T\\\n    .rename({0: \"missing values\"}, axis=\"columns\")\n\n\nWe see that more_code_info is always null and that termination_date if often null. Most of the openfda* columns are also almost always empty.\nWe can keep only the columns with no missing values\n\n\nCode\n# This line can seem complicated, run pieces of each to understand\nkept_columns = list(\n    missing_values.columns[(missing_values.iloc[0] == 0).values]\n)\n\n\n\n\nCode\ndf_kept = df.select(kept_columns)\n\n\n\n\nCode\ndf_kept.head(2)\n\n\n\n\nCode\ndf_kept.printSchema()\n\n\n\n\nCode\ndf_kept.count()"
  },
  {
    "objectID": "core/notebooks/notebook07_json-format.html#filtering-by-string-values",
    "href": "core/notebooks/notebook07_json-format.html#filtering-by-string-values",
    "title": "Using JSON data with Python",
    "section": "Filtering by string values",
    "text": "Filtering by string values\nCases from South San Francisco\n\n\nCode\ndf.filter(df.city == \"South San Francisco\")\\\n    .toPandas()\n\n\nRemark. Once again, we use .toPandas() to pretty format the results in the notebook. But it’s a BAD idea to do this if the spark DataFrame is large, since it requires a collect()\nAside from filtering strings by a perfect match, there are plenty of other powerful ways to filter by strings in pyspark :\n\ndf.filter(df.city.contains('San Francisco')): returns rows where strings of a column contain a provided substring. In our example, filtering by rows which contain the substring “San Francisco” would be a good way to get all rows in San Francisco, instead of just “South San Francisco”.\ndf.filter(df.city.startswith('San')): Returns rows where a string starts with a provided substring.\ndf.filter(df.city.endswith('ice')): Returns rows where a string starts with a provided substring.\ndf.filter(df.city.isNull()): Returns rows where values in a provided column are null.\ndf.filter(df.city.isNotNull()): Opposite of the above.\ndf.filter(df.city.like('San%')): Performs a SQL-like query containing the LIKE clause.\ndf.filter(df.city.rlike('[A-Z]*ice$')): Performs a regexp filter.\ndf.filter(df.city.isin('San Francisco', 'Los Angeles')): Looks for rows where the string value of a column matches any of the provided strings exactly.\n\nYou can try some of these to understand\n\n\nCode\ndf.filter(df.city.contains('San Francisco'))\\\n    .toPandas()\n\n\n\n\nCode\ndf.filter(df.city.isin('San Francisco', 'Los Angeles')).toPandas()"
  },
  {
    "objectID": "core/notebooks/notebook07_json-format.html#filtering-by-date-values",
    "href": "core/notebooks/notebook07_json-format.html#filtering-by-date-values",
    "title": "Using JSON data with Python",
    "section": "Filtering by Date Values",
    "text": "Filtering by Date Values\nIn addition to filtering by strings, we can also filter by columns where the values are stored as dates or datetimes (or strings that can be inferred as dates). Perhaps the most useful way to filter dates is by using the between() method, which allows us to find results within a certain date range. Here we find all the results which were reported in the years 2013 and 2014:\n\n\nCode\ndf.filter(df.city == \"South San Francisco\")\\\n    .filter(df.report_date.between('2013-01-01 00:00:00','2015-03-11 00:00:00'))\\\n    .toPandas()\n\n\n\n\n\n\n\n\nCaution\n\n\n\nIs Spark smart enough to understand that the string in column report_date contains a date?\n\n\n\n\nCode\ndf.filter(df.city == \"South San Francisco\")\\\n    .filter(df.center_classification_date.between('2013-01-01 00:00:00','2013-12-31 00:00:00'))\\\n    .toPandas()\n\n\n\n\nCode\ndf_dates = df.select([c for c in df.columns if c.endswith(\"date\")])\n\ndf_dates.printSchema()\n\n\n\n\nCode\ndf_dates.show(5)\n\n\nColumns are not dates (DateType) but strings (StringType). When comparing report_date with '2013-01-01 00:00:00' and '2015-03-11 00:00:00', we are comparing strings and are lucky enough that in unicode '-' &lt; '0' &lt; '...' &lt; '9' so that 2013-.... is less that any string starting with 20130..., while any string starting with 2013... is less than any string starting with 2015...\n\n\n\n\n\n\nCaution\n\n\n\nIf some field in a Jason string is meant to represent a date or a datetime object, spark should be given a hint.\nJson loaders (from Python) as well as the Spark Json reader have optional arguments that can be used to indicate the date parser to be used."
  },
  {
    "objectID": "core/notebooks/notebook07_json-format.html#handling-complex-types",
    "href": "core/notebooks/notebook07_json-format.html#handling-complex-types",
    "title": "Using JSON data with Python",
    "section": "Handling complex types",
    "text": "Handling complex types\nBridging the gap between tabular and semi-structured data.\n\n\n\n\n\n\nNote\n\n\n\nSQL, R, Pandas …\n\n\nstruct, array, map\n\n\nCode\n# struct\n\n\nThe problems we faced after loading data from the json file pertained to the fact that column fda was of complex StrucType() type. We shall revisit this dataframe.\n\n\nCode\ndf = spark.read.json(filename, multiLine=True)\n\n\nThe dataframe schema df.schema which is of type StructType (defined in pyspark.sql.types) can be converted to a json string which in turn can be converted into a Python dictionary.\n\n\nCode\ndf = spark.read.json(filename, multiLine=True)\n\nsj = json.loads(df.schema.json())\n\n\nWe equip the dataframe with a primary key\n\n\nCode\nfrom pyspark.sql import Window\n\nw = Window.orderBy(col(\"center_classification_date\"))\n\ndf = (\n  df\n    .withColumn(\"row_id\", fn.row_number()\n    .over(w))\n)\n\n\n\n\nCode\n[(f['name'], f['type']) \n for f in sj['fields'] if not isinstance(f['type'], str)]\n\n\nColumn openfda has type StrucType() with fields with composite type.\n\n\nCode\n{f['type']['type']\n for f in sj['fields'] if not isinstance(f['type'], str)}\n\n\nProjecting on row_id and openfda.* leads to a (partially) flattened datafame, that, thanks to the row_id column can be joined with the original dataframe.\n\n\nCode\ndf_proj = df.select('row_id', 'openfda.*')\n\ndf_proj.printSchema()\n\n\nWe can inspect the length of the arrays.\n\n\nCode\n# array\ndf_proj.select(\n    fn.max(fn.size(col(\"application_number\"))).alias(\"Max\"), \n    fn.min(fn.size(col(\"application_number\"))).alias(\"min\"), \n    fn.avg(fn.size(col(\"application_number\"))).alias(\"Mean\")).show(1)\n\n\nIn some rows, the size of the array is -1 because the field is NULL.\n\n\nCode\n(\n  df_proj\n    .where(fn.size(col(\"application_number\"))&gt;1)\n    .select(\"row_id\")\n    .show(5)\n)\n\n\nAn array column can be exploded. This is like pivoting into long form. The result contains one row per item in the array.\n\n\nCode\n(\n  df_proj\n    .select('row_id', 'application_number')\n    .withColumn(\"exploded\", fn.explode(col(\"application_number\")))\n    .select('row_id', 'exploded')\n    .groupBy('row_id')\n    .agg(fn.count('exploded').alias(\"n_lignes\"))\n    .where(\"n_lignes &gt; 1\")\n    .show(5)\n)"
  },
  {
    "objectID": "core/notebooks/notebook10_graphx.html",
    "href": "core/notebooks/notebook10_graphx.html",
    "title": "",
    "section": "",
    "text": "CodeShow All CodeHide All CodeView Source\n\n\n\n\n\n\nCode\nfrom graphframes import GraphFrame\nfrom pyspark import SparkConf, SparkContext\nfrom pyspark.sql import SparkSession\n\nconf = SparkConf().setAppName(\"Spark graphx Course\")\nsc = SparkContext(conf=conf)  # no need for Spark 3...\n\nspark = (SparkSession\n         .builder\n         .appName(\"Spark graphx Course\")\n         .getOrCreate()\n         )\n\nspark._sc is sc\n\n\n\n---------------------------------------------------------------------------\nModuleNotFoundError                       Traceback (most recent call last)\nCell In[1], line 1\n----&gt; 1 from graphframes import GraphFrame\n      2 from pyspark import SparkConf, SparkContext\n      3 from pyspark.sql import SparkSession\n\nModuleNotFoundError: No module named 'graphframes'\n\n\n\n\n\nCode\nv = spark.createDataFrame([\n    (\"a\", \"Alice\", 34),\n    (\"b\", \"Bob\", 36),\n    (\"c\", \"Charlie\", 30),\n], [\"id\", \"name\", \"age\"])\n# Create an Edge DataFrame with \"src\" and \"dst\" columns\ne = spark.createDataFrame([\n    (\"a\", \"b\", \"friend\"),\n    (\"b\", \"c\", \"follow\"),\n    (\"c\", \"b\", \"follow\"),\n], [\"src\", \"dst\", \"relationship\"])\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[2], line 1\n----&gt; 1 v = spark.createDataFrame([\n      2     (\"a\", \"Alice\", 34),\n      3     (\"b\", \"Bob\", 36),\n      4     (\"c\", \"Charlie\", 30),\n      5 ], [\"id\", \"name\", \"age\"])\n      6 # Create an Edge DataFrame with \"src\" and \"dst\" columns\n      7 e = spark.createDataFrame([\n      8     (\"a\", \"b\", \"friend\"),\n      9     (\"b\", \"c\", \"follow\"),\n     10     (\"c\", \"b\", \"follow\"),\n     11 ], [\"src\", \"dst\", \"relationship\"])\n\nNameError: name 'spark' is not defined\n\n\n\n\n\nCode\ng = GraphFrame(v, e)\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[3], line 1\n----&gt; 1 g = GraphFrame(v, e)\n\nNameError: name 'GraphFrame' is not defined"
  },
  {
    "objectID": "core/notebooks/notebook14.html",
    "href": "core/notebooks/notebook14.html",
    "title": "Using with pyspark for data preprocessing",
    "section": "",
    "text": "Code\nimport os\nimport sys\nimport pyarrow as pa\nimport comet as co\nimport pyarrow.parquet as pq\n\n\n\n---------------------------------------------------------------------------\nModuleNotFoundError                       Traceback (most recent call last)\nCell In[1], line 4\n      2 import sys\n      3 import pyarrow as pa\n----&gt; 4 import comet as co\n      5 import pyarrow.parquet as pq\n\nModuleNotFoundError: No module named 'comet'\n\n\n\n\n\nCode\nfrom pyspark.sql import Window\nimport pyspark.sql.functions as func\nfrom pyspark.sql.types import *\nfrom pyspark.sql.functions import col, lit\n\n\n\n\nCode\nfrom functools import reduce\n\n\n\n\nCode\nos.environ['PYSPARK_PYTHON'] = sys.executable\nos.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable\n\n\n\n\nCode\nfrom pyspark.sql import SparkSession\n\nspark = (SparkSession\n    .builder\n    .appName(\"Web data\")         \n    .getOrCreate()\n)\n\n\n25/01/15 06:08:46 WARN Utils: Your hostname, boucheron-Precision-5480 resolves to a loopback address: 127.0.1.1; using 192.168.10.120 instead (on interface wlp0s20f3)\n25/01/15 06:08:47 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\nSetting default log level to \"WARN\".\nTo adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n25/01/15 06:08:47 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n\n\n\n\nCode\n%timeit\ninput_path = './'\n\ninput_file = os.path.join(input_path, 'webdata.parquet')\n\ndf = spark.read.parquet(input_file)\n\n\n\n---------------------------------------------------------------------------\nAnalysisException                         Traceback (most recent call last)\nCell In[6], line 6\n      2 input_path = './'\n      4 input_file = os.path.join(input_path, 'webdata.parquet')\n----&gt; 6 df = spark.read.parquet(input_file)\n\nFile ~/Documents/IFEBY310/.venv/lib/python3.12/site-packages/pyspark/sql/readwriter.py:544, in DataFrameReader.parquet(self, *paths, **options)\n    533 int96RebaseMode = options.get(\"int96RebaseMode\", None)\n    534 self._set_opts(\n    535     mergeSchema=mergeSchema,\n    536     pathGlobFilter=pathGlobFilter,\n   (...)\n    541     int96RebaseMode=int96RebaseMode,\n    542 )\n--&gt; 544 return self._df(self._jreader.parquet(_to_seq(self._spark._sc, paths)))\n\nFile ~/Documents/IFEBY310/.venv/lib/python3.12/site-packages/py4j/java_gateway.py:1322, in JavaMember.__call__(self, *args)\n   1316 command = proto.CALL_COMMAND_NAME +\\\n   1317     self.command_header +\\\n   1318     args_command +\\\n   1319     proto.END_COMMAND_PART\n   1321 answer = self.gateway_client.send_command(command)\n-&gt; 1322 return_value = get_return_value(\n   1323     answer, self.gateway_client, self.target_id, self.name)\n   1325 for temp_arg in temp_args:\n   1326     if hasattr(temp_arg, \"_detach\"):\n\nFile ~/Documents/IFEBY310/.venv/lib/python3.12/site-packages/pyspark/errors/exceptions/captured.py:185, in capture_sql_exception.&lt;locals&gt;.deco(*a, **kw)\n    181 converted = convert_exception(e.java_exception)\n    182 if not isinstance(converted, UnknownException):\n    183     # Hide where the exception came from that shows a non-Pythonic\n    184     # JVM exception message.\n--&gt; 185     raise converted from None\n    186 else:\n    187     raise\n\nAnalysisException: [PATH_NOT_FOUND] Path does not exist: file:/home/boucheron/Documents/IFEBY310/core/notebooks/webdata.parquet.\n\n\n\n\n\nCode\ndf.printSchema()\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[7], line 1\n----&gt; 1 df.printSchema()\n\nNameError: name 'df' is not defined\n\n\n\n\n\nCode\ndf.rdd.getNumPartitions()\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[8], line 1\n----&gt; 1 df.rdd.getNumPartitions()\n\nNameError: name 'df' is not defined\n\n\n\n\n\nCode\n%timeit\ndfa = pq.read_table(input_file)\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[9], line 2\n      1 get_ipython().run_line_magic('timeit', '')\n----&gt; 2 dfa = pq.read_table(input_file)\n\nNameError: name 'pq' is not defined\n\n\n\n\n\nCode\nprint(dfa.schema)\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[10], line 1\n----&gt; 1 print(dfa.schema)\n\nNameError: name 'dfa' is not defined\n\n\n\n\n\nCode\n( \n    df.select('xid')\n      .distinct()\n      .explain()\n)\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[11], line 2\n      1 ( \n----&gt; 2     df.select('xid')\n      3       .distinct()\n      4       .explain()\n      5 )\n\nNameError: name 'df' is not defined\n\n\n\n\n\nCode\n# \n( \n    df.select('xid')\n      .distinct()\n      .count()\n)\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[12], line 3\n      1 # \n      2 ( \n----&gt; 3     df.select('xid')\n      4       .distinct()\n      5       .count()\n      6 )\n\nNameError: name 'df' is not defined\n\n\n\n\n\nCode\nxid_partition = Window.partitionBy('xid')\nn_events = func.count(col('action')).over(xid_partition)\ndf = df.withColumn('n_events', n_events)\ndf.head(n=2)\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[13], line 3\n      1 xid_partition = Window.partitionBy('xid')\n      2 n_events = func.count(col('action')).over(xid_partition)\n----&gt; 3 df = df.withColumn('n_events', n_events)\n      4 df.head(n=2)\n\nNameError: name 'df' is not defined\n\n\n\n\n\nCode\n( \n  df\n    .groupBy('xid')\n    .agg(func.count('action'))\n    .head(5)\n)\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[14], line 2\n      1 ( \n----&gt; 2   df\n      3     .groupBy('xid')\n      4     .agg(func.count('action'))\n      5     .head(5)\n      6 )\n\nNameError: name 'df' is not defined\n\n\n\n\n\nCode\ndf = df.repartitionByRange(20, 'xid')\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[15], line 1\n----&gt; 1 df = df.repartitionByRange(20, 'xid')\n\nNameError: name 'df' is not defined\n\n\n\n\n\nCode\ndf.persist()\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[16], line 1\n----&gt; 1 df.persist()\n\nNameError: name 'df' is not defined\n\n\n\n\n\nCode\nxid_partition = Window.partitionBy('xid')\n\nmax_date = (\n  func\n    .max(col('date'))\n    .over(xid_partition)\n)\n\nn_days_since_last_event = func.datediff(func.current_date(), max_date)\n\ndf = df.withColumn('n_days_since_last_event',\n                   n_days_since_last_event)\n\ndf.head(n=2)\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[17], line 11\n      3 max_date = (\n      4   func\n      5     .max(col('date'))\n      6     .over(xid_partition)\n      7 )\n      9 n_days_since_last_event = func.datediff(func.current_date(), max_date)\n---&gt; 11 df = df.withColumn('n_days_since_last_event',\n     12                    n_days_since_last_event)\n     14 df.head(n=2)\n\nNameError: name 'df' is not defined\n\n\n\n\n\nCode\nn_events = func.count(col('action')).over(xid_partition)\n\ndf = df.withColumn('n_events', n_events)\n\ndf.head(n=2)\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[18], line 3\n      1 n_events = func.count(col('action')).over(xid_partition)\n----&gt; 3 df = df.withColumn('n_events', n_events)\n      5 df.head(n=2)\n\nNameError: name 'df' is not defined\n\n\n\n\n\nCode\nxid_device_partition = xid_partition.partitionBy('device')\n\nn_events_per_device = (\n    func.count(col('action'))\n        .over(xid_device_partition)\n)\n\ndf = df.withColumn('n_events_per_device', n_events_per_device)\ndf.head(n=2)\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[19], line 8\n      1 xid_device_partition = xid_partition.partitionBy('device')\n      3 n_events_per_device = (\n      4     func.count(col('action'))\n      5         .over(xid_device_partition)\n      6 )\n----&gt; 8 df = df.withColumn('n_events_per_device', n_events_per_device)\n      9 df.head(n=2)\n\nNameError: name 'df' is not defined\n\n\n\n\n\nCode\nrank_device = (\n  func\n    .dense_rank()\n    .over(xid_partition.orderBy('device'))\n)\n\nn_unique_device = (\n    func\n      .last(rank_device)\n      .over(xid_partition)\n)\n\ndf = df.withColumn('n_device', n_unique_device)\n\ndf.head(n=2)\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[20], line 13\n      1 rank_device = (\n      2   func\n      3     .dense_rank()\n      4     .over(xid_partition.orderBy('device'))\n      5 )\n      7 n_unique_device = (\n      8     func\n      9       .last(rank_device)\n     10       .over(xid_partition)\n     11 )\n---&gt; 13 df = df.withColumn('n_device', n_unique_device)\n     15 df.head(n=2)\n\nNameError: name 'df' is not defined\n\n\n\n\n\nCode\ndf.printSchema()\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[21], line 1\n----&gt; 1 df.printSchema()\n\nNameError: name 'df' is not defined\n\n\n\n\n\nCode\n(\n  df\n    .where(col('n_device') &gt; 1)\n    .select('xid', \n            'device', \n            'n_events', \n            'n_device', \n            'n_events_per_device')\n    .head(n=8)\n)\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[22], line 2\n      1 (\n----&gt; 2   df\n      3     .where(col('n_device') &gt; 1)\n      4     .select('xid', \n      5             'device', \n      6             'n_events', \n      7             'n_device', \n      8             'n_events_per_device')\n      9     .head(n=8)\n     10 )\n\nNameError: name 'df' is not defined\n\n\n\n\n\nCode\ndef n_events_transformer(df):\n    xid_partition = Window.partitionBy('xid')\n    n_events = func.count(col('action')).over(xid_partition)\n    \n    df = df.withColumn('n_events', n_events)\n\n    return df\n\n\n\n\nCode\ndef my_count_window_transform(df, input_col, output_col, part_col):\n    w = Window.partitionBy(part_col)\n    out_col = func.count(col(input_col)).over(w)\n\n    return df.withColumn(output_col, out_col)\n\n\n\n\nCode\ndef hour_transformer(df):\n    hour = func.hour(col('date'))\n    df = df.withColumn('hour', hour)\n    return df\n\ndef weekday_transformer(df):\n    weekday = func.date_format(col('date'), 'EEEE')\n    df = df.withColumn('weekday', weekday)\n    return df\n\n\n\n\nCode\ndef n_events_per_hour_transformer(df):\n    xid_hour_partition = Window.partitionBy('xid', 'hour')\n    n_events_per_hour = func.count(col('action')).over(xid_hour_partition)\n    df = df.withColumn('n_events_per_hour', n_events_per_hour)\n    return df\n\n\n\n\nCode\ndef n_events_per_weekday_transformer(df):\n    xid_weekday_partition = Window.partitionBy('xid', 'weekday')\n    n_events_per_weekday = func.count(col('action')).over(xid_weekday_partition)\n    df = df.withColumn('n_events_per_weekday', n_events_per_weekday)\n    return df\n\ndef n_days_since_last_event_transformer(df):\n    xid_partition = Window.partitionBy('xid')\n    max_date = func.max(col('date')).over(xid_partition)\n    n_days_since_last_event = func.datediff(func.current_date(), max_date)\n    df = df.withColumn('n_days_since_last_event',\n                       n_days_since_last_event + lit(0.1))\n    return df\n\ndef n_days_since_last_action_transformer(df):\n    xid_partition_action = Window.partitionBy('xid', 'action')\n    max_date = func.max(col('date')).over(xid_partition_action)\n    n_days_since_last_action = func.datediff(func.current_date(),\n                                                        max_date)\n    df = df.withColumn('n_days_since_last_action',\n                       n_days_since_last_action + lit(0.1))\n    return df\n\ndef n_unique_day_transformer(df):\n    xid_partition = Window.partitionBy('xid')\n    dayofyear = func.dayofyear(col('date'))\n    rank_day = func.dense_rank().over(xid_partition.orderBy(dayofyear))\n    n_unique_day = func.last(rank_day).over(xid_partition)\n    df = df.withColumn('n_unique_day', n_unique_day)\n    return df\n\ndef n_unique_hour_transformer(df):\n    xid_partition = Window.partitionBy('xid')\n    rank_hour = func.dense_rank().over(xid_partition.orderBy('hour'))\n    n_unique_hour = func.last(rank_hour).over(xid_partition)\n    df = df.withColumn('n_unique_hour', n_unique_hour)\n    return df\n\ndef n_events_per_device_transformer(df):\n    xid_device_partition = Window.partitionBy('xid', 'device')\n    n_events_per_device = func.count(func.col('device')) \\\n        .over(xid_device_partition)\n    df = df.withColumn('n_events_per_device', n_events_per_device)\n    return df\n\ndef n_unique_device_transformer(df):\n    xid_partition = Window.partitionBy('xid')\n    rank_device = func.dense_rank().over(xid_partition.orderBy('device'))\n    n_unique_device = func.last(rank_device).over(xid_partition)\n    df = df.withColumn('n_device', n_unique_device)\n    return df\n\ndef n_actions_per_category_id_transformer(df):\n    xid_category_id_partition = Window.partitionBy('xid', 'category_id',\n                                                   'action')\n    n_actions_per_category_id = func.count(func.col('action')) \\\n        .over(xid_category_id_partition)\n    df = df.withColumn('n_actions_per_category_id', n_actions_per_category_id)\n    return df\n\ndef n_unique_category_id_transformer(df):\n    xid_partition = Window.partitionBy('xid')\n    rank_category_id = func.dense_rank().over(xid_partition\\\n                                              .orderBy('category_id'))\n    n_unique_category_id = func.last(rank_category_id).over(xid_partition)\n    df = df.withColumn('n_unique_category_id', n_unique_category_id)\n    return df\n\ndef n_events_per_category_id_transformer(df):\n    xid_category_id_partition = Window.partitionBy('xid', 'category_id')\n    n_events_per_category_id = func.count(func.col('action')) \\\n        .over(xid_category_id_partition)\n    df = df.withColumn('n_events_per_category_id', n_events_per_category_id)\n    return df\n\ndef n_events_per_website_id_transformer(df):\n    xid_website_id_partition = Window.partitionBy('xid', 'website_id')\n    n_events_per_website_id = func.count(col('action'))\\\n        .over(xid_website_id_partition)\n    df = df.withColumn('n_events_per_website_id', n_events_per_website_id)\n    return df"
  },
  {
    "objectID": "core/notebooks/notebookxx_pg_pandas_spark.html",
    "href": "core/notebooks/notebookxx_pg_pandas_spark.html",
    "title": "PostgreSQL and Spark",
    "section": "",
    "text": "Reading and sriting Spark Dataframes from and to Databases\nCode\nimport pyspark\nfrom pyspark import SparkConf\nfrom pyspark.sql import SparkSession\n\nimport os\nimport getpass"
  },
  {
    "objectID": "core/notebooks/notebookxx_pg_pandas_spark.html#connect-to-pg-server",
    "href": "core/notebooks/notebookxx_pg_pandas_spark.html#connect-to-pg-server",
    "title": "PostgreSQL and Spark",
    "section": "Connect to Pg server",
    "text": "Connect to Pg server\n\n\nCode\nulogin = getpass.getuser()\npw = getpass.getpass()\n\n\n\n---------------------------------------------------------------------------\nStdinNotImplementedError                  Traceback (most recent call last)\nCell In[2], line 2\n      1 ulogin = getpass.getuser()\n----&gt; 2 pw = getpass.getpass()\n\nFile ~/Documents/IFEBY310/.venv/lib/python3.12/site-packages/ipykernel/kernelbase.py:1256, in Kernel.getpass(self, prompt, stream)\n   1254 if not self._allow_stdin:\n   1255     msg = \"getpass was called, but this frontend does not support input requests.\"\n-&gt; 1256     raise StdinNotImplementedError(msg)\n   1257 if stream is not None:\n   1258     import warnings\n\nStdinNotImplementedError: getpass was called, but this frontend does not support input requests.\n\n\n\nSpark jdbc readers and writers rely on a collection of options. Some options are used repeatedly. In order to avoid cut and paste, we pack them in a dictionary.\n\n\nCode\ndico_jdbc_pg = {\n    \"url\":  \"jdbc:postgresql://localhost:5434/bd_2023-24\",  \n    \"user\":  ulogin, \n    \"password\":  pw, \n    \"driver\":  \"org.postgresql.Driver\"\n}\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[3], line 4\n      1 dico_jdbc_pg = {\n      2     \"url\":  \"jdbc:postgresql://localhost:5434/bd_2023-24\",  \n      3     \"user\":  ulogin, \n----&gt; 4     \"password\":  pw, \n      5     \"driver\":  \"org.postgresql.Driver\"\n      6 }\n\nNameError: name 'pw' is not defined\n\n\n\n\n\nCode\ndbschema = 'nycflights'"
  },
  {
    "objectID": "core/notebooks/notebookxx_pg_pandas_spark.html#reading-spark-dataframes-from-a-postgresql-database",
    "href": "core/notebooks/notebookxx_pg_pandas_spark.html#reading-spark-dataframes-from-a-postgresql-database",
    "title": "PostgreSQL and Spark",
    "section": "Reading Spark Dataframes from a PostgreSQL database",
    "text": "Reading Spark Dataframes from a PostgreSQL database\n\n\nCode\nspark_home = \"/home/boucheron/.local/share/spark-3.5.0-bin-hadoop3\"\n\n\n\nTo get started you will need to include the JDBC driver for your particular database on the spark classpath.\n\n\n\nCode\nspark = (\n  SparkSession \n    .builder \n    .appName(\"Python Spark SQL basic example\") \n    .config(\"spark.jars\", \n            spark_home + \"/jars/\" + \"postgresql-42.7.2.jar\") \n    .getOrCreate()\n)\n\n\n25/01/14 22:39:39 WARN Utils: Your hostname, boucheron-Precision-5480 resolves to a loopback address: 127.0.1.1; using 192.168.10.120 instead (on interface wlp0s20f3)\n25/01/14 22:39:39 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n25/01/14 22:39:39 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\nSetting default log level to \"WARN\".\nTo adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel)."
  },
  {
    "objectID": "core/notebooks/notebookxx_pg_pandas_spark.html#downloading-a-table-to-spark",
    "href": "core/notebooks/notebookxx_pg_pandas_spark.html#downloading-a-table-to-spark",
    "title": "PostgreSQL and Spark",
    "section": "Downloading a table to Spark",
    "text": "Downloading a table to Spark\nWe rely on dictionary union and dictionary unpacking to set the options.\n\n\nCode\ndf_airlines = (\n  spark\n    .read\n    .format(\"jdbc\")\n    .options(**(dico_jdbc_pg | {'dbtable': 'nycflights.airlines'}))\n    .load()\n)\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[7], line 5\n      1 df_airlines = (\n      2   spark\n      3     .read\n      4     .format(\"jdbc\")\n----&gt; 5     .options(**(dico_jdbc_pg | {'dbtable': 'nycflights.airlines'}))\n      6     .load()\n      7 )\n\nNameError: name 'dico_jdbc_pg' is not defined\n\n\n\n\n\nCode\ndf_airlines.show(5)\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[8], line 1\n----&gt; 1 df_airlines.show(5)\n\nNameError: name 'df_airlines' is not defined"
  },
  {
    "objectID": "core/notebooks/notebookxx_pg_pandas_spark.html#querying-the-database",
    "href": "core/notebooks/notebookxx_pg_pandas_spark.html#querying-the-database",
    "title": "PostgreSQL and Spark",
    "section": "Querying the database",
    "text": "Querying the database\n\n\nCode\nquery = \"\"\"\n    SELECT DISTINCT fl.carrier, al.name, fl.origin, fl.dest\n    FROM nycflights.airlines al JOIN \n        nycflights.flights fl ON (fl.carrier=al.carrier)\n\"\"\"\n\n\n\n\nCode\ndf_query = (\n  spark\n    .read\n    .format(\"jdbc\")\n    .options(**(dico_jdbc_pg | {'query': query}))\n    .load()\n)\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[10], line 5\n      1 df_query = (\n      2   spark\n      3     .read\n      4     .format(\"jdbc\")\n----&gt; 5     .options(**(dico_jdbc_pg | {'query': query}))\n      6     .load()\n      7 )\n\nNameError: name 'dico_jdbc_pg' is not defined\n\n\n\n\n\nCode\ndf_query.show()\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[11], line 1\n----&gt; 1 df_query.show()\n\nNameError: name 'df_query' is not defined"
  },
  {
    "objectID": "core/notebooks/notebookxx_pg_pandas_spark.html#the-end",
    "href": "core/notebooks/notebookxx_pg_pandas_spark.html#the-end",
    "title": "PostgreSQL and Spark",
    "section": "The end",
    "text": "The end\n\n\nCode\nspark.stop()"
  },
  {
    "objectID": "core/notebooks/xcitibike_spark.html",
    "href": "core/notebooks/xcitibike_spark.html",
    "title": "Building parquet dataset from extracted csv files",
    "section": "",
    "text": "Code\nimport os\nimport sys\nimport re \nimport pandas as pd\nimport datetime\nfrom tqdm import tqdm\n\nos.environ['PYSPARK_PYTHON'] = sys.executable\nos.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable\nCode\ndata_dir = \"../data\"\n# os.path.exists(data_dir)\n\nextract_dir = os.path.join(data_dir, \"xcitibike\")\nif not os.path.exists(extract_dir):\n    os.mkdir(extract_dir)\n\nparquet_dir = os.path.join(data_dir, \"pq_citibike\")\nif not os.path.exists(parquet_dir):\n    os.mkdir(parquet_dir)\n\ncheckpoint_dir = os.path.join(data_dir, \"citibike_charlie\")\nif not os.path.exists(checkpoint_dir):\n    os.mkdir(checkpoint_dir)\n\n\n\n---------------------------------------------------------------------------\nFileNotFoundError                         Traceback (most recent call last)\nCell In[2], line 6\n      4 extract_dir = os.path.join(data_dir, \"xcitibike\")\n      5 if not os.path.exists(extract_dir):\n----&gt; 6     os.mkdir(extract_dir)\n      8 parquet_dir = os.path.join(data_dir, \"pq_citibike\")\n      9 if not os.path.exists(parquet_dir):\n\nFileNotFoundError: [Errno 2] No such file or directory: '../data/xcitibike'\nCode\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql import functions as fn\nfrom pyspark.sql.functions import pandas_udf\nfrom pyspark.sql.types import BooleanType\nfrom pyspark.sql.functions import PandasUDFType\nCode\nspark = (SparkSession\n    .builder\n    .appName(\"Spark building citibike parquet file\")\n    .getOrCreate()\n)\n\n\n25/01/15 06:09:07 WARN Utils: Your hostname, boucheron-Precision-5480 resolves to a loopback address: 127.0.1.1; using 192.168.10.120 instead (on interface wlp0s20f3)\n25/01/15 06:09:07 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\nSetting default log level to \"WARN\".\nTo adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n25/01/15 06:09:08 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\nCode\nspark.sparkContext.setCheckpointDir(checkpoint_dir)\n\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[5], line 1\n----&gt; 1 spark.sparkContext.setCheckpointDir(checkpoint_dir)\n\nNameError: name 'checkpoint_dir' is not defined\nCode\n@pandas_udf(BooleanType())\ndef detect_non_ISO(s: pd.Series) -&gt; bool:\n    r = s.str.match(r\"\\d+/\\d+/\\d+\").any()\n    return r\n\n@pandas_udf(\"string\")\ndef make_iso(s: pd.Series) -&gt; pd.Series:\n    t = s.str.split(' ', expand=True)\n    u = t[0].str.split('/')\n    v = u.map(lambda x  : [x[2], x[0], x[1]]).str.join('-')\n    w = v.combine(t[1], lambda x, y : ' '.join([x, y]))\n    return w\nCode\ndicts_rename = {\n    1: {\n 'tripduration': 'trip_duration',\n 'starttime': 'started_at',\n 'stoptime': 'ended_at',\n 'bikeid': 'bike_id',\n 'usertype': 'user_type',\n 'start station latitude': 'start_lat',\n 'start station longitude': 'start_lng',\n 'end station latitude': 'end_lat',\n 'end station longitude': 'end_lng'\n} ,\n  2:  {\n 'Trip Duration': 'trip_duration',\n  'Start Time': 'started_at',\n  'Stop Time': 'ended_at',\n  'Start Station Latitude': 'start_lat',\n  'Start Station Longitude': 'start_lng',\n  'End Station Latitude': 'end_lat',\n  'End Station Longitude': 'end_lng'    \n}\n}\nCode\nfor (root, dirs ,files) in tqdm(os.walk(extract_dir, topdown=True)):\n    if dirs:\n        continue\n\n    for flnm in files:\n        if not flnm.endswith('.csv'):  \n            continue\n\n        fpath = os.path.join(root, flnm)\n        df = spark.read.option(\"header\",\"true\").csv(fpath)\n\n        df = (\n            df.withColumnsRenamed(dicts_rename[1])\n            .withColumnsRenamed(dicts_rename[2])\n        )\n\n        df = df.toDF(*[c.replace(' ','_').lower() for c in df.columns])\n\n        if re.match(r\"\\d+/\\d+/\\d+\", df.select(\"started_at\").first()[0]):\n            df = (\n                   df\n                    .withColumn('started_at', make_iso(fn.col(\"started_at\")))\n                    .withColumn('ended_at', make_iso(fn.col(\"ended_at\")))\n            ) \n\n        df = df.withColumns(\n                {\n                'started_at': fn.to_timestamp(fn.col(\"started_at\")),\n                'ended_at': fn.to_timestamp(fn.col(\"ended_at\"))\n                }\n            )   \n\n        df = df.withColumns(\n                {\n                    'start_year': fn.year(fn.col('started_at')),\n                    'start_month': fn.month(fn.col('ended_at'))\n                }\n            )\n\n        df.checkpoint(eager=True)\n\n        # df.printSchema()\n\n        df.write.parquet(\n            parquet_dir, \n            partitionBy=['start_year', 'start_month'], \n            mode=\"append\"\n        )\n\n\n0it [00:00, ?it/s]0it [00:00, ?it/s]\nCode\n# spark.stop()"
  },
  {
    "objectID": "core/notebooks/xcitibike_spark.html#references",
    "href": "core/notebooks/xcitibike_spark.html#references",
    "title": "Building parquet dataset from extracted csv files",
    "section": "References",
    "text": "References\nPython vectorized string computations"
  },
  {
    "objectID": "cours-equipe.html",
    "href": "cours-equipe.html",
    "title": "Équipe enseignante",
    "section": "",
    "text": "S. Boucheron Professeur de Mathématiques à l’Université Paris Cité/Laboratoire de Probabilités, Statistiques et Modélisatiion LPSM."
  },
  {
    "objectID": "cours-equipe.html#cours",
    "href": "cours-equipe.html#cours",
    "title": "Équipe enseignante",
    "section": "",
    "text": "S. Boucheron Professeur de Mathématiques à l’Université Paris Cité/Laboratoire de Probabilités, Statistiques et Modélisatiion LPSM."
  },
  {
    "objectID": "cours-equipe.html#travaux-dirigés",
    "href": "cours-equipe.html#travaux-dirigés",
    "title": "Équipe enseignante",
    "section": " Travaux dirigés",
    "text": "Travaux dirigés\n\n\n\nNom\nHoraire\nSalle\n\n\n\n\nStéphane Boucheron\nVendredi 15h45 - 18h15\n2004/5 Sophie Germain\n\n\nCristina Sirangelo\nVendredi 15h45 - 18h15\n2006 Sophie Germain\n\n\nAmine Souiri\nJeudi 13h30 - 16h00\n2006 Sophie Germain\n\n\nSylvain Schmitz\n\n\n\n\nAmélie Gheerbrant\n\n\n\n\nAnatole Dahan"
  },
  {
    "objectID": "course-team.html",
    "href": "course-team.html",
    "title": "Course team",
    "section": "",
    "text": "Teachers 2024-25\n\nC. Sirangelo Professeur d’Informatique à l’Université Paris Cité/Institut de Recherche en Informatique Fondamentale.\nS. Boucheron Professeur de Mathématiques à l’Université Paris Cité/Laboratoire de Probabilités, Statistiques et Modélisatiion LPSM.\n\n\n\n\n\n\n Former contributors\n\nStéphane Gaiffas\nSothéa Has\nAmélie Gheerbrant\nVlady Ravelomanana",
    "crumbs": [
      "Information",
      "Team"
    ]
  },
  {
    "objectID": "notebooks-listings.html",
    "href": "notebooks-listings.html",
    "title": "Notebooks",
    "section": "",
    "text": "Slides provide the framework for the course. You can use them to familiarise yourself with the material for the next lesson, and/or to review previous lessons.\nIcon  points to chalkboard arguments.\n\n\n\n\n\n\n\n\n\nMode d’emploi\n\n\n\nSlides use revealjs de . They are best viewed using a browser.\nKeyboard shortcut  takes you to the Help page.\n\n\n\n\n\n\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Description\n        \n         \n          Jupyter notebook\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\nDate\n\n\nTitle\n\n\nDescription\n\n\nJupyter notebook\n\n\n\n\n\n\nJan 17, 2025\n\n\nPython Stack for Data Science\n\n\nPython\n\n\n\n\n\n\n\nJan 17, 2025\n\n\nNumpy and Scipy\n\n\nPython, Numpy, Scipy, Plotly\n\n\n   \n\n\n\n\nJan 24, 2025\n\n\nTable wranglig with Pandas\n\n\nPython, Pandas\n\n\n   \n\n\n\n\nJan 31, 2025\n\n\nTable wranglig with Dask\n\n\nPython, Dask\n\n\n \n\n\n\n\nFeb 6, 2025\n\n\nSpark Resilient Distributed Datasets\n\n\nPySpark, Spark, RDD\n\n\n   \n\n\n\n\nFeb 7, 2025\n\n\nSpark SQL\n\n\nPySpark, Spark, SQL\n\n\n   \n\n\n\n\nFeb 21, 2025\n\n\nPandas on Spark and SparklyR\n\n\nPySpark, Spark, Pandas, R\n\n\n \n\n\n\n\nMar 7, 2025\n\n\nJSON format\n\n\n \n\n\n \n\n\n\n\nMar 14, 2025\n\n\nWeb data\n\n\n \n\n\n \n\n\n\n\nMar 21, 2025\n\n\nSpark again\n\n\n \n\n\n \n\n\n\n\n\nNo matching items",
    "crumbs": [
      "Notebooks"
    ]
  },
  {
    "objectID": "slides-listings.html",
    "href": "slides-listings.html",
    "title": "Slides",
    "section": "",
    "text": "Slides provide the framework for the course. You can use them to familiarise yourself with the material for the next lesson, and/or to review previous lessons.\nIcon  points to chalkboard arguments.\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Titre\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Description\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\nDate\n\n\nTitre\n\n\nDescription\n\n\n\n\n\n\nJan 17, 2025\n\n\nIntroduction to Big Data\n\n\n\n\n\n\n\nJan 17, 2025\n\n\nPython data stack : Numpy et Scipy\n\n\n\n\n\n\n\nJan 24, 2025\n\n\nPython data stack : Pandas \n\n\n\n\n\n\n\nJan 31, 2025\n\n\nPython data stack : Dask\n\n\n\n\n\n\n\nFeb 6, 2025\n\n\nSpark : RDDs\n\n\n\n\n\n\n\nFeb 7, 2025\n\n\nSpark : SQL\n\n\n\n\n\n\n\nMar 7, 2025\n\n\nJSON\n\n\n\n\n\n\n\nMar 14, 2025\n\n\nFile formats for Big Data\n\n\n\n\n\n\n\nMar 21, 2025\n\n\nSpark: a deeper dive\n\n\n\n\n\n\n\nMar 28, 2025\n\n\nSpark: tips\n\n\n\n\n\n\n\nApr 4, 2025\n\n\nSpark: Applications\n\n\n\n\n\n\n\nApr 11, 2025\n\n\nSpark: Applications\n\n\n\n\n\n\n\nMay 2, 2025\n\n\nPandas on Spark and sparklyr\n\n\n\n\n\n\n\n\nNo matching items\n\n\n\n\n\n\n\n\nMode d’emploi\n\n\n\nSlides use revealjs de . They are best viewed using a browser.\nKeyboard shortcut  takes you to the Help page.",
    "crumbs": [
      "Slides"
    ]
  },
  {
    "objectID": "weeks/week-2.html",
    "href": "weeks/week-2.html",
    "title": "Week 2",
    "section": "",
    "text": "Important\n\n\n\n\nRoom: Sophie Germain 0014\nFriday 24 January 2025 15h45-17h45\n Calendar",
    "crumbs": [
      "Journal",
      "Week 2"
    ]
  },
  {
    "objectID": "weeks/week-2.html#lecture-slides",
    "href": "weeks/week-2.html#lecture-slides",
    "title": "Week 2",
    "section": "Lecture : slides",
    "text": "Lecture : slides\nWe shall briefly come back to several parts of\n\n Python Data Science Stack\n\nData processing\nVisualization",
    "crumbs": [
      "Journal",
      "Week 2"
    ]
  },
  {
    "objectID": "weeks/week-2.html#notebooks",
    "href": "weeks/week-2.html#notebooks",
    "title": "Week 2",
    "section": "Notebooks",
    "text": "Notebooks\nYou shall have gone through (on your own)\n\n Jupyter notebook I : tour of Python\n html: tour of Python\n\nYou shall have gone through\n\n Jupyter notebook II : tour of numpy\n html: tour of numpy\n\nup to Section Sparse Matrices (not included)\nWe shall spend most of the lecture on\n\n Jupyter notebook III : tour of pandas\n html: tour of Pandas",
    "crumbs": [
      "Journal",
      "Week 2"
    ]
  },
  {
    "objectID": "weeks/week-2.html#references",
    "href": "weeks/week-2.html#references",
    "title": "Week 2",
    "section": "References",
    "text": "References\nYou can watch the Introduction to NumPy conference presented at EuroSciPy 2023\n\nThe Pandas book by Wes McKinney\nPandas exercises on Kaggle\n\n\nContenders to Pandas are gaining attention: Polars",
    "crumbs": [
      "Journal",
      "Week 2"
    ]
  },
  {
    "objectID": "weeks/week-2.html#logistics",
    "href": "weeks/week-2.html#logistics",
    "title": "Week 2",
    "section": "Logistics",
    "text": "Logistics\n To work the jupyter notebooks, install python 3, and modules related to jupyter: jupyter-cache, jupyter_client, jupyter_core, jupyterlab_widgets (this induces the installation of dependencies).\nhttps://jupyter.org\nDownload the jupyter notebooks from notebooks listings.\n If you do not already have an ENT account, follow instructions on Moodle to get one. You shall need this account to connect to PostGres cluster.\n\n\nBack to Agenda ⏎",
    "crumbs": [
      "Journal",
      "Week 2"
    ]
  },
  {
    "objectID": "weeks/week-4.html",
    "href": "weeks/week-4.html",
    "title": "Weeks 4-5",
    "section": "",
    "text": "Important\n\n\n\n\n 2 sessions during Week 4/0 session during Week 5\nThursday 3 February 2025 Buffon RH04A 10h45-12h45\nFriday 4 February 2025 Sophie Germain 0014 15h45-17h45\n Calendar",
    "crumbs": [
      "Journal",
      "Week 4"
    ]
  },
  {
    "objectID": "weeks/week-4.html#lecture-slides",
    "href": "weeks/week-4.html#lecture-slides",
    "title": "Weeks 4-5",
    "section": "Lecture : slides",
    "text": "Lecture : slides\n\n Spark low level APIs: RDD\n Spark high level APIs: SQL\n\nWe may come back to several parts of\n\n Python Data Science Stack\n Dask",
    "crumbs": [
      "Journal",
      "Week 4"
    ]
  },
  {
    "objectID": "weeks/week-4.html#notebooks",
    "href": "weeks/week-4.html#notebooks",
    "title": "Weeks 4-5",
    "section": "Notebooks",
    "text": "Notebooks\nWe shall spend most of the lectures on\n\n Jupyter notebook V : Spark RDD\n html: Spark RDD\n\n Jupyter notebook VI : Spark SQL\n html: Spark SQL\n\nand possibly compare with:\nDask Tutorial\nYou shall have gone through (on your own)\n\n Jupyter notebook I : tour of Python\n html: tour of Python\n\n Jupyter notebook II : tour of numpy\n html: tour of numpy\n\n Jupyter notebook III: tour of pandas\n html: tour of Pandas",
    "crumbs": [
      "Journal",
      "Week 4"
    ]
  },
  {
    "objectID": "weeks/week-4.html#references",
    "href": "weeks/week-4.html#references",
    "title": "Weeks 4-5",
    "section": "References",
    "text": "References\n\n\n\nSpark Definitive Guide\nSpark\nSpark\nData pipelines\nNext Generation Databases NoSQLand Big Data, Guy Harrison\nMastering Spark on R\n\n\n\nParquet\n‘Arrow’\nDask Docs\nDask Examples\nDask Code\nDask Blog\nPandas cheatsheet",
    "crumbs": [
      "Journal",
      "Week 4"
    ]
  },
  {
    "objectID": "weeks/week-4.html#logistics",
    "href": "weeks/week-4.html#logistics",
    "title": "Weeks 4-5",
    "section": "Logistics",
    "text": "Logistics\n pyspark\n To work the jupyter notebooks, install python 3, and modules related to jupyter: jupyter-cache, jupyter_client, jupyter_core, jupyterlab_widgets (this induces the installation of dependencies).\nhttps://jupyter.org\nDownload the jupyter notebooks from notebooks listings.\n If you do not already have an ENT account, follow instructions on Moodle to get one. You shall need this account to connect to PostGres cluster.\n\n\nBack to Agenda ⏎",
    "crumbs": [
      "Journal",
      "Week 4"
    ]
  },
  {
    "objectID": "core/slides/slides01_introduction.html#who-are-we",
    "href": "core/slides/slides01_introduction.html#who-are-we",
    "title": "Big data technologies",
    "section": "Who are we ?",
    "text": "Who are we ?\n\n\n\n\n\n\nStéphane Boucheron\nLPSM\nStatistics \n\nhttps://stephane-v-boucheron.fr\n\n\n\n\n\n\nCristina Sirangelo\nIRIF\nData Science, Databases \n\nhttps://www.irif.fr/~amelie/"
  },
  {
    "objectID": "core/slides/slides01_introduction.html#course-logistics-1",
    "href": "core/slides/slides01_introduction.html#course-logistics-1",
    "title": "Big data technologies",
    "section": "Course logistics",
    "text": "Course logistics\n\n24 hours = 2 hours \\(\\times\\) 12 weeks : classes + hands-on\nAgenda\n\nAbout the hands-on\n\nHands-on and homeworks using Jupyter/Quarto notebooks\nUsing a Docker image  built for the course\n Hands-on must be carried out using your own laptop"
  },
  {
    "objectID": "core/slides/slides01_introduction.html#course-logistics-2",
    "href": "core/slides/slides01_introduction.html#course-logistics-2",
    "title": "Big data technologies",
    "section": "Course logistics",
    "text": "Course logistics\n\n course : https://s-v-b.github.io/IFEBY310\n Bookmark it !\n Follow the steps described on the tools page:\n\n\nhttps://s-v-b.github.io/IFEBY310/tools\n\n\nUse"
  },
  {
    "objectID": "core/slides/slides01_introduction.html#course-evaluation",
    "href": "core/slides/slides01_introduction.html#course-evaluation",
    "title": "Big data technologies",
    "section": "Course evaluation",
    "text": "Course evaluation\n\nEvaluation using homeworks and a final project\nFind a friend: all work done by pairs of students\nAll your work goes in your private git repository and nowhere else: no emails !\nAll your homework will be using quarto files"
  },
  {
    "objectID": "core/slides/slides01_introduction.html#why-docker-what-is-it",
    "href": "core/slides/slides01_introduction.html#why-docker-what-is-it",
    "title": "Big data technologies",
    "section": "Why docker ? What is it ?",
    "text": "Why docker ? What is it ?\n\nDon’t mess with your python env. and configuration files\nEverything in embedded in a container (better than a Virtual Machine)\nA container is an instance of an image\n\nSame image = same environment for everybody\nSame image = no {version, dependencies, install} problems\nIt is an entreprise standard used everywhere now!"
  },
  {
    "objectID": "core/slides/slides01_introduction.html#docker-1",
    "href": "core/slides/slides01_introduction.html#docker-1",
    "title": "Big data technologies",
    "section": "\n docker\n",
    "text": "docker\n\n\nHave a look at https://s-v-b.github.io/IFEBY310/tools\nHave a look at the Dockerfile to explain a little bit how the image is built\nPerform a quick demo on how to use the docker image\n\n\n\n\n\n\n\nAnd that’s it for logistics !"
  },
  {
    "objectID": "core/slides/slides01_introduction.html#big-data-1",
    "href": "core/slides/slides01_introduction.html#big-data-1",
    "title": "Big data technologies",
    "section": "Big data",
    "text": "Big data\n\nMoore’s Law: computing power doubled every two years between 1975 and 2012\nNowadays, less than two years and a half\nRapid growth of datasets: internet activity, social networks, genomics, physics, censor networks, IOT, …\nData size trends: doubles every year according to IDC executive summary\nData deluge: Today, data is growing faster than computing power"
  },
  {
    "objectID": "core/slides/slides01_introduction.html#question",
    "href": "core/slides/slides01_introduction.html#question",
    "title": "Big data technologies",
    "section": "Question \n",
    "text": "Question \n\n\nHow do we catch up to process the data deluge and to learn from it ?"
  },
  {
    "objectID": "core/slides/slides01_introduction.html#order-of-magnitudes",
    "href": "core/slides/slides01_introduction.html#order-of-magnitudes",
    "title": "Big data technologies",
    "section": "Order of magnitudes",
    "text": "Order of magnitudes\nbit\nA bit is a value of either a 1 or 0 (on or off)\nbyte (B)\nA byte is made of 8 bits\n\n1 character, e.g., “a”, is one byte\n\nKilobyte (KB)\nA kilobyte is \\(1024 =2^{10}\\) bytes\n\n\n2 or 3 paragraphs of ASCII text"
  },
  {
    "objectID": "core/slides/slides01_introduction.html#some-more-comparisons",
    "href": "core/slides/slides01_introduction.html#some-more-comparisons",
    "title": "Big data technologies",
    "section": "Some more comparisons",
    "text": "Some more comparisons\nMegabyte (MB)\nA megabyte is \\(1 048 576=2^{20}\\) B or \\(1 024\\) KB\n\n\n873 pages of plain text\n\n4 books (200 pages or 240 000 characters)\n\nGigabyte (GB)\nA gigabyte is \\(1 073 741 824=2^{30}\\) B, \\(1 024\\) MB or \\(1 048 576\\) KB\n\n\n894 784 pages of plain text (1 200 characters)\n\n4 473 books (200 pages or 240 000 characters)\n\n640 web pages (with 1.6 MB average file size)\n\n341 digital pictures (with 3 MB average file size)\n\n256 MP3 audio files (with 4 MB average file size)\n\n1,5 650 MB CD"
  },
  {
    "objectID": "core/slides/slides01_introduction.html#even-more",
    "href": "core/slides/slides01_introduction.html#even-more",
    "title": "Big data technologies",
    "section": "Even more",
    "text": "Even more\nTerabyte (TB)\nA terabyte is \\(1 099 511 627 776=2^{40}\\) B, 1 024 GB or 1 048 576 MB.\n\n\n916 259 689 pages of plain text (1 200 characters)\n\n4 581 298 books (200 pages or 240 000 characters)\n\n655 360 web pages (with 1.6 MB average file size)\n\n349 525 digital pictures (with 3 MB average file size)\n\n262 144 MP3 audio files (with 4 MB average file size)\n\n1 613 650 MB CD’s\n\n233 4.38 GB DVDs\n\n40 25 GB Blu-ray discs"
  },
  {
    "objectID": "core/slides/slides01_introduction.html#the-deluge",
    "href": "core/slides/slides01_introduction.html#the-deluge",
    "title": "Big data technologies",
    "section": "The deluge",
    "text": "The deluge\nPetabyte (PB)\nA petabyte is 1 024 TB, 1 048 576 GB or 1 073 741 824 MB\n\\[1125899906842624 = 2^{50} \\quad\\text{Bytes}\\]\n\n\n938 249 922 368 pages of plain text (1 200 characters)\n\n4 691 249 611 books (200 pages or 240 000 characters)\n\n671 088 640 web pages (with 1.6 MB average file size)\n\n357 913 941 digital pictures (with 3 MB average file size)\n\n268 435 456 MP3 audio files (with 4 MB average file size)\n\n1 651 910 650 MB CD’s\n\n239 400 4.38 GB DVDs\n\n41 943 25 GB Blu-ray discs\n\nExabyte, etc.\n\n1 EB = 1 exabyte = 1 024 PB\n1 ZB = 1 zettabyte = 1 024 EB"
  },
  {
    "objectID": "core/slides/slides01_introduction.html#some-figures",
    "href": "core/slides/slides01_introduction.html#some-figures",
    "title": "Big data technologies",
    "section": "Some figures",
    "text": "Some figures\nYou have every single second1:\n\nAt least 8,000 tweets sent\n900+ photos posted on Instagram\nThousands of Skype calls made\nOver 70,000 Google searches performed\nAround 80,000 YouTube videos viewed\nOver 2 million emails sent\nhttps://www.internetlivestats.com"
  },
  {
    "objectID": "core/slides/slides01_introduction.html#some-figures-1",
    "href": "core/slides/slides01_introduction.html#some-figures-1",
    "title": "Big data technologies",
    "section": "Some figures",
    "text": "Some figures\nThere are1:\n\n\n5 billion web pages as of mid-2019 (indexed web)\n\nand we expected2:\n\n\n4.8 ZB of annual IP traffic in 2022\n\nNote that\n\n\n1 ZB \\(\\approx\\) 36 000 years of HD video\nNetflix’s entire catalog is \\(\\approx\\) 3.5 years of HD video\nhttps://www.worldwidewebsize.comCisco’s Visual Networking Index"
  },
  {
    "objectID": "core/slides/slides01_introduction.html#some-figures-2",
    "href": "core/slides/slides01_introduction.html#some-figures-2",
    "title": "Big data technologies",
    "section": "Some figures",
    "text": "Some figures\nMore figures :\n\nfacebook daily logs: 60TB\n1000 genomes project: 200TB\nGoogle web index: 10+ PB\nCost of 1TB of storage: ~$35\nTime to read 1TB from disk: 3 hours if 100MB/s"
  },
  {
    "objectID": "core/slides/slides01_introduction.html#latency-numbers",
    "href": "core/slides/slides01_introduction.html#latency-numbers",
    "title": "Big data technologies",
    "section": "Latency numbers",
    "text": "Latency numbers\n\n\n\n\n\n\n\n\n\nMemory type\nLatency(ns)\nLatency(us)\n(ms)\n\n\n\n\nL1 cache reference\n0.5 ns\n\n\n\n\n\nL2 cache reference\n7 ns\n\n\n14x L1 cache\n\n\nMain memory reference\n100 ns\n\n\n20x L2, 200x L1\n\n\nCompress 1K bytes with Zippy/Snappy\n3,000 ns\n3 us\n\n\n\n\nSend 1K bytes over 1 Gbps network\n10,000 ns\n10 us\n\n\n\n\nRead 4K randomly from SSD*\n150,000 ns\n150 us\n\n~1GB/sec SSD\n\n\nRead 1 MB sequentially from memory\n250,000 ns\n250 us\n\n\n\n\nRound trip within same datacenter\n500,000 ns\n500 us\n\n\n\n\nRead 1 MB sequentially from SSD*\n1,000,000 ns\n1,000 us\n1 ms\n~1GB/sec SSD, 4X memory\n\n\nDisk seek\n10,000,000 ns\n10,000 us\n10 ms\n20x datacenter roundtrip\n\n\nRead 1 MB sequentially from disk\n20,000,000 ns\n20,000 us\n20 ms\n80x memory, 20x SSD\n\n\nSend packet US -&gt; Europe -&gt; US\n150,000,000 ns\n150,000 us\n150 ms\n600x memory"
  },
  {
    "objectID": "core/slides/slides01_introduction.html#section-2",
    "href": "core/slides/slides01_introduction.html#section-2",
    "title": "Big data technologies",
    "section": "",
    "text": "traceroute to mathscinet.ams.org (104.238.176.204), 64 hops max\n  1   192.168.10.1  3,149ms  1,532ms  1,216ms \n  2   192.168.0.254  1,623ms  1,397ms  1,309ms \n  3   78.196.1.254  2,571ms  2,120ms  2,371ms \n  4   78.255.140.126  2,813ms  2,621ms  2,200ms \n  5   78.254.243.86  2,626ms  2,528ms  2,517ms \n  6   78.254.253.42  2,517ms  4,129ms  2,671ms \n  7   78.254.242.54  2,535ms  2,258ms  2,350ms \n  8   *  *  * \n  9   195.66.224.191  12,231ms  11,718ms  12,486ms \n 10   *  *  * \n 11   63.218.14.58  26,213ms  19,264ms  18,949ms \n 12   63.218.231.106  29,135ms  22,078ms  17,954ms"
  },
  {
    "objectID": "core/slides/slides01_introduction.html#latency-numbers-1",
    "href": "core/slides/slides01_introduction.html#latency-numbers-1",
    "title": "Big data technologies",
    "section": "Latency numbers",
    "text": "Latency numbers\n\nReading 1MB from disk = 100 x reading 1MB from memory\nSending packet from US to Europe to US = 1 000 000 x main memory reference\n\nGeneral tendency\nTrue in general, not always:\n\nmemory operations : fastest\ndisk operations : slow\nnetwork operations : slowest"
  },
  {
    "objectID": "core/slides/slides01_introduction.html#latency-numbers-httpswww.eecs.berkeley.edurcsresearchinteractive_latency.html",
    "href": "core/slides/slides01_introduction.html#latency-numbers-httpswww.eecs.berkeley.edurcsresearchinteractive_latency.html",
    "title": "Big data technologies",
    "section": "Latency numbers 1(https://www.eecs.berkeley.edu/~rcs/research/interactive_latency.html)",
    "text": "Latency numbers 1(https://www.eecs.berkeley.edu/~rcs/research/interactive_latency.html)\n\n\n\nhttps://www.eecs.berkeley.edu/~rcs/research/interactive_latency.html"
  },
  {
    "objectID": "core/slides/slides01_introduction.html#latency-numbers-for-mortals",
    "href": "core/slides/slides01_introduction.html#latency-numbers-for-mortals",
    "title": "Big data technologies",
    "section": "Latency numbers for mortals",
    "text": "Latency numbers for mortals\nMultiply all durations by a billion \\(10^9\\)\n\n\n\n\n\n\n\nMemory type\nLatency\nHuman duration\n\n\n\nL1 cache reference\n0.5 s\nOne heart beat (0.5 s)\n\n\nL2 cache reference\n7 s\nLong yawn\n\n\nMain memory reference\n100 s\nBrushing your teeth\n\n\nSend 2K bytes over 1 Gbps network\n5.5 hr\nFrom lunch to end of work day\n\n\nSSD random read\n1.7 days\nA normal weekend\n\n\nRead 1 MB sequentially from memory\n2.9 days\nA long weekend\n\n\nRound trip within same datacenter\n5.8 days\nA medium vacation\n\n\nRead 1 MB sequentially from SSD\n11.6 days\nWaiting for almost 2 weeks for a delivery\n\n\nDisk seek\n16.5 weeks\nA semester in university\n\n\nRead 1 MB sequentially from disk\n7.8 months\nAlmost producing a new human being\n\n\nSend packet US -&gt; Europe -&gt; US\n4.8 years\nAverage time it takes to complete a bachelor’s degree"
  },
  {
    "objectID": "core/slides/slides01_introduction.html#challenges-with-big-datasets",
    "href": "core/slides/slides01_introduction.html#challenges-with-big-datasets",
    "title": "Big data technologies",
    "section": "Challenges with big datasets",
    "text": "Challenges with big datasets\n\nLarge data don’t fit on a single hard-drive\nOne large (and expensive) machine can’t process or store all the data\nFor computations how do we stream data from the disk to the different layers of memory ?\nConcurrent accesses to the data: disks cannot be read in parallel"
  },
  {
    "objectID": "core/slides/slides01_introduction.html#solutions",
    "href": "core/slides/slides01_introduction.html#solutions",
    "title": "Big data technologies",
    "section": "Solutions",
    "text": "Solutions\n\nCombine several machines containing hard drives and processors on a network\nUsing commodity hardware: cheap, common architecture i.e. processor + RAM + disk\nScalability = more machines on the network\nPartition the data across the machines"
  },
  {
    "objectID": "core/slides/slides01_introduction.html#challenges-1",
    "href": "core/slides/slides01_introduction.html#challenges-1",
    "title": "Big data technologies",
    "section": "Challenges",
    "text": "Challenges\nDealing with distributed computations adds software complexity\n\nScheduling\n\nHow to split the work across machines? Must exploit and optimize data locality since moving data is very expensive\n\nReliability\n\nHow to handle failures? Commodity (cheap) hardware fails more often. @Google [1%, 5%] HD failure/year and 0.2% DIMM failure/year\n\nUneven performance of machines\n\nsome nodes are slower than others"
  },
  {
    "objectID": "core/slides/slides01_introduction.html#section-3",
    "href": "core/slides/slides01_introduction.html#section-3",
    "title": "Big data technologies",
    "section": "",
    "text": "Problems sketched in\n\n\nNext Generation Dabases describes the challenges faces by database industry between 1995 and 2015, that is during the onset of the data deluge"
  },
  {
    "objectID": "core/slides/slides01_introduction.html#solutions-1",
    "href": "core/slides/slides01_introduction.html#solutions-1",
    "title": "Big data technologies",
    "section": "Solutions",
    "text": "Solutions\n\nSchedule, manage and coordinate threads and resources using appropriate software\nLocks to limit access to resources\nReplicate data for faster reading and reliability"
  },
  {
    "objectID": "core/slides/slides01_introduction.html#is-it-hpc",
    "href": "core/slides/slides01_introduction.html#is-it-hpc",
    "title": "Big data technologies",
    "section": "Is it HPC ?",
    "text": "Is it HPC ?\n\nHigh Performance Computing (HPC)\nParallel computing\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nFor HPC, scaling up means using a bigger machine\n\nHuge performance increase for medium scale problems\n\nVery expensive, specialized machines, lots of processors and memory\n\n\n\n\n\n\n\n\n\n\n\n\n\nNo"
  },
  {
    "objectID": "core/slides/slides01_introduction.html#and",
    "href": "core/slides/slides01_introduction.html#and",
    "title": "Big data technologies",
    "section": "\n and \n",
    "text": "and \n\n\nGoogle committed to a number of key tenants when designing its data center architecture. Most significantly —and at the time, uniquely— Google committed to massively parallelizing and distributing processing across very large numbers of commodity servers.\n\n\nGoogle also adopted a “Jedis build their own lightsabers” attitude: very little third party —and virtually no commercial— software would be found in the Google architecture.\n\n\nBuild was considered better than buy at Google.\n\nFrom Next Generation Dabases"
  },
  {
    "objectID": "core/slides/slides01_introduction.html#the-big-data-universe",
    "href": "core/slides/slides01_introduction.html#the-big-data-universe",
    "title": "Big data technologies",
    "section": "The Big Data universe",
    "text": "The Big Data universe\nMany technologies combining software and cloud computing"
  },
  {
    "objectID": "core/slides/slides01_introduction.html#the-big-data-universe-still-expanding",
    "href": "core/slides/slides01_introduction.html#the-big-data-universe-still-expanding",
    "title": "Big data technologies",
    "section": "The Big Data universe (still expanding)",
    "text": "The Big Data universe (still expanding)\nOften used with/for with Machine Learning (or AI)"
  },
  {
    "objectID": "core/slides/slides01_introduction.html#tools",
    "href": "core/slides/slides01_introduction.html#tools",
    "title": "Big data technologies",
    "section": "Tools \n",
    "text": "Tools \n\n\nSoftwares such as HadoopMR (Hadoop Map Reduce) and more recently Spark and Dask cope with these challenges\nThey are distributed computational engines: softwares that ease the development of distributed algorithms\n\nThey run on clusters (several machine on a network), managed by a resource manager such as :\n\n\n Yarn : https://hadoop.apache.org/docs/current/hadoop-yarn/hadoop-yarn-site/YARN.html\n\nKubernetes : https://kubernetes.io\n\n\nA resource manager ensures that the tasks running on the cluster do not try to use the same resources all at once"
  },
  {
    "objectID": "core/slides/slides01_introduction.html#apache-spark-1",
    "href": "core/slides/slides01_introduction.html#apache-spark-1",
    "title": "Big data technologies",
    "section": "Apache Spark\n",
    "text": "Apache Spark\n\nThe course will focus mainly on Spark for big data processing\n\n\nhttps://spark.apache.org\n\n\n\nSpark is an enterprise standard  (cf https://spark.apache.org/powered-by.html)\nOne of the most used big data processing framework\n\nOpen source\n\nThe predecessor of Spark is Hadoop\nSee Chapter 2 in Next Generation Dabases\nGuy Harrison"
  },
  {
    "objectID": "core/slides/slides01_introduction.html#hadoop",
    "href": "core/slides/slides01_introduction.html#hadoop",
    "title": "Big data technologies",
    "section": "Hadoop",
    "text": "Hadoop\n\nHadoop has a simple API and good fault tolerance (tolerance to nodes failing midway through a processing job)\nThe cost is lots of data shuffling across the network\nWith intermediate computations written to disk over the network which we know is very time expensive\n\nIt is made of three components:\n\nHDFS (Highly Distributed File System) inspired from GoogleFileSystem, see https://ai.google/research/pubs/pub51\nYARN (Yet Another Ressource Negociator) for processing management.\nMapReduce inspired from Google for processing again.https://research.google.com/archive/mapreduce.html"
  },
  {
    "objectID": "core/slides/slides01_introduction.html#section-4",
    "href": "core/slides/slides01_introduction.html#section-4",
    "title": "Big data technologies",
    "section": "",
    "text": "The Hadoop 1.0 architecture is powerful and easy to understand, but it is limited to MapReduce workloads and it provides limited flexibility with regard to scheduling and resource allocation.\n\n\nIn the Hadoop 2.0 architecture, YARN (Yet Another Resource Negotiator or, recursively, YARN Application Resource Negotiator) improves scalability and flexibility by splitting the roles of the Task Tracker into two processes.\n\n\nA Resource Manager controls access to the clusters resources (memory, CPU, etc.) while the Application Manager (one per job) controls task execution.\n\nGuy Harrison. Next Generation Database"
  },
  {
    "objectID": "core/slides/slides01_introduction.html#mapreduces-wordcount-example",
    "href": "core/slides/slides01_introduction.html#mapreduces-wordcount-example",
    "title": "Big data technologies",
    "section": "MapReduce’s wordcount example",
    "text": "MapReduce’s wordcount example"
  },
  {
    "objectID": "core/slides/slides01_introduction.html#spark",
    "href": "core/slides/slides01_introduction.html#spark",
    "title": "Big data technologies",
    "section": "Spark",
    "text": "Spark\nAdvantages of Spark over HadoopMR ?\n\n\nIn-memory storage: use RAM for fast iterative computations\n\nLower overhead for starting jobs\n\nSimple and expressive with Scala, Python, R, Java APIs\n\nHigher level libraries with SparkSQL, SparkStreaming, etc.\n\nDisadvantages of Spark over HadoopMR ?\n\n\nSpark requires servers with more CPU and more memory\n\nBut still much cheaper than HPC\n\nSpark is much faster than Hadoop\n\n\nHadoop uses disk and network\n\n\nSpark tries to use memory as much as possible for operations while minimizing network use"
  },
  {
    "objectID": "core/slides/slides01_introduction.html#spark-versus-hadoop",
    "href": "core/slides/slides01_introduction.html#spark-versus-hadoop",
    "title": "Big data technologies",
    "section": "\nSpark versus Hadoop\n",
    "text": "Spark versus Hadoop\n\n\n\n\n\n\n\n\n\nHadoopMR\nSpark\n\n\n\nStorage\nDisk\nin-memory or disk\n\n\nOperations\nMap, reduce\nMap, reduce, join, sample, …\n\n\nExecution model\nBatch\nBatch, interactive, streaming \n\n\nProgramming environments\nJava\nScala, Java, Python, R"
  },
  {
    "objectID": "core/slides/slides01_introduction.html#spark-and-hadoop-comparison",
    "href": "core/slides/slides01_introduction.html#spark-and-hadoop-comparison",
    "title": "Big data technologies",
    "section": "\nSpark and Hadoop comparison",
    "text": "Spark and Hadoop comparison\nFor logistic regression training (a simple classification algorithm which requires several passes on a dataset)"
  },
  {
    "objectID": "core/slides/slides01_introduction.html#the-spark-stack",
    "href": "core/slides/slides01_introduction.html#the-spark-stack",
    "title": "Big data technologies",
    "section": "The Spark stack",
    "text": "The Spark stack"
  },
  {
    "objectID": "core/slides/slides01_introduction.html#the-spark-stack-1",
    "href": "core/slides/slides01_introduction.html#the-spark-stack-1",
    "title": "Big data technologies",
    "section": "The Spark stack",
    "text": "The Spark stack"
  },
  {
    "objectID": "core/slides/slides01_introduction.html#spark-can-run-everywhere",
    "href": "core/slides/slides01_introduction.html#spark-can-run-everywhere",
    "title": "Big data technologies",
    "section": "\nSpark can run “everywhere”",
    "text": "Spark can run “everywhere”"
  },
  {
    "objectID": "core/slides/slides01_introduction.html#section-5",
    "href": "core/slides/slides01_introduction.html#section-5",
    "title": "Big data technologies",
    "section": "",
    "text": "https://mesos.apache.org: Apache Mesos abstracts CPU, memory, storage, and other compute resources away from machines (physical or virtual), enabling fault-tolerant and elastic distributed systems to easily be built and run effectively. Mesos is built using the same principles as the Linux kernel, only at a different level of abstraction. The Mesos kernel runs on every machine and provides applications (e.g., Hadoop, Spark, Kafka, Elasticsearch) with API’s for resource management and scheduling across entire datacenter and cloud environments.\nhttps://kubernetes.io Kubernetes, also known as K8s, is an open-source system for automating deployment, scaling, and management of containerized applications."
  },
  {
    "objectID": "core/slides/slides01_introduction.html#very-tentative-agenda-for-the-course",
    "href": "core/slides/slides01_introduction.html#very-tentative-agenda-for-the-course",
    "title": "Big data technologies",
    "section": "Very tentative agenda for the course",
    "text": "Very tentative agenda for the course\nWeeks 1, 2 and 3  The Python data-science stack for medium-scale problems\nWeeks 4 and 5  Introduction to spark and its low-level API\nWeeks 6, 7 and 8 Spark’s high level API: .stress[spark.sql]. Data from different formats and sources\nWeek 9  Run a job on a cluster with spark-submit, monitoring, mistakes and debugging\nWeeks 10, 11, 12  Introduction to spark applications and spark-streaming"
  },
  {
    "objectID": "core/slides/slides01_introduction.html#main-tools-for-the-course-tentative",
    "href": "core/slides/slides01_introduction.html#main-tools-for-the-course-tentative",
    "title": "Big data technologies",
    "section": "Main tools for the course (tentative…)",
    "text": "Main tools for the course (tentative…)\nInfrastructure\n\n\n\nPython stack\n\n\n\nData Visualization"
  },
  {
    "objectID": "core/slides/slides01_introduction.html#main-tools-for-the-course-tentative-1",
    "href": "core/slides/slides01_introduction.html#main-tools-for-the-course-tentative-1",
    "title": "Big data technologies",
    "section": "Main tools for the course (tentative…)",
    "text": "Main tools for the course (tentative…)\nBig data processing\n\n\n\nData storage / formats / querying"
  },
  {
    "objectID": "core/slides/slides01_introduction.html#learning-resources",
    "href": "core/slides/slides01_introduction.html#learning-resources",
    "title": "Big data technologies",
    "section": "Learning resources",
    "text": "Learning resources\n\nSpark Documentation Website http://spark.apache.org/docs/latest/\nAPI docs http://spark.apache.org/docs/latest/api/scala/index.html http://spark.apache.org/docs/latest/api/python/\nDatabricks learning notebooks https://databricks.com/resources\nStackOverflow https://stackoverflow.com/tags/apache-spark https://stackoverflow.com/tags/pyspark\nMore advanced http://books.japila.pl/apache-spark-internals/\nMisc. Next Generation Databases: NoSQLand Big Data by Guy HarrisonData Pipelines Pocket Reference by J. Densmore"
  },
  {
    "objectID": "core/slides/slides01_introduction.html#learning-resources-1",
    "href": "core/slides/slides01_introduction.html#learning-resources-1",
    "title": "Big data technologies",
    "section": "Learning Resources",
    "text": "Learning Resources\n\n\n\n\nBook: Spark The Definitive Guide http://shop.oreilly.com/product/0636920034957.do https://github.com/databricks/Spark-The-Definitive-Guide\n\n\n\n\n\n\nAbove all"
  },
  {
    "objectID": "core/slides/slides01_introduction.html#data-centers-1",
    "href": "core/slides/slides01_introduction.html#data-centers-1",
    "title": "Big data technologies",
    "section": "Data centers",
    "text": "Data centers\nWonder what a datacenter looks like ?\n\nHave a look at http://www.google.com/about/datacenters"
  },
  {
    "objectID": "core/slides/slides01_introduction.html#data-centers-2",
    "href": "core/slides/slides01_introduction.html#data-centers-2",
    "title": "Big data technologies",
    "section": "Data centers",
    "text": "Data centers\nWonder what a datacenter looks like ?"
  },
  {
    "objectID": "core/slides/slides01_introduction.html#data-centers-3",
    "href": "core/slides/slides01_introduction.html#data-centers-3",
    "title": "Big data technologies",
    "section": "Data centers",
    "text": "Data centers\nWonder what a datacenter looks like ?"
  },
  {
    "objectID": "core/slides/slides02_python_ds_stack.html#features-of-python-1",
    "href": "core/slides/slides02_python_ds_stack.html#features-of-python-1",
    "title": "Python Data Science Stack",
    "section": "Features of Python\n",
    "text": "Features of Python\n\n\nHigh-level data types (tuples, dict, list, set, etc.)\nStandard libraries with batteries included\n\nString services,\nRegular expressions\nDatetime\n…\n\n\nLibraries for scientific computing\nEasy and efficient I/O, many file formats\nOS, threading, multiprocessing\nNetworking, email, html, webserver, scrapping\nCan be extended with C/C++ and easily accelerated (cython, numba, pypy)\nTons of external libraries"
  },
  {
    "objectID": "core/slides/slides02_python_ds_stack.html#features-of-python-2",
    "href": "core/slides/slides02_python_ds_stack.html#features-of-python-2",
    "title": "Python Data Science Stack",
    "section": "Features of Python\n",
    "text": "Features of Python"
  },
  {
    "objectID": "core/slides/slides02_python_ds_stack.html#the-stackoverflow-2023-survey",
    "href": "core/slides/slides02_python_ds_stack.html#the-stackoverflow-2023-survey",
    "title": "Python Data Science Stack",
    "section": "The stackoverflow 2023 survey\n",
    "text": "The stackoverflow 2023 survey"
  },
  {
    "objectID": "core/slides/slides02_python_ds_stack.html#python-popularity-growth",
    "href": "core/slides/slides02_python_ds_stack.html#python-popularity-growth",
    "title": "Python Data Science Stack",
    "section": "\nPython popularity growth",
    "text": "Python popularity growth"
  },
  {
    "objectID": "core/slides/slides02_python_ds_stack.html#python-popularity-growth-1",
    "href": "core/slides/slides02_python_ds_stack.html#python-popularity-growth-1",
    "title": "Python Data Science Stack",
    "section": "\nPython popularity growth",
    "text": "Python popularity growth"
  },
  {
    "objectID": "core/slides/slides02_python_ds_stack.html#numpy",
    "href": "core/slides/slides02_python_ds_stack.html#numpy",
    "title": "Python Data Science Stack",
    "section": "Numpy",
    "text": "Numpy\n\n\n\n\n\n\n\n\nnumpy is all about multi-dimensional arrays and matrices\nhigh-level computation such as\n\nlinear algebra: numpy.linalg\n\nrandom number generation:numpy.random\n\n\n\nFast but not optimized for multi-threaded architectures\nNot for distributed multi-machine settings"
  },
  {
    "objectID": "core/slides/slides02_python_ds_stack.html#scipy",
    "href": "core/slides/slides02_python_ds_stack.html#scipy",
    "title": "Python Data Science Stack",
    "section": "Scipy",
    "text": "Scipy\n\n\n\n\n\n\n\n\nscipy extends numpy with extra modules:\n\noptimization,\nintegration,\nFFT, signal and image processing\n…\n\n\nSparse matrix formats in scipy.sparse"
  },
  {
    "objectID": "core/slides/slides02_python_ds_stack.html#pandas",
    "href": "core/slides/slides02_python_ds_stack.html#pandas",
    "title": "Python Data Science Stack",
    "section": "Pandas",
    "text": "Pandas\n\n\n\n\n\n\n\n\npandas builds upon numpy to provide a high-performance, easy-to-use DataFrame object, with high-level data processing\nEasy I/O with most data format : csv, json, hdf5, feather, parquet, etc.\n\nSQL semantics: select, filter, join, groupby, agg, , where, etc.\nVery large general-purpose library for data processing, not distributed, medium scale data only\n\n\n\n\n\n\n\n\n\n\nLinks\n\n\n\nPandas online book\nPandas homepage\nPolars homepage\nPolars versus Pandas"
  },
  {
    "objectID": "core/slides/slides02_python_ds_stack.html#dask",
    "href": "core/slides/slides02_python_ds_stack.html#dask",
    "title": "Python Data Science Stack",
    "section": "Dask",
    "text": "Dask\n\n\n\n\n\n\n\n\ndask is roughly a distributed and parallel pandas\n\nSame API has pandas !\nTask scheduling, lazy evaluation, distributed dataframes\nStill young and far behind spark, but can be useful\nEasier than spark, full Python (no JVM)\n\n\n\n\n\n\n\n\n\n\nLinks\n\n\n\nDask homepage"
  },
  {
    "objectID": "core/slides/slides02_python_ds_stack.html#pyspark",
    "href": "core/slides/slides02_python_ds_stack.html#pyspark",
    "title": "Python Data Science Stack",
    "section": "Pyspark",
    "text": "Pyspark\n\n\n\n\n\n\n\n\npyspark is the python API to spark, a big data processing framework\nWe will use it a lot in this course\nNative API to spark is scala: pyspark can be slower (much slower if you are not careful)\n\n\n\n\n\n\n\n\n\n\nLinks\n\n\n\nPyspark documentation\nSpark Apache Project"
  },
  {
    "objectID": "core/slides/slides02_python_ds_stack.html#sqlalchemy",
    "href": "core/slides/slides02_python_ds_stack.html#sqlalchemy",
    "title": "Python Data Science Stack",
    "section": "SQLAlchemy",
    "text": "SQLAlchemy\n\n\n\n\n\n\nObject Relational Model (ORM)\nODBC\n\n\n\n\n\n\n\n\n\nLinks\n\n\n\nSQl Alchemy homepage\npsycopg2\npsycopg documentation"
  },
  {
    "objectID": "core/slides/slides02_python_ds_stack.html#pyarrow",
    "href": "core/slides/slides02_python_ds_stack.html#pyarrow",
    "title": "Python Data Science Stack",
    "section": "Pyarrow",
    "text": "Pyarrow\n\n\n\n\n\n\n\nThe universal columnar format and multi-language toolbox for fast data interchange and in-memory analytics\n\n\nApache Arrow defines a language-independent columnar memory format for flat and hierarchical data, organized for efficient analytic operations on modern hardware like CPUs and GPUs. The Arrow memory format also supports zero-copy reads for lightning-fast data access without serialization overhead.\n\n\n\n\n\n\n\n\n\n\nLinks\n\n\n\nApache Arrow Project Homepage\nPyarrow documentation"
  },
  {
    "objectID": "core/slides/slides02_python_ds_stack.html#matplotlib",
    "href": "core/slides/slides02_python_ds_stack.html#matplotlib",
    "title": "Python Data Science Stack",
    "section": "Matplotlib",
    "text": "Matplotlib\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmatplotlib provides versatile 2D plotting capabilities\n\nscientific computing\ndata visualization\n\n\nLarge and customizable library\nThe historical one, somewhat low-level when plotting things related to data\n\n\n\n\n\n\n\n\n\nLinks\n\n\nMatplotlib Homepage"
  },
  {
    "objectID": "core/slides/slides02_python_ds_stack.html#plotly",
    "href": "core/slides/slides02_python_ds_stack.html#plotly",
    "title": "Python Data Science Stack",
    "section": "Plotly",
    "text": "Plotly\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAn interactive visualization library for web browsers based on javascript graphic library d3.js\n\nWith a clean and simple python interface, can be used in a jupyter notebook\nInteractions enabled by default (zoom, etc.) and fast rendering\nVery good looking plots with good default parameters\n\n\n\n\n\n\n\n\n\nLinks\n\n\n\nPlotly homepage"
  },
  {
    "objectID": "core/slides/slides02_python_ds_stack.html#altair",
    "href": "core/slides/slides02_python_ds_stack.html#altair",
    "title": "Python Data Science Stack",
    "section": "Altair",
    "text": "Altair\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVega-Altair: Declarative Visualization in Python\nVega-Altair is a declarative visualization library for Python. Its simple, friendly and consistent API, built on top of the powerful Vega-Lite grammar, empowers you to spend less time writing code and more time exploring your data.\n\n\n\n\n\n\n\n\n\n\nLinks\n\n\n\nAltair homepage\nVega-Lite: A Grammar of Interactive Graphics"
  },
  {
    "objectID": "core/slides/slides02_python_ds_stack.html#dash",
    "href": "core/slides/slides02_python_ds_stack.html#dash",
    "title": "Python Data Science Stack",
    "section": "Dash",
    "text": "Dash\n\n\n\n\n\n\n\n\nLinks\n\n\nDash homepage"
  },
  {
    "objectID": "core/slides/slides02_python_ds_stack.html#shiny",
    "href": "core/slides/slides02_python_ds_stack.html#shiny",
    "title": "Python Data Science Stack",
    "section": "Shiny",
    "text": "Shiny\n\n\n\n\n\n\n\n\nLinks\n\n\nShiny homepage"
  },
  {
    "objectID": "core/slides/slides02_python_ds_stack.html#pure-python-interfaces",
    "href": "core/slides/slides02_python_ds_stack.html#pure-python-interfaces",
    "title": "Python Data Science Stack",
    "section": "Pure Python interfaces",
    "text": "Pure Python interfaces\n\n\n\n\nWays to use all these tools\n\nWrite a script script.py and use python directly in a CLI : python script.py\nUse the ipython interactive shell"
  },
  {
    "objectID": "core/slides/slides02_python_ds_stack.html#interfaces-jupyter",
    "href": "core/slides/slides02_python_ds_stack.html#interfaces-jupyter",
    "title": "Python Data Science Stack",
    "section": "Interfaces : Jupyter",
    "text": "Interfaces : Jupyter\n\n\n\n\n\nUse jupyter: a web application that allows to create and run documents, called notebooks (with .ipynb extension)\nNotebooks can contain code, equations, visualizations, text, etc. (literate programming)\nEach notebook has a kernel running a python/R,Julia, … thread\nA problem: a ipynb file is a json document. Leads to bad code diff, a problem with git versioning\n\n\n\n\n\n\n\n\n\n\nLinks\n\n\n\njupyter\njupyterlab\npolynote"
  },
  {
    "objectID": "core/slides/slides02_python_ds_stack.html#quarto",
    "href": "core/slides/slides02_python_ds_stack.html#quarto",
    "title": "Python Data Science Stack",
    "section": "Quarto",
    "text": "Quarto"
  },
  {
    "objectID": "core/slides/slides02_python_ds_stack.html#interfaceside-vs-code-and-other-editors",
    "href": "core/slides/slides02_python_ds_stack.html#interfaceside-vs-code-and-other-editors",
    "title": "Python Data Science Stack",
    "section": "Interfaces/IDE : VS Code (and other editors)",
    "text": "Interfaces/IDE : VS Code (and other editors)"
  },
  {
    "objectID": "core/slides/slides02_python_ds_stack.html#reticulate",
    "href": "core/slides/slides02_python_ds_stack.html#reticulate",
    "title": "Python Data Science Stack",
    "section": "Reticulate",
    "text": "Reticulate\n\n\nReticulate embeds a Python session within your R session, enabling seamless, high-performance interoperability. If you are an R developer that uses Python for some of your work or a member of data science team that uses both languages, reticulate can dramatically streamline your workflow!\n\n\n\n\n\n\n\n\n\n\nLinks\n\n\n\nReticulate homepage"
  },
  {
    "objectID": "core/slides/slides02_python_ds_stack.html#py2r",
    "href": "core/slides/slides02_python_ds_stack.html#py2r",
    "title": "Python Data Science Stack",
    "section": "Py2R",
    "text": "Py2R\n\n\nPython has several well-written packages for statistics and data science, but CRAN, R’s central repository, contains thousands of packages implementing sophisticated statistical algorithms that have been field-tested over many years. Thanks to the rpy2 package, Pythonistas can take advantage of the great work already done by the R community. rpy2 provides an interface that allows you to run R in Python processes. Users can move between languages and use the best of both programming languages.\n\n\n\nrpy2 homepage"
  },
  {
    "objectID": "core/slides/slides02_python_ds_stack.html#statistics",
    "href": "core/slides/slides02_python_ds_stack.html#statistics",
    "title": "Python Data Science Stack",
    "section": "Statistics",
    "text": "Statistics\n\n\nstatsmodels"
  },
  {
    "objectID": "core/slides/slides02_python_ds_stack.html#machine-learning",
    "href": "core/slides/slides02_python_ds_stack.html#machine-learning",
    "title": "Python Data Science Stack",
    "section": "Machine learning",
    "text": "Machine learning\n\nscikit-learn\nxgboost\nlightgbm\nvowpalwabbit\n…"
  },
  {
    "objectID": "core/slides/slides02_python_ds_stack.html#deep-learning",
    "href": "core/slides/slides02_python_ds_stack.html#deep-learning",
    "title": "Python Data Science Stack",
    "section": "Deep learning",
    "text": "Deep learning\n\nkeras\ntensorflow\npytorch\n…"
  },
  {
    "objectID": "core/slides/slides02_python_ds_stack.html#getting-faster",
    "href": "core/slides/slides02_python_ds_stack.html#getting-faster",
    "title": "Python Data Science Stack",
    "section": "Getting faster",
    "text": "Getting faster\n\n\nnumba, cython, cupy"
  },
  {
    "objectID": "core/slides/slides02_python_ds_stack.html#and",
    "href": "core/slides/slides02_python_ds_stack.html#and",
    "title": "Python Data Science Stack",
    "section": "And …",
    "text": "And …\n\nPython APIs for most databases and clouds\nProcessing and plotting tools for Geospatial data\nImage processing\nWeb development, web scrapping\n\namong many many many other things…"
  },
  {
    "objectID": "core/slides/slides03_sparkrdd.html#principles",
    "href": "core/slides/slides03_sparkrdd.html#principles",
    "title": "Apache and RDD",
    "section": "Principles",
    "text": "Principles\nSpark computing framework deals with many complex issues: fault tolerance, slow machines, big datasets, etc.\n\nIt follows the next guideline\nHere is an operation, run it on all the data.\n\n\n\n\n\n\nNote\n\n\n\nI do not care where it runs\nFeel free to run it twice on different nodes\n\n\n\n\n\n\nJobs are divided in tasks that are executed by the workers\n\n\n\n\n\n\nNote\n\n\n\nHow do we deal with failure? Launch another task!\n\nHow do we deal with stragglers? Launch another task!  … and kill the original task"
  },
  {
    "objectID": "core/slides/slides03_sparkrdd.html#a-picture",
    "href": "core/slides/slides03_sparkrdd.html#a-picture",
    "title": "Apache and RDD",
    "section": "A picture",
    "text": "A picture\n\nSpark Cluster Overview"
  },
  {
    "objectID": "core/slides/slides03_sparkrdd.html#job",
    "href": "core/slides/slides03_sparkrdd.html#job",
    "title": "Apache and RDD",
    "section": "Job",
    "text": "Job\nA job in Spark represents a complete computation triggered by an action in the application code.\nWhen you invoke an action (such as collect(), saveAsTextFile(), etc.) on a Spark RDD, DataFrame, or Dataset, it triggers the execution of one or more jobs.\n\nEach job consists of one or more stages, where each stage represents a set of tasks that can be executed in parallel.\nJobs in Spark are created by transformations that have no dependency on each other, meaning each stage can execute independently."
  },
  {
    "objectID": "core/slides/slides03_sparkrdd.html#task",
    "href": "core/slides/slides03_sparkrdd.html#task",
    "title": "Apache and RDD",
    "section": "Task",
    "text": "Task\nA task is the smallest unit of work in Spark and represents the execution of a computation on a single partition of data.\n\nTasks are created for each partition of the RDD, DataFrame, or Dataset involved in the computation.\n\n\nSpark’s execution engine assigns tasks to individual executor nodes in the cluster for parallel execution.\n\n\nTasks are executed within the context of a specific stage, and each task typically operates on a subset of the data distributed across the cluster.\n\n\nThe number of tasks within a stage depends on the number of partitions of the input data and the degree of parallelism configured for the Spark application.\n\n\nIn summary, a job represents the entire computation triggered by an action, composed of one or more stages, each of which is divided into smaller units of work called tasks.\n\n\nTasks operate on individual partitions of the data in parallel to achieve efficient and scalable distributed computation in Spark."
  },
  {
    "objectID": "core/slides/slides03_sparkrdd.html#api",
    "href": "core/slides/slides03_sparkrdd.html#api",
    "title": "Apache and RDD",
    "section": "API",
    "text": "API\nAn API allows a user to interact with the software\nSpark is implemented in Scala and runs on the JVM (Java Virtual Machine)\n\nMultiple Application Programming Interfaces (APIs):\n\nScala (JVM)\nJava (JVM)\n Python\n R\n\n\n\nThis course uses primarily the Python API. Easier to learn than Scala and Java\n\n\n\n\n\n\n\nAbout the R APIs\n\n\nSee Mastering Spark in R"
  },
  {
    "objectID": "core/slides/slides03_sparkrdd.html#digression-on-acronym-api-application-programming-interface",
    "href": "core/slides/slides03_sparkrdd.html#digression-on-acronym-api-application-programming-interface",
    "title": "Apache and RDD",
    "section": "Digression on acronym API (Application Programming Interface)",
    "text": "Digression on acronym API (Application Programming Interface)\nSee https://en.wikipedia.org/wiki/API for more on this acronym\n In Python language, look at interface and corresponding chapter Interfaces, Protocols and ABCs in Fluent Python\n\n For R there are in fact two APIs, or two packages that offer a Spark API\n\nsparklyr\nSparkR\n\nSee Mastering Spark with R by Javier Luraschi, Kevin Kuo, Edgar Ruiz"
  },
  {
    "objectID": "core/slides/slides03_sparkrdd.html#architecture",
    "href": "core/slides/slides03_sparkrdd.html#architecture",
    "title": "Apache and RDD",
    "section": "Architecture",
    "text": "Architecture\n\n\nWhen you interact with Spark through its API, you send instructions to the Driver\n\nThe Driver is the central coordinator\nIt communicates with distributed workers called executors\nCreates a logical directed acyclic graph (DAG) of operations\nMerges operations that can be merged\nSplits the operations in tasks (smallest unit of work in Spark)\nSchedules the tasks and send them to the executors\nTracks data and tasks\n\n\n\n\n\n\n\nSpark Cluster Overview\n\n\nExample\n\nExample of DAG: map(f) - map(g) - filter(h) - reduce(l)\nmap(f o g)"
  },
  {
    "objectID": "core/slides/slides03_sparkrdd.html#sparkcontext-versus-sparksession",
    "href": "core/slides/slides03_sparkrdd.html#sparkcontext-versus-sparksession",
    "title": "Apache and RDD",
    "section": "SparkContext versus SparkSession",
    "text": "SparkContext versus SparkSession\nSparkContext and SparkSession serve different purposes\n\nSparkContext was the main entry point for Spark applications in first versions of Apache Spark.\nSparkContext represented the connection to a Spark cluster, allowing the application to interact with the cluster manager.\nSparkContext was responsible for coordinating and managing the execution of jobs and tasks.\nSparkContext provided APIs for creating RDDs (Resilient Distributed Datasets), which were the primary abstraction in Spark for representing distributed data."
  },
  {
    "objectID": "core/slides/slides03_sparkrdd.html#sparkcontext-object",
    "href": "core/slides/slides03_sparkrdd.html#sparkcontext-object",
    "title": "Apache and RDD",
    "section": "SparkContext object",
    "text": "SparkContext object\nYour python session interacts with the driver through a SparkContext object\n\nIn the Spark interactive shell  An object of class SparkContext is automatically created in the session and named sc\nIn a jupyter notebook  Create a SparkContext object using:\n\n&gt;&gt;&gt; from pyspark import SparkConf, SparkContext\n\n&gt;&gt;&gt; conf = (\n  SparkConf()\n  .setAppName(appName)\n  .setMaster(master)\n)\n&gt;&gt;&gt; sc = SparkContext(conf=conf)"
  },
  {
    "objectID": "core/slides/slides03_sparkrdd.html#sparksession",
    "href": "core/slides/slides03_sparkrdd.html#sparksession",
    "title": "Apache and RDD",
    "section": " SparkSession",
    "text": "SparkSession\nIn Spark 2.0 and later versions, SparkContext is still available but is not the primary entry point.\nInstead, SparkSession is preferred.\nSparkSession was introduced in Spark 2.0 as a higher-level abstraction that encapsulates SparkContext, SQLContext, and HiveContext.\nSparkSession provides a unified entry point for Spark functionality, integrating Structured APIs:\n\nSQL,\nDataFrame,\nDataset\n\nand the traditional RDD-based APIs."
  },
  {
    "objectID": "core/slides/slides03_sparkrdd.html#what-sparksession",
    "href": "core/slides/slides03_sparkrdd.html#what-sparksession",
    "title": "Apache and RDD",
    "section": "What SparkSession?",
    "text": "What SparkSession?\nSparkSession is designed to make it easier to work with structured data (like data stored in tables or files with a schema) using Spark’s DataFrame and Dataset APIs.\n\nSparkSession also provides built-in support for reading data from various sources (like Parquet, JSON, JDBC, etc.) into DataFrames and writing DataFrames back to different formats.\n\n\nAdditionally, SparkSession simplifies the configuration of Spark properties and provides a Spark SQL CLI and a Spark Shell with SQL and DataFrame support.\n\n\n\n\n\n\n\n\nNote\n\n\nSparkSession internally creates and manages a SparkContext, so when you create a SparkSession, you don’t need to create a SparkContext separately."
  },
  {
    "objectID": "core/slides/slides03_sparkrdd.html#section-3",
    "href": "core/slides/slides03_sparkrdd.html#section-3",
    "title": "Apache and RDD",
    "section": "",
    "text": "SparkContext is lower-level and primarily focused on managing the execution of Spark jobs and interacting with the cluster\nSparkSession provides a higher-level, more user-friendly interface for working with structured data and integrates various Spark functionalities, including SQL, DataFrame, and Dataset APIs."
  },
  {
    "objectID": "core/slides/slides03_sparkrdd.html#rdds-and-running-model",
    "href": "core/slides/slides03_sparkrdd.html#rdds-and-running-model",
    "title": "Apache and RDD",
    "section": "RDDs and running model",
    "text": "RDDs and running model\nSpark programs are written in terms of operations on RDDs\n\nRDD stands for Resilient Distributed Dataset \nAn immutable distributed collection of objects spread across the cluster disks or memory\nRDDs can contain any type of Python, Java, or Scala objects, including user-defined classes\nParallel transformations and actions can be applied to RDDs\nRDDs are automatically rebuilt on machine failure"
  },
  {
    "objectID": "core/slides/slides03_sparkrdd.html#creating-a-rdd",
    "href": "core/slides/slides03_sparkrdd.html#creating-a-rdd",
    "title": "Apache and RDD",
    "section": "Creating a RDD",
    "text": "Creating a RDD\nFrom an iterable object iterator1 (e.g. a Python list, etc.):\nlines = sc.parallelize(iterator)\nFrom a text file:\nlines = sc.textFile(\"/path/to/file.txt\")\nwhere lines is the resulting RDD, and sc the spark context\n\n\n\n\n\n\nRemarks\n\n\n\nparallelize not really used in practice\nIn real life: load data from external storage\nExternal storage is often HDFS (Hadoop Distributed File System)\nCan read most formats (json, csv, xml, parquet, orc, etc.)\n\n\n\n\nSee Chapter 17 Iterators, Generators, … in Fluent Python"
  },
  {
    "objectID": "core/slides/slides03_sparkrdd.html#operations-on-rdd",
    "href": "core/slides/slides03_sparkrdd.html#operations-on-rdd",
    "title": "Apache and RDD",
    "section": "Operations on RDD",
    "text": "Operations on RDD\nTwo families of operations can be performed on RDDs\n\n\nTransformations  Operations on RDDs which return a new RDD  Lazy evaluation\n\n\n\n\nActions  Operations on RDDs that return some other data type  Triggers computations\n\n\n\n\n\n\n What is lazy evaluation ?"
  },
  {
    "objectID": "core/slides/slides03_sparkrdd.html#section-4",
    "href": "core/slides/slides03_sparkrdd.html#section-4",
    "title": "Apache and RDD",
    "section": "",
    "text": "When a transformation is called on a RDD:\n\nThe operation is not immediately performed\nSpark internally records that this operation has been requested\nComputations are triggered only if an action requires the result of this transformation at some point"
  },
  {
    "objectID": "core/slides/slides03_sparkrdd.html#transformations-1",
    "href": "core/slides/slides03_sparkrdd.html#transformations-1",
    "title": "Apache and RDD",
    "section": "Transformations",
    "text": "Transformations\nThe most important transformation is map\n\n\n\ntransformation\ndescription\n\n\n\n\nmap(f)\napply a function f to each element of the RDD\n\n\n\n\nHere is an example:\n&gt;&gt;&gt; rdd = sc.parallelize([2, 3, 4])\n&gt;&gt;&gt; (\n  rdd\n    .map(lambda x: list(range(1, x)))\n    .collect()\n)\n[[1], [1, 2], [1, 2, 3]]\n\n\n\nWe have to call collect (an action) otherwise nothing happens\nOnce again, transformation map is lazily evaluated \n\n\n\n\n\n\n\n\n\nNote\n\n\n\nIn Python, three options for passing functions into Spark\n\nfor short functions: lambda expressions (anonymous functions)\ntop-level functions\nlocally/user defined functions with def"
  },
  {
    "objectID": "core/slides/slides03_sparkrdd.html#transformations-2",
    "href": "core/slides/slides03_sparkrdd.html#transformations-2",
    "title": "Apache and RDD",
    "section": "Transformations",
    "text": "Transformations\nPassing functions to map:\n\nInvolves serialization with pickle\nSpark sends the entire pickled function to worker nodes\n\n\n\n\n\n\n\nWarning\n\n\nIf the function is an object method:\n\nThe whole object is pickled since the method contains references to the object (self) and references to attributes of the object\nThe whole object can be large\nThe whole object may not be serializable with pickle\n\n\n\n\n\n\nGo to notebook05_sparkrdd.ipynb"
  },
  {
    "objectID": "core/slides/slides03_sparkrdd.html#pythons-corner",
    "href": "core/slides/slides03_sparkrdd.html#pythons-corner",
    "title": "Apache and RDD",
    "section": "Python’s corner",
    "text": "Python’s corner\n\n[it for it in map(lambda x : list(range(1, x)), [1, 2, 3])]"
  },
  {
    "objectID": "core/slides/slides03_sparkrdd.html#serialization",
    "href": "core/slides/slides03_sparkrdd.html#serialization",
    "title": "Apache and RDD",
    "section": " Serialization",
    "text": "Serialization\n\nConverting an object from its in-memory structure to a binary or text-oriented format for storage or transmission, in a way that allows the future reconstruction of a clone of the object on the same system or on a different one.\n\n\n\nThe pickle module supports serialization of arbitrary Python objects to a binary format\n\nfrom Fluent Python by Ramalho\n\n\n\nSee also cloudpickle"
  },
  {
    "objectID": "core/slides/slides03_sparkrdd.html#transformations-continued",
    "href": "core/slides/slides03_sparkrdd.html#transformations-continued",
    "title": "Apache and RDD",
    "section": "Transformations (continued)",
    "text": "Transformations (continued)\nflatMap\n\n\n\n\n\n\n\ntransformation\ndescription\n\n\n\n\nflatMap(f)\napply f to each element of the RDD, then flattens the results\n\n\n\n\n\n\n\n\n\n\nExample\n\n\n&gt;&gt;&gt; rdd = sc.parallelize([2, 3, 4, 5])\n&gt;&gt;&gt; (\n  rdd\n    .flatMap(lambda x: range(1, x))\n    .collect()\n)\n[1, 1, 2, 1, 2, 3, 1, 2, 3, 4]"
  },
  {
    "objectID": "core/slides/slides03_sparkrdd.html#pythons-corner-is-there-any-flatmap-function",
    "href": "core/slides/slides03_sparkrdd.html#pythons-corner-is-there-any-flatmap-function",
    "title": "Apache and RDD",
    "section": "Python’s corner: is there any flatMap() function?",
    "text": "Python’s corner: is there any flatMap() function?\nNested list comprehensions\n\n[o for it in map(lambda x : list(range(1, x)), [1, 2, 3, 4])   for o in it]\n\n[1, 1, 2, 1, 2, 3]\n\n\n\n\nimport itertools\n\n[o for o in itertools.chain.from_iterable(map(lambda x : list(range(1, x)), [1, 2, 3, 4]))]\n\n[1, 1, 2, 1, 2, 3]\n\n\n\n\n\nflatten = itertools.chain.from_iterable\n\n[o for o in  flatten(map(lambda x : list(range(1, x)), [1, 2, 3, 4]))]\n\n[1, 1, 2, 1, 2, 3]\n\n\n\nFrom https://discuss.python.org/t/add-built-in-flatmap-function-to-functools/21137\nhttps://docs.python.org/3/library/itertools.html"
  },
  {
    "objectID": "core/slides/slides03_sparkrdd.html#transformations-continued-1",
    "href": "core/slides/slides03_sparkrdd.html#transformations-continued-1",
    "title": "Apache and RDD",
    "section": "Transformations (continued)",
    "text": "Transformations (continued)\nfilter allows to filter an RDD\n\n\n\n\n\n\n\ntransformation\ndescription\n\n\n\n\nfilter(f)\nReturn an RDD consisting of only elements that pass the condition f passed to filter()\n\n\n\n\n\n\n\n\n\n\nExample\n\n\n&gt;&gt;&gt; rdd = sc.parallelize(range(10))\n&gt;&gt;&gt; rdd.filter(lambda x: x % 2 == 0).collect()\n[0, 2, 4, 6, 8]"
  },
  {
    "objectID": "core/slides/slides03_sparkrdd.html#pythons-corner-1",
    "href": "core/slides/slides03_sparkrdd.html#pythons-corner-1",
    "title": "Apache and RDD",
    "section": "Python’s corner",
    "text": "Python’s corner\nUsing list comprehensions\n\nlll = list(range(10))\nspam = lambda x: x % 2 == 0\n\n[o  for o in lll if spam(o)]\n\n[0, 2, 4, 6, 8]\n\n\n\nTweaking filterfalse from itertools\n\n[o for o in itertools.filterfalse(lambda x : not x% 2==0, lll)]\n\n[0, 2, 4, 6, 8]"
  },
  {
    "objectID": "core/slides/slides03_sparkrdd.html#transformations-distinct-and-sample",
    "href": "core/slides/slides03_sparkrdd.html#transformations-distinct-and-sample",
    "title": "Apache and RDD",
    "section": "Transformations: distinct and sample",
    "text": "Transformations: distinct and sample\n\n\n\n\n\n\n\ntransformation\ndescription\n\n\n\n\ndistinct()\nRemoves duplicates\n\n\nsample(withReplacement, fraction, [seed])\nSample an RDD, with or without replacement\n\n\n\n\n\n\n\n\n\n\nExample\n\n\n&gt;&gt;&gt; rdd = sc.parallelize([1, 1, 4, 2, 1, 3, 3])\n&gt;&gt;&gt; rdd.distinct().collect()\n[1, 2, 3, 4]"
  },
  {
    "objectID": "core/slides/slides03_sparkrdd.html#pythons-corner-2",
    "href": "core/slides/slides03_sparkrdd.html#pythons-corner-2",
    "title": "Apache and RDD",
    "section": "Python’s corner",
    "text": "Python’s corner"
  },
  {
    "objectID": "core/slides/slides03_sparkrdd.html#transformations-3",
    "href": "core/slides/slides03_sparkrdd.html#transformations-3",
    "title": "Apache and RDD",
    "section": "Transformations",
    "text": "Transformations\nWe have also pseudo-set-theoretical operations\n\n\n\n\n\n\n\ntransformation\ndescription\n\n\n\n\nunion(otherRdd)\nReturns union with otherRdd\n\n\ninstersection(otherRdd)\nReturns intersection with otherRdd\n\n\nsubtract(otherRdd)\nReturn each value in self that is not contained in otherRdd.\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nIf there are duplicates in the input RDD, the result of union() will contain duplicates (fixed with distinct())\nintersection() removes all duplicates (including duplicates from a single RDD)\nPerformance of intersection() is much worse than union() since it requires a shuffle to identify common elements\nsubtract also requires a shuffle"
  },
  {
    "objectID": "core/slides/slides03_sparkrdd.html#transformations-4",
    "href": "core/slides/slides03_sparkrdd.html#transformations-4",
    "title": "Apache and RDD",
    "section": "Transformations",
    "text": "Transformations\nWe have also pseudo-set-theoretical operations\n\n\n\n\n\n\n\ntransformation\ndescription\n\n\n\n\nunion(otherRdd)\nReturns union with otherRdd\n\n\ninstersection(otherRdd)\nReturns intersection with otherRdd\n\n\nsubtract(otherRdd)\nReturn each value in self that is not contained in otherRdd.\n\n\n\n\n\n\n\n\n\n\nExample with union and distinct\n\n\n&gt;&gt;&gt; rdd1 = sc.parallelize(range(5))\n&gt;&gt;&gt; rdd2 = sc.parallelize(range(3, 9))\n&gt;&gt;&gt; rdd3 = rdd1.union(rdd2)\n&gt;&gt;&gt; rdd3.collect()\n[0, 1, 2, 3, 4, 3, 4, 5, 6, 7, 8]\n&gt;&gt;&gt; rdd3.distinct().collect()\n[0, 1, 2, 3, 4, 5, 6, 7, 8]\n\n\n\n\n\n\nHow does Spark decide whether two RDD items are equal?"
  },
  {
    "objectID": "core/slides/slides03_sparkrdd.html#pythons-corner-3",
    "href": "core/slides/slides03_sparkrdd.html#pythons-corner-3",
    "title": "Apache and RDD",
    "section": "Python’s corner",
    "text": "Python’s corner\n\n# %%\nspam = list(range(5)) + list(range(3, 9))\n[o for o in set(spam)]\n\n\n\n\n\n\nHow does Python decide whether two objects are equal/identical?\nSee also all_unique() from more_itertools"
  },
  {
    "objectID": "core/slides/slides03_sparkrdd.html#about-shuffles",
    "href": "core/slides/slides03_sparkrdd.html#about-shuffles",
    "title": "Apache and RDD",
    "section": " About shuffles",
    "text": "About shuffles\n\nCertain operations trigger a shuffle\nIt is Spark’s mechanism for redistributing data so as to modify the partitioning\nIt involves moving data across executors and machines, making shuffle a complex and costly operation\nMore on shuffles later"
  },
  {
    "objectID": "core/slides/slides03_sparkrdd.html#performance-impact",
    "href": "core/slides/slides03_sparkrdd.html#performance-impact",
    "title": "Apache and RDD",
    "section": " Performance Impact",
    "text": "Performance Impact\n\nA shuffle involves\n\ndisk I/O,\ndata serialization\nnetwork I/O.\n\n\n\n\nTo organize data for the shuffle, Spark generates sets of tasks:\n\nmap tasks to organize the data and\nreduce tasks to aggregate it\n\n\n\n\n\nThis vocabulary comes from MapReduce and does not directly relate to Spark’s map and reduce operations."
  },
  {
    "objectID": "core/slides/slides03_sparkrdd.html#transformations-5",
    "href": "core/slides/slides03_sparkrdd.html#transformations-5",
    "title": "Apache and RDD",
    "section": "Transformations",
    "text": "Transformations\nAnother pseudo set operation\n\n\n\n\n\n\n\ntransformation\ndescription\n\n\n\n\ncartesian(otherRdd)\nReturn the Cartesian product of this RDD and another one\n\n\n\n\n\n\n\n\n\n\nExample\n\n\n&gt;&gt;&gt; rdd1 = sc.parallelize([1, 2])\n&gt;&gt;&gt; rdd2 = sc.parallelize([\"a\", \"b\"])\n&gt;&gt;&gt; rdd1.cartesian(rdd2).collect()\n[(1, 'a'), (1, 'b'), (2, 'a'), (2, 'b')]\n\n\n\n cartesian() is very expensive for large RDDs\n\n\n\nLet’s go to notebook05_sparkrdd.ipynb"
  },
  {
    "objectID": "core/slides/slides03_sparkrdd.html#actions-1",
    "href": "core/slides/slides03_sparkrdd.html#actions-1",
    "title": "Apache and RDD",
    "section": "Actions",
    "text": "Actions\n\n\ncollect() brings the RDD back to the driver\n\n\n\ntransformation\ndescription\n\n\n\n\ncollect()\nReturn all elements from the RDD\n\n\n\nExample\n&gt;&gt;&gt; rdd = sc.parallelize([1, 2, 3, 3])\n&gt;&gt;&gt; rdd.collect()\n[1, 2, 3, 3]\n\n\n\n\n\n Be sure that the retrieved data fits in the driver memory !\nUseful when developping and working on small data for testing\n We’ll use it a lot here, but we don’t use it in real-world problems"
  },
  {
    "objectID": "core/slides/slides03_sparkrdd.html#actions-2",
    "href": "core/slides/slides03_sparkrdd.html#actions-2",
    "title": "Apache and RDD",
    "section": "Actions",
    "text": "Actions\nCounts matter!\n\n\n\n\n\n\n\ntransformation\ndescription\n\n\n\n\ncount()\nReturn the number of elements in the RDD\n\n\ncountByValue()\nReturn the count of each unique value in the RDD as a dictionary of {value: count} pairs.\n\n\n\n\n\n\n\n\n\nExample\n\n\n&gt;&gt;&gt; rdd = sc.parallelize([1, 3, 1, 2, 2, 2])\n&gt;&gt;&gt; rdd.count()\n6\n&gt;&gt;&gt; rdd.countByValue()\ndefaultdict(int, {1: 2, 3: 1, 2: 3})"
  },
  {
    "objectID": "core/slides/slides03_sparkrdd.html#pythons-corner-4",
    "href": "core/slides/slides03_sparkrdd.html#pythons-corner-4",
    "title": "Apache and RDD",
    "section": "Python’s corner",
    "text": "Python’s corner\n\nspam = [1, 3, 1, 2, 2, 2]\n\nlen(spam)\n\nfrom collections import Counter\n\nCounter(spam)\n\nCounter({2: 3, 1: 2, 3: 1})\n\n\n\n\nhttps://docs.python.org/3/library/collections.html#collections.Counter"
  },
  {
    "objectID": "core/slides/slides03_sparkrdd.html#actions-cherry-picking",
    "href": "core/slides/slides03_sparkrdd.html#actions-cherry-picking",
    "title": "Apache and RDD",
    "section": "Actions: cherry-picking",
    "text": "Actions: cherry-picking\nHow to get some (but not all) values in an RDD ?\n\n\n\n\n\n\n\naction\ndescription\n\n\n\n\ntake(n)\nReturn n elements from the RDD (deterministic)\n\n\ntop(n)\nReturn first n elements from the RDD (descending order)\n\n\ntakeOrdered(num, key=None)\nGet the N elements from a RDD ordered in ascending order or as specified by the optional key function.\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\ntake(n) returns n elements from the RDD and attempts to minimize the number of partitions it accesses\n the result may be a biased collection\ncollect and take may return the elements in an order you don’t expect"
  },
  {
    "objectID": "core/slides/slides03_sparkrdd.html#pythons-corner-5",
    "href": "core/slides/slides03_sparkrdd.html#pythons-corner-5",
    "title": "Apache and RDD",
    "section": "Python’s corner",
    "text": "Python’s corner\n\nlist(itertools.islice(list(range(10)), 3))\n\n[0, 1, 2]"
  },
  {
    "objectID": "core/slides/slides03_sparkrdd.html#actions-3",
    "href": "core/slides/slides03_sparkrdd.html#actions-3",
    "title": "Apache and RDD",
    "section": "Actions",
    "text": "Actions\nHow to get some values in an RDD?\n\n\n\n\n\n\n\naction\ndescription\n\n\n\n\ntake(n)\nReturn n elements from the RDD (deterministic)\n\n\ntop(n)\nReturn first n elements from the RDD (decending order)\n\n\ntakeOrdered(num, key=None)\nGet the $N $elements from a RDD ordered in ascending order or as specified by the optional key function.\n\n\n\n\n\n\n\n\n\n\nExample\n\n\n&gt;&gt;&gt; rdd = sc.parallelize([(3, 'a'), (1, 'b'), (2, 'd')])\n&gt;&gt;&gt; rdd.takeOrdered(2)\n[(1, 'b'), (2, 'd')]\n&gt;&gt;&gt; rdd.takeOrdered(2, key=lambda x: x[1])\n[(3, 'a'), (1, 'b')]"
  },
  {
    "objectID": "core/slides/slides03_sparkrdd.html#actions-reductions",
    "href": "core/slides/slides03_sparkrdd.html#actions-reductions",
    "title": "Apache and RDD",
    "section": "Actions: reduction(s)",
    "text": "Actions: reduction(s)\n\n\n\n\n\n\n\n\naction\ndescription\n\n\n\n\nreduce(f)\nReduces the elements of this RDD using the specified commutative and associative binary operator f.\n\n\nfold(zeroValue, op)\nSame as reduce() but with the provided zero value.\n\n\n\n\n\n\nop(x, y) is allowed to modify x and return it as its result value to avoid object allocation; however, it should not modify y.\nreduce applies some operation to pairs of elements until there is just one left. Throws an exception for empty collections.\nfold has initial zero-value: defined for empty collections."
  },
  {
    "objectID": "core/slides/slides03_sparkrdd.html#actions-reductions-1",
    "href": "core/slides/slides03_sparkrdd.html#actions-reductions-1",
    "title": "Apache and RDD",
    "section": "Actions: reduction(s)",
    "text": "Actions: reduction(s)\n\n\n\n\n\n\n\n\naction\ndescription\n\n\n\n\nreduce(f)\nReduces the elements of this RDD using the specified commutative and associative binary operator f.\n\n\nfold(zeroValue, op)\nSame as reduce() but with the provided zero value.\n\n\n\n\n\n\n\n\n\n\n\nExample\n\n\n&gt;&gt;&gt; rdd = sc.parallelize([1, 2, 3])\n&gt;&gt;&gt; rdd.reduce(lambda a, b: a + b)\n6\n&gt;&gt;&gt; rdd.fold(0, lambda a, b: a + b)\n6"
  },
  {
    "objectID": "core/slides/slides03_sparkrdd.html#actions-reductions-2",
    "href": "core/slides/slides03_sparkrdd.html#actions-reductions-2",
    "title": "Apache and RDD",
    "section": "Actions: reduction(s)",
    "text": "Actions: reduction(s)\n\n\n\n\n\n\n\n\naction\ndescription\n\n\n\n\nreduce(f)\nReduces the elements of this RDD using the specified commutative and associative binary operator f.\n\n\nfold(zeroValue, op)\nSame as reduce() but with the provided zero value.\n\n\n\n\n\n\n\n\n\n\n\nWarning\n\n\nWith fold, solutions can depend on the number of partitions\n&gt;&gt;&gt; rdd = sc.parallelize([1, 2, 4], 2) # RDD with 2 partitions\n&gt;&gt;&gt; rdd.fold(2.5, lambda a, b: a + b)\n14.5\n\nRDD has 2 partition: say [1, 2] and [4]\nSum in the partitions: 2.5 + (1 + 2) = 5.5 and 2.5 + (4) = 6.5\nSum over partitions: 2.5 + (5.5 + 6.5) = 14.5"
  },
  {
    "objectID": "core/slides/slides03_sparkrdd.html#actions-reductions-3",
    "href": "core/slides/slides03_sparkrdd.html#actions-reductions-3",
    "title": "Apache and RDD",
    "section": "Actions: reduction(s)",
    "text": "Actions: reduction(s)\n\n\n\n\n\n\n\n\naction\ndescription\n\n\n\n\nreduce(f)\nReduces the elements of this RDD using the specified commutative and associative binary operator f.\n\n\nfold(zeroValue, op)\nSame as reduce() but with the provided zero value.\n\n\n\n\n\n\n\n\n\n\n\nWarning\n\n\nSolutions can depend on the number of partitions\n&gt;&gt;&gt; rdd = sc.parallelize([1, 2, 3], 5) # RDD with 5 partitions\n&gt;&gt;&gt; rdd.fold(2, lambda a, b: a + b)\n\n\n\n\n\n\n\n\n\nNote\n\n\nBack to Jupyter notebook V : Spark RDD"
  },
  {
    "objectID": "core/slides/slides03_sparkrdd.html#actions-reductions-4",
    "href": "core/slides/slides03_sparkrdd.html#actions-reductions-4",
    "title": "Apache and RDD",
    "section": "Actions: reduction(s)",
    "text": "Actions: reduction(s)\n\n\n\n\n\n\n\n\naction\ndescription\n\n\n\n\nreduce(f)\nReduces the elements of this RDD using the specified commutative and associative binary operator f.\n\n\nfold(zeroValue, op)\nSame as reduce() but with the provided zero value.\n\n\n\n\n\n\n\n\n\n\n\nWarning\n\n\nSolutions can depend on the number of partitions\n&gt;&gt;&gt; rdd = sc.parallelize([1, 2, 3], 5) # RDD with 5 partitions\n&gt;&gt;&gt; rdd.fold(2, lambda a, b: a + b)\n18\n\nYes, even if there is less partitions than elements !\n18 = 2 * 5 + (1+2+3) + 2"
  },
  {
    "objectID": "core/slides/slides03_sparkrdd.html#pythons-corner-6",
    "href": "core/slides/slides03_sparkrdd.html#pythons-corner-6",
    "title": "Apache and RDD",
    "section": "Python’s corner",
    "text": "Python’s corner\n\nfrom functools import reduce\n\nreduce(lambda a, b: a + b,  [1, 2, 3])\n1reduce(lambda a, b: a + b,  [1, 2, 3], 2)\n\n\n1\n\ninitial argument used to initialize the accumulator. The default is 0\n\n\n\n\n8"
  },
  {
    "objectID": "core/slides/slides03_sparkrdd.html#actions-aggregate",
    "href": "core/slides/slides03_sparkrdd.html#actions-aggregate",
    "title": "Apache and RDD",
    "section": "Actions : aggregate",
    "text": "Actions : aggregate\n\n\n\n\n\n\n\n\naction\ndescription\n\n\n\n\naggregate(zero, seqOp, combOp)\nSimilar to reduce() but used to return a different type\n\n\n\n\n\nAggregates the elements of each partition, and then the results for all the partitions, given aggregation functions and zero value.\n\nseqOp(acc, val): function to combine the elements of a partition from the RDD (val) with an accumulator (acc).  The result type may differ from the RDD type (if any)\ncombOp: function that merges the accumulators of two partitions\nIn both functions, the first argument can be modified while the second cannot"
  },
  {
    "objectID": "core/slides/slides03_sparkrdd.html#actions-aggregate-1",
    "href": "core/slides/slides03_sparkrdd.html#actions-aggregate-1",
    "title": "Apache and RDD",
    "section": "Actions : aggregate",
    "text": "Actions : aggregate\n\n\n\n\n\n\n\n\naction\ndescription\n\n\n\n\naggregate(zero, seqOp, combOp)\nSimilar to reduce() but used to return a different type\n\n\n\n\n\n\n\n\n\n\n\nExample\n\n\n&gt;&gt;&gt; seqOp = lambda x, y: (x[0] + y, x[1] + 1)\n&gt;&gt;&gt; combOp = lambda x, y: (x[0] + y[0], x[1] + y[1])\n&gt;&gt;&gt; sc.parallelize([1, 2, 3, 4]).aggregate((0, 0), seqOp, combOp)\n(10, 4)\n&gt;&gt;&gt; ( \n      sc.parallelize([])\n        .aggregate((0, 0), seqOp, combOp)\n)\n(0, 0)\n\n\n\n\n\n\nBack to Jupyter notebook V : Spark RDD"
  },
  {
    "objectID": "core/slides/slides03_sparkrdd.html#actions-4",
    "href": "core/slides/slides03_sparkrdd.html#actions-4",
    "title": "Apache and RDD",
    "section": "Actions",
    "text": "Actions\nThe foreach action\n\n\n\naction\ndescription\n\n\n\n\nforeach(f)\nApply a function f to each element of a RDD\n\n\n\n\n\nPerforms an action on all of the elements in the RDD without returning any result to the driver.\nExample : insert records into a database with f\n\n\n\n The foreach() action performs computations on each element of the RDD without bringing it back to the driver"
  },
  {
    "objectID": "core/slides/slides03_sparkrdd.html#lazy-evaluation-and-persistence",
    "href": "core/slides/slides03_sparkrdd.html#lazy-evaluation-and-persistence",
    "title": "Apache and RDD",
    "section": "Lazy evaluation and persistence",
    "text": "Lazy evaluation and persistence\n\nSpark RDDs are lazily evaluated\nEach time an action is called on a RDD, this RDD and all its dependencies are recomputed\nIf you plan to reuse a RDD multiple times, you should use persistence\n\n\n\n\n\n\n\nNote\n\n\n\nLazy evaluation helps spark to reduce the number of passes over the data it has to make by grouping operations together\nNo substantial benefit to writing a single complex map instead of chaining together many simple operations\nUsers are free to organize their program into smaller, more manageable operations"
  },
  {
    "objectID": "core/slides/slides03_sparkrdd.html#persistence-1",
    "href": "core/slides/slides03_sparkrdd.html#persistence-1",
    "title": "Apache and RDD",
    "section": "Persistence",
    "text": "Persistence\nHow to use persistence ?\n\n\n\n\n\n\n\nmethod\ndescription\n\n\n\n\ncache()\nPersist the RDD in memory\n\n\npersist(storageLevel)\nPersist the RDD according to storageLevel\n\n\n\n\n These methods must be called before the action, and do not trigger the actual computation"
  },
  {
    "objectID": "core/slides/slides03_sparkrdd.html#usage-of-storagelevel",
    "href": "core/slides/slides03_sparkrdd.html#usage-of-storagelevel",
    "title": "Apache and RDD",
    "section": "Usage of storageLevel",
    "text": "Usage of storageLevel\npyspark.StorageLevel(\n  useDisk, useMemory, useOffHeap, deserialized, replication=1\n)"
  },
  {
    "objectID": "core/slides/slides03_sparkrdd.html#shades-of-persistence",
    "href": "core/slides/slides03_sparkrdd.html#shades-of-persistence",
    "title": "Apache and RDD",
    "section": "Shades of persistence",
    "text": "Shades of persistence\n\nWhat does persistence in memory mean?\nMake StorageLevel explicit\nAny difference between cache() and persist() with useMemory?\nWhy do we call persistence caching?"
  },
  {
    "objectID": "core/slides/slides03_sparkrdd.html#options-for-persistence",
    "href": "core/slides/slides03_sparkrdd.html#options-for-persistence",
    "title": "Apache and RDD",
    "section": "Options for persistence",
    "text": "Options for persistence\n\nOptions for persistence\n\n\n\n\n\n\n\nargument\ndescription\n\n\n\n\nuseDisk\nAllow caching to use disk if True\n\n\nuseMemory\nAllow caching to use memory if True\n\n\nuseOffHeap\nStore data outside of JVM heap if True. Useful if using some in-memory storage system (such a Tachyon)\n\n\ndeserialized\nCache data without serialization if True\n\n\nreplication\nNumber of replications of the cached data\n\n\n\n\nreplication: If you are caching data that is expensive to compute, you can use replication. If one machine fails, data does not need to be recomputed."
  },
  {
    "objectID": "core/slides/slides03_sparkrdd.html#options-for-persistence-1",
    "href": "core/slides/slides03_sparkrdd.html#options-for-persistence-1",
    "title": "Apache and RDD",
    "section": "Options for persistence",
    "text": "Options for persistence\n\nOptions for persistence\n\n\n\n\n\n\n\nargument\ndescription\n\n\n\n\nuseDisk\nAllow caching to use disk if True\n\n\nuseMemory\nAllow caching to use memory if True\n\n\nuseOffHeap\nStore data outside of JVM heap if True. Useful if using some in-memory storage system (such a Tachyon)\n\n\ndeserialized\nCache data without serialization if True\n\n\nreplication\nNumber of replications of the cached data\n\n\n\n\ndeserialized :\n\nSerialization consists in converting data to some binary format\nTo the best of our knowledge, PySpark only support serialized caching (using pickle)"
  },
  {
    "objectID": "core/slides/slides03_sparkrdd.html#options-for-persistence-2",
    "href": "core/slides/slides03_sparkrdd.html#options-for-persistence-2",
    "title": "Apache and RDD",
    "section": "Options for persistence",
    "text": "Options for persistence\n\nOptions for persistence\n\n\n\n\n\n\n\nargument\ndescription\n\n\n\n\nuseDisk\nAllow caching to use disk if True\n\n\nuseMemory\nAllow caching to use memory if True\n\n\nuseOffHeap\nStore data outside of JVM heap if True. Useful if using some in-memory storage system (such a Tachyon)\n\n\ndeserialized\nCache data without serialization if True\n\n\nreplication\nNumber of replications of the cached data\n\n\n\n\n\nuseOffHeap\n\n\n\n\n\nData cached in the JVM heap by default\nVery interesting alternative in-memory solutions such as tachyon\nDon’t forget that spark is scala running on the JVM"
  },
  {
    "objectID": "core/slides/slides03_sparkrdd.html#back-to-options-for-persistence",
    "href": "core/slides/slides03_sparkrdd.html#back-to-options-for-persistence",
    "title": "Apache and RDD",
    "section": "Back to options for persistence",
    "text": "Back to options for persistence\nStorageLevel(useDisk, useMemory, useOffHeap, deserialized, replication)\nYou can use these constants:\nDISK_ONLY = StorageLevel(True, False, False, False, 1)\nDISK_ONLY_2 = StorageLevel(True, False, False, False, 2)\nMEMORY_AND_DISK = StorageLevel(True, True, False, True, 1)\nMEMORY_AND_DISK_2 = StorageLevel(True, True, False, True, 2)\nMEMORY_AND_DISK_SER = StorageLevel(True, True, False, False, 1)\nMEMORY_AND_DISK_SER_2 = StorageLevel(True, True, False, False, 2)\nMEMORY_ONLY = StorageLevel(False, True, False, True, 1)\nMEMORY_ONLY_2 = StorageLevel(False, True, False, True, 2)\nMEMORY_ONLY_SER = StorageLevel(False, True, False, False, 1)\nMEMORY_ONLY_SER_2 = StorageLevel(False, True, False, False, 2)\nOFF_HEAP = StorageLevel(False, False, True, False, 1)\nand simply call for instance\nrdd.persist(MEMORY_AND_DISK)"
  },
  {
    "objectID": "core/slides/slides03_sparkrdd.html#persistence-2",
    "href": "core/slides/slides03_sparkrdd.html#persistence-2",
    "title": "Apache and RDD",
    "section": "Persistence",
    "text": "Persistence\nWhat if you attempt to cache too much data to fit in memory ?\nSpark will automatically evict old partitions using a Least Recently Used (LRU) cache policy:\n\nFor the memory-only storage levels, it will recompute these partitions the next time they are accessed\nFor the memory-and-disk ones, it will write them out to disk\n\nUse unpersist() to RDDs to manually remove them from the cache"
  },
  {
    "objectID": "core/slides/slides03_sparkrdd.html#reminder-about-passing-functions",
    "href": "core/slides/slides03_sparkrdd.html#reminder-about-passing-functions",
    "title": "Apache and RDD",
    "section": "Reminder: about passing functions ",
    "text": "Reminder: about passing functions \n\n\n\n\n\n\n\nWarning\n\n\nWhen passing functions, you can inadvertently serialize the object containing the function.\n\n\n\n\nIf you pass a function that:\n\nis the member of an object (a method)\ncontains references to fields in an object\n\nthen Spark sends the entire object to worker nodes, which can be much larger than the bit of information you need\n\n\n\n\n\n\nCaution\n\n\nThis can cause your program to fail, if your class contains objects that Python can’t pickle"
  },
  {
    "objectID": "core/slides/slides03_sparkrdd.html#about-passing-functions",
    "href": "core/slides/slides03_sparkrdd.html#about-passing-functions",
    "title": "Apache and RDD",
    "section": "About passing functions",
    "text": "About passing functions\nPassing a function with field references (don’t do this !  )\nclass SearchFunctions(object):\n  \n  def __init__(self, query):\n      self.query = query\n\n  def isMatch(self, s):\n      return self.query in s\n\n  def getMatchesFunctionReference(self, rdd):\n      # Problem: references all of \"self\" in \"self.isMatch\"\n      return rdd.filter(self.isMatch)\n\n  def getMatchesMemberReference(self, rdd):\n      # Problem: references all of \"self\" in \"self.query\"\n      return rdd.filter(lambda x: self.query in x)\n\n\n\n\n\n\nTip\n\n\nInstead, just extract the fields you need from your object into a local variable and pass that in"
  },
  {
    "objectID": "core/slides/slides03_sparkrdd.html#about-passing-functions-1",
    "href": "core/slides/slides03_sparkrdd.html#about-passing-functions-1",
    "title": "Apache and RDD",
    "section": "About passing functions",
    "text": "About passing functions\nPython function passing without field references\nclass WordFunctions(object):\n  ...\n\ndef getMatchesNoReference(self, rdd):\n  # Safe: extract only the field we need into a local variable\n  query = self.query\n  return rdd.filter(lambda x: query in x)\n\nMuch better!"
  },
  {
    "objectID": "core/slides/slides03_sparkrdd.html#pair-rdd-key-value-pairs-1",
    "href": "core/slides/slides03_sparkrdd.html#pair-rdd-key-value-pairs-1",
    "title": "Apache and RDD",
    "section": "Pair RDD: key-value pairs",
    "text": "Pair RDD: key-value pairs\nIt’s roughly a RDD where each element is a tuple with two elements: a key and a value\n\n\nFor numerous tasks, such as aggregations tasks, storing information as (key, value) pairs into RDD is very convenient\nSuch RDDs are called PairRDD\nPair RDDs expose new operations such as grouping together data with the same key, and grouping together two different RDDs"
  },
  {
    "objectID": "core/slides/slides03_sparkrdd.html#creating-a-pair-rdd",
    "href": "core/slides/slides03_sparkrdd.html#creating-a-pair-rdd",
    "title": "Apache and RDD",
    "section": "Creating a pair RDD",
    "text": "Creating a pair RDD\nCalling map with a function returning a tuple with two elements\n&gt;&gt;&gt; rdd = sc.parallelize([[1, \"a\", 7], [2, \"b\", 13], [2, \"c\", 17]])\n&gt;&gt;&gt; rdd = rdd.map(lambda x: (x[0], x[1:]))\n&gt;&gt;&gt; rdd.collect()\n[(1, ['a', 7]), (2, ['b', 13]), (2, ['c', 17])]"
  },
  {
    "objectID": "core/slides/slides03_sparkrdd.html#warning-5",
    "href": "core/slides/slides03_sparkrdd.html#warning-5",
    "title": "Apache and RDD",
    "section": " Warning",
    "text": "Warning\nAll elements of a PairRDD must be tuples with two elements (the key and the value)\n&gt;&gt;&gt; rdd = sc.parallelize([[1, \"a\", 7], [2, \"b\", 13], [2, \"c\", 17]])\n&gt;&gt;&gt; rdd.keys().collect()\n[1, 2, 2]\n&gt;&gt;&gt; rdd.values().collect()\n['a', 'b', 'c']\n\nFor things to work as expected you must do\n&gt;&gt;&gt; rdd = sc.parallelize([[1, \"a\", 7], [2, \"b\", 13], [2, \"c\", 17]])\\\n      .map(lambda x: (x[0], x[1:]))\n&gt;&gt;&gt; rdd.keys().collect()\n[1, 2, 2]\n&gt;&gt;&gt; rdd.values().collect()\n[['a', 7], ['b', 13], ['c', 17]]"
  },
  {
    "objectID": "core/slides/slides03_sparkrdd.html#transformations-for-a-single-pairrdd",
    "href": "core/slides/slides03_sparkrdd.html#transformations-for-a-single-pairrdd",
    "title": "Apache and RDD",
    "section": "Transformations for a single PairRDD",
    "text": "Transformations for a single PairRDD\n\n\n\n\n\n\n\n\ntransformation\ndescription\n\n\n\n\nkeys()\nReturn an RDD containing the keys\n\n\nvalues()\nReturn an RDD containing the values\n\n\nsortByKey()\nReturn an RDD sorted by the key\n\n\nmapValues(f)\nApply a function f to each value of a pair RDD without changing the key\n\n\nflatMapValues(f)\nPass each value in the key-value pair RDD through a flatMap function f without changing the keys"
  },
  {
    "objectID": "core/slides/slides03_sparkrdd.html#transformations-for-a-single-pairrdd-1",
    "href": "core/slides/slides03_sparkrdd.html#transformations-for-a-single-pairrdd-1",
    "title": "Apache and RDD",
    "section": "Transformations for a single PairRDD",
    "text": "Transformations for a single PairRDD\n\n\n\n\n\n\n\n\ntransformation\ndescription\n\n\n\n\nkeys()\nReturn an RDD containing the keys\n\n\nvalues()\nReturn an RDD containing the values\n\n\nsortByKey()\nReturn an RDD sorted by the key\n\n\nmapValues(f)\nApply a function f to each value of a pair RDD without changing the key\n\n\nflatMapValues(f)\nPass each value in the key-value pair RDD through a flatMap function f without changing the keys\n\n\n\n\n\nExample with mapValues\n&gt;&gt;&gt; rdd = sc.parallelize([(\"a\", \"x y z\"), (\"b\", \"p r\")])\n&gt;&gt;&gt; rdd.mapValues(lambda v: v.split(' ')).collect()\n[('a', ['x', 'y', 'z']), ('b', ['p', 'r'])]"
  },
  {
    "objectID": "core/slides/slides03_sparkrdd.html#transformations-for-a-single-pairrdd-2",
    "href": "core/slides/slides03_sparkrdd.html#transformations-for-a-single-pairrdd-2",
    "title": "Apache and RDD",
    "section": "Transformations for a single PairRDD",
    "text": "Transformations for a single PairRDD\n\n\n\n\n\n\n\n\ntransformation\ndescription\n\n\n\n\nkeys()\nReturn an RDD containing the keys\n\n\nvalues()\nReturn an RDD containing the values\n\n\nsortByKey()\nReturn an RDD sorted by the key\n\n\nmapValues(f)\nApply a function f to each value of a pair RDD without changing the key\n\n\nflatMapValues(f)\nPass each value in the key-value pair RDD through a flatMap function f without changing the keys\n\n\n\n\n\nExample with flatMapValues\n&gt;&gt;&gt; texts = sc.parallelize([(\"a\", \"x y z\"), (\"b\", \"p r\")])\n&gt;&gt;&gt; tokenize = lambda x: x.split(\" \")\n&gt;&gt;&gt; texts.flatMapValues(tokenize).collect()\n[('a', 'x'), ('a', 'y'), ('a', 'z'), ('b', 'p'), ('b', 'r')]"
  },
  {
    "objectID": "core/slides/slides03_sparkrdd.html#transformations-for-a-single-pairrdd-keyed",
    "href": "core/slides/slides03_sparkrdd.html#transformations-for-a-single-pairrdd-keyed",
    "title": "Apache and RDD",
    "section": "Transformations for a single PairRDD (keyed)",
    "text": "Transformations for a single PairRDD (keyed)\n\n\n\n\n\n\n\n\ntransformation\ndescription\n\n\n\n\ngroupByKey()\nGroup values with the same key\n\n\nreduceByKey(f)\nMerge the values for each key using an associative reduce function f.\n\n\nfoldByKey(f)\nMerge the values for each key using an associative reduce function f.\n\n\ncombineByKey(createCombiner, mergeValue, mergeCombiners, [partitioner])\nGeneric function to combine the elements for each key using a custom set of aggregation functions."
  },
  {
    "objectID": "core/slides/slides03_sparkrdd.html#transformations-for-a-single-pairrdd-keyed-1",
    "href": "core/slides/slides03_sparkrdd.html#transformations-for-a-single-pairrdd-keyed-1",
    "title": "Apache and RDD",
    "section": "Transformations for a single PairRDD (keyed)",
    "text": "Transformations for a single PairRDD (keyed)\n\n\n\n\n\n\n\n\ntransformation\ndescription\n\n\n\n\ngroupByKey()\nGroup values with the same key\n\n\nreduceByKey(f)\nMerge the values for each key using an associative reduce function f.\n\n\nfoldByKey(f)\nMerge the values for each key using an associative reduce function f.\n\n\ncombineByKey(createCombiner, mergeValue, mergeCombiners, [partitioner])\nGeneric function to combine the elements for each key using a custom set of aggregation functions.\n\n\n\n\n\nExample with groupByKey\n&gt;&gt;&gt; rdd = sc.parallelize([\n        (\"a\", 1), (\"b\", 1), (\"a\", 1), \n        (\"b\", 3), (\"c\", 42)\n        ])\n&gt;&gt;&gt; rdd.groupByKey().mapValues(list).collect()\n[('c', [42]), ('b', [1, 3]), ('a', [1, 1])]"
  },
  {
    "objectID": "core/slides/slides03_sparkrdd.html#groupbykey-internals",
    "href": "core/slides/slides03_sparkrdd.html#groupbykey-internals",
    "title": "Apache and RDD",
    "section": "groupByKey() internals",
    "text": "groupByKey() internals\n\nGrouping locally\n Shuffling\nPartitionning\nRelation to reduceByKey()"
  },
  {
    "objectID": "core/slides/slides03_sparkrdd.html#transformations-for-a-single-pairrdd-keyed-2",
    "href": "core/slides/slides03_sparkrdd.html#transformations-for-a-single-pairrdd-keyed-2",
    "title": "Apache and RDD",
    "section": "Transformations for a single PairRDD (keyed)",
    "text": "Transformations for a single PairRDD (keyed)\n\n\n\n\n\n\n\n\ntransformation\ndescription\n\n\n\n\ngroupByKey()\nGroup values with the same key\n\n\nreduceByKey(f)\nMerge the values for each key using an associative reduce function f.\n\n\nfoldByKey(f)\nMerge the values for each key using an associative reduce function f.\n\n\ncombineByKey(createCombiner, mergeValue, mergeCombiners, [partitioner])\nGeneric function to combine the elements for each key using a custom set of aggregation functions.\n\n\n\n\n\nExample with reduceByKey\n&gt;&gt;&gt; rdd = sc.parallelize([(\"a\", 1), (\"b\", 1), (\"a\", 1)])\n&gt;&gt;&gt; rdd.reduceByKey(lambda a, b: a + b).collect()\n[('a', 2), ('b', 1)]\n\nThe reducing occurs first locally (within partitions)\nThen, a shuffle is performed with the local results to reduce globally"
  },
  {
    "objectID": "core/slides/slides03_sparkrdd.html#reducebykey-in-picture",
    "href": "core/slides/slides03_sparkrdd.html#reducebykey-in-picture",
    "title": "Apache and RDD",
    "section": "ReduceByKey in picture",
    "text": "ReduceByKey in picture"
  },
  {
    "objectID": "core/slides/slides03_sparkrdd.html#transformations-for-a-single-pairrdd-keyed-3",
    "href": "core/slides/slides03_sparkrdd.html#transformations-for-a-single-pairrdd-keyed-3",
    "title": "Apache and RDD",
    "section": "Transformations for a single PairRDD (keyed)",
    "text": "Transformations for a single PairRDD (keyed)\n\n\n\n\n\n\n\n\ntransformation\ndescription\n\n\n\n\ngroupByKey()\nGroup values with the same key\n\n\nreduceByKey(f)\nMerge the values for each key using an associative reduce function f.\n\n\nfoldByKey(f)\nMerge the values for each key using an associative reduce function f.\n\n\ncombineByKey(createCombiner, mergeValue, mergeCombiners, [partitioner])\nGeneric function to combine the elements for each key using a custom set of aggregation functions.\n\n\n\n\n\ncombineByKey Transforms an RDD[(K, V)] into another RDD of type RDD[(K, C)] for a combined type C that can be different from V\n\n\nThe user must define\n\ncreateCombiner : which turns a V into a C\nmergeValue : to merge a V into a C\nmergeCombiners : to combine two C’s into a single one"
  },
  {
    "objectID": "core/slides/slides03_sparkrdd.html#transformations-for-a-single-pairrdd-keyed-4",
    "href": "core/slides/slides03_sparkrdd.html#transformations-for-a-single-pairrdd-keyed-4",
    "title": "Apache and RDD",
    "section": "Transformations for a single PairRDD (keyed)",
    "text": "Transformations for a single PairRDD (keyed)\n\n\n\n\n\n\n\n\ntransformation\ndescription\n\n\n\n\ngroupByKey()\nGroup values with the same key\n\n\nreduceByKey(f)\nMerge the values for each key using an associative reduce function f.\n\n\nfoldByKey(f)\nMerge the values for each key using an associative reduce function f.\n\n\ncombineByKey(createCombiner, mergeValue, mergeCombiners, [partitioner])\nGeneric function to combine the elements for each key using a custom set of aggregation functions.\n\n\n\n\n\nIn this example\n\ncreateCombiner : converts the value to str\nmergeValue : concatenates two str\nmergeCombiners : concatenates two str\n\n&gt;&gt;&gt; rdd = sc.parallelize([('a', 1), ('b', 2), ('a', 13)])\n&gt;&gt;&gt; def add(a, b):\n        return a + str(b)\n&gt;&gt;&gt; rdd.combineByKey(str, add, add).collect()\n[('a', '113'), ('b', '2')]"
  },
  {
    "objectID": "core/slides/slides03_sparkrdd.html#transformations-for-two-pairrdd",
    "href": "core/slides/slides03_sparkrdd.html#transformations-for-two-pairrdd",
    "title": "Apache and RDD",
    "section": "Transformations for two PairRDD",
    "text": "Transformations for two PairRDD\n\n\n\n\n\n\n\ntransformation\ndescription\n\n\n\n\nsubtractByKey(other)\nRemove elements with a key present in the other RDD.\n\n\njoin(other)\nInner join with other RDD.\n\n\nrightOuterJoin(other)\nRight join with other RDD.\n\n\nleftOuterJoin(other)\nLeft join with other RDD.\n\n\n\n\nRight join: the key must be present in the first RDD\nLeft join: the key must be present in the other RDD"
  },
  {
    "objectID": "core/slides/slides03_sparkrdd.html#transformations-for-two-pairrdd-1",
    "href": "core/slides/slides03_sparkrdd.html#transformations-for-two-pairrdd-1",
    "title": "Apache and RDD",
    "section": "Transformations for two PairRDD",
    "text": "Transformations for two PairRDD\n\nJoin operations are mainly used through the high-level API: DataFrame objects and the spark.sql API\nWe will use them a lot with the high-level API (DataFrame from spark.sql)\n\n\n\nBack to Jupyter notebook V : Spark RDD"
  },
  {
    "objectID": "core/slides/slides03_sparkrdd.html#actions-for-a-single-pairrdd",
    "href": "core/slides/slides03_sparkrdd.html#actions-for-a-single-pairrdd",
    "title": "Apache and RDD",
    "section": "Actions for a single PairRDD",
    "text": "Actions for a single PairRDD\n\n\n\n\n\n\n\naction\ndescription\n\n\n\n\ncountByKey()\nCount the number of elements for each key.\n\n\nlookup(key)\nReturn all the values associated with the provided key.\n\n\ncollectAsMap()\nReturn the key-value pairs in this RDD to the master as a Python dictionary.\n\n\n\n\n\nBack to Jupyter notebook V : Spark RDD"
  },
  {
    "objectID": "core/slides/slides03_sparkrdd.html#data-partitionning",
    "href": "core/slides/slides03_sparkrdd.html#data-partitionning",
    "title": "Apache and RDD",
    "section": "Data partitionning",
    "text": "Data partitionning\n\nSome operations on PairRDDs, such as join, require to scan the data more than once\nPartitionning the RDDs in advance can reduce network communications\nWhen a key-oriented dataset is reused several times, partitionning can improve performance\nIn Spark: you can choose which keys will appear on the same node, but no explicit control of which worker node each key goes to."
  },
  {
    "objectID": "core/slides/slides03_sparkrdd.html#data-partitionning-1",
    "href": "core/slides/slides03_sparkrdd.html#data-partitionning-1",
    "title": "Apache and RDD",
    "section": "Data partitionning",
    "text": "Data partitionning\nIn practice, you can specify the number of partitions with\nrdd.partitionBy(100)\n\nYou can also use a custom partition function hash such that hash(key) returns a hash value\nimport urlparse\n\n&gt;&gt;&gt; def hash_domain(url):\n        # Returns a hash associated to the domain of a website\n        return hash(urlparse.urlparse(url).netloc)\n\nrdd.partitionBy(20, hash_domain) # Create 20 partitions\nTo have finer control on partitionning, you must use the Scala API."
  },
  {
    "objectID": "core/slides/slides03_sparkrdd.html#section-7",
    "href": "core/slides/slides03_sparkrdd.html#section-7",
    "title": "Apache and RDD",
    "section": "",
    "text": "Partitionning tweaking\nShuffles monitoring"
  },
  {
    "objectID": "core/slides/slides04_sparkl.html#pyspark-overview",
    "href": "core/slides/slides04_sparkl.html#pyspark-overview",
    "title": "Spark SQL",
    "section": "PySpark overview",
    "text": "PySpark overview\n\nOfficial documentation"
  },
  {
    "objectID": "core/slides/slides04_sparkl.html#overview",
    "href": "core/slides/slides04_sparkl.html#overview",
    "title": "Spark SQL",
    "section": "Overview",
    "text": "Overview\n\nSpark SQL is a library included in Spark since version 1.3\nSpark Dataframes was introduced with version\nIt provides an easier interface to process tabular data\nInstead of RDDs, we deal with DataFrames\nSince Spark 1.6, there is also the concept of Datasets, but only for Scala and Java"
  },
  {
    "objectID": "core/slides/slides04_sparkl.html#sparkcontext-and-sparksession",
    "href": "core/slides/slides04_sparkl.html#sparkcontext-and-sparksession",
    "title": "Spark SQL",
    "section": "SparkContext and SparkSession",
    "text": "SparkContext and SparkSession\n\nBefore Spark 2, there was only SparkContext and SQLContext\nAll core functionality was accessed with SparkContext\nAll SQL functionality needed the SQLContext, which can be created from an SparkContext\nWith Spark 2 came the SparkSession class\nSparkSession is the .stress[global entry-point] for everything Spark-related\n\n\n\nSparkContext was enough for handling RDDs\nPurpose of SQLContext ?\nCould we use SparkSession to handle RDDs?"
  },
  {
    "objectID": "core/slides/slides04_sparkl.html#sparkcontext-and-sparksession-1",
    "href": "core/slides/slides04_sparkl.html#sparkcontext-and-sparksession-1",
    "title": "Spark SQL",
    "section": "SparkContext and SparkSession",
    "text": "SparkContext and SparkSession\nBefore Spark 2\n&gt;&gt;&gt; from pyspark import SparkConf, SparkContext\n&gt;&gt;&gt; from pyspark.sql import SQLContext\n\n&gt;&gt;&gt; conf = SparkConf().setAppName(appName).setMaster(master)\n&gt;&gt;&gt; sc = SparkContext(conf = conf)\n&gt;&gt;&gt; sql_context = new SQLContext(sc)\n\nSince Spark 2\n\nfrom pyspark.sql import SparkSession\n\napp_name = \"Spark Dataframes\"\n\nspark = (\n  SparkSession \n    .builder \n    .appName(app_name) \n#        .master(master) \n#        .config(\"spark.some.config.option\", \"some-value\") \\\n    .getOrCreate()\n)"
  },
  {
    "objectID": "core/slides/slides04_sparkl.html#dataframe-1",
    "href": "core/slides/slides04_sparkl.html#dataframe-1",
    "title": "Spark SQL",
    "section": "DataFrame",
    "text": "DataFrame\n\nThe main entity of Spark SQL is the DataFrame\nA DataFrame is actually an RDD of Rows with a schema\nA schema gives the names of the columns and their types\nRow is a class representing a row of the DataFrame.\nIt can be used almost as a python list, with its size equal to the number of columns in the schema.\n\n\nRow-oriented or column-oriented?"
  },
  {
    "objectID": "core/slides/slides04_sparkl.html#dataframe-2",
    "href": "core/slides/slides04_sparkl.html#dataframe-2",
    "title": "Spark SQL",
    "section": "DataFrame",
    "text": "DataFrame\n\nfrom pyspark.sql import Row\n\nrow1 = Row(name=\"John\", age=21)\nrow2 = Row(name=\"James\", age=32)\nrow3 = Row(name=\"Jane\", age=18)\nrow1['name']\n\n\n\n'John'\n\n\n\ndf = spark.createDataFrame([row1, row2, row3])\ndf\n\n\n\nDataFrame[name: string, age: bigint]\n\n\n\ndf.show()\n\n\n\n+-----+---+\n| name|age|\n+-----+---+\n| John| 21|\n|James| 32|\n| Jane| 18|\n+-----+---+\n\n\n\n\nRelate Row to named tuple or dictionary\nWhat does .show() ?"
  },
  {
    "objectID": "core/slides/slides04_sparkl.html#dataframe-3",
    "href": "core/slides/slides04_sparkl.html#dataframe-3",
    "title": "Spark SQL",
    "section": "DataFrame",
    "text": "DataFrame\n\ndf.printSchema()\n\n\n\nroot\n |-- name: string (nullable = true)\n |-- age: long (nullable = true)\n\n\n\nYou can access the underlying RDD object using .rdd\n\nprint(df.rdd.toDebugString().decode(\"utf-8\"))\n\n\n\n(20) MapPartitionsRDD[10] at javaToPython at NativeMethodAccessorImpl.java:0 []\n |   MapPartitionsRDD[9] at javaToPython at NativeMethodAccessorImpl.java:0 []\n |   SQLExecutionRDD[8] at javaToPython at NativeMethodAccessorImpl.java:0 []\n |   MapPartitionsRDD[7] at javaToPython at NativeMethodAccessorImpl.java:0 []\n |   MapPartitionsRDD[4] at applySchemaToPythonRDD at NativeMethodAccessorImpl.java:0 []\n |   MapPartitionsRDD[3] at map at SerDeUtil.scala:69 []\n |   MapPartitionsRDD[2] at mapPartitions at SerDeUtil.scala:117 []\n |   PythonRDD[1] at RDD at PythonRDD.scala:53 []\n |   ParallelCollectionRDD[0] at readRDDFromFile at PythonRDD.scala:289 []\n\n\n\ndf.rdd.getNumPartitions()\n\n\n\n20"
  },
  {
    "objectID": "core/slides/slides04_sparkl.html#creating-dataframes",
    "href": "core/slides/slides04_sparkl.html#creating-dataframes",
    "title": "Spark SQL",
    "section": "Creating DataFrames",
    "text": "Creating DataFrames\n\nWe can use the method createDataFrame from the SparkSession instance\nCan be used to create a Spark DataFrame from:\n\na pandas.DataFrame object\na local python list\nan RDD\n\nFull documentation can be found in the [API docs]"
  },
  {
    "objectID": "core/slides/slides04_sparkl.html#creating-dataframes-1",
    "href": "core/slides/slides04_sparkl.html#creating-dataframes-1",
    "title": "Spark SQL",
    "section": "Creating DataFrames",
    "text": "Creating DataFrames\n\nrows = [\n        Row(name=\"John\", age=21, gender=\"male\"),\n        Row(name=\"Jane\", age=25, gender=\"female\"),\n        Row(name=\"Albert\", age=46, gender=\"male\")\n    ]\ndf = spark.createDataFrame(rows)\ndf.show()\n\n\n\n+------+---+------+\n|  name|age|gender|\n+------+---+------+\n|  John| 21|  male|\n|  Jane| 25|female|\n|Albert| 46|  male|\n+------+---+------+"
  },
  {
    "objectID": "core/slides/slides04_sparkl.html#creating-dataframes-2",
    "href": "core/slides/slides04_sparkl.html#creating-dataframes-2",
    "title": "Spark SQL",
    "section": "Creating DataFrames",
    "text": "Creating DataFrames\n\ncolumn_names = [\"name\", \"age\", \"gender\"]\nrows = [\n        [\"John\", 21, \"male\"],\n        [\"James\", 25, \"female\"],\n        [\"Albert\", 46, \"male\"]\n    ]\ndf = spark.createDataFrame(rows, column_names)\ndf.show()\n\n\n\n+------+---+------+\n|  name|age|gender|\n+------+---+------+\n|  John| 21|  male|\n| James| 25|female|\n|Albert| 46|  male|\n+------+---+------+"
  },
  {
    "objectID": "core/slides/slides04_sparkl.html#creating-dataframes-3",
    "href": "core/slides/slides04_sparkl.html#creating-dataframes-3",
    "title": "Spark SQL",
    "section": "Creating DataFrames",
    "text": "Creating DataFrames\n\ncolumn_names = [\"name\", \"age\", \"gender\"]\n\nsc = spark._sc\n\nrdd = sc.parallelize([\n        (\"John\", 21, \"male\"),\n        (\"James\", 25, \"female\"),\n        (\"Albert\", 46, \"male\")\n    ])\n\ndf = spark.createDataFrame(rdd, column_names)\ndf.show()\n\n\n\n+------+---+------+\n|  name|age|gender|\n+------+---+------+\n|  John| 21|  male|\n| James| 25|female|\n|Albert| 46|  male|\n+------+---+------+"
  },
  {
    "objectID": "core/slides/slides04_sparkl.html#schema-and-types",
    "href": "core/slides/slides04_sparkl.html#schema-and-types",
    "title": "Spark SQL",
    "section": "Schema and Types",
    "text": "Schema and Types\n\nA DataFrame always contains a schema\nThe schema defines the column names and types\nIn all previous examples, the schema was inferred\nThe schema of a DataFrame is represented by the class types.StructType [API doc]\nWhen creating a DataFrame, the schema can be either inferred or defined by the user\n\n\nfrom pyspark.sql.types import *\n\ndf.schema\n# StructType(List(StructField(name,StringType,true),\n#                 StructField(age,IntegerType,true),\n#                 StructField(gender,StringType,true)))\n\n\n\nStructType([StructField('name', StringType(), True), StructField('age', LongType(), True), StructField('gender', StringType(), True)])\n\n\n\ncheck absence of quotation\nSpark has its own collection (tree) of types. The Python counterparts are defined in pyspark.sql.types"
  },
  {
    "objectID": "core/slides/slides04_sparkl.html#creating-a-custom-schema",
    "href": "core/slides/slides04_sparkl.html#creating-a-custom-schema",
    "title": "Spark SQL",
    "section": "Creating a custom Schema",
    "text": "Creating a custom Schema\n\nfrom pyspark.sql.types import *\n\nschema = StructType([\n    StructField(\"name\", StringType(), True),\n    StructField(\"age\", IntegerType(), True),\n    StructField(\"gender\", StringType(), True)\n])\n\nrows = [(\"John\", 21, \"male\")]\ndf = spark.createDataFrame(rows, schema)\ndf.printSchema()\ndf.show()\n\n\n\nroot\n |-- name: string (nullable = true)\n |-- age: integer (nullable = true)\n |-- gender: string (nullable = true)\n\n+----+---+------+\n|name|age|gender|\n+----+---+------+\n|John| 21|  male|\n+----+---+------+"
  },
  {
    "objectID": "core/slides/slides04_sparkl.html#types-supported-by-spark-sql",
    "href": "core/slides/slides04_sparkl.html#types-supported-by-spark-sql",
    "title": "Spark SQL",
    "section": "Types supported by Spark SQL",
    "text": "Types supported by Spark SQL\n\nStringType\nIntegerType\nLongType\nFloatType\nDoubleType\nBooleanType\nDateType\nTimestampType\n...\n\nThe full list of types can be found in [API doc]"
  },
  {
    "objectID": "core/slides/slides04_sparkl.html#reading-data-from-sources",
    "href": "core/slides/slides04_sparkl.html#reading-data-from-sources",
    "title": "Spark SQL",
    "section": "Reading data from sources",
    "text": "Reading data from sources\n\nData is usually read from external sources (move the algorithms, not the data)\nSpark SQL provides connectors to read from many different sources:\n\nText files (CSV, JSON)\nDistributed tabular files (Parquet, ORC)\nIn-memory data sources (Apache Arrow)\nGeneral relational Databases (via JDBC)\nThird-party connectors to connect to many other databases\nAnd you can create your own connector for Spark (in Scala)"
  },
  {
    "objectID": "core/slides/slides04_sparkl.html#reading-data-from-sources-1",
    "href": "core/slides/slides04_sparkl.html#reading-data-from-sources-1",
    "title": "Spark SQL",
    "section": "Reading data from sources",
    "text": "Reading data from sources\n\nIn all cases, the syntax is similar:\n\nspark.read.{source}(path)\n\nSpark supports different file systems to look at the data:\n\nLocal files: file://path/to/file or just path/to/file\nHDFS (Hadoop Distributed FileSystem): hdfs://path/to/file\nAmazon S3: s3://path/to/file"
  },
  {
    "objectID": "core/slides/slides04_sparkl.html#reading-from-a-csv-file",
    "href": "core/slides/slides04_sparkl.html#reading-from-a-csv-file",
    "title": "Spark SQL",
    "section": "Reading from a CSV file",
    "text": "Reading from a CSV file\n\npath_to_csv = \"../../../../Downloads/tips.csv\"\ndf = spark.read.csv(path_to_csv)\n\n\n\n\n\ndf = (\n  spark.read\n    .format('csv')\n    .option('header', 'true')\n    .option('sep', \",\")\n    .load(path_to_csv)\n)\n\n\n\n\n\nmy_csv_options = {\n  'header': True,\n  'sep': ';',\n}\n\ndf = (\n  spark\n    .read\n    .csv(path_to_csv, **my_csv_options)\n)"
  },
  {
    "objectID": "core/slides/slides04_sparkl.html#reading-from-a-csv-file-1",
    "href": "core/slides/slides04_sparkl.html#reading-from-a-csv-file-1",
    "title": "Spark SQL",
    "section": "Reading from a CSV file",
    "text": "Reading from a CSV file\nMain options\nSome important options of the CSV reader are listed here:\n\n\n\n\n\n\n\nOption\nDescription\n\n\n\n\nsep\nThe separator character\n\n\nheader\nIf “true”, the first line contains the column names\n\n\ninferSchema\nIf “true”, the column types will be guessed from the contents\n\n\ndateFormat\nA string representing the format of the date columns\n\n\n\nThe full list of options can be found in the API Docs"
  },
  {
    "objectID": "core/slides/slides04_sparkl.html#reading-from-other-file-types",
    "href": "core/slides/slides04_sparkl.html#reading-from-other-file-types",
    "title": "Spark SQL",
    "section": "Reading from other file types",
    "text": "Reading from other file types\n## JSON file\ndf = spark.read.json(\"/path/to/file.json\")\ndf = spark.read.format(\"json\").load(\"/path/to/file.json\")\n## Parquet file (distributed tabular data)\ndf = spark.read.parquet(\"hdfs://path/to/file.parquet\")\ndf = spark.read.format(\"parquet\").load(\"hdfs://path/to/file.parquet\")\n## ORC file (distributed tabular data)\ndf = spark.read.orc(\"hdfs://path/to/file.orc\")\ndf = spark.read.format(\"orc\").load(\"hdfs://path/to/file.orc\")"
  },
  {
    "objectID": "core/slides/slides04_sparkl.html#reading-from-external-databases",
    "href": "core/slides/slides04_sparkl.html#reading-from-external-databases",
    "title": "Spark SQL",
    "section": "Reading from external databases",
    "text": "Reading from external databases\n\nWe can use JDBC drivers (Java) to read from relational databases\nExamples of databases: Oracle, PostgreSQL, MySQL, etc.\nThe java driver file must be uploaded to the cluster before trying to access\nThis operation can be very heavy. When available, specific connectors should be used\nSpecific connectors are often provided by third-party libraries"
  },
  {
    "objectID": "core/slides/slides04_sparkl.html#reading-from-external-databases-1",
    "href": "core/slides/slides04_sparkl.html#reading-from-external-databases-1",
    "title": "Spark SQL",
    "section": "Reading from external databases",
    "text": "Reading from external databases\n\nspark = (\n  SparkSession \n    .builder \n    .appName(\"Python Spark SQL basic example\") \n    .config(\"spark.jars\", \n            spark_home + \"/jars/\" + \"postgresql-42.7.2.jar\") \n    .getOrCreate()\n)\n\n\n\n\n\ndf = (\n  spark\n    .read.format(\"jdbc\") \n    .option(\"url\", \"jdbc:postgresql:dbserver\") \n    .option(\"dbtable\", \"schema.tablename\") \n    .option(\"user\", usrnm) \n    .option(\"password\", pwd) \n    .load()\n)\n\n\n\n\nor\ndf = spark.read.jdbc(\n      url=\"jdbc:postgresql:dbserver\",\n      table=\"schema.tablename\"\n      properties={\n          \"user\": \"username\",\n          \"password\": \"p4ssw0rd\"\n      }\n)\n\ndf_airlines = (\n  spark\n    .read\n    .format(\"jdbc\")\n    .options(**(dico_jdbc_pg | {'dbtable': 'nycflights.airlines'}))\n    .load()\n)"
  },
  {
    "objectID": "core/slides/slides04_sparkl.html#spark-sql-as-a-substitute-for-hiveql",
    "href": "core/slides/slides04_sparkl.html#spark-sql-as-a-substitute-for-hiveql",
    "title": "Spark SQL",
    "section": "Spark SQL as a Substitute for HiveQL",
    "text": "Spark SQL as a Substitute for HiveQL\n\n Hive (Hadoop InteractiVE)\n\nDevlopped by  dring 2000’s\nReleased 2010 as Apache project\n\n\nHiveQL: SQL-like interface to query data stored in various databases and file systems that integrate with Hadoop.\nHive on wikipedia\n\n\nApache Hive is a data warehouse software project, built on top of Apache Hadoop for providing data query and analysis.[3][4] Hive gives an SQL-like interface to query data stored in various databases and file systems that integrate with Hadoop. Traditional SQL queries must be implemented in the MapReduce Java API to execute SQL applications and queries over distributed data. Hive provides the necessary SQL abstraction to integrate SQL-like queries (HiveQL) into the underlying Java without the need to implement queries in the low-level Java API. Since most data warehousing applications work with SQL-based querying languages, Hive aids the portability of SQL-based applications to Hadoop.[5] While initially developed by Facebook, Apache Hive is used and developed by other companies such as Netflix and the Financial Industry Regulatory Authority (FINRA).[6][7] Amazon maintains a software fork of Apache Hive included in Amazon Elastic MapReduce on Amazon Web Services.[8]"
  },
  {
    "objectID": "core/slides/slides04_sparkl.html#performing-queries",
    "href": "core/slides/slides04_sparkl.html#performing-queries",
    "title": "Spark SQL",
    "section": "Performing queries",
    "text": "Performing queries\n\nSpark SQL is designed to be compatible with ANSI SQL queries\nSpark SQL allows SQL-like queries to be evaluated on Spark DataFrames (and on many other tables)\nSpark DataFrames have to be tagged as temporary views\nSpark SQL Queries can be submitted using spark.sql()\n\n Method sql for class SparkSession provides access to SQLContext"
  },
  {
    "objectID": "core/slides/slides04_sparkl.html#performing-queries-1",
    "href": "core/slides/slides04_sparkl.html#performing-queries-1",
    "title": "Spark SQL",
    "section": "Performing queries",
    "text": "Performing queries\n\ncolumn_names = [\"name\", \"age\", \"gender\"]\nrows = [\n        [\"John\", 21, \"male\"],\n        [\"Jane\", 25, \"female\"]\n    ]\ndf = spark.createDataFrame(rows, column_names)\n\ndf.show()\n\n\n\n+----+---+------+\n|name|age|gender|\n+----+---+------+\n|John| 21|  male|\n|Jane| 25|female|\n+----+---+------+\n\n\n\n\n\n## Create a temporary view from the DataFrame\ndf.createOrReplaceTempView(\"new_view\")\n\n## Define the query\nquery = \"\"\"\n  SELECT name, age \n  FROM new_view \n  WHERE gender='male'\n\"\"\"\n\nmen_df = spark.sql(query)\nmen_df.show()\n\n\n\n+----+---+\n|name|age|\n+----+---+\n|John| 21|\n+----+---+"
  },
  {
    "objectID": "core/slides/slides04_sparkl.html#using-the-api",
    "href": "core/slides/slides04_sparkl.html#using-the-api",
    "title": "Spark SQL",
    "section": "Using the API",
    "text": "Using the API\nSQL queries form an expresive feature, it’s not the best way to code a complex logic\n\nErrors are harder to find in strings\nQueries makes the code less modular\n\n\nThe Spark dataframe API offers a developper-friendly API for implementing\n\nRelational algebra \\(\\sigma, \\pi, \\bowtie, \\cup, \\cap, \\setminus\\)\nPartitionning GROUP BY\nAggregation and Window functions\n\n\n\nCompare the Spark Dataframe API with:\n dplyr, dtplyr, dbplyr in R Tidyverse\n Pandas\nChaining and/or piping enable modular query construction"
  },
  {
    "objectID": "core/slides/slides04_sparkl.html#basic-single-tables-operations-methodsverbs",
    "href": "core/slides/slides04_sparkl.html#basic-single-tables-operations-methodsverbs",
    "title": "Spark SQL",
    "section": "Basic Single Tables Operations (methods/verbs)",
    "text": "Basic Single Tables Operations (methods/verbs)\n\n\n\n\n\n\n\nOperation\nDescription\n\n\n\n\nselect\nChooses columns from the table \\(\\pi\\)\n\n\nselectExpr\nChooses columns and expressions from table \\(\\pi\\)\n\n\nwhere\nFilters rows based on a boolean rule \\(\\sigma\\)\n\n\nlimit\nLimits the number of rows LIMIT ...\n\n\norderBy\nSorts the DataFrame based on one or more columns ORDER BY ...\n\n\nalias\nChanges the name of a column AS ...\n\n\ncast\nChanges the type of a column\n\n\nwithColumn\nAdds a new column"
  },
  {
    "objectID": "core/slides/slides04_sparkl.html#select",
    "href": "core/slides/slides04_sparkl.html#select",
    "title": "Spark SQL",
    "section": "SELECT",
    "text": "SELECT\n\n## SQL query:\nquery = \"\"\"\n  SELECT name, age \n  FROM table\n\"\"\"\n\n## Using Spark SQL API:\n( \n  df.select(\"name\", \"age\")\n    .show()\n)\n\n\n\n+----+---+\n|name|age|\n+----+---+\n|John| 21|\n|Jane| 25|\n+----+---+"
  },
  {
    "objectID": "core/slides/slides04_sparkl.html#select-continued",
    "href": "core/slides/slides04_sparkl.html#select-continued",
    "title": "Spark SQL",
    "section": "SELECT (continued)",
    "text": "SELECT (continued)\nThe argument of select() is *cols where cols can be built from column names (strings), column expressions like df.age + 10, lists\n\ndf.select( df.name.alias(\"nom\"), df.age+10 ).show()\n\n\n\n+----+----------+\n| nom|(age + 10)|\n+----+----------+\n|John|        31|\n|Jane|        35|\n+----+----------+\n\n\n\n\ndf.select([c for c in df.columns if \"a\" in c]).show()\n\n\n\n+----+---+\n|name|age|\n+----+---+\n|John| 21|\n|Jane| 25|\n+----+---+"
  },
  {
    "objectID": "core/slides/slides04_sparkl.html#selectexpr",
    "href": "core/slides/slides04_sparkl.html#selectexpr",
    "title": "Spark SQL",
    "section": "selectExpr",
    "text": "selectExpr\n###  A variant of select() that accepts SQL expressions.\n&gt;&gt;&gt; df.selectExpr(\"age * 2\", \"abs(age)\").collect()"
  },
  {
    "objectID": "core/slides/slides04_sparkl.html#where",
    "href": "core/slides/slides04_sparkl.html#where",
    "title": "Spark SQL",
    "section": "WHERE",
    "text": "WHERE\n\n## In a SQL query:\nquery = \"\"\"\n  SELECT * \n  FROM table \n  WHERE age &gt; 21\n\"\"\"\n\n## Using Spark SQL API:\ndf.where(\"age &gt; 21\").show()\n\n## Alternatively:\n# df.where(df['age'] &gt; 21).show()\n# df.where(df.age &gt; 21).show()\n# df.select(\"*\").where(\"age &gt; 21\").show()\n\n\n\n+----+---+------+\n|name|age|gender|\n+----+---+------+\n|Jane| 25|female|\n+----+---+------+"
  },
  {
    "objectID": "core/slides/slides04_sparkl.html#limit",
    "href": "core/slides/slides04_sparkl.html#limit",
    "title": "Spark SQL",
    "section": "LIMIT",
    "text": "LIMIT\n\n## In a SQL query:\nquery = \"\"\"\n  SELECT * \n  FROM table \n  LIMIT 1\n\"\"\"\n\n## Using Spark SQL API:\n( \n  df.limit(1)\n    .show()\n)\n\n## Or even\ndf.select(\"*\").limit(1).show()\n\n\n\n+----+---+------+\n|name|age|gender|\n+----+---+------+\n|John| 21|  male|\n+----+---+------+\n\n+----+---+------+\n|name|age|gender|\n+----+---+------+\n|John| 21|  male|\n+----+---+------+"
  },
  {
    "objectID": "core/slides/slides04_sparkl.html#order-by",
    "href": "core/slides/slides04_sparkl.html#order-by",
    "title": "Spark SQL",
    "section": "ORDER BY",
    "text": "ORDER BY\n\n## In a SQL query:\nquery = \"\"\"\n  SELECT * \n  FROM table \n  ORDER BY name ASC\n\"\"\"\n\n## Using Spark SQL API:\ndf.orderBy(df.name.asc()).show()\n\n\n\n+----+---+------+\n|name|age|gender|\n+----+---+------+\n|Jane| 25|female|\n|John| 21|  male|\n+----+---+------+"
  },
  {
    "objectID": "core/slides/slides04_sparkl.html#alias-name-change",
    "href": "core/slides/slides04_sparkl.html#alias-name-change",
    "title": "Spark SQL",
    "section": "ALIAS (name change)",
    "text": "ALIAS (name change)\n\n## In a SQL query:\nquery = \"\"\"\n  SELECT name, age, gender AS sex \n  FROM table\n\"\"\"\n\n## Using Spark SQL API:\ndf.select(\n    df.name, \n    df.age, \n    df.gender.alias('sex')\n  ).show()\n\n\n\n+----+---+------+\n|name|age|   sex|\n+----+---+------+\n|John| 21|  male|\n|Jane| 25|female|\n+----+---+------+"
  },
  {
    "objectID": "core/slides/slides04_sparkl.html#cast-type-change",
    "href": "core/slides/slides04_sparkl.html#cast-type-change",
    "title": "Spark SQL",
    "section": "CAST (type change)",
    "text": "CAST (type change)\n\n## In a SQL query:\nquery = \"\"\"\n  SELECT name, cast(age AS float) AS age_f \n  FROM table\n\"\"\"\n\n## Using Spark SQL API:\ndf.select(\n  df.name, \n  df.age.cast(\"float\").alias(\"age_f\")\n).show()\n\n## Or\nnew_age_col = df.age.cast(\"float\").alias(\"age_f\")\n\ndf.select(df.name, new_age_col).show()\n\n\n\n+----+-----+\n|name|age_f|\n+----+-----+\n|John| 21.0|\n|Jane| 25.0|\n+----+-----+\n\n+----+-----+\n|name|age_f|\n+----+-----+\n|John| 21.0|\n|Jane| 25.0|\n+----+-----+"
  },
  {
    "objectID": "core/slides/slides04_sparkl.html#adding-new-columns",
    "href": "core/slides/slides04_sparkl.html#adding-new-columns",
    "title": "Spark SQL",
    "section": "Adding new columns",
    "text": "Adding new columns\n\n## In a SQL query:\nquery = \"SELECT *, 12*age AS age_months FROM table\"\n\n## Using Spark SQL API:\ndf.withColumn(\"age_months\", df.age * 12).show()\n\n## Or\ndf.select(\"*\", \n          (df.age * 12).alias(\"age_months\")\n  ).show()\n\n\n\n+----+---+------+----------+\n|name|age|gender|age_months|\n+----+---+------+----------+\n|John| 21|  male|       252|\n|Jane| 25|female|       300|\n+----+---+------+----------+\n\n+----+---+------+----------+\n|name|age|gender|age_months|\n+----+---+------+----------+\n|John| 21|  male|       252|\n|Jane| 25|female|       300|\n+----+---+------+----------+"
  },
  {
    "objectID": "core/slides/slides04_sparkl.html#basic-operations",
    "href": "core/slides/slides04_sparkl.html#basic-operations",
    "title": "Spark SQL",
    "section": "Basic operations",
    "text": "Basic operations\n\nThe full list of operations that can be applied to a DataFrame can be found in the [DataFrame doc]\nThe list of operations on columns can be found in the [Column docs]"
  },
  {
    "objectID": "core/slides/slides04_sparkl.html#column-functions-1",
    "href": "core/slides/slides04_sparkl.html#column-functions-1",
    "title": "Spark SQL",
    "section": "Column functions",
    "text": "Column functions\n\nOften, we need to make many transformations using one or more functions\nSpark SQL has a package called functions with many functions available for that\nSome of those functions are only for aggregations  Examples: avg, sum, etc. We will cover them later\nSome others are for column transformation or operations  Examples:\n\nsubstr, concat, … (string and regex manipulation)\ndatediff, … (timestamp and duration)\nfloor, … (numerics)\n\nThe full list is, once again, in the [API docs]"
  },
  {
    "objectID": "core/slides/slides04_sparkl.html#column-functions-2",
    "href": "core/slides/slides04_sparkl.html#column-functions-2",
    "title": "Spark SQL",
    "section": "Column functions",
    "text": "Column functions\nTo use these functions, we first need to import them:\n\nfrom pyspark.sql import functions as fn\n\n\n\n\nNote: the “as fn” part is important to avoid confusion with native Python functions such as “sum”"
  },
  {
    "objectID": "core/slides/slides04_sparkl.html#numeric-functions-examples",
    "href": "core/slides/slides04_sparkl.html#numeric-functions-examples",
    "title": "Spark SQL",
    "section": "Numeric functions examples",
    "text": "Numeric functions examples\n\nfrom pyspark.sql import functions as fn\n\ncolumns = [\"brand\", \"cost\"]\ndf = spark.createDataFrame([\n        (\"garnier\", 3.49),\n        (\"elseve\", 2.71)\n        ], columns)\n\nround_cost = fn.round(df.cost, 1)\nfloor_cost = fn.floor(df.cost)\nceil_cost = fn.ceil(df.cost)\n\ndf.withColumn('round', round_cost)\\\n        .withColumn('floor', floor_cost)\\\n        .withColumn('ceil', ceil_cost)\\\n        .show()\n\n\n\n+-------+----+-----+-----+----+\n|  brand|cost|round|floor|ceil|\n+-------+----+-----+-----+----+\n|garnier|3.49|  3.5|    3|   4|\n| elseve|2.71|  2.7|    2|   3|\n+-------+----+-----+-----+----+"
  },
  {
    "objectID": "core/slides/slides04_sparkl.html#string-functions-examples",
    "href": "core/slides/slides04_sparkl.html#string-functions-examples",
    "title": "Spark SQL",
    "section": "String functions examples",
    "text": "String functions examples\n\nfrom pyspark.sql import functions as fn\n\ncolumns = [\"first_name\", \"last_name\"]\n\ndf = spark.createDataFrame([\n        (\"John\", \"Doe\"),\n        (\"Mary\", \"Jane\")\n  ], \n  columns      \n)\n\nlast_name_initial = fn.substring(df.last_name, 0, 1)\nname = fn.concat_ws(\" \", df.first_name, last_name_initial)\ndf.withColumn(\"name\", name).show()\n\n\n\n+----------+---------+------+\n|first_name|last_name|  name|\n+----------+---------+------+\n|      John|      Doe|John D|\n|      Mary|     Jane|Mary J|\n+----------+---------+------+"
  },
  {
    "objectID": "core/slides/slides04_sparkl.html#date-functions-examples",
    "href": "core/slides/slides04_sparkl.html#date-functions-examples",
    "title": "Spark SQL",
    "section": "Date functions examples",
    "text": "Date functions examples\n\nfrom datetime import date\nfrom pyspark.sql import functions as fn\n\ndf = spark.createDataFrame([\n        (date(2015, 1, 1), date(2015, 1, 15)),\n        (date(2015, 2, 21), date(2015, 3, 8)),\n        ], [\"start_date\", \"end_date\"]\n    )\ndays_between = fn.datediff(df.end_date, df.start_date)\nstart_month = fn.month(df.start_date)\n\ndf.withColumn('days_between', days_between)\\\n        .withColumn('start_month', start_month)\\\n        .show()\n\n\n\n+----------+----------+------------+-----------+\n|start_date|  end_date|days_between|start_month|\n+----------+----------+------------+-----------+\n|2015-01-01|2015-01-15|          14|          1|\n|2015-02-21|2015-03-08|          15|          2|\n+----------+----------+------------+-----------+"
  },
  {
    "objectID": "core/slides/slides04_sparkl.html#conditional-transformations",
    "href": "core/slides/slides04_sparkl.html#conditional-transformations",
    "title": "Spark SQL",
    "section": "Conditional transformations",
    "text": "Conditional transformations\n\nIn the functions package is a special function called when\nThis function is used to create a new column which value depends on the value of other columns\notherwise is used to match “the rest”\nCombination between conditions can be done using \"&\" for “and” and \"|\" for “or”"
  },
  {
    "objectID": "core/slides/slides04_sparkl.html#examples",
    "href": "core/slides/slides04_sparkl.html#examples",
    "title": "Spark SQL",
    "section": "Examples",
    "text": "Examples\n\ndf = spark.createDataFrame([\n        (\"John\", 21, \"male\"),\n        (\"Jane\", 25, \"female\"),\n        (\"Albert\", 46, \"male\"),\n        (\"Brad\", 49, \"super-hero\")\n    ], [\"name\", \"age\", \"gender\"])\n\nsupervisor = fn.when(df.gender == 'male', 'Mr. Smith')\\\n        .when(df.gender == 'female', 'Miss Jones')\\\n        .otherwise('NA')\n\ndf.withColumn(\"supervisor\", supervisor).show()\n\n\n\n+------+---+----------+----------+\n|  name|age|    gender|supervisor|\n+------+---+----------+----------+\n|  John| 21|      male| Mr. Smith|\n|  Jane| 25|    female|Miss Jones|\n|Albert| 46|      male| Mr. Smith|\n|  Brad| 49|super-hero|        NA|\n+------+---+----------+----------+"
  },
  {
    "objectID": "core/slides/slides04_sparkl.html#functions-in-relational-database-management-systems",
    "href": "core/slides/slides04_sparkl.html#functions-in-relational-database-management-systems",
    "title": "Spark SQL",
    "section": " Functions in Relational Database Management Systems",
    "text": "Functions in Relational Database Management Systems\nCompare functions defined in pyspark.sql.functions with functions specified in ANSI SQL and defined in popular RDBMs\nPostgreSQL Documentation\n Section on Functions and Operators\n\n\n\nIn RDBMs functions serve many purposes\n\nquerying\nsystem administration\ntriggers\n…"
  },
  {
    "objectID": "core/slides/slides04_sparkl.html#user-defined-functions",
    "href": "core/slides/slides04_sparkl.html#user-defined-functions",
    "title": "Spark SQL",
    "section": "User-defined functions",
    "text": "User-defined functions\n\nWhen you need a transformation that is not available in the functions package, you can create a User Defined Function (UDF)\nWarning: the performance of this can be very very low\nSo, it should be used only when you are sure the operation cannot be done with available functions\nTo create an UDF, use functions.udf, passing a lambda or a named functions\nIt is similar to the map operation of RDDs"
  },
  {
    "objectID": "core/slides/slides04_sparkl.html#example",
    "href": "core/slides/slides04_sparkl.html#example",
    "title": "Spark SQL",
    "section": "Example",
    "text": "Example\n\nfrom pyspark.sql import functions as fn\nfrom pyspark.sql.types import StringType\n\ndf = spark.createDataFrame([(1, 3), (4, 2)], [\"first\", \"second\"])\n\ndef my_func(col_1, col_2):\n        if (col_1 &gt; col_2):\n            return \"{} is bigger than {}\".format(col_1, col_2)\n        else:\n            return \"{} is bigger than {}\".format(col_2, col_1)\n\nmy_udf = fn.udf(my_func, StringType())\n\ndf.withColumn(\"udf\", my_udf(df['first'], df['second'])).show()\n\n\n\n+-----+------+------------------+\n|first|second|               udf|\n+-----+------+------------------+\n|    1|     3|3 is bigger than 1|\n|    4|     2|4 is bigger than 2|\n+-----+------+------------------+"
  },
  {
    "objectID": "core/slides/slides04_sparkl.html#performing-joins",
    "href": "core/slides/slides04_sparkl.html#performing-joins",
    "title": "Spark SQL",
    "section": "Performing joins",
    "text": "Performing joins\n\nSpark SQL supports joins between two DataFrame\nAs in ANSI SQL, a join rule must be defined\nThe rule can either be a set of join keys (equi-join), or a conditional rule (\\(\\theta\\)-join)\nJoin with conditional rules (\\(\\theta\\)-joins) in Spark can be very heavy\nSeveral types of joins are available, default is inner\n\nSyntax for \\(\\texttt{left_df} \\bowtie_{\\texttt{cols}} \\texttt{right_df}\\) is simple:\nleft_df.join(\n  other=right_df, \n  on=cols, \n  how=join_type\n)\n\ncols contains a column name or a list of column names\njoin_type is the type of join"
  },
  {
    "objectID": "core/slides/slides04_sparkl.html#examples-1",
    "href": "core/slides/slides04_sparkl.html#examples-1",
    "title": "Spark SQL",
    "section": "Examples",
    "text": "Examples\n\nfrom datetime import date\n\nproducts = spark.createDataFrame([\n        ('1', 'mouse', 'microsoft', 39.99),\n        ('2', 'keyboard', 'logitech', 59.99),\n    ], ['prod_id', 'prod_cat', 'prod_brand', 'prod_value'])\n\npurchases = spark.createDataFrame([\n        (date(2017, 11, 1), 2, '1'),\n        (date(2017, 11, 2), 1, '1'),\n        (date(2017, 11, 5), 1, '2'),\n    ], ['date', 'quantity', 'prod_id'])\n\n# The default join type is the \"INNER\" join\npurchases.join(products, 'prod_id').show()\n\n\n\n+-------+----------+--------+--------+----------+----------+\n|prod_id|      date|quantity|prod_cat|prod_brand|prod_value|\n+-------+----------+--------+--------+----------+----------+\n|      1|2017-11-01|       2|   mouse| microsoft|     39.99|\n|      1|2017-11-02|       1|   mouse| microsoft|     39.99|\n|      2|2017-11-05|       1|keyboard|  logitech|     59.99|\n+-------+----------+--------+--------+----------+----------+"
  },
  {
    "objectID": "core/slides/slides04_sparkl.html#examples-2",
    "href": "core/slides/slides04_sparkl.html#examples-2",
    "title": "Spark SQL",
    "section": "Examples",
    "text": "Examples\n\n# We can also use a query string (not recommended)\nproducts.createOrReplaceTempView(\"products\")\npurchases.createOrReplaceTempView(\"purchases\")\n\nquery = \"\"\"\n  SELECT * \n  FROM  purchases AS prc INNER JOIN \n        products AS prd \n    ON (prc.prod_id = prd.prod_id)\n\"\"\"\n\nspark.sql(query).show()\n\n\n\n+----------+--------+-------+-------+--------+----------+----------+\n|      date|quantity|prod_id|prod_id|prod_cat|prod_brand|prod_value|\n+----------+--------+-------+-------+--------+----------+----------+\n|2017-11-01|       2|      1|      1|   mouse| microsoft|     39.99|\n|2017-11-02|       1|      1|      1|   mouse| microsoft|     39.99|\n|2017-11-05|       1|      2|      2|keyboard|  logitech|     59.99|\n+----------+--------+-------+-------+--------+----------+----------+"
  },
  {
    "objectID": "core/slides/slides04_sparkl.html#examples-3",
    "href": "core/slides/slides04_sparkl.html#examples-3",
    "title": "Spark SQL",
    "section": "Examples",
    "text": "Examples\n\nnew_purchases = spark.createDataFrame([\n        (date(2017, 11, 1), 2, '1'),\n        (date(2017, 11, 2), 1, '3'),\n    ], ['date', 'quantity', 'prod_id_x']\n)\n\njoin_rule = new_purchases.prod_id_x == products.prod_id\n\nnew_purchases.join(products, join_rule, 'left').show()\n\n\n\n+----------+--------+---------+-------+--------+----------+----------+\n|      date|quantity|prod_id_x|prod_id|prod_cat|prod_brand|prod_value|\n+----------+--------+---------+-------+--------+----------+----------+\n|2017-11-01|       2|        1|      1|   mouse| microsoft|     39.99|\n|2017-11-02|       1|        3|   NULL|    NULL|      NULL|      NULL|\n+----------+--------+---------+-------+--------+----------+----------+"
  },
  {
    "objectID": "core/slides/slides04_sparkl.html#performing-joins-some-remarks",
    "href": "core/slides/slides04_sparkl.html#performing-joins-some-remarks",
    "title": "Spark SQL",
    "section": "Performing joins: some remarks",
    "text": "Performing joins: some remarks\n\nSpark removes the duplicated column in the DataFrame it outputs after a join operation\nWhen joining using columns with nulls, Spark just skips those\n\n&gt;&gt;&gt; df1.show()               &gt;&gt;&gt; df2.show()\n+----+-----+                 +----+-----+\n|  id| name|                 |  id| dept|\n+----+-----+                 +----+-----+\n| 123|name1|                 |null|sales|\n| 456|name3|                 | 223|Legal|\n|null|name2|                 | 456|   IT|\n+----+-----+                 +----+-----+\n\n&gt;&gt;&gt; df1.join(df2, \"id\").show\n+---+-----+-----+\n| id| name| dept|\n+---+-----+-----+\n|123|name1|sales|\n|456|name3|   IT|\n+---+-----+-----+"
  },
  {
    "objectID": "core/slides/slides04_sparkl.html#join-types",
    "href": "core/slides/slides04_sparkl.html#join-types",
    "title": "Spark SQL",
    "section": "Join types",
    "text": "Join types\n\n\n\n\n\n\n\n\nSQL Join Type\nIn Spark (synonyms)\nDescription\n\n\n\n\nINNER\n\"inner\"\nData from left and right matching both ways (intersection)\n\n\nFULL OUTER\n\"outer\", \"full\", \"fullouter\"\nAll rows from left and right with extra data if present (union)\n\n\nLEFT OUTER\n\"leftouter\", \"left\"\nRows from left with extra data from right if present\n\n\nRIGHT OUTER\n\"rightouter\", \"right\"\nRows from right with extra data from left if present\n\n\nLEFT SEMI\n\"leftsemi\"\nData from left with a match with right\n\n\nLEFT ANTI\n\"leftanti\"\nData from left with no match with right\n\n\nCROSS\n\"cross\"\nCartesian product of left and right (never used)"
  },
  {
    "objectID": "core/slides/slides04_sparkl.html#join-types-1",
    "href": "core/slides/slides04_sparkl.html#join-types-1",
    "title": "Spark SQL",
    "section": "Join types",
    "text": "Join types"
  },
  {
    "objectID": "core/slides/slides04_sparkl.html#inner-join-inner",
    "href": "core/slides/slides04_sparkl.html#inner-join-inner",
    "title": "Spark SQL",
    "section": "Inner join (“inner”)",
    "text": "Inner join (“inner”)\n&gt;&gt;&gt; inner = df_left.join(df_right, \"id\", \"inner\")\n\ndf_left                df_right             \n+---+-----+            +---+-----+\n| id|value|            | id|value|\n+---+-----+            +---+-----+\n|  1|   A1|            |  3|   A3|\n|  2|   A2|            |  4| A4_1|\n|  3|   A3|            |  4|   A4|\n|  4|   A4|            |  5|   A5|\n+---+-----+            |  6|   A6|\n                       +---+-----+\ninner\n+---+-----+-----+\n| id|value|value|\n+---+-----+-----+\n|  3|   A3|   A3|\n|  4|   A4|   A4|\n|  4|   A4| A4_1|\n+---+-----+-----+"
  },
  {
    "objectID": "core/slides/slides04_sparkl.html#outer-join-outer-full-or-fullouter",
    "href": "core/slides/slides04_sparkl.html#outer-join-outer-full-or-fullouter",
    "title": "Spark SQL",
    "section": "Outer join (“outer”, “full” or “fullouter”)",
    "text": "Outer join (“outer”, “full” or “fullouter”)\n&gt;&gt;&gt; outer = df_left.join(df_right, \"id\", \"outer\")\ndf_left                df_right             \n+---+-----+            +---+-----+\n| id|value|            | id|value|\n+---+-----+            +---+-----+\n|  1|   A1|            |  3|   A3|\n|  2|   A2|            |  4| A4_1|\n|  3|   A3|            |  4|   A4|\n|  4|   A4|            |  5|   A5|\n+---+-----+            |  6|   A6|\n                       +---+-----+\nouter\n+---+-----+-----+\n| id|value|value|\n+---+-----+-----+\n|  1|   A1| null|\n|  2|   A2| null|\n|  3|   A3|   A3|\n|  4|   A4|   A4|\n|  4|   A4| A4_1|\n|  5| null|   A5|\n|  6| null|   A6|\n+---+-----+-----+"
  },
  {
    "objectID": "core/slides/slides04_sparkl.html#left-join-leftouter-or-left",
    "href": "core/slides/slides04_sparkl.html#left-join-leftouter-or-left",
    "title": "Spark SQL",
    "section": "Left join (“leftouter” or “left” )",
    "text": "Left join (“leftouter” or “left” )\n&gt;&gt;&gt; left = df_left.join(df_right, \"id\", \"left\")\n\ndf_left                df_right             \n+---+-----+            +---+-----+\n| id|value|            | id|value|\n+---+-----+            +---+-----+\n|  1|   A1|            |  3|   A3|\n|  2|   A2|            |  4| A4_1|\n|  3|   A3|            |  4|   A4|\n|  4|   A4|            |  5|   A5|\n+---+-----+            |  6|   A6|\n                       +---+-----+\nleft\n+---+-----+-----+\n| id|value|value|\n+---+-----+-----+\n|  1|   A1| null|\n|  2|   A2| null|\n|  3|   A3|   A3|\n|  4|   A4|   A4|\n|  4|   A4| A4_1|\n+---+-----+-----+"
  },
  {
    "objectID": "core/slides/slides04_sparkl.html#right-rightouter-or-right",
    "href": "core/slides/slides04_sparkl.html#right-rightouter-or-right",
    "title": "Spark SQL",
    "section": "Right (“rightouter” or “right”)",
    "text": "Right (“rightouter” or “right”)\n&gt;&gt;&gt; right = df_left.join(df_right, \"id\", \"right\")\n\ndf_left                df_right             \n+---+-----+            +---+-----+\n| id|value|            | id|value|\n+---+-----+            +---+-----+\n|  1|   A1|            |  3|   A3|\n|  2|   A2|            |  4| A4_1|\n|  3|   A3|            |  4|   A4|\n|  4|   A4|            |  5|   A5|\n+---+-----+            |  6|   A6|\n                       +---+-----+\nright\n+---+-----+-----+\n| id|value|value|\n+---+-----+-----+\n|  3|   A3|   A3|\n|  4|   A4|   A4|\n|  4|   A4| A4_1|\n|  5| null|   A5|\n|  6| null|   A6|\n+---+-----+-----+"
  },
  {
    "objectID": "core/slides/slides04_sparkl.html#left-semi-join-leftsemi",
    "href": "core/slides/slides04_sparkl.html#left-semi-join-leftsemi",
    "title": "Spark SQL",
    "section": "Left semi join (“leftsemi”)",
    "text": "Left semi join (“leftsemi”)\n&gt;&gt;&gt; left_semi = df_left.join(df_right, \"id\", \"leftsemi\")\n\ndf_left                df_right             \n+---+-----+            +---+-----+\n| id|value|            | id|value|\n+---+-----+            +---+-----+\n|  1|   A1|            |  3|   A3|\n|  2|   A2|            |  4| A4_1|\n|  3|   A3|            |  4|   A4|\n|  4|   A4|            |  5|   A5|\n+---+-----+            |  6|   A6|\n                       +---+-----+\nleft_semi\n+---+-----+\n| id|value|\n+---+-----+\n|  3|   A3|\n|  4|   A4|\n+---+-----+"
  },
  {
    "objectID": "core/slides/slides04_sparkl.html#left-anti-joint-leftanti",
    "href": "core/slides/slides04_sparkl.html#left-anti-joint-leftanti",
    "title": "Spark SQL",
    "section": "Left anti joint (“leftanti”)",
    "text": "Left anti joint (“leftanti”)\n&gt;&gt;&gt; left_anti = df_left.join(df_right, \"id\", \"leftanti\")\n\ndf_left                df_right             \n+---+-----+            +---+-----+\n| id|value|            | id|value|\n+---+-----+            +---+-----+\n|  1|   A1|            |  3|   A3|\n|  2|   A2|            |  4| A4_1|\n|  3|   A3|            |  4|   A4|\n|  4|   A4|            |  5|   A5|\n+---+-----+            |  6|   A6|\n                       +---+-----+\nleft_anti\n+---+-----+\n| id|value|\n+---+-----+\n|  1|   A1|\n|  2|   A2|\n+---+-----+"
  },
  {
    "objectID": "core/slides/slides04_sparkl.html#performing-joins-1",
    "href": "core/slides/slides04_sparkl.html#performing-joins-1",
    "title": "Spark SQL",
    "section": "Performing joins",
    "text": "Performing joins\n\nNode-to-node communication strategy\nPer node computation strategy\n\n\nSection *“How Spark Performs Joins”"
  },
  {
    "objectID": "core/slides/slides04_sparkl.html#performing-aggregations",
    "href": "core/slides/slides04_sparkl.html#performing-aggregations",
    "title": "Spark SQL",
    "section": "Performing aggregations",
    "text": "Performing aggregations\n\nMaybe the most used operations in SQL and Spark SQL\nSimilar to SQL, we use \"group by\" to perform aggregations\nWe usually can call the aggregation function just after groupBy  Namely, we use groupBy().agg()\nMany aggregation functions in pyspark.sql.functions\nSome examples:\n\nNumerical: fn.avg, fn.sum, fn.min, fn.max, etc.\nGeneral: fn.first, fn.last, fn.count, fn.countDistinct, etc."
  },
  {
    "objectID": "core/slides/slides04_sparkl.html#examples-4",
    "href": "core/slides/slides04_sparkl.html#examples-4",
    "title": "Spark SQL",
    "section": "Examples",
    "text": "Examples\n\nfrom pyspark.sql import functions as fn\n\nproducts = spark.createDataFrame([\n        ('1', 'mouse', 'microsoft', 39.99),\n        ('2', 'mouse', 'microsoft', 59.99),\n        ('3', 'keyboard', 'microsoft', 59.99),\n        ('4', 'keyboard', 'logitech', 59.99),\n        ('5', 'mouse', 'logitech', 29.99),\n    ], ['prod_id', 'prod_cat', 'prod_brand', 'prod_value'])\n\nproducts.groupBy('prod_cat').avg('prod_value').show()\n\n# Or\nproducts.groupBy('prod_cat').agg(fn.avg('prod_value')).show()\n\n\n\n+--------+-----------------+\n|prod_cat|  avg(prod_value)|\n+--------+-----------------+\n|   mouse|43.32333333333333|\n|keyboard|            59.99|\n+--------+-----------------+\n\n+--------+-----------------+\n|prod_cat|  avg(prod_value)|\n+--------+-----------------+\n|   mouse|43.32333333333333|\n|keyboard|            59.99|\n+--------+-----------------+"
  },
  {
    "objectID": "core/slides/slides04_sparkl.html#examples-5",
    "href": "core/slides/slides04_sparkl.html#examples-5",
    "title": "Spark SQL",
    "section": "Examples",
    "text": "Examples\n\nfrom pyspark.sql import functions as fn\n\nproducts.groupBy('prod_brand', 'prod_cat')\\\n        .agg(fn.avg('prod_value')).show()\n\n\n\n+----------+--------+---------------+\n|prod_brand|prod_cat|avg(prod_value)|\n+----------+--------+---------------+\n| microsoft|   mouse|          49.99|\n| microsoft|keyboard|          59.99|\n|  logitech|keyboard|          59.99|\n|  logitech|   mouse|          29.99|\n+----------+--------+---------------+"
  },
  {
    "objectID": "core/slides/slides04_sparkl.html#examples-6",
    "href": "core/slides/slides04_sparkl.html#examples-6",
    "title": "Spark SQL",
    "section": "Examples",
    "text": "Examples\n\nfrom pyspark.sql import functions as fn\n\nproducts.groupBy('prod_brand').agg(\n    fn.round(fn.avg('prod_value'), 1).alias('average'),\n    fn.ceil(fn.sum('prod_value')).alias('sum'),\n    fn.min('prod_value').alias('min')\n).show()\n\n\n\n+----------+-------+---+-----+\n|prod_brand|average|sum|  min|\n+----------+-------+---+-----+\n| microsoft|   53.3|160|39.99|\n|  logitech|   45.0| 90|29.99|\n+----------+-------+---+-----+"
  },
  {
    "objectID": "core/slides/slides04_sparkl.html#examples-7",
    "href": "core/slides/slides04_sparkl.html#examples-7",
    "title": "Spark SQL",
    "section": "Examples",
    "text": "Examples\n\n# Using an SQL query\nproducts.createOrReplaceTempView(\"products\")\n\nquery = \"\"\"\n  SELECT\n    prod_brand,\n    round(avg(prod_value), 1) AS average,\n    min(prod_value) AS min\n  FROM products\n  GROUP BY prod_brand\n\"\"\"\n\nspark.sql(query).show()\n\n\n\n+----------+-------+-----+\n|prod_brand|average|  min|\n+----------+-------+-----+\n| microsoft|   53.3|39.99|\n|  logitech|   45.0|29.99|\n+----------+-------+-----+"
  },
  {
    "objectID": "core/slides/slides04_sparkl.html#window-analytic-functions",
    "href": "core/slides/slides04_sparkl.html#window-analytic-functions",
    "title": "Spark SQL",
    "section": "Window (analytic) functions",
    "text": "Window (analytic) functions\n\nA very, very powerful feature\nThey allow to solve complex problems\nANSI SQL2003 allows for a window_clause in aggregate function calls, the addition of which makes those functions into window functions\nA good article about this feature is [here]\n\nSee also :\nhttps://www.postgresql.org/docs/current/tutorial-window.html\n\n\nA window function performs a calculation across a set of table rows that are somehow related to the current row. This is comparable to the type of calculation that can be done with an aggregate function. However, window functions do not cause rows to become grouped into a single output row like non-window aggregate calls would. Instead, the rows retain their separate identities. Behind the scenes, the window function is able to access more than just the current row of the query result."
  },
  {
    "objectID": "core/slides/slides04_sparkl.html#window-functions-1",
    "href": "core/slides/slides04_sparkl.html#window-functions-1",
    "title": "Spark SQL",
    "section": "Window functions",
    "text": "Window functions\n\nIt’s similar to aggregations, but the number of rows doesn’t change\nInstead, new columns are created, and the aggregated values are duplicated for values of the same “group”\nThere are\n\n“traditional” aggregations, such as min, max, avg, sum and\n“special” types, such as lag, lead, rank"
  },
  {
    "objectID": "core/slides/slides04_sparkl.html#numerical-window-functions",
    "href": "core/slides/slides04_sparkl.html#numerical-window-functions",
    "title": "Spark SQL",
    "section": "Numerical window functions",
    "text": "Numerical window functions\n\nfrom pyspark.sql import Window\nfrom pyspark.sql import functions as fn\n\n# First, we create the Window definition\nwindow = Window.partitionBy('prod_brand')\n\n# Then, we can use \"over\" to aggregate on this window\navg = fn.avg('prod_value').over(window)\n\n# Finally, we can it as a classical column\nproducts.withColumn('avg_brand_value', fn.round(avg, 2)).show()\n\n\n\n+-------+--------+----------+----------+---------------+\n|prod_id|prod_cat|prod_brand|prod_value|avg_brand_value|\n+-------+--------+----------+----------+---------------+\n|      4|keyboard|  logitech|     59.99|          44.99|\n|      5|   mouse|  logitech|     29.99|          44.99|\n|      1|   mouse| microsoft|     39.99|          53.32|\n|      2|   mouse| microsoft|     59.99|          53.32|\n|      3|keyboard| microsoft|     59.99|          53.32|\n+-------+--------+----------+----------+---------------+"
  },
  {
    "objectID": "core/slides/slides04_sparkl.html#numerical-window-functions-1",
    "href": "core/slides/slides04_sparkl.html#numerical-window-functions-1",
    "title": "Spark SQL",
    "section": "Numerical window functions",
    "text": "Numerical window functions\n\nfrom pyspark.sql import Window\nfrom pyspark.sql import functions as fn\n\n# The window can be defined on multiple columns\nwindow = Window.partitionBy('prod_brand', 'prod_cat')\n\navg = fn.avg('prod_value').over(window)\n\nproducts.withColumn('avg_value', fn.round(avg, 2)).show()\n\n\n\n+-------+--------+----------+----------+---------+\n|prod_id|prod_cat|prod_brand|prod_value|avg_value|\n+-------+--------+----------+----------+---------+\n|      4|keyboard|  logitech|     59.99|    59.99|\n|      5|   mouse|  logitech|     29.99|    29.99|\n|      3|keyboard| microsoft|     59.99|    59.99|\n|      1|   mouse| microsoft|     39.99|    49.99|\n|      2|   mouse| microsoft|     59.99|    49.99|\n+-------+--------+----------+----------+---------+"
  },
  {
    "objectID": "core/slides/slides04_sparkl.html#numerical-window-functions-2",
    "href": "core/slides/slides04_sparkl.html#numerical-window-functions-2",
    "title": "Spark SQL",
    "section": "Numerical window functions",
    "text": "Numerical window functions\n\nfrom pyspark.sql import Window\nfrom pyspark.sql import functions as fn\n\n# Multiple windows can be defined\nwindow1 = Window.partitionBy('prod_brand')\nwindow2 = Window.partitionBy('prod_cat')\n\navg_brand = fn.avg('prod_value').over(window1)\navg_cat = fn.avg('prod_value').over(window2)\n\nproducts \\\n  .withColumn('avg_by_brand', fn.round(avg_brand, 2)) \\\n  .withColumn('avg_by_cat', fn.round(avg_cat, 2)) \\\n  .show()\n\n\n\n+-------+--------+----------+----------+------------+----------+\n|prod_id|prod_cat|prod_brand|prod_value|avg_by_brand|avg_by_cat|\n+-------+--------+----------+----------+------------+----------+\n|      4|keyboard|  logitech|     59.99|       44.99|     59.99|\n|      3|keyboard| microsoft|     59.99|       53.32|     59.99|\n|      5|   mouse|  logitech|     29.99|       44.99|     43.32|\n|      1|   mouse| microsoft|     39.99|       53.32|     43.32|\n|      2|   mouse| microsoft|     59.99|       53.32|     43.32|\n+-------+--------+----------+----------+------------+----------+"
  },
  {
    "objectID": "core/slides/slides04_sparkl.html#lag-and-lead",
    "href": "core/slides/slides04_sparkl.html#lag-and-lead",
    "title": "Spark SQL",
    "section": "Lag and Lead",
    "text": "Lag and Lead\n\nlag and lead are special functions used over an ordered window\nThey are used to take the “previous” and “next” value within the window\nVery useful in datasets with a date column for instance"
  },
  {
    "objectID": "core/slides/slides04_sparkl.html#lag-and-lead-1",
    "href": "core/slides/slides04_sparkl.html#lag-and-lead-1",
    "title": "Spark SQL",
    "section": "Lag and Lead",
    "text": "Lag and Lead\n\npurchases = spark.createDataFrame([\n        (date(2017, 11, 1), 'mouse'),\n        (date(2017, 11, 2), 'mouse'),\n        (date(2017, 11, 4), 'keyboard'),\n        (date(2017, 11, 6), 'keyboard'),\n        (date(2017, 11, 9), 'keyboard'),\n        (date(2017, 11, 12), 'mouse'),\n        (date(2017, 11, 18), 'keyboard')\n    ], ['date', 'prod_cat'])\npurchases.show()\n\n\n\n+----------+--------+\n|      date|prod_cat|\n+----------+--------+\n|2017-11-01|   mouse|\n|2017-11-02|   mouse|\n|2017-11-04|keyboard|\n|2017-11-06|keyboard|\n|2017-11-09|keyboard|\n|2017-11-12|   mouse|\n|2017-11-18|keyboard|\n+----------+--------+"
  },
  {
    "objectID": "core/slides/slides04_sparkl.html#lag-and-lead-2",
    "href": "core/slides/slides04_sparkl.html#lag-and-lead-2",
    "title": "Spark SQL",
    "section": "Lag and Lead",
    "text": "Lag and Lead\n\nwindow = Window.partitionBy('prod_cat').orderBy('date')\n\nprev_purch = fn.lag('date', 1).over(window)\nnext_purch = fn.lead('date', 1).over(window)\n\npurchases\\\n  .withColumn('prev', prev_purch)\\\n  .withColumn('next', next_purch)\\\n  .orderBy('prod_cat', 'date')\\\n  .show()\n\n\n\n+----------+--------+----------+----------+\n|      date|prod_cat|      prev|      next|\n+----------+--------+----------+----------+\n|2017-11-04|keyboard|      NULL|2017-11-06|\n|2017-11-06|keyboard|2017-11-04|2017-11-09|\n|2017-11-09|keyboard|2017-11-06|2017-11-18|\n|2017-11-18|keyboard|2017-11-09|      NULL|\n|2017-11-01|   mouse|      NULL|2017-11-02|\n|2017-11-02|   mouse|2017-11-01|2017-11-12|\n|2017-11-12|   mouse|2017-11-02|      NULL|\n+----------+--------+----------+----------+"
  },
  {
    "objectID": "core/slides/slides04_sparkl.html#rank-denserank-and-rownumber",
    "href": "core/slides/slides04_sparkl.html#rank-denserank-and-rownumber",
    "title": "Spark SQL",
    "section": "Rank, DenseRank and RowNumber",
    "text": "Rank, DenseRank and RowNumber\n\nAnother set of useful “special” functions\nAlso used on ordered windows\nThey create a rank, or an order of the items within the window"
  },
  {
    "objectID": "core/slides/slides04_sparkl.html#rank-and-rownumber",
    "href": "core/slides/slides04_sparkl.html#rank-and-rownumber",
    "title": "Spark SQL",
    "section": "Rank and RowNumber",
    "text": "Rank and RowNumber\n\ncontestants = spark.createDataFrame([\n    ('veterans', 'John', 3000),\n    ('veterans', 'Bob', 3200),\n    ('veterans', 'Mary', 4000),\n    ('young', 'Jane', 4000),\n    ('young', 'April', 3100),\n    ('young', 'Alice', 3700),\n    ('young', 'Micheal', 4000)], \n  ['category', 'name', 'points']\n)\n\ncontestants.show()\n\n\n\n+--------+-------+------+\n|category|   name|points|\n+--------+-------+------+\n|veterans|   John|  3000|\n|veterans|    Bob|  3200|\n|veterans|   Mary|  4000|\n|   young|   Jane|  4000|\n|   young|  April|  3100|\n|   young|  Alice|  3700|\n|   young|Micheal|  4000|\n+--------+-------+------+"
  },
  {
    "objectID": "core/slides/slides04_sparkl.html#rank-and-rownumber-1",
    "href": "core/slides/slides04_sparkl.html#rank-and-rownumber-1",
    "title": "Spark SQL",
    "section": "Rank and RowNumber",
    "text": "Rank and RowNumber\n\nwindow = Window.partitionBy('category')\\\n        .orderBy(contestants.points.desc())\n\nrank = fn.rank().over(window)\ndense_rank = fn.dense_rank().over(window)\nrow_number = fn.row_number().over(window)\n\ncontestants\\\n        .withColumn('rank', rank)\\\n        .withColumn('dense_rank', dense_rank)\\\n        .withColumn('row_number', row_number)\\\n        .orderBy('category', fn.col('points').desc())\\\n        .show()\n\n\n\n+--------+-------+------+----+----------+----------+\n|category|   name|points|rank|dense_rank|row_number|\n+--------+-------+------+----+----------+----------+\n|veterans|   Mary|  4000|   1|         1|         1|\n|veterans|    Bob|  3200|   2|         2|         2|\n|veterans|   John|  3000|   3|         3|         3|\n|   young|   Jane|  4000|   1|         1|         1|\n|   young|Micheal|  4000|   1|         1|         2|\n|   young|  Alice|  3700|   3|         2|         3|\n|   young|  April|  3100|   4|         3|         4|\n+--------+-------+------+----+----------+----------+"
  },
  {
    "objectID": "core/slides/slides04_sparkl.html#writing-dataframes-1",
    "href": "core/slides/slides04_sparkl.html#writing-dataframes-1",
    "title": "Spark SQL",
    "section": "Writing dataframes",
    "text": "Writing dataframes\n\nVery similar to reading. Output formats are the same: csv, json, parquet, orc, jdbc, etc. Note that write is an action\nInstead of df.read.{source} use df.write.{target}\nMain option is mode with possible values:\n\n\"append\": append contents of this DataFrame to existing data.\n\"overwrite\": overwrite existing data\n\"error\": throw an exception if data already exists\n\"ignore\": silently ignore this operation if data already exists.\n\n\nExample\n\nproducts.write.csv('./products.csv')\nproducts.write.mode('overwrite').parquet('./produits.parquet')\nproducts.write.format('parquet').save('./produits_2.parquet')"
  },
  {
    "objectID": "core/slides/slides04_sparkl.html#query-planning-and-optimization",
    "href": "core/slides/slides04_sparkl.html#query-planning-and-optimization",
    "title": "Spark SQL",
    "section": "Query planning and optimization",
    "text": "Query planning and optimization\nA lot happens under the hood when executing an action on a DataFrame. The query goes through the following exectution stages:\n\nLogical Analysis\nCaching Replacement\nLogical Query Optimization (using rule-based and cost-based optimizations)\nPhysical Planning\nPhysical Optimization (e.g. Whole-Stage Java Code Generation or Adaptive Query Execution)\nConstructing the RDD of Internal Binary Rows (that represents the structured query in terms of Spark Core’s RDD API)\n\n\n\nhttps://jaceklaskowski.gitbooks.io/mastering-spark-sql/spark-sql.html]"
  },
  {
    "objectID": "core/slides/slides04_sparkl.html#query-planning-and-optimization-1",
    "href": "core/slides/slides04_sparkl.html#query-planning-and-optimization-1",
    "title": "Spark SQL",
    "section": "Query planning and optimization",
    "text": "Query planning and optimization\n\n\n\n\n\nhttps://jaceklaskowski.gitbooks.io/mastering-spark-sql/spark-sql.html]"
  },
  {
    "objectID": "core/slides/slides04_sparkl.html#references",
    "href": "core/slides/slides04_sparkl.html#references",
    "title": "Spark SQL",
    "section": "References",
    "text": "References\nPySpark Quickstart\nPandas on Spark User’s Guide"
  },
  {
    "objectID": "core/slides/slides09_dask.html#bird-eye-big-picture",
    "href": "core/slides/slides09_dask.html#bird-eye-big-picture",
    "title": "Dask",
    "section": "Bird-eye Big Picture",
    "text": "Bird-eye Big Picture\n\nDask in picture"
  },
  {
    "objectID": "core/slides/slides09_dask.html#section",
    "href": "core/slides/slides09_dask.html#section",
    "title": "Dask",
    "section": "",
    "text": "Overview - dask’s place in the universe.\nDelayed - the single-function way to parallelize general python code.\nDataframe - parallelized operations on many pandas dataframes spread across your cluster"
  },
  {
    "objectID": "core/slides/slides09_dask.html#flavours-of-big-data",
    "href": "core/slides/slides09_dask.html#flavours-of-big-data",
    "title": "Dask",
    "section": "Flavours of (big) data",
    "text": "Flavours of (big) data\n\n\n\n\n\n\n\n\n\nType\nTypical size\nFeatures\nTool\n\n\n\n\nSmall data\nFew GigaBytes\nFits in RAM\nPandas\n\n\nMedium data\nLess than 2 Terabytes\nDoes not fit in RAM, fits on hard drive\nDask\n\n\nLarge data\nPetabytes\nDoes not fit on hard drive\nSpark"
  },
  {
    "objectID": "core/slides/slides09_dask.html#sources",
    "href": "core/slides/slides09_dask.html#sources",
    "title": "Dask",
    "section": "Sources",
    "text": "Sources\nDask Tutorial\nDask FAQ"
  },
  {
    "objectID": "core/slides/slides09_dask.html#trends",
    "href": "core/slides/slides09_dask.html#trends",
    "title": "Dask",
    "section": "Trends",
    "text": "Trends\n\nDask adoption metrics"
  },
  {
    "objectID": "core/slides/slides09_dask.html#delayed-in-a-nutshell",
    "href": "core/slides/slides09_dask.html#delayed-in-a-nutshell",
    "title": "Dask",
    "section": "Delayed (in a nutshell)",
    "text": "Delayed (in a nutshell)\n\nThe single-function way to parallelize general python code"
  },
  {
    "objectID": "core/slides/slides09_dask.html#imports",
    "href": "core/slides/slides09_dask.html#imports",
    "title": "Dask",
    "section": "Imports",
    "text": "Imports\n\nimport dask\n\ndask.config.set(scheduler='threads')\ndask.config.set({'dataframe.query-planning': True})\n\n\n\n&lt;dask.config.set at 0x7756d013e900&gt;\n\n\n\nimport dask.dataframe as dd\nimport dask.bag as db\n\n\n\n\n\nfrom dask import delayed\nimport dask.threaded\n\nfrom dask.distributed import Client\nfrom dask.diagnostics import ProgressBar\nfrom dask.diagnostics import Profiler, ResourceProfiler, CacheProfiler"
  },
  {
    "objectID": "core/slides/slides09_dask.html#localcluster",
    "href": "core/slides/slides09_dask.html#localcluster",
    "title": "Dask",
    "section": "LocalCluster",
    "text": "LocalCluster\nDask can set itself up easily in your Python session if you create a LocalCluster object, which sets everything up for you.\n\n# from dask.distributed import LocalCluster\n\n# cluster = LocalCluster()\n# client = cluster.get_client()"
  },
  {
    "objectID": "core/slides/slides09_dask.html#normal-dask-work",
    "href": "core/slides/slides09_dask.html#normal-dask-work",
    "title": "Dask",
    "section": "Normal Dask work …",
    "text": "Normal Dask work …\nAlternatively, you can skip this part, and Dask will operate within a thread pool contained entirely with your local process."
  },
  {
    "objectID": "core/slides/slides09_dask.html#delaying-pyhton-tasks",
    "href": "core/slides/slides09_dask.html#delaying-pyhton-tasks",
    "title": "Dask",
    "section": "Delaying Pyhton tasks",
    "text": "Delaying Pyhton tasks"
  },
  {
    "objectID": "core/slides/slides09_dask.html#a-job-i",
    "href": "core/slides/slides09_dask.html#a-job-i",
    "title": "Dask",
    "section": "A job (I)",
    "text": "A job (I)\n\ndef inc(x):\n  return x + 1\n\ndef double(x):\n  return x * 2\n\ndef add(x, y):\n  return x + y"
  },
  {
    "objectID": "core/slides/slides09_dask.html#a-job-ii-piecing-elements-together",
    "href": "core/slides/slides09_dask.html#a-job-ii-piecing-elements-together",
    "title": "Dask",
    "section": "A job (II): piecing elements together",
    "text": "A job (II): piecing elements together\n\ndata = [1, 2, 3, 4, 5]\n\noutput = []\n\nfor x in data:\n1  a = inc(x)\n2  b = double(x)\n3  c = add(a, b)\n  output.append(c)\n  \ntotal = sum(output)\n  \ntotal \n\n\n1\n\nIncrement x\n\n2\n\nMultiply x by 2\n\n3\n\nc == (x+1) + 2*x == 3*x+1\n\n\n\n\n\n\n50"
  },
  {
    "objectID": "core/slides/slides09_dask.html#delaying-existing-functions",
    "href": "core/slides/slides09_dask.html#delaying-existing-functions",
    "title": "Dask",
    "section": "Delaying existing functions",
    "text": "Delaying existing functions\n\noutput = []\n\nfor x in data:\n1  a = dask.delayed(inc)(x)\n  b = dask.delayed(double)(x) \n  c = dask.delayed(add)(a, b) \n  output.append(c)\n  \n2total = dask.delayed(sum)(output)\n  \ntotal\n\n\n1\n\nDecorating inc using dask.delayed()\n\n2\n\nDecorating sum()\n\n\n\n\n\n\nDelayed('sum-b042864a-ca75-4d5d-9921-92cf16c401d2')\n\n\n\n1total.compute()\n\n\n1\n\nCollecting the results\n\n\n\n\n\n\n50"
  },
  {
    "objectID": "core/slides/slides09_dask.html#another-way-of-using-decorators",
    "href": "core/slides/slides09_dask.html#another-way-of-using-decorators",
    "title": "Dask",
    "section": "Another way of using decorators",
    "text": "Another way of using decorators\n\n1@dask.delayed\ndef inc(x):\n  return x + 1\n\n@dask.delayed\ndef double(x):\n  return x * 2\n\n@dask.delayed\ndef add(x, y):\n  return x + y\n\ndata = [1, 2, 3, 4, 5]\n\n2output = []\nfor x in data:\n  a = inc(x)\n  b = double(x)\n  c = add(a, b)\n  output.append(c)\n  \ntotal = dask.delayed(sum)(output)\ntotal\n3total.compute()\n\n\n1\n\nDecorating the definition\n\n2\n\nReusing the Python code\n\n3\n\nCollecting results\n\n\n\n\n\n\n50"
  },
  {
    "objectID": "core/slides/slides09_dask.html#visualizing-the-task-graph",
    "href": "core/slides/slides09_dask.html#visualizing-the-task-graph",
    "title": "Dask",
    "section": "Visualizing the task graph",
    "text": "Visualizing the task graph\n\n\ntotal.visualize()"
  },
  {
    "objectID": "core/slides/slides09_dask.html#another-job",
    "href": "core/slides/slides09_dask.html#another-job",
    "title": "Dask",
    "section": "Another job",
    "text": "Another job\n\nDATA = []\n\n@dask.delayed\ndef inc(x):\n  return x + 1\n\n@dask.delayed\ndef add_data(x):\n  DATA.append(x)\n\n@dask.delayed\ndef sum_data(x):\n  return sum(DATA) + x\n\na = inc(1)\nb = add_data(a)\nc = inc(3)\nd = add_data(c)\ne = inc(5)\nf = sum_data(e)\nf.compute()\n\n\n\n6"
  },
  {
    "objectID": "core/slides/slides09_dask.html#a-flawed-task-graph",
    "href": "core/slides/slides09_dask.html#a-flawed-task-graph",
    "title": "Dask",
    "section": "A flawed task graph",
    "text": "A flawed task graph\n\n\nf.visualize()"
  },
  {
    "objectID": "core/slides/slides09_dask.html#fixing",
    "href": "core/slides/slides09_dask.html#fixing",
    "title": "Dask",
    "section": "Fixing",
    "text": "Fixing\n\n\n\nfrom dask.graph_manipulation import bind\n\ng = bind(sum_data, [b, d])(e)\n\ng.compute()\n\n\n\n12\n\n\n\nThe result of the evaluation of sum_data() depends not only on its argument, hence on the Delayed e, but also on the side effects of add_data(), that is on the Delayed b and d\nNote that not only the DAG was wrong but the result obtained above was not the intended result.\n\n\n\n\ng.visualize()"
  },
  {
    "objectID": "core/slides/slides09_dask.html#section-1",
    "href": "core/slides/slides09_dask.html#section-1",
    "title": "Dask",
    "section": "",
    "text": "By default, Dask Delayed uses the threaded scheduler in order to avoid data transfer costs\n\n\nConsider using multi-processing scheduler or dask.distributed scheduler on a local machine or on a cluster if your code does not release the GIL well (computations that are dominated by pure Python code, or computations wrapping external code and holding onto it)."
  },
  {
    "objectID": "core/slides/slides09_dask.html#importing-the-usual-suspects",
    "href": "core/slides/slides09_dask.html#importing-the-usual-suspects",
    "title": "Dask",
    "section": "Importing the usual suspects",
    "text": "Importing the usual suspects\n\nimport numpy as np\n1import pandas as pd\n\n2import dask.dataframe as dd\nimport dask.array as da\nimport dask.bag as db\n\n\n1\n\nStandard dataframes in Python\n\n2\n\nParallelized and distributed dataframes in Python"
  },
  {
    "objectID": "core/slides/slides09_dask.html#bird-eye-view",
    "href": "core/slides/slides09_dask.html#bird-eye-view",
    "title": "Dask",
    "section": "Bird-eye view",
    "text": "Bird-eye view"
  },
  {
    "objectID": "core/slides/slides09_dask.html#section-2",
    "href": "core/slides/slides09_dask.html#section-2",
    "title": "Dask",
    "section": "",
    "text": "Dask Dataframes parallelize the popular pandas library, providing:\n\n\n\n\nLarger-than-memory execution for single machines, allowing you to process data that is larger than your available RAM\n\n\n\n\n\n\nParallel execution for faster processing\n\n\n\n\n\n\nDistributed computation for terabyte-sized datasets\n\n\n\n\n\nDask Dataframes are similar to Apache Spark, but use the familiar pandas API and memory model\n\n\nOne Dask dataframe is simply a coordinated collection of pandas dataframes on different computers"
  },
  {
    "objectID": "core/slides/slides09_dask.html#section-3",
    "href": "core/slides/slides09_dask.html#section-3",
    "title": "Dask",
    "section": "",
    "text": "Dask DataFrame helps you process large tabular data by parallelizing Pandas, either on your laptop for larger-than-memory computing, or on a distributed cluster of computers.\n\n\n\n\nColumn of four squares collectively labeled as a Dask DataFrame with a single constituent square labeled as a pandas DataFrame\n\n\n\nJust pandas: Dask DataFrames are a collection of many pandas DataFrames.\n\n\nThe API is the same1. The execution is the same \n\n\nLarge scale: Works on 100 GiB on a laptop, or 100 TiB on a cluster.\n\n\nEasy to use: Pure Python, easy to set up and debug.\n\n\nDask DataFrames coordinate many pandas DataFrames/Series arranged along the index. A Dask DataFrame is partitioned row-wise, grouping rows by index value for efficiency. These pandas objects may live on disk or on other machines.\n\nThe Dask Dataframe API is a subset of the Pandas API"
  },
  {
    "objectID": "core/slides/slides09_dask.html#creating-a-dask-dataframe",
    "href": "core/slides/slides09_dask.html#creating-a-dask-dataframe",
    "title": "Dask",
    "section": "Creating a dask dataframe",
    "text": "Creating a dask dataframe\n\n\nindex = pd.date_range(\"2021-09-01\", \n                      periods=2400, \n                      freq=\"1H\")\n\ndf = pd.DataFrame({\n  \"a\": np.arange(2400), \n  \"b\": list(\"abcaddbe\" * 300)}, \n  index=index)\n  \n1ddf = dd.from_pandas(df, npartitions=20)\n\n2ddf.head()\n\n\n\n1\n\nIn Dask, proper partitioning is a key performance issue\n\n2\n\nThe dataframe API is (almost) the same as in Pandas!\n\n\n\n\n\n\n\n\n\n\n\na\nb\n\n\n\n\n2021-09-01 00:00:00\n0\na\n\n\n2021-09-01 01:00:00\n1\nb\n\n\n2021-09-01 02:00:00\n2\nc\n\n\n2021-09-01 03:00:00\n3\na\n\n\n2021-09-01 04:00:00\n4\nd\n\n\n\n\n\n\n\n\n\n\npandas programmers just need to learn the key differences when working with distributed computing systems to make the Dask transition easily."
  },
  {
    "objectID": "core/slides/slides09_dask.html#inside-the-dataframe",
    "href": "core/slides/slides09_dask.html#inside-the-dataframe",
    "title": "Dask",
    "section": "Inside the dataframe",
    "text": "Inside the dataframe\n\n\n\nA sketch of the interplay between index and partitioning\n\n\nddf.divisions\n\n\n(Timestamp('2021-09-01 00:00:00'),\n Timestamp('2021-09-06 00:00:00'),\n Timestamp('2021-09-11 00:00:00'),\n Timestamp('2021-09-16 00:00:00'),\n Timestamp('2021-09-21 00:00:00'),\n Timestamp('2021-09-26 00:00:00'),\n Timestamp('2021-10-01 00:00:00'),\n Timestamp('2021-10-06 00:00:00'),\n Timestamp('2021-10-11 00:00:00'),\n Timestamp('2021-10-16 00:00:00'),\n Timestamp('2021-10-21 00:00:00'),\n Timestamp('2021-10-26 00:00:00'),\n Timestamp('2021-10-31 00:00:00'),\n Timestamp('2021-11-05 00:00:00'),\n Timestamp('2021-11-10 00:00:00'),\n Timestamp('2021-11-15 00:00:00'),\n Timestamp('2021-11-20 00:00:00'),\n Timestamp('2021-11-25 00:00:00'),\n Timestamp('2021-11-30 00:00:00'),\n Timestamp('2021-12-05 00:00:00'),\n Timestamp('2021-12-09 23:00:00'))\n\n\nA dataframe has a task graph\n\nddf.visualize()\n\n\n\n\n\n\n\n\n\n\nTODO\n\n\n\nWhat’s in a partition?\n\n1ddf.partitions[1]\n\n\n1\n\nThis is the second class of the partition\n\n\n\n\n\n\nDask DataFrame Structure:\n\n\n\n\n\na\nb\n\n\nnpartitions=1\n\n\n\n\n\n\n2021-09-06\nint64\nstring\n\n\n2021-09-11\n...\n...\n\n\n\n\nDask Name: partitions, 2 expressions\n\n\nSlicing\n\n1ddf[\"2021-10-01\":\"2021-10-09 5:00\"]\n\n\n1\n\nLike slicing NumPy arrays or pandas DataFrame.\n\n\n\n\n\n\nDask DataFrame Structure:\n\n\n\n\n\na\nb\n\n\nnpartitions=2\n\n\n\n\n\n\n2021-10-01 00:00:00.000000000\nint64\nstring\n\n\n2021-10-06 00:00:00.000000000\n...\n...\n\n\n2021-10-09 05:00:59.999999999\n...\n...\n\n\n\n\nDask Name: loc, 2 expressions"
  },
  {
    "objectID": "core/slides/slides09_dask.html#dask-dataframes-contd",
    "href": "core/slides/slides09_dask.html#dask-dataframes-contd",
    "title": "Dask",
    "section": "Dask dataframes (cont’d)",
    "text": "Dask dataframes (cont’d)\n\nDask DataFrames coordinate many Pandas DataFrames/Series arranged along an index.\n\n\nWe define a Dask DataFrame object with the following components:\n\n\n\n\nA Dask graph with a special set of keys designating partitions, such as (‘x’, 0), (‘x’, 1), …\n\n\n\n\n\n\nA name to identify which keys in the Dask graph refer to this DataFrame, such as ‘x’\n\n\n\n\n\n\nAn empty Pandas object containing appropriate metadata (e.g. column names, dtypes, etc.)\n\n\n\n\n\n\nA sequence of partition boundaries along the index called divisions"
  },
  {
    "objectID": "core/slides/slides09_dask.html#methods",
    "href": "core/slides/slides09_dask.html#methods",
    "title": "Dask",
    "section": "Methods",
    "text": "Methods\n\n\n\n( \n  ddf.a\n    .mean()\n)\n\n\n\n&lt;dask_expr.expr.Scalar: expr=df['a'].mean(), dtype=float64&gt;\n\n\n\n( \n  ddf.a\n    .mean()\n    .compute()\n)\n\n\n\nnp.float64(1199.5)\n\n\n\n(\n  ddf\n    .b\n    .unique()\n)\n\n\n\nDask Series Structure:\nnpartitions=20\n    string\n       ...\n     ...  \n       ...\n       ...\nDask Name: unique, 3 expressions\nExpr=Unique(frame=df['b'])"
  },
  {
    "objectID": "core/slides/slides09_dask.html#reading-and-writing-from-parquet",
    "href": "core/slides/slides09_dask.html#reading-and-writing-from-parquet",
    "title": "Dask",
    "section": "Reading and writing from parquet",
    "text": "Reading and writing from parquet\nfname = 'fhvhv_tripdata_2022-11.parquet'\ndpath = '../../../../Downloads/'\n\nglobpath = 'fhvhv_tripdata_20*-*.parquet'\n\n!ls -l ../../../../Downloads/fhvhv_tripdata_20*-*.parquet\n\nimport os\n\nos.path.expanduser('~' + '/Documents')\n\n\n\n'/home/boucheron/Documents'\n\n\n%%time \n\ndata = dd.read_parquet(\n  os.path.join(dpath, globpath),\n  categories= ['PULocationID',\n               'DOLocationID'], \n  engine='auto'\n)\ntype(data)\n#| eval: false\ndf = data.to_dask_dataframe()\n\ndf.info()\ndf._meta.dtypes\n\ndf.npartitions\n\n\n\n\n\ndf.head()\n\n\n\n\n\ntype(df)\n\n\n\n\n\ndf._meta.dtypes\n\n\n\n\n\ndf._meta_nonempty\n\n\n\n\n\ndf.info()\n\n\n\n\n\ndf.divisions\n\n\n\n\n\ndf.describe(include=\"all\")"
  },
  {
    "objectID": "core/slides/slides09_dask.html#partitioning-and-saving-to-parquet",
    "href": "core/slides/slides09_dask.html#partitioning-and-saving-to-parquet",
    "title": "Dask",
    "section": "Partitioning and saving to parquet",
    "text": "Partitioning and saving to parquet\n\nimport pyarrow as pa\n\nschm = pa.Schema.from_pandas(df._meta)\n\nschm\n\n\n\n\n\ndf.PULocationID.unique().compute()\n\n\n\n\n\ndf.to_parquet( \n  'fhvhv_tripdata_2022-11',\n  partition_on= ['PULocationID'],\n  engine='pyarrow', \n  schema = schm\n  )\n\n\n\n\n\ndf.info(memory_usage=True)"
  },
  {
    "objectID": "core/slides/slides09_dask.html#section-4",
    "href": "core/slides/slides09_dask.html#section-4",
    "title": "Dask",
    "section": "",
    "text": "After you have generated a task graph, it is the scheduler’s job to execute it (see Scheduling).\n\n\nBy default, for the majority of Dask APIs, when you call compute() on a Dask object, Dask uses the thread pool on your computer (a.k.a threaded scheduler) to run computations in parallel. This is true for Dask Array, Dask DataFrame, and Dask Delayed. The exception being Dask Bag which uses the multiprocessing scheduler by default.\n\n\nIf you want more control, use the distributed scheduler instead. Despite having “distributed” in it’s name, the distributed scheduler works well on both single and multiple machines. Think of it as the “advanced scheduler”."
  },
  {
    "objectID": "core/slides/slides09_dask.html#section-5",
    "href": "core/slides/slides09_dask.html#section-5",
    "title": "Dask",
    "section": "",
    "text": "Dask schedulers come with diagnostics to help you understand the performance characteristics of your computations\n\n\nBy using these diagnostics and with some thought, we can often identify the slow parts of troublesome computations\n\n\nThe single-machine and distributed schedulers come with different diagnostic tools\n\n\nThese tools are deeply integrated into each scheduler, so a tool designed for one will not transfer over to the other"
  },
  {
    "objectID": "core/slides/slides09_dask.html#dask-query-optimization",
    "href": "core/slides/slides09_dask.html#dask-query-optimization",
    "title": "Dask",
    "section": "Dask query optimization",
    "text": "Dask query optimization\nDemo"
  },
  {
    "objectID": "core/slides/slides09_dask.html#visualize-task-graphs",
    "href": "core/slides/slides09_dask.html#visualize-task-graphs",
    "title": "Dask",
    "section": "Visualize task graphs",
    "text": "Visualize task graphs"
  },
  {
    "objectID": "core/slides/slides09_dask.html#single-threaded-scheduler-and-a-normal-python-profiler",
    "href": "core/slides/slides09_dask.html#single-threaded-scheduler-and-a-normal-python-profiler",
    "title": "Dask",
    "section": "Single threaded scheduler and a normal Python profiler",
    "text": "Single threaded scheduler and a normal Python profiler"
  },
  {
    "objectID": "core/slides/slides09_dask.html#diagnostics-for-the-single-machine-scheduler",
    "href": "core/slides/slides09_dask.html#diagnostics-for-the-single-machine-scheduler",
    "title": "Dask",
    "section": "Diagnostics for the single-machine scheduler",
    "text": "Diagnostics for the single-machine scheduler"
  },
  {
    "objectID": "core/slides/slides09_dask.html#diagnostics-for-the-distributed-scheduler-and-dashboard",
    "href": "core/slides/slides09_dask.html#diagnostics-for-the-distributed-scheduler-and-dashboard",
    "title": "Dask",
    "section": "Diagnostics for the distributed scheduler and dashboard",
    "text": "Diagnostics for the distributed scheduler and dashboard"
  },
  {
    "objectID": "core/slides/slides09_dask.html#reference",
    "href": "core/slides/slides09_dask.html#reference",
    "title": "Dask",
    "section": " Reference",
    "text": "Reference\n\nDocs\nExamples\nCode\nBlog\nTutorial"
  },
  {
    "objectID": "core/slides/slides09_dask.html#ask-for-help",
    "href": "core/slides/slides09_dask.html#ask-for-help",
    "title": "Dask",
    "section": " Ask for help",
    "text": "Ask for help\n\ndask tag on Stack Overflow, for usage questions\ngithub issues for bug reports and feature requests\ngitter chat for general, non-bug, discussion"
  },
  {
    "objectID": "core/slides/slides09_dask.html#books",
    "href": "core/slides/slides09_dask.html#books",
    "title": "Dask",
    "section": " Books",
    "text": "Books\n\nScaling Python with Dask\nData Science with Python and Dask\n[Dask Definitive Guide (to appear 2025)]"
  },
  {
    "objectID": "core/slides/slides09_dask.html#blogs",
    "href": "core/slides/slides09_dask.html#blogs",
    "title": "Dask",
    "section": "Blogs",
    "text": "Blogs"
  },
  {
    "objectID": "core/slides/slides09_dask.html#loading-a-parquet-file",
    "href": "core/slides/slides09_dask.html#loading-a-parquet-file",
    "title": "Dask",
    "section": "Loading a Parquet file",
    "text": "Loading a Parquet file\n\ndpath = '/home/boucheron/Dropbox/MMD-2021/DATA/ny_corpus_prq/'\n\nglobpath = '*/*.parquet'\n\ndata = dd.read_parquet(\n  os.path.join(dpath, globpath),\n  engine='auto'\n)\n\n\n\n\n\n\ndata.info\n\n\n&lt;bound method DataFrame.info of Dask DataFrame Structure:\n                 title   topic    text             date\nnpartitions=77                                         \n                string  string  string  category[known]\n                   ...     ...     ...              ...\n...                ...     ...     ...              ...\n                   ...     ...     ...              ...\n                   ...     ...     ...              ...\nDask Name: read_parquet, 1 expression\nExpr=ReadParquetFSSpec(92994fd)&gt;"
  },
  {
    "objectID": "core/slides/slides09_dask.html#section-6",
    "href": "core/slides/slides09_dask.html#section-6",
    "title": "Dask",
    "section": "",
    "text": "( \n  data\n    .groupby(\"topic\")\n    .count()\n)\n\n\n\nDask DataFrame Structure:\n\n\n\n\n\ntitle\ntext\ndate\n\n\nnpartitions=1\n\n\n\n\n\n\n\n\nint64\nint64\nint64\n\n\n\n...\n...\n...\n\n\n\n\nDask Name: count, 2 expressions"
  },
  {
    "objectID": "core/slides/slides09_dask.html#section-7",
    "href": "core/slides/slides09_dask.html#section-7",
    "title": "Dask",
    "section": "",
    "text": "ddf = dd.read_parquet(\n    \"s3://dask-data/nyc-taxi/nyc-2015.parquet/part.*.parquet\",\n    columns=[\n      \"passenger_count\", \n      \"tip_amount\"],\n    storage_options={\"anon\": True},\n)\n\n\n\n\n\nresult = (\n  ddf\n    .groupby(\"passenger_count\")\n    .tip_amount\n    .mean()\n#    .compute()\n)\n\nresult\n\n\n\nDask Series Structure:\nnpartitions=1\n    float64\n        ...\nDask Name: getitem, 4 expressions\nExpr=((ReadParquetFSSpec(117185e)[['passenger_count', 'tip_amount']]).mean(observed=False, chunk_kwargs={'numeric_only': False}, aggregate_kwargs={'numeric_only': False}, _slice='tip_amount'))['tip_amount']\n\n\n\nimport dask.dataframe as dd\nfrom dask.distributed import Client\n\n\n\n\n\nclient = Client()\nclient\n\n\n\n\n     \n    \n        Client\n        Client-23ac4916-e27e-11ef-a64c-300505fc3398\n        \n\n\n\nConnection method: Cluster object\nCluster type: distributed.LocalCluster\n\n\nDashboard: http://127.0.0.1:8787/status\n\n\n\n\n\n\n        \n\n        \n            \n            Cluster Info\n            \n    \n    \n    \n        LocalCluster\n        62d0ccd4\n        \n\n\n\nDashboard: http://127.0.0.1:8787/status\nWorkers: 5\n\n\nTotal threads: 20\nTotal memory: 30.96 GiB\n\n\nStatus: running\nUsing processes: True\n\n\n\n\n\n        \n            \n                Scheduler Info\n            \n\n            \n    \n         \n        \n            Scheduler\n            Scheduler-4a341215-7814-4fde-a35c-f90f4492536c\n            \n\n\n\nComm: tcp://127.0.0.1:45181\nWorkers: 5\n\n\nDashboard: http://127.0.0.1:8787/status\nTotal threads: 20\n\n\nStarted: Just now\nTotal memory: 30.96 GiB\n\n\n\n\n        \n    \n\n    \n        \n            Workers\n        \n\n        \n        \n             \n            \n            \n                \n                    Worker: 0\n                \n                \n\n\n\nComm: tcp://127.0.0.1:34133\nTotal threads: 4\n\n\nDashboard: http://127.0.0.1:39875/status\nMemory: 6.19 GiB\n\n\nNanny: tcp://127.0.0.1:46805\n\n\n\nLocal directory: /tmp/dask-scratch-space/worker-31p6doxl\n\n\n\n\n            \n            \n        \n        \n        \n             \n            \n            \n                \n                    Worker: 1\n                \n                \n\n\n\nComm: tcp://127.0.0.1:41389\nTotal threads: 4\n\n\nDashboard: http://127.0.0.1:37229/status\nMemory: 6.19 GiB\n\n\nNanny: tcp://127.0.0.1:43601\n\n\n\nLocal directory: /tmp/dask-scratch-space/worker-m9q13h76\n\n\n\n\n            \n            \n        \n        \n        \n             \n            \n            \n                \n                    Worker: 2\n                \n                \n\n\n\nComm: tcp://127.0.0.1:34985\nTotal threads: 4\n\n\nDashboard: http://127.0.0.1:41965/status\nMemory: 6.19 GiB\n\n\nNanny: tcp://127.0.0.1:34219\n\n\n\nLocal directory: /tmp/dask-scratch-space/worker-1yjf2_ei\n\n\n\n\n            \n            \n        \n        \n        \n             \n            \n            \n                \n                    Worker: 3\n                \n                \n\n\n\nComm: tcp://127.0.0.1:35209\nTotal threads: 4\n\n\nDashboard: http://127.0.0.1:33913/status\nMemory: 6.19 GiB\n\n\nNanny: tcp://127.0.0.1:42609\n\n\n\nLocal directory: /tmp/dask-scratch-space/worker-_sj1rvoz\n\n\n\n\n            \n            \n        \n        \n        \n             \n            \n            \n                \n                    Worker: 4\n                \n                \n\n\n\nComm: tcp://127.0.0.1:42041\nTotal threads: 4\n\n\nDashboard: http://127.0.0.1:45571/status\nMemory: 6.19 GiB\n\n\nNanny: tcp://127.0.0.1:35539\n\n\n\nLocal directory: /tmp/dask-scratch-space/worker-tgi7deul"
  },
  {
    "objectID": "core/notebooks/notebook06_sparksql.html#select-projection",
    "href": "core/notebooks/notebook06_sparksql.html#select-projection",
    "title": "DataFrame",
    "section": "SELECT (projection)",
    "text": "SELECT (projection)\n\n\nCode\ndf.createOrReplaceTempView(\"table\")    \nquery = \"SELECT name, age FROM table\"\nspark.sql(query).show()\n\n\n+----+---+\n|name|age|\n+----+---+\n|John| 21|\n|Jane| 25|\n+----+---+\n\n\n\n\n\nCode\ndf.select(\"name\", \"age\").show()\n\n\n+----+---+\n|name|age|\n+----+---+\n|John| 21|\n|Jane| 25|\n+----+---+"
  },
  {
    "objectID": "core/notebooks/notebook06_sparksql.html#where-filter-selection",
    "href": "core/notebooks/notebook06_sparksql.html#where-filter-selection",
    "title": "DataFrame",
    "section": "WHERE (filter, selection)",
    "text": "WHERE (filter, selection)\n\n\nCode\ndf.createOrReplaceTempView(\"table\")\nquery = \"SELECT * FROM table WHERE age &gt; 21\"\nspark.sql(query).show()\n\n\n+----+---+------+\n|name|age|gender|\n+----+---+------+\n|Jane| 25|female|\n+----+---+------+\n\n\n\n\n\nCode\ndf.where(\"age &gt; 21\").show()\n\n\n+----+---+------+\n|name|age|gender|\n+----+---+------+\n|Jane| 25|female|\n+----+---+------+\n\n\n\n\n\nCode\n# Alternatively:\ndf.where(df['age'] &gt; 21).show()\n\n\n+----+---+------+\n|name|age|gender|\n+----+---+------+\n|Jane| 25|female|\n+----+---+------+\n\n\n\n\n\nCode\ndf.where(df.age &gt; 21).show()\n\n\n+----+---+------+\n|name|age|gender|\n+----+---+------+\n|Jane| 25|female|\n+----+---+------+\n\n\n\n\n\nCode\n( \n    df.where(\"age &gt; 21\")\n      .select([\"name\", \"age\"])\n      .show()\n)\n\n\n+----+---+\n|name|age|\n+----+---+\n|Jane| 25|\n+----+---+"
  },
  {
    "objectID": "core/notebooks/notebook06_sparksql.html#alias-rename",
    "href": "core/notebooks/notebook06_sparksql.html#alias-rename",
    "title": "DataFrame",
    "section": "ALIAS (rename)",
    "text": "ALIAS (rename)\n\n\nCode\ndf.createOrReplaceTempView(\"table\")\nquery = \"SELECT name, age, gender AS sex FROM table\"\nspark.sql(query).show()\n\n\n+----+---+------+\n|name|age|   sex|\n+----+---+------+\n|John| 21|  male|\n|Jane| 25|female|\n+----+---+------+\n\n\n\n\n\nCode\ntype(df.age)\n\n\npyspark.sql.column.Column\n\n\n\n\nCode\ndf.select(df.name, df.age, df.gender.alias('sex')).show()\n\n\n+----+---+------+\n|name|age|   sex|\n+----+---+------+\n|John| 21|  male|\n|Jane| 25|female|\n+----+---+------+"
  },
  {
    "objectID": "core/notebooks/notebook06_sparksql.html#select-projection-π",
    "href": "core/notebooks/notebook06_sparksql.html#select-projection-π",
    "title": "DataFrame",
    "section": "SELECT (projection \\(π\\))",
    "text": "SELECT (projection \\(π\\))\n\n\nCode\ndf.createOrReplaceTempView(\"table\")    \n\nquery = \"\"\"\n    SELECT \n        name, age \n    FROM \n        table\n\"\"\"\n\nspark.sql(query).show()\n\n\n+----+---+\n|name|age|\n+----+---+\n|John| 21|\n|Jane| 25|\n+----+---+\n\n\n\nUsing the API:\n\n\nCode\n(\n    df\n        .select(\"name\", \"age\")\n        .show()\n)\n\n\n+----+---+\n|name|age|\n+----+---+\n|John| 21|\n|Jane| 25|\n+----+---+\n\n\n\nπ(df, \"name\", \"age\")"
  },
  {
    "objectID": "core/notebooks/notebook06_sparksql.html#where-filter-selection-σ",
    "href": "core/notebooks/notebook06_sparksql.html#where-filter-selection-σ",
    "title": "DataFrame",
    "section": "WHERE (filter, selection, \\(σ\\))",
    "text": "WHERE (filter, selection, \\(σ\\))\n\n\nCode\ndf.createOrReplaceTempView(\"table\")\n\nquery = \"\"\"\n    SELECT \n        * \n    FROM \n        table\n    WHERE \n        age &gt; 21\n\"\"\"\n\nspark.sql(query).show()\n\n\n+----+---+------+\n|name|age|gender|\n+----+---+------+\n|Jane| 25|female|\n+----+---+------+\n\n\n\nUsing the API\n\n\nCode\n( \n    df\n        .where(\"age &gt; 21\")\n        .show()\n)\n\n\n+----+---+------+\n|name|age|gender|\n+----+---+------+\n|Jane| 25|female|\n+----+---+------+\n\n\n\nThis implements σ(df, \"age &gt; 21\")\n\n\nCode\n# Alternatively:\n( \n    df\n      .where(df['age'] &gt; 21)\n      .show()\n)\n\n\n+----+---+------+\n|name|age|gender|\n+----+---+------+\n|Jane| 25|female|\n+----+---+------+\n\n\n\n\n\nCode\n( \n    df\n      .where(df.age &gt; 21)\n      .show()\n)\n\n\n+----+---+------+\n|name|age|gender|\n+----+---+------+\n|Jane| 25|female|\n+----+---+------+\n\n\n\nMethod chaining allows to construct complex queries\n\n\nCode\n( \n    df\n      .where(\"age &gt; 21\")\n      .select([\"name\", \"age\"])\n      .show()\n)\n\n\n+----+---+\n|name|age|\n+----+---+\n|Jane| 25|\n+----+---+\n\n\n\nThis implements\n    σ(df, \"age &gt; 21\") |&gt;\n    π([\"name\", \"age\"])"
  }
]