{
  "hash": "12177466914cdb6625f5fe92b55c4b38",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Numpy : broadcasting`\"\ndate: \"`r Sys.Date()\"\n\njupyter: python3\n\nengine: jupyter\n---\n\n\n\n\n# Centering and scaling a data matrix\n\n\nIn the sequel  $X$ is a (data) numerical matrix, that is an element of $\\mathbb{R}^{n \\times p}$. The rows of $X$ (the individuals) are the sample points. Each sample point is a tuple of $p$ elements (the so-called variables). \n\nThe *sample mean* is defined as \n$$\n\\overline{X}= \\begin{pmatrix} \\frac{1}{n}\\sum_{i=1}^n X_{i,j}\\end{pmatrix}_{j\\leq p}\n$$\nIn linear algebra, $\\overline{X}$ is the result of  vector matrix multiplication:\n$$\n\\overline{X} = \\frac{1}{n}\\begin{pmatrix} 1 & \\ldots & 1\\end{pmatrix} \\times X\n$$\nHere, we view $\\overline{X}_n$ as a row vector built from column averages.\n\nCentering $X$ consists in subtracting the column average from each matrix element. \n$$X - \\begin{pmatrix}1 \\\\ \\vdots \\\\ 1\\end{pmatrix} \\times \\overline{X}$$\n\nNote that centering consists on projecting the columns of $X$ \non the $n-1$ dimensional subspace of $\\mathbb{R}^n$ that is orthogonal to $\\begin{pmatrix} 1 & \\ldots  &  1\\end{pmatrix}^\\top$:\n\\begin{align*}\nX - \\begin{pmatrix}1 \\\\ \\vdots \\\\ 1\\end{pmatrix} \\times \\overline{X}\n  & =\n  \\underbrace{ \n    \\left(\\text{Id}_n - \n      \\frac{1}{n}\n      \\begin{pmatrix} 1 \\\\ \\vdots \\\\ 1\\end{pmatrix} \n      \\begin{pmatrix} 1 & \\ldots  &  1\\end{pmatrix}\\right)}_{\\text{projectior matrix}} X\n\\end{align*}\nLet us call $Y$ the matrix obtained from centering the columns of $X$.\n\nScaling $Y$ consists of dividing each coefficient of $Y$ by $1/\\sqrt{n}$ times the (Euclidean) norm of its column. \n\nLet us call $\\sigma_j$ the Euclidean \nnorm of column $j$ of $Y$ ($1\\leq j \\leq p$) divided by $1/\\sqrt{n}$:\n$$\n\\sigma_j^2 = \\frac{1}{n}\\sum_{i=1}^n Y_{i,j}^2 = \\frac{1}{n} \\sum_{i=1}^n \\left(X_{i,j} - \\overline{X}_j\\right)^2\n$$ \nThis is also the standard deviation of the $j^{\\text{n}}$ column of $X$.\n\nThe standardized matrix $Z$ is obtained from the next multiplication\n$$\nZ = Y \\times \n  \\begin{pmatrix} \n    \\sigma_1 & 0        &  \\ldots      &     & 0 \\\\\n    0        & \\sigma_2 &   0    &     & \\vdots \\\\\n    \\vdots   & 0         & \\ddots &     & \\vdots  \\\\\n    0        & \\ldots   &        &     & \\sigma_d       \n  \\end{pmatrix}^{-1}\n$$\n\nNote that for $i\\leq n$, $j \\leq p$:\n$$Z_{i,j} = \\frac{X_{i,j} - \\overline{X}_j}{\\sigma_j}$$\n\n$Z$ is called the $z$-score matrix. It shows up in many statistical analyses. \n\nIn the statistical computing environment `R`, a function called `scale()` computes the $z$-score matrix. On may ask whether NumPy offers such a function.\n\n\n\n# Scaling in NumPy (standardization)\n\nIn NumPy, there is no single function equivalent to R's `scale()` function. However, you can  achieve the same result using *broadcasting*.\n\n::: {#c4de5a89 .cell execution_count=1}\n``` {.python .cell-code}\nimport numpy as np\n```\n:::\n\n\nLet us first generate a toy data matrix with random (Gaussian) coefficients. This is an opportunity to introduce `np.random`. \n\nWe will work with $n=5$  and $p=3$. \n\nWe first build a (positive definite) covariance matrix. We ensure \npositive definiteness starting from the Cholesky factorization. \n\n::: {#9eaf33cd .cell execution_count=2}\n``` {.python .cell-code}\n# %%\nL = np.array([  \n  [1., 0., 0.], \n  [.5, 1., 0.], \n  [.5, .5, 1.]])\nC = L @ L.transpose()\n# L is the Cholesky factor of C\n```\n:::\n\n\n::: {#a95e8a47 .cell execution_count=3}\n``` {.python .cell-code}\n# Generate a sample of 5 independent normal vectors with mean (1, 2, 3) and covariance C\nmu = np.array([1, 2, 3])\n```\n:::\n\n\n::: {#93d82afb .cell execution_count=4}\n``` {.python .cell-code}\nX = np.random.randn(5,3) @ L.transpose() + mu\n```\n:::\n\n\nIn NumPy, function `mean` with well chosen optional `axis` argument returns a 1D  array filled with column averages\n\n::: {.callout-note}\n\nWe just did something strange: we added a matrix with shape $(5,3)$ and a vector with length $3$. In linear algebra, this is not legitimate. We just used the device called *broadcasting*. See below. \n\n:::\n\n::: {#bb55f130 .cell execution_count=5}\n``` {.python .cell-code}\n# Compute column averages\n# That is compute arithmetic mean along axis `0`\nemp_mean = np.mean(X, axis=0)\n```\n:::\n\n\nWe can magically center the columns using broadcasting. \n\n::: {#7ce41198 .cell execution_count=6}\n``` {.python .cell-code}\nX_centered = X - emp_mean\n```\n:::\n\n\nIf broadcasting were note possible, we could still achieve the result by resorting to NumPy  implementation of matrix multiplication\n\n::: {#93d7dad9 .cell execution_count=7}\n``` {.python .cell-code}\nX - np.ones((5,1), dtype=np.float16) @ emp_mean.reshape(1,3)\n```\n\n::: {.cell-output .cell-output-display execution_count=19}\n```\narray([[ 0.29217189, -0.61560995,  0.06531655],\n       [ 0.67773779,  0.47890199, -0.4490209 ],\n       [-1.94985335, -0.97326014, -1.66136559],\n       [ 1.06293708, -0.53083919, -0.47012529],\n       [-0.08299341,  1.64080728,  2.51519522]])\n```\n:::\n:::\n\n\nis a centered version of `X`:\n\n::: {#162140e1 .cell execution_count=8}\n``` {.python .cell-code}\nX_centered - (X - np.ones((5,1), dtype=np.float16) @ emp_mean.reshape(1,3))\n```\n\n::: {.cell-output .cell-output-display execution_count=20}\n```\narray([[0., 0., 0.],\n       [0., 0., 0.],\n       [0., 0., 0.],\n       [0., 0., 0.],\n       [0., 0., 0.]])\n```\n:::\n:::\n\n\nWe compute now the column standard deviations using function `np.std` with `axis` argument set to `0`\n\n::: {#43c39cf8 .cell execution_count=9}\n``` {.python .cell-code}\nemp_std = np.std(X, axis=0)\n```\n:::\n\n\nThe $z$-score matrix is obtained using another broadcasting operation.\n\n::: {#be53ec75 .cell execution_count=10}\n``` {.python .cell-code}\nZ = (X - emp_mean) / emp_std  # X_centered / emp_std\n```\n:::\n\n\nFinally,  let us perform the sanity checks :\n\n::: {#366f5b59 .cell execution_count=11}\n``` {.python .cell-code}\n# %%\nnp.mean(Z, axis=0)  # Z is column centered\n```\n\n::: {.cell-output .cell-output-display execution_count=23}\n```\narray([ 1.08246745e-16,  4.44089210e-17, -8.88178420e-17])\n```\n:::\n:::\n\n\n::: {#90b1730e .cell execution_count=12}\n``` {.python .cell-code}\nnp.std(Z, axis=0)   # Z is standardized\n```\n\n::: {.cell-output .cell-output-display execution_count=24}\n```\narray([1., 1., 1.])\n```\n:::\n:::\n\n\n::: {.callout-note}\n\nAlternatively, we can use `scipy.stats.zscore` which provides R's `scale()`-like functionality:\n\n::: {#461cc8da .cell execution_count=13}\n``` {.python .cell-code}\nfrom scipy.stats import zscore\nX_scaled_scipy = zscore(X, axis=0)\nX_scaled_scipy\n```\n\n::: {.cell-output .cell-output-display execution_count=25}\n```\narray([[ 0.27899773, -0.64678637,  0.04735261],\n       [ 0.64717831,  0.5031551 , -0.32552713],\n       [-1.86193364, -1.02254909, -1.20444186],\n       [ 1.0150088 , -0.55772255, -0.34082719],\n       [-0.0792512 ,  1.72390291,  1.82344357]])\n```\n:::\n:::\n\n\n`scipy.stats.zscore` centers and scales the data by default (equivalent to R's `scale()` with default arguments). \n\nCentering and standardization are classical preprocessing steps before Principal Component Analysis (and before many Machine Learning procedures).\n\n:::\n\n# How does broadcasting work ?\n\n## Why broadcasting ? (from the documentation)\n\n> The term broadcasting describes how NumPy treats arrays with different shapes during arithmetic operations. Subject to certain constraints, the smaller array is “broadcast” across the larger array so that they have compatible shapes. Broadcasting provides a means of vectorizing array operations so that looping occurs in C instead of Python. It does this without making needless copies of data and usually leads to efficient algorithm implementations. There are, however, cases where broadcasting is a bad idea because it leads to inefficient use of memory that slows computation.\n\n## How ?\n\n> When operating on two arrays, NumPy compares their shapes element-wise. It starts with the trailing (i.e. rightmost) dimension and works its way left. Two dimensions are compatible when\n>\n> - they are equal, or\n> - one of them is 1.\n\nIn our setting the shape of `X` and `emp_mean` are `(5,3)` and `(3)`. The rightmost dimensions are equal, hence compatible.\n\n> Input arrays do not need to have the same number of dimensions. The resulting array will have the same number of dimensions as the input array with the greatest number of dimensions, where the size of each dimension is the largest size of the corresponding dimension among the input arrays. Note that missing dimensions are assumed to have size one.\n\nIn our setting, `emp_mean` is (virtually) reshaped to `(1,3)` and the two arrays are fully compatible. To match the leading dimension of `X`, we can stack three copies of reshaped `emp_mean`, this is just like multiplying `emp_mean`  by `np.array([1.], shape=(3,1))`.   \n\n> When either of the dimensions compared is one, the other is used. In other words, dimensions with size 1 are stretched or “copied” to match the other.\n\n\n# References \n\n\n[Official Numpy documentation](https://numpy.org/devdocs/user/basics.broadcasting.html#basics-broadcasting)\n\n",
    "supporting": [
      "notebook02-2_numpy_files"
    ],
    "filters": [],
    "includes": {}
  }
}