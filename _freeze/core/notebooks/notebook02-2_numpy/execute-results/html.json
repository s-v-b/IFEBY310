{
  "hash": "66d1944e7bdb96bda69413c20bf7494b",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Numpy : broadcasting`\"\ndate: \"`r Sys.Date()\"\n\njupyter: python3\n\nengine: jupyter\n---\n\n\n\n\n# Centering and scaling a data matrix\n\n\nIn the sequel  $X$ is a (data) numerical matrix, that is an element of $\\mathbb{R}^{n \\times p}$. The rows of $X$ (the individuals) are the sample points. Each sample point is a tuple of $p$ elements (the so-called variables). \n\nThe *sample mean* is defined as \n$$\n\\overline{X}= \\begin{pmatrix} \\frac{1}{n}\\sum_{i=1}^n X_{i,j}\\end{pmatrix}_{j\\leq p}\n$$\n\nIn linear algebra, $\\overline{X}$ is the result of  vector matrix multiplication:\n$$\n\\overline{X} = \\frac{1}{n}\\begin{pmatrix} 1 & \\ldots & 1\\end{pmatrix} \\times X\n$$\n\nHere, we view $\\overline{X}_n$ as a row vector built from column averages.\n\nCentering $X$ consists in subtracting the column average from each matrix element. \n$$X - \\begin{pmatrix}1 \\\\ \\vdots \\\\ 1\\end{pmatrix} \\times \\overline{X}$$\n\nNote that centering consists on projecting the columns of $X$ \non the $n-1$ dimensional subspace of $\\mathbb{R}^n$ that is orthogonal to $\\begin{pmatrix} 1 & \\ldots  &  1\\end{pmatrix}^\\top$:\n\n\\begin{align*}\nX - \\begin{pmatrix}1 \\\\ \\vdots \\\\ 1\\end{pmatrix} \\times \\overline{X}\n  & =\n  \\underbrace{ \n    \\left(\\text{Id}_n - \n      \\frac{1}{n}\n      \\begin{pmatrix} 1 \\\\ \\vdots \\\\ 1\\end{pmatrix} \n      \\begin{pmatrix} 1 & \\ldots  &  1\\end{pmatrix}\\right)}_{\\text{projector matrix}} X\n\\end{align*}\n\nLet us call $Y$ the matrix obtained from centering the columns of $X$.\n\nScaling $Y$ consists of dividing each coefficient of $Y$ by $1/\\sqrt{n}$ times the (Euclidean) norm of its column. \n\nLet us call $\\sigma_j$ the Euclidean \nnorm of column $j$ of $Y$ ($1\\leq j \\leq p$) divided by $1/\\sqrt{n}$:\n$$\n\\sigma_j^2 = \\frac{1}{n}\\sum_{i=1}^n Y_{i,j}^2 = \\frac{1}{n} \\sum_{i=1}^n \\left(X_{i,j} - \\overline{X}_j\\right)^2\n$$ \n\nThis is also the standard deviation of the $j^{\\text{n}}$ column of $X$.\n\nThe standardized matrix $Z$ is obtained from the next multiplication\n$$\nZ = Y \\times \n  \\begin{pmatrix} \n    \\sigma_1 & 0        &  \\ldots      &     & 0 \\\\\n    0        & \\sigma_2 &   0    &     & \\vdots \\\\\n    \\vdots   & 0         & \\ddots &     & \\vdots  \\\\\n    0        & \\ldots   &        &     & \\sigma_d       \n  \\end{pmatrix}^{-1}\n$$\n\nNote that for $i\\leq n$, $j \\leq p$:\n$$Z_{i,j} = \\frac{X_{i,j} - \\overline{X}_j}{\\sigma_j}$$\n\n$Z$ is called the $z$-score matrix. It shows up in many statistical analyses. \n\nIn the statistical computing environment `R`, a function called `scale()` computes the $z$-score matrix. On may ask whether NumPy offers such a function.\n\n\n\n# Scaling in NumPy (standardization)\n\nIn NumPy, there is no single function equivalent to R's `scale()` function. However, you can  achieve the same result using *broadcasting*.\n\n::: {#import-np .cell execution_count=1}\n``` {.python .cell-code}\nimport numpy as np\n```\n:::\n\n\nLet us first generate a toy data matrix with random (Gaussian) coefficients. This is an opportunity to introduce `np.random`. \n\nWe will work with $n=5$  and $p=3$. \n\nWe first build a (positive definite) covariance matrix. We ensure \npositive definiteness starting from the Cholesky factorization. \n\n::: {#0b31d612 .cell execution_count=2}\n``` {.python .cell-code}\n# %%\n#| label: build-covariance\nL = np.array([  \n  [1., 0., 0.], \n  [.5, 1., 0.], \n  [.5, .5, 1.]])\nC = L @ L.transpose()\n# L is the Cholesky factor of C\n```\n:::\n\n\n::: {#define-expectation .cell execution_count=3}\n``` {.python .cell-code}\n# Generate a sample of 5 independent normal vectors with mean (1, 2, 3) and covariance C\nmu = np.array([1, 2, 3])\n```\n:::\n\n\n::: {#generate-mulivariate-normal .cell execution_count=4}\n``` {.python .cell-code}\nX = np.random.randn(5,3) @ L.transpose() + mu\n```\n:::\n\n\nIn NumPy, function `mean` with well chosen optional `axis` argument returns a 1D  array filled with column averages\n\n::: {.callout-note}\n\nWe just did something strange: we added a matrix with shape $(5,3)$ and a vector with length $3$. In linear algebra, this is not legitimate. We just used the device called *broadcasting*. See below. \n\n:::\n\n::: {#compute-mean .cell execution_count=5}\n``` {.python .cell-code}\n# Compute column averages\n# That is compute arithmetic mean along axis `0`\nemp_mean = np.mean(X, axis=0)\n```\n:::\n\n\nWe can magically center the columns using broadcasting. \n\n::: {#center-data-broadcast .cell execution_count=6}\n``` {.python .cell-code}\nX_centered = X - emp_mean\n```\n:::\n\n\nIf broadcasting were note possible, we could still achieve the result by resorting to NumPy  implementation of matrix multiplication\n\n::: {#cell-center-data-substract .cell execution_count=7}\n``` {.python .cell-code}\nX - np.ones((5,1), dtype=np.float16) @ emp_mean.reshape(1,3)\n```\n\n::: {#center-data-substract .cell-output .cell-output-display execution_count=7}\n```\narray([[ 0.41846646, -0.72225791, -0.78003353],\n       [-2.50364484, -0.7034133 , -1.21231068],\n       [-0.41703883,  0.21260877, -0.25539926],\n       [ 1.62442711,  1.52986466,  1.19133333],\n       [ 0.8777901 , -0.31680222,  1.05641013]])\n```\n:::\n:::\n\n\nis a centered version of `X`:\n\n::: {#cell-center-sanity-check .cell execution_count=8}\n``` {.python .cell-code}\nX_centered - (X - np.ones((5,1), dtype=np.float16) @ emp_mean.reshape(1,3))\n```\n\n::: {#center-sanity-check .cell-output .cell-output-display execution_count=8}\n```\narray([[0., 0., 0.],\n       [0., 0., 0.],\n       [0., 0., 0.],\n       [0., 0., 0.],\n       [0., 0., 0.]])\n```\n:::\n:::\n\n\nWe compute now the column standard deviations using function `np.std` with `axis` argument set to `0`\n\n::: {#computer-std .cell execution_count=9}\n``` {.python .cell-code}\nemp_std = np.std(X, axis=0)\n```\n:::\n\n\nThe $z$-score matrix is obtained using another broadcasting operation.\n\n::: {#standardize-broadcast .cell execution_count=10}\n``` {.python .cell-code}\nZ = (X - emp_mean) / emp_std  # X_centered / emp_std\n```\n:::\n\n\nFinally,  let us perform the sanity checks :\n\n::: {#cfcab3ad .cell execution_count=11}\n``` {.python .cell-code}\n# %%\n#| label: sanity-check-z-matrix-mean\nnp.mean(Z, axis=0)  # Z is column centered\n```\n\n::: {.cell-output .cell-output-display execution_count=11}\n```\narray([-4.44089210e-17, -3.33066907e-17,  2.22044605e-16])\n```\n:::\n:::\n\n\n::: {#cell-sanity-check-z-matrix-std .cell execution_count=12}\n``` {.python .cell-code}\nnp.std(Z, axis=0)   # Z is standardized\n```\n\n::: {#sanity-check-z-matrix-std .cell-output .cell-output-display execution_count=12}\n```\narray([1., 1., 1.])\n```\n:::\n:::\n\n\n::: {.callout-note}\n\nAlternatively, we can use `scipy.stats.zscore` which provides R's `scale()`-like functionality:\n\n::: {#cell-scipy-scale-zscore .cell execution_count=13}\n``` {.python .cell-code}\nfrom scipy.stats import zscore\nX_scaled_scipy = zscore(X, axis=0)\nX_scaled_scipy\n```\n\n::: {#scipy-scale-zscore .cell-output .cell-output-display execution_count=13}\n```\narray([[ 0.29550841, -0.86295532, -0.80637625],\n       [-1.76799861, -0.84043975, -1.25325195],\n       [-0.29450026,  0.25402542, -0.26402441],\n       [ 1.14712152,  1.82788563,  1.23156617],\n       [ 0.61986894, -0.37851598,  1.09208644]])\n```\n:::\n:::\n\n\n`scipy.stats.zscore` centers and scales the data by default (equivalent to R's `scale()` with default arguments). \n\nCentering and standardization are classical preprocessing steps before Principal Component Analysis (and before many Machine Learning procedures).\n\n:::\n\n# How does broadcasting work ?\n\n## Why broadcasting ? (from the documentation)\n\n> The term broadcasting describes how NumPy treats arrays with different shapes during arithmetic operations. Subject to certain constraints, the smaller array is “broadcast” across the larger array so that they have compatible shapes. Broadcasting provides a means of vectorizing array operations so that looping occurs in C instead of Python. It does this without making needless copies of data and usually leads to efficient algorithm implementations. There are, however, cases where broadcasting is a bad idea because it leads to inefficient use of memory that slows computation.\n\n## How ?\n\n> When operating on two arrays, NumPy compares their shapes element-wise. It starts with the trailing (i.e. rightmost) dimension and works its way left. Two dimensions are compatible when\n>\n> - they are equal, or\n> - one of them is 1.\n\nIn our setting the shape of `X` and `emp_mean` are `(5,3)` and `(3)`. The rightmost dimensions are equal, hence compatible.\n\n> Input arrays do not need to have the same number of dimensions. The resulting array will have the same number of dimensions as the input array with the greatest number of dimensions, where the size of each dimension is the largest size of the corresponding dimension among the input arrays. Note that missing dimensions are assumed to have size one.\n\nIn our setting, `emp_mean` is (virtually) reshaped to `(1,3)` and the two arrays are fully compatible. To match the leading dimension of `X`, we can stack three copies of reshaped `emp_mean`, this is just like multiplying `emp_mean`  by `np.array([1.], shape=(3,1))`.   \n\n> When either of the dimensions compared is one, the other is used. In other words, dimensions with size 1 are stretched or “copied” to match the other.\n\n\n\n# Variations on scaling/centering\n\n## Alternatives to mean/standard deviation\n\nIn statistics, it is commonplace to sketch a probability distribution (empirical or not) using a location and a spread/scale parameter. If we add a constant to every outcome, the location parameter is shifted by the constant, the scale/spread parameter is left invariant. If we multiply every outcome by a constant, the location and the spread/scale parameters are multiplied by the constant. \n\nChoosing the expectation/mean as the location parameter is traditional, it is motivated by the fact the expectation minimizes the mean quadratic error, but it may not be a panacea. The empirical mean is well known for its sensitivity to outliers. In words, it lacks robustness. Picking the standard deviation as the scale/spread parameter leads to the same issues. \n\nMany alternative location/scale parameters have been proposed aiming to achieve good tradeoffs between robustness and computability. The median is a popular location parameter, and the median absolute deviation around the median is a popular scale/spread parameter. \n\nTODO: median and median deviation around the median \n\n## Using functions as objects for a more flexible function\n\n::: {#my_scaling .cell execution_count=14}\n``` {.python .cell-code}\ndef my_scaling(X, center=None, scale=None):\n  \"\"\"\n  Center and scale a data matrix using flexible location and spread functions.\n  \n  This function demonstrates how to use functions as objects in Python to create\n  a flexible scaling function that can work with different location and scale\n  parameters. By default, it performs z-score standardization (centering by mean\n  and scaling by standard deviation), but can be configured to use robust\n  alternatives like median and median absolute deviation.\n  \n  Parameters\n  ----------\n  X : array_like\n      Input data matrix of shape (n, p) where n is the number of observations\n      and p is the number of variables. Each row is an observation, each column\n      is a variable.\n  center : callable, None, or False, optional\n      Function to compute the location parameter along axis=0. \n      - If None (default), uses `np.mean` for column-wise means.\n      - If a callable (e.g., `np.median`), applies it to compute location.\n      - If False, no centering is performed (location set to zeros).\n  scale : callable, None, or False, optional\n      Function to compute the spread/scale parameter along axis=0.\n      - If None (default), uses `np.std` for column-wise standard deviations.\n      - If a callable (e.g., `median_absdev`), applies it to compute spread.\n      - If False, no scaling is performed (spread set to ones).\n  \n  Returns\n  -------\n  X_scaled : ndarray\n      Centered and scaled data matrix of the same shape as X.\n      Each column is centered by subtracting its location parameter and\n      scaled by dividing by its spread parameter.\n      Formula: (X - location) / spread\n  \n  Notes\n  -----\n  This function illustrates the use of functions as first-class objects in Python.\n  By passing different functions for `center` and `scale`, you can achieve:\n  - Standard z-score: `my_scaling(X)` or `my_scaling(X, np.mean, np.std)`\n  - Robust scaling: `my_scaling(X, np.median, median_absdev)`\n  - Centering only: `my_scaling(X, scale=False)`\n  - Scaling only: `my_scaling(X, center=False)`\n  \n  Examples\n  --------\n  >>> import numpy as np\n  >>> X = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n  >>> # Standard z-score standardization\n  >>> my_scaling(X)\n  array([[-1.22474487, -1.22474487, -1.22474487],\n         [ 0.        ,  0.        ,  0.        ],\n         [ 1.22474487,  1.22474487,  1.22474487]])\n  \n  >>> # Robust scaling using median and MAD\n  >>> my_scaling(X, center=np.median, scale=median_absdev)\n  array([[-1., -1., -1.],\n         [ 0.,  0.,  0.],\n         [ 1.,  1.,  1.]])\n  \n  >>> # Centering only (no scaling)\n  >>> my_scaling(X, scale=False)\n  array([[-3., -3., -3.],\n         [ 0.,  0.,  0.],\n         [ 3.,  3.,  3.]])\n  \"\"\"\n  if center is None:\n      center = np.mean\n\n  if center:\n      location = center(X, axis=0)\n  else:\n      location = np.zeros(X.shape[1])\n  \n  if scale is None:\n      fscale = np.std\n  \n  if scale:\n      spread = scale(X, axis=0)\n  else:\n      spread = np.ones(X.shape[1])\n\n  return (X - location) / spread\n```\n:::\n\n\n::: {#median_absdev .cell execution_count=15}\n``` {.python .cell-code}\ndef median_absdev(X, axis=0):\n  \"\"\"\n  Compute the median absolute deviation (MAD) around the median.\n  \n  The median absolute deviation is a robust measure of statistical dispersion.\n  It is defined as the median of the absolute deviations from the median of\n  the data. Unlike the standard deviation, it is less sensitive to outliers.\n  \n  Parameters\n  ----------\n  X : array_like\n      Input array or object that can be converted to an array.\n      For a 2D array, rows typically represent observations and columns\n      represent variables.\n  axis : int, optional\n      Axis along which the median and MAD are computed. Default is 0.\n      For a 2D array with axis=0, computes MAD for each column.\n      For axis=1, computes MAD for each row.\n  \n  Returns\n  -------\n  mad : ndarray\n      Median absolute deviation along the specified axis.\n      The shape of the output is the same as the input array with the\n      specified axis removed.\n  \n  Notes\n  -----\n  The MAD is computed as:\n      MAD = median(|X - median(X)|)\n  \n  This function first computes the median along the specified axis, then\n  computes the absolute deviations from that median, and finally returns\n  the median of those absolute deviations.\n  \n  Examples\n  --------\n  >>> import numpy as np\n  >>> X = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n  >>> median_absdev(X, axis=0)\n  array([3., 3., 3.])\n  \n  >>> # For a single column\n  >>> x = np.array([1, 2, 3, 4, 5, 100])  # 100 is an outlier\n  >>> median_absdev(x)\n  1.5  # Robust to the outlier\n  \"\"\"\n  med = np.median(X, axis=axis)\n  \n  return np.median(np.abs(X - med), axis=axis)\n```\n:::\n\n\n`scipy.stats.median_abs_deviation()` offers a function for computing the MAD. Besides an `axis` parameter, it proposes `nan_policy` parameter. \n\n# References \n\n\n[Official Numpy documentation](https://numpy.org/devdocs/user/basics.broadcasting.html#basics-broadcasting)\n\n",
    "supporting": [
      "notebook02-2_numpy_files"
    ],
    "filters": [],
    "includes": {}
  }
}