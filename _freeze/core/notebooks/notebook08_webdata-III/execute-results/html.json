{
  "hash": "7be3e3c46705ba57e68e7cb4e2c10177",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: Using with `pyspark` for data preprocessing\njupyter: python3\nexecute: \n  eval: true\n---\n\nWe want to use pyspark to preprocess a potentially huge dataset used for web-marketing.\n\n## Data description\n\nThe data is a `parquet` file which contains a dataframe with 8 columns:\n\n- `xid`: unique user id\n- `action`: type of action. 'C' is a click, 'O' or 'VSL' is a web-display\n- `date`: date of the action\n- `website_id`: unique id of the website\n- `url`: url of the webpage\n- `category_id`: id of the display\n- `zipcode`: postal zipcode of the user\n- `device`: type of device used by the user\n\n::: {.callout-note title=\"Questions\"}\n\n- According to you, how was that data asset collected?\n- In the likely context (digital marketing), what is a *web display*?\n- What are the different values of `category_id`? What are their respective frequencies?\n- What are the 10 most frequent websites (according to the data)?\n- What are the different devices used across the dataset? \n- When was the dataset collected?\n:::\n\n\n## Q1. Some statistics / computations\n\nUsing `pyspark.sql`,  we want to do the following things:\n\n1. Compute the total number of unique users\n2. Construct a column containing the total number of actions per user\n3. Construct a column containing the number of days since the last action of the user\n4. Construct a column containing the number of actions of each user for each modality of device \n\n## Q2. Binary classification\n\nThen, we want to construct a *classifier* to predict the click on  category 1204 (`category_id`). \n\nHere is an agenda for this:\n\n1. Construction of a *feature matrix* where each line gathers information about a single  user (identified by `xid`).\n2. In the *feature matrix*, we need to keep only the rows (users) reporting exposition to the display in category 1204\n3. Using this *training* dataset, *train* a *binary classifier*, and evaluate the classifier using a *precision/recall* curve computed on test data.\n\n::: {.callout-note}\n\nViewing this dataset and the task at hand, we might be tempted to just filter rows satisfying condition `category_id=='1204'`, and \nfit a model that predict `action` given the other columns (or given some of them). This is not what the agenda suggests. Why?  \n\n\n:::\n\n\n# Download/read the data and have a look\n\n## Round up the usual suspects\n\n::: {#4db55f74 .cell execution_count=1}\n``` {.python .cell-code}\nimport os\nimport sys\nimport requests, zipfile, io\n\nfrom time import perf_counter\n# from pathlib import Path\n```\n:::\n\n\n::: {#3699fa0c .cell execution_count=2}\n``` {.python .cell-code}\nos.environ['PYSPARK_PYTHON'] = sys.executable\nos.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable\n```\n:::\n\n\n::: {#db02e3e5 .cell execution_count=3}\n``` {.python .cell-code}\nfrom pyspark import SparkConf, SparkContext\nfrom pyspark.sql import SparkSession\n```\n:::\n\n\n::: {#332ab024 .cell execution_count=4}\n``` {.python .cell-code}\nfrom pyspark.sql import Window\nimport pyspark.sql.functions as func\nfrom pyspark.sql.types import *\nfrom pyspark.sql.functions import col, lit\n```\n:::\n\n\n::: {#3fdf9204 .cell execution_count=5}\n``` {.python .cell-code}\nfrom functools import reduce\nimport numpy as np\nfrom scipy.sparse import csr_matrix\n```\n:::\n\n\nThe next imports are used at the end of the notebook. \n\n::: {#436987b6 .cell execution_count=6}\n``` {.python .cell-code}\nfrom sklearn.preprocessing import MaxAbsScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import precision_recall_curve, f1_score\n```\n:::\n\n\n## Set up the spark client\n\n::: {.callout-note}\n\nAs we are not interacting with a preexisting cluster, we actually need to start a server and a client. So far, we used the `local` mode. Even with a laptop/desktop, several modes are available\n\n- `local`\n- `standalone`\n- \n\n:::\n\n\n::: {.callout-note}\n\n### Spark in standalone mode\n\n\n\nTo launch spark in `standalone` mode, assuming the current working directory is `$SPARK_HOME` \n\nAssume that `hadoop` is running. This can be checked by monitoring java processes running on the local machine \n\n```{.jps}\n$ jps \n30033 Jps\n>>> 18898 NodeManager\n>>> 17735 NameNode\n>>> 18216 SecondaryNameNode\n>>> 17965 DataNode\n>>> 18542 ResourceManager\n```\n`NameNode` `DataNode` and `SecondaryNameNode` have been launched by `start-dfs.sh`, while `NodeManager` and  `ResourceManager` have been \nlaunched by `start-yarn`.\n\nWe can monitor `NameNode` at `http://localhost:9870`\n\nThen we may launch a `master`  and a `worker` process. \n\n```{.bash}\n$ ./sbin/start-master.sh --ip localhost\n>>> starting org.apache.spark.deploy.master.Master, logging to ...\n$ ./sbin/start-worker.sh spark://localhost:7077\n```\n\nWe may now launch the  instance of `SparkSession`, setting explicitly the `master` node and the port to communicate with the `master`.\n\n::: {#4afb4a5f .cell execution_count=7}\n``` {.python .cell-code}\n# spark =  (\n#     SparkSession.builder \n#             .appName(\"Spark webdata\") \n#             .master(\"spark://localhost:7077\") \n#             .config(\"spark.driver.memory\", \"16G\") \n#             .config(\"spark.driver.maxResultSize\", \"0\") \n#             .getOrCreate()\n# )\n```\n:::\n\n\nSparkUI should be reachable at `localhost:4040`. \n\nThe `master` can be monitored at `localhost:8080`\n\n:::\n\n::: {#e5d15bda .cell execution_count=8}\n``` {.python .cell-code}\nspark = (\n    SparkSession\n        .builder\n        .appName(\"Taming Webdata\")\n        .config(\"spark.driver.memory\", \"16G\") \n        .config(\"spark.driver.maxResultSize\", \"0\")\n        .getOrCreate()\n)\n\nsc = spark._sc\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWARNING: Using incubator modules: jdk.incubator.vector\nUsing Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n26/02/16 10:02:38 WARN Utils: Your hostname, boucheron-Precision-5480, resolves to a loopback address: 127.0.1.1; using 172.23.32.10 instead (on interface eth0)\n26/02/16 10:02:38 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\nUsing Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\nSetting default log level to \"WARN\".\nTo adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n26/02/16 10:02:39 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n```\n:::\n:::\n\n\n### Prepare for `checkpointing`\n\n::: {.callout-note}\n\nHandling this modest dataset will put our Spark server under strain. To make our life easier, we should prepare for caching and \ncheckpointing.\n\n:::\n\n::: {#0dd66f3a .cell execution_count=9}\n``` {.python .cell-code}\n(\n    spark.sparkContext.setCheckpointDir(\"file://\"+ os.getcwd())\n)\n```\n:::\n\n\n### Downloading dataset (if needed)\n\n::: {#4f18d260 .cell execution_count=10}\n``` {.python .cell-code}\npath = './data/webdata.parquet'\n```\n:::\n\n\n::: {#bb9611a8 .cell execution_count=11}\n``` {.python .cell-code}\nif not os.path.exists(path):\n    url = \"https://s-v-b.github.io/IFEBY310/data/webdata.parquet.zip\"\n    r = requests.get(url)\n    z = zipfile.ZipFile(io.BytesIO(r.content))\n    z.extractall(path='./data/')\n```\n:::\n\n\nThe default file system used by the spark object may be the local filesystem or hdfs. We explicitly ask for using the local filesystem.\n\n::: {#3243b2e0 .cell execution_count=12}\n``` {.python .cell-code}\nfile_path = 'file://' + os.path.abspath(path)\n\ndf = spark.read.parquet(file_path)\n```\n:::\n\n\n::: {.callout-note}\n\nHave a look at [Spark UI](http://localhost:4040/jobs/). Did the preceding chunk trigger an action? If yes, how many stages? tasks? \n\nTry \n\n```{.python}\n!tree -L 1 data/webdata.parquet/ \n```\n\nto check  wether the parquet file is partitioned. Check wether the different partitions are read concurrently. \n\n:::\n\n# Let  us select the correct users and build a training dataset\n\nWe construct a ETL (Extract Transform Load) process on this data using the `pyspark.sql` API.\n\n\n::: {.callout-note}\n\nETL (Extract Transform Load) and ELT (Extract Load Transform). Check that you understand these two acronyms. What do they stand for? What is the difference?\n\n:::\n\n## Extraction\n\nHere *extraction* is just about reading the data.\n\n::: {.callout-note}\n\nWhat else coud *extraction* be about?\n\n:::\n\n::: {#89bc1f32 .cell execution_count=13}\n``` {.python .cell-code}\ndf.show(n=3)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n+--------------------+------+-------------------+----------+--------------------+-----------+-------+------+\n|                 xid|action|               date|website_id|                 url|category_id|zipcode|device|\n+--------------------+------+-------------------+----------+--------------------+-----------+-------+------+\n|001ff9b6-5383-422...|     O|2017-01-25 07:02:18|         3|http://www.8chanc...|     1002.0|  11370|   SMP|\n|0056ab7a-3cba-4ed...|     O|2016-12-28 09:47:08|        54|http://www.salair...|     1002.0|  86000|   DSK|\n|005ae4ab-363a-41a...|     O|2017-01-27 22:21:06|        74|http://www.realit...|     1002.0|  49700|   DSK|\n+--------------------+------+-------------------+----------+--------------------+-----------+-------+------+\nonly showing top 3 rows\n```\n:::\n:::\n\n\n::: {.callout-note}\n\nHow many stages and tasks did action `show()` triggered? \n\nLook at the plan details in tab SQL/Dataframe on [Spark UI](http://localhost:4040/jobs/). What are the different steps? Is there any shuffle?\n\n:::\n\nMuch of our work will consist in computing `window` functions over windows defined by a partition over `xid` (users). This comes from \nthe fact that most of our features will be built by inspecting the events related with a given user (`xid`). \nComputing window functions over partitions is likely to trigger shuffles. \n\n::: {#e104acae .cell execution_count=14}\n``` {.python .cell-code}\ndf = df.repartition(20, 'xid')\n```\n:::\n\n\n::: {.callout-note}\n\nIs repartition an action or a transformation?\n\n:::\n\n::: {#eaeb9f53 .cell execution_count=15}\n``` {.python .cell-code}\ndf.cache()\n```\n\n::: {.cell-output .cell-output-display execution_count=13}\n```\nDataFrame[xid: string, action: string, date: timestamp, website_id: string, url: string, category_id: float, zipcode: string, device: string]\n```\n:::\n:::\n\n\n::: {#3606e624 .cell execution_count=16}\n``` {.python .cell-code}\n# sc.setCheckpointDir(\"file://\" + os.path.abspath(os.path.curdir))   \n\ndf.checkpoint()\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n\r[Stage 2:===============================================>         (10 + 2) / 12]\r\r[Stage 4:>                                                        (0 + 20) / 20]\r\r[Stage 4:============================>                           (10 + 10) / 20]\r\r                                                                                \r\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=14}\n```\nDataFrame[xid: string, action: string, date: timestamp, website_id: string, url: string, category_id: float, zipcode: string, device: string]\n```\n:::\n:::\n\n\n::: {.callout-note}\n\n### Questions\n\n- What does `checkpoint()` actually do? \n- Is it different from `df.cache()`?\n- How many jobs did `checkpoint()` trigger?\n- Did `checkpoint()` trigger writing in checkpoint directory?\n\n:::\n\nThe dataframe `df` has been repartitioned in 20 partitions along column `xid`.\nLet us check that the number of distinct values of `xid` is balanced. \n\n::: {#18d40c61 .cell execution_count=17}\n``` {.python .cell-code}\ndf.explain()\n```\n:::\n\n\n::: {.callout-note}\n\nEvaluate  the preceding chunk.  How do the final and initial compare? Identify shuffle stages?\n\n:::\n\n::: {#6329dcba .cell execution_count=18}\n``` {.python .cell-code}\ndef foo(x): yield len(set(x))\n\nn_xid_per_partitions = ( \n    df.rdd\n    .map(lambda x : x.xid)\n    .mapPartitions(foo)\n    .collect()\n)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n\r[Stage 8:>                                                        (0 + 20) / 20]\r\r[Stage 8:==>                                                      (1 + 19) / 20]\r\r[Stage 8:=====>                                                   (2 + 18) / 20]\r\r[Stage 8:=============================================>           (16 + 4) / 20]\r\r                                                                                \r\n```\n:::\n:::\n\n\n::: {.callout-note}\n\n`yield` is a Python keyword that indicates that `foo` is a function *generator*. Try `foo(range(10))` and then `next(foo(range(10)))` and finally\n\n```{.python}\nbar = foo(range(10))\nnext(bar)\nnext(bar)\n```\n\nto guess what this means.\n\n- Read the documentation of `mapPartitions()`\n- Read chapter `Iterators, Generators, ...`  in [Fluent Python](https://www.oreilly.com/library/view/fluent-python-2nd/9781492056348/)\n\n\n:::\n\n\nIn the sequel, we may query dataframe `df`  using `spark.sql()`. To make this possible, we have to register the dataframe as  a temporary view.\n\n::: {#create-webdata-view .cell execution_count=19}\n``` {.python .cell-code}\nspark.sql(\"SHOW TABLES ;\").show()\n\ndf.createOrReplaceTempView(\"webdata\")\n\nspark.sql(\"SHOW TABLES ;\").show()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n+---------+---------+-----------+\n|namespace|tableName|isTemporary|\n+---------+---------+-----------+\n+---------+---------+-----------+\n\n+---------+---------+-----------+\n|namespace|tableName|isTemporary|\n+---------+---------+-----------+\n|         |  webdata|       true|\n+---------+---------+-----------+\n\n```\n:::\n:::\n\n\n::: {#a7435b53 .cell execution_count=20}\n``` {.python .cell-code}\nspark.sql(\"DESC EXTENDED webdata ;\").show()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n+-----------+---------+-------+\n|   col_name|data_type|comment|\n+-----------+---------+-------+\n|        xid|   string|   NULL|\n|     action|   string|   NULL|\n|       date|timestamp|   NULL|\n| website_id|   string|   NULL|\n|        url|   string|   NULL|\n|category_id|    float|   NULL|\n|    zipcode|   string|   NULL|\n|     device|   string|   NULL|\n+-----------+---------+-------+\n\n```\n:::\n:::\n\n\n::: {.callout-note}\n\nWhen handling managed tables (not views) and combined with `ANALYZE TABLE`, `DESC EXTENDED` can provide precious information on column statistics. \n\n:::\n\n\n## Transformation of the data\n\nAt this step we compute a lot of extra things from the data. The aim is to build *features* that describe users. The features will eventually be used by ML (Machine Learning) methods.\n\nEach feature is built by feeding a *transformer* with a dataframe. Each  transformer returns a dataframe with one more column.\n\nThe prospective list of transformers is:\n\n```\nhour_transformer\nweekday_transformer\nn_actions_per_category_id_transformer\nn_days_since_last_action_transformer\nn_days_since_last_event_transformer\nn_events_per_category_id_transformer\nn_events_per_device_transformer\nn_events_per_hour_transformer\nn_events_per_website_id_transformer\nn_events_per_weekday_transformer\nn_unique_day_transformer\nn_unique_device_transformer\nn_unique_hour_transformer\n```\n\nEach transformer (besides `hour_transformer` and `weekday_transformer`)is defined by \n\n- a `Window` object (optional, some are used several times), \n- a column function that defines the added column, and \n- a name  for the added column. \n\nThe transformer is named after the name  of the added column. All transformers have the same signature. \n\n::: {#dec2f439 .cell execution_count=21}\n``` {.python .cell-code}\ndef hour_transformer(df):\n    hour = func.hour(col('date'))\n    df = df.withColumn('hour', hour)\n    return df\n\ndef weekday_transformer(df):\n    weekday = func.date_format(col('date'), 'EEEE')\n    df = df.withColumn('weekday', weekday)\n    return df\n```\n:::\n\n\n::: {#e579c0c0 .cell execution_count=22}\n``` {.python .cell-code}\ndef n_events_transformer(df):\n    xid_partition = Window.partitionBy('xid')\n    n_events = func.count(col('action')).over(xid_partition)\n    \n    df = df.withColumn('n_events', n_events)\n\n    return df\n\ndef n_events_per_action_transformer(df):\n    xid_action_partition = Window.partitionBy('xid', 'action')\n    n_events_per_action = func.count(col('action')).over(xid_action_partition)\n\n    df = df.withColumn('n_events_per_action', n_events_per_action)\n    \n    return df\n\n\n\ndef n_events_per_hour_transformer(df):\n    xid_hour_partition = Window.partitionBy('xid', 'hour')\n    n_events_per_hour = (\n        func.count(col('action')).over(xid_hour_partition)\n    )\n    df = df.withColumn('n_events_per_hour', n_events_per_hour)\n    return df\n\ndef n_events_per_weekday_transformer(df):\n    xid_weekday_partition = Window.partitionBy('xid', 'weekday')\n    n_events_per_weekday = func.count(col('action')).over(xid_weekday_partition)\n    df = df.withColumn('n_events_per_weekday', n_events_per_weekday)\n    return df\n\ndef n_days_since_last_event_transformer(df):\n    xid_partition = Window.partitionBy('xid')\n    max_date = func.max(col('date')).over(xid_partition)\n    n_days_since_last_event = func.datediff(func.current_date(), max_date)\n    df = df.withColumn('n_days_since_last_event',\n                       n_days_since_last_event + lit(0.1))\n    return df\n\ndef n_days_since_last_action_transformer(df):\n    xid_partition_action = Window.partitionBy('xid', 'action')\n    max_date = func.max(col('date')).over(xid_partition_action)\n    n_days_since_last_action = func.datediff(func.current_date(),\n                                                        max_date)\n    df = df.withColumn('n_days_since_last_action',\n                       n_days_since_last_action + lit(0.1))\n    return df\n\ndef n_unique_day_transformer(df):\n    xid_partition = Window.partitionBy('xid')\n    dayofyear = func.dayofyear(col('date'))\n    rank_day = func.dense_rank().over(xid_partition.orderBy(dayofyear))\n    n_unique_day = func.last(rank_day).over(xid_partition)\n    df = df.withColumn('n_unique_day', n_unique_day)\n    return df\n\ndef n_unique_hour_transformer(df):\n    xid_partition = Window.partitionBy('xid')\n    rank_hour = func.dense_rank().over(xid_partition.orderBy('hour'))\n    n_unique_hour = func.last(rank_hour).over(xid_partition)\n    df = df.withColumn('n_unique_hour', n_unique_hour)\n    return df\n\ndef n_events_per_device_transformer(df):\n    xid_device_partition = Window.partitionBy('xid', 'device')\n    n_events_per_device = func.count(func.col('device')) \\\n        .over(xid_device_partition)\n    df = df.withColumn('n_events_per_device', n_events_per_device)\n    return df\n\ndef n_unique_device_transformer(df):\n    xid_partition = Window.partitionBy('xid')\n    rank_device = func.dense_rank().over(xid_partition.orderBy('device'))\n    n_unique_device = func.last(rank_device).over(xid_partition)\n    df = df.withColumn('n_device', n_unique_device)\n    return df\n\ndef n_actions_per_category_id_transformer(df):\n    xid_category_id_partition = Window.partitionBy('xid', 'category_id',\n                                                   'action')\n    n_actions_per_category_id = func.count(func.col('action')) \\\n        .over(xid_category_id_partition)\n    df = df.withColumn('n_actions_per_category_id', n_actions_per_category_id)\n    return df\n\ndef n_unique_category_id_transformer(df):\n    xid_partition = Window.partitionBy('xid')\n    rank_category_id = func.dense_rank().over(xid_partition\\\n                                              .orderBy('category_id'))\n    n_unique_category_id = func.last(rank_category_id).over(xid_partition)\n    df = df.withColumn('n_unique_category_id', n_unique_category_id)\n    return df\n\ndef n_events_per_category_id_transformer(df):\n    xid_category_id_partition = Window.partitionBy('xid', 'category_id')\n    n_events_per_category_id = func.count(func.col('action')) \\\n        .over(xid_category_id_partition)\n    df = df.withColumn('n_events_per_category_id', n_events_per_category_id)\n    return df\n\ndef n_events_per_website_id_transformer(df):\n    xid_website_id_partition = Window.partitionBy('xid', 'website_id')\n    n_events_per_website_id = func.count(col('action'))\\\n        .over(xid_website_id_partition)\n    df = df.withColumn('n_events_per_website_id', n_events_per_website_id)\n    return df\n```\n:::\n\n\n::: {#f3268ed7 .cell execution_count=23}\n``` {.python .cell-code}\ntransformers = [\n    hour_transformer,\n    weekday_transformer,\n    n_events_per_hour_transformer,\n    n_events_per_weekday_transformer,\n    n_days_since_last_event_transformer,\n    n_days_since_last_action_transformer,\n    n_unique_day_transformer,\n    n_unique_hour_transformer,\n    n_events_per_device_transformer,\n    n_unique_device_transformer,\n    n_actions_per_category_id_transformer,\n    n_events_per_category_id_transformer,\n    n_events_per_website_id_transformer,\n]\n```\n:::\n\n\n::: {.callout-warning}\n\n### DRY compliance\n\nThe code above looks repetitive. Its structure is not transparent. \n\nThe code does not emphasize the fact that transformers can not be applied in an arbitrary order (columns `hour`  and `weekday` should be built before columns `n_events_per_hour`, `n_events_per_weekday`).\n\nIs it possible to take advantage of the fact that the dataframe can be partitioned according to `xid` in advance? \n\n:::\n\n\nAs a warmup  we first apply the two transformers that do not involve computing window functions. \n\n::: {#77d2b0f3 .cell execution_count=24}\n``` {.python .cell-code}\ndf = hour_transformer(df)\ndf = weekday_transformer(df)\n```\n:::\n\n\n::: {#28cbaaca .cell execution_count=25}\n``` {.python .cell-code}\ndf.cache()\n```\n\n::: {.cell-output .cell-output-display execution_count=22}\n```\nDataFrame[xid: string, action: string, date: timestamp, website_id: string, url: string, category_id: float, zipcode: string, device: string, hour: int, weekday: string]\n```\n:::\n:::\n\n\n::: {#7d474008 .cell execution_count=26}\n``` {.python .cell-code}\ndf.checkpoint()\n```\n\n::: {.cell-output .cell-output-display execution_count=23}\n```\nDataFrame[xid: string, action: string, date: timestamp, website_id: string, url: string, category_id: float, zipcode: string, device: string, hour: int, weekday: string]\n```\n:::\n:::\n\n\n::: {#6529486b .cell execution_count=27}\n``` {.python .cell-code}\ndf.explain()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n== Physical Plan ==\nAdaptiveSparkPlan isFinalPlan=true\n+- == Final Plan ==\n   ResultQueryStage 1\n   +- TableCacheQueryStage 0\n      +- InMemoryTableScan [xid#0, action#1, date#2, website_id#3, url#4, category_id#5, zipcode#6, device#7, hour#283, weekday#284]\n            +- InMemoryRelation [xid#0, action#1, date#2, website_id#3, url#4, category_id#5, zipcode#6, device#7, hour#283, weekday#284], StorageLevel(disk, memory, deserialized, 1 replicas)\n                  +- AdaptiveSparkPlan isFinalPlan=true\n                  +- == Final Plan ==\n                     ResultQueryStage 1\n                     +- *(1) Project [xid#0, action#1, date#2, website_id#3, url#4, category_id#5, zipcode#6, device#7, hour(date#2, Some(Europe/Paris)) AS hour#283, date_format(date#2, EEEE, Some(Europe/Paris)) AS weekday#284]\n                        +- TableCacheQueryStage 0\n                           +- InMemoryTableScan [action#1, category_id#5, date#2, device#7, url#4, website_id#3, xid#0, zipcode#6]\n                                 +- InMemoryRelation [xid#0, action#1, date#2, website_id#3, url#4, category_id#5, zipcode#6, device#7], StorageLevel(disk, memory, deserialized, 1 replicas)\n                                       +- AdaptiveSparkPlan isFinalPlan=true\n                     +- == Final Plan ==\n                        ResultQueryStage 1\n                        +- ShuffleQueryStage 0\n                           +- Exchange hashpartitioning(xid#0, 20), REPARTITION_BY_NUM, [plan_id=37]\n                              +- *(1) ColumnarToRow\n                                 +- FileScan parquet [xid#0,action#1,date#2,website_id#3,url#4,category_id#5,zipcode#6,device#7] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/home/boucheron/Documents/IFEBY310/core/notebooks/data/webdata.pa..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<xid:string,action:string,date:timestamp,website_id:string,url:string,category_id:float,zip...\n                     +- == Initial Plan ==\n                        Exchange hashpartitioning(xid#0, 20), REPARTITION_BY_NUM, [plan_id=22]\n                        +- FileScan parquet [xid#0,action#1,date#2,website_id#3,url#4,category_id#5,zipcode#6,device#7] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/home/boucheron/Documents/IFEBY310/core/notebooks/data/webdata.pa..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<xid:string,action:string,date:timestamp,website_id:string,url:string,category_id:float,zip...\n                  +- == Initial Plan ==\n                     Project [xid#0, action#1, date#2, website_id#3, url#4, category_id#5, zipcode#6, device#7, hour(date#2, Some(Europe/Paris)) AS hour#283, date_format(date#2, EEEE, Some(Europe/Paris)) AS weekday#284]\n                     +- InMemoryTableScan [action#1, category_id#5, date#2, device#7, url#4, website_id#3, xid#0, zipcode#6]\n                           +- InMemoryRelation [xid#0, action#1, date#2, website_id#3, url#4, category_id#5, zipcode#6, device#7], StorageLevel(disk, memory, deserialized, 1 replicas)\n                                 +- AdaptiveSparkPlan isFinalPlan=true\n               +- == Final Plan ==\n                  ResultQueryStage 1\n                  +- ShuffleQueryStage 0\n                     +- Exchange hashpartitioning(xid#0, 20), REPARTITION_BY_NUM, [plan_id=37]\n                        +- *(1) ColumnarToRow\n                           +- FileScan parquet [xid#0,action#1,date#2,website_id#3,url#4,category_id#5,zipcode#6,device#7] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/home/boucheron/Documents/IFEBY310/core/notebooks/data/webdata.pa..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<xid:string,action:string,date:timestamp,website_id:string,url:string,category_id:float,zip...\n               +- == Initial Plan ==\n                  Exchange hashpartitioning(xid#0, 20), REPARTITION_BY_NUM, [plan_id=22]\n                  +- FileScan parquet [xid#0,action#1,date#2,website_id#3,url#4,category_id#5,zipcode#6,device#7] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/home/boucheron/Documents/IFEBY310/core/notebooks/data/webdata.pa..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<xid:string,action:string,date:timestamp,website_id:string,url:string,category_id:float,zip...\n+- == Initial Plan ==\n   InMemoryTableScan [xid#0, action#1, date#2, website_id#3, url#4, category_id#5, zipcode#6, device#7, hour#283, weekday#284]\n      +- InMemoryRelation [xid#0, action#1, date#2, website_id#3, url#4, category_id#5, zipcode#6, device#7, hour#283, weekday#284], StorageLevel(disk, memory, deserialized, 1 replicas)\n            +- AdaptiveSparkPlan isFinalPlan=true\n            +- == Final Plan ==\n               ResultQueryStage 1\n               +- *(1) Project [xid#0, action#1, date#2, website_id#3, url#4, category_id#5, zipcode#6, device#7, hour(date#2, Some(Europe/Paris)) AS hour#283, date_format(date#2, EEEE, Some(Europe/Paris)) AS weekday#284]\n                  +- TableCacheQueryStage 0\n                     +- InMemoryTableScan [action#1, category_id#5, date#2, device#7, url#4, website_id#3, xid#0, zipcode#6]\n                           +- InMemoryRelation [xid#0, action#1, date#2, website_id#3, url#4, category_id#5, zipcode#6, device#7], StorageLevel(disk, memory, deserialized, 1 replicas)\n                                 +- AdaptiveSparkPlan isFinalPlan=true\n                     +- == Final Plan ==\n                        ResultQueryStage 1\n                        +- ShuffleQueryStage 0\n                           +- Exchange hashpartitioning(xid#0, 20), REPARTITION_BY_NUM, [plan_id=37]\n                              +- *(1) ColumnarToRow\n                                 +- FileScan parquet [xid#0,action#1,date#2,website_id#3,url#4,category_id#5,zipcode#6,device#7] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/home/boucheron/Documents/IFEBY310/core/notebooks/data/webdata.pa..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<xid:string,action:string,date:timestamp,website_id:string,url:string,category_id:float,zip...\n                     +- == Initial Plan ==\n                        Exchange hashpartitioning(xid#0, 20), REPARTITION_BY_NUM, [plan_id=22]\n                        +- FileScan parquet [xid#0,action#1,date#2,website_id#3,url#4,category_id#5,zipcode#6,device#7] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/home/boucheron/Documents/IFEBY310/core/notebooks/data/webdata.pa..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<xid:string,action:string,date:timestamp,website_id:string,url:string,category_id:float,zip...\n            +- == Initial Plan ==\n               Project [xid#0, action#1, date#2, website_id#3, url#4, category_id#5, zipcode#6, device#7, hour(date#2, Some(Europe/Paris)) AS hour#283, date_format(date#2, EEEE, Some(Europe/Paris)) AS weekday#284]\n               +- InMemoryTableScan [action#1, category_id#5, date#2, device#7, url#4, website_id#3, xid#0, zipcode#6]\n                     +- InMemoryRelation [xid#0, action#1, date#2, website_id#3, url#4, category_id#5, zipcode#6, device#7], StorageLevel(disk, memory, deserialized, 1 replicas)\n                           +- AdaptiveSparkPlan isFinalPlan=true\n               +- == Final Plan ==\n                  ResultQueryStage 1\n                  +- ShuffleQueryStage 0\n                     +- Exchange hashpartitioning(xid#0, 20), REPARTITION_BY_NUM, [plan_id=37]\n                        +- *(1) ColumnarToRow\n                           +- FileScan parquet [xid#0,action#1,date#2,website_id#3,url#4,category_id#5,zipcode#6,device#7] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/home/boucheron/Documents/IFEBY310/core/notebooks/data/webdata.pa..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<xid:string,action:string,date:timestamp,website_id:string,url:string,category_id:float,zip...\n               +- == Initial Plan ==\n                  Exchange hashpartitioning(xid#0, 20), REPARTITION_BY_NUM, [plan_id=22]\n                  +- FileScan parquet [xid#0,action#1,date#2,website_id#3,url#4,category_id#5,zipcode#6,device#7] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/home/boucheron/Documents/IFEBY310/core/notebooks/data/webdata.pa..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<xid:string,action:string,date:timestamp,website_id:string,url:string,category_id:float,zip...\n\n\n```\n:::\n:::\n\n\n::: {.callout-note}\n\n- Did the two tranformations trigger jobs executions?\n- Did `cache()` trigger a job?  (check on Spark UI)\n- Did `checkpoint()` trigger  job(s)? If yes, could you spot shuffle stages?\n\n:::\n\n\nNext we run the transformers that only involve `partitionBy('xid')`.\n\n::: {#9e0ee9ec .cell execution_count=28}\n``` {.python .cell-code}\ndf = n_events_transformer(df)\ndf = n_days_since_last_event_transformer(df)\ndf = n_unique_day_transformer(df)\ndf = n_unique_hour_transformer(df)\ndf = n_unique_device_transformer(df)\ndf = n_unique_category_id_transformer(df)\n```\n:::\n\n\nRun `df.explain()`. \n\n\n::: {.callout-note}\n\n:::\n\n::: {#e42384ed .cell execution_count=29}\n``` {.python .cell-code}\ndf.checkpoint()\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n\r[Stage 14:>                                                       (0 + 20) / 20]\r\r                                                                                \r\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=26}\n```\nDataFrame[xid: string, action: string, date: timestamp, website_id: string, url: string, category_id: float, zipcode: string, device: string, hour: int, weekday: string, n_events: bigint, n_days_since_last_event: double, n_unique_day: int, n_unique_hour: int, n_device: int, n_unique_category_id: int]\n```\n:::\n:::\n\n\n::: {.callout-note}\n\n\n- How many jobs were triggered by `checkpoint()` ?\n- How many stages ? tasks ?\n- Could you spot shuffle reads and writes ?\n- Have a look at detailed plans on Spark UI.  \n\n:::\n\n\nWe run now the other transformers. These ones rely on refined partitions.\n\n::: {#e0c0d0d6 .cell execution_count=30}\n``` {.python .cell-code}\ndf = n_events_per_action_transformer(df)\n\ndf.checkpoint()\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n\r[Stage 16:>                                                       (0 + 20) / 20]\r\r[Stage 16:==>                                                     (1 + 19) / 20]\r\r[Stage 16:===================>                                    (7 + 13) / 20]\r\r[Stage 16:=====================================================>  (19 + 1) / 20]\r\r                                                                                \r\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=27}\n```\nDataFrame[xid: string, action: string, date: timestamp, website_id: string, url: string, category_id: float, zipcode: string, device: string, hour: int, weekday: string, n_events: bigint, n_days_since_last_event: double, n_unique_day: int, n_unique_hour: int, n_device: int, n_unique_category_id: int, n_events_per_action: bigint]\n```\n:::\n:::\n\n\n::: {.callout-note}\n\n- How many jobs were triggered by `checkpoint()` ?\n- How many stages ? tasks ?\n- Could you spot shuffle reads and writes ?\n- Have a look at detailed plans on Spark UI.  \n\n:::\n\n::: {#88966487 .cell execution_count=31}\n``` {.python .cell-code}\ndf = n_events_per_hour_transformer(df)\ndf = n_events_per_weekday_transformer(df)\ndf = n_days_since_last_event_transformer(df)\ndf = n_days_since_last_action_transformer(df)\ndf = n_events_per_device_transformer(df)\ndf = n_actions_per_category_id_transformer(df)\ndf = n_events_per_category_id_transformer(df)\ndf = n_events_per_website_id_transformer(df)\n```\n:::\n\n\n::: {#11f5f948 .cell execution_count=32}\n``` {.python .cell-code}\ndf.checkpoint()\n\ndf.explain(mode=\"codegen\")\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n\r[Stage 18:>                                                       (0 + 20) / 20]\r\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nFound 14 WholeStageCodegen subtrees.\n== Subtree 1 / 14 (maxMethodCodeSize:154; maxConstantPoolSize:134(0.20% used); numInnerClasses:0) ==\n*(1) Sort [xid#0 ASC NULLS FIRST], false, 0\n+- TableCacheQueryStage 0\n   +- InMemoryTableScan [xid#0, action#1, date#2, website_id#3, url#4, category_id#5, zipcode#6, device#7, hour#283, weekday#284]\n         +- InMemoryRelation [xid#0, action#1, date#2, website_id#3, url#4, category_id#5, zipcode#6, device#7, hour#283, weekday#284], StorageLevel(disk, memory, deserialized, 1 replicas)\n               +- AdaptiveSparkPlan isFinalPlan=true\n                  +- == Final Plan ==\n                     ResultQueryStage 1\n                     +- *(1) Project [xid#0, action#1, date#2, website_id#3, url#4, category_id#5, zipcode#6, device#7, hour(date#2, Some(Europe/Paris)) AS hour#283, date_format(date#2, EEEE, Some(Europe/Paris)) AS weekday#284]\n                        +- TableCacheQueryStage 0\n                           +- InMemoryTableScan [action#1, category_id#5, date#2, device#7, url#4, website_id#3, xid#0, zipcode#6]\n                                 +- InMemoryRelation [xid#0, action#1, date#2, website_id#3, url#4, category_id#5, zipcode#6, device#7], StorageLevel(disk, memory, deserialized, 1 replicas)\n                                       +- AdaptiveSparkPlan isFinalPlan=true\n                     +- == Final Plan ==\n                        ResultQueryStage 1\n                        +- ShuffleQueryStage 0\n                           +- Exchange hashpartitioning(xid#0, 20), REPARTITION_BY_NUM, [plan_id=37]\n                              +- *(1) ColumnarToRow\n                                 +- FileScan parquet [xid#0,action#1,date#2,website_id#3,url#4,category_id#5,zipcode#6,device#7] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/home/boucheron/Documents/IFEBY310/core/notebooks/data/webdata.pa..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<xid:string,action:string,date:timestamp,website_id:string,url:string,category_id:float,zip...\n                     +- == Initial Plan ==\n                        Exchange hashpartitioning(xid#0, 20), REPARTITION_BY_NUM, [plan_id=22]\n                        +- FileScan parquet [xid#0,action#1,date#2,website_id#3,url#4,category_id#5,zipcode#6,device#7] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/home/boucheron/Documents/IFEBY310/core/notebooks/data/webdata.pa..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<xid:string,action:string,date:timestamp,website_id:string,url:string,category_id:float,zip...\n                  +- == Initial Plan ==\n                     Project [xid#0, action#1, date#2, website_id#3, url#4, category_id#5, zipcode#6, device#7, hour(date#2, Some(Europe/Paris)) AS hour#283, date_format(date#2, EEEE, Some(Europe/Paris)) AS weekday#284]\n                     +- InMemoryTableScan [action#1, category_id#5, date#2, device#7, url#4, website_id#3, xid#0, zipcode#6]\n                           +- InMemoryRelation [xid#0, action#1, date#2, website_id#3, url#4, category_id#5, zipcode#6, device#7], StorageLevel(disk, memory, deserialized, 1 replicas)\n                                 +- AdaptiveSparkPlan isFinalPlan=true\n               +- == Final Plan ==\n                  ResultQueryStage 1\n                  +- ShuffleQueryStage 0\n                     +- Exchange hashpartitioning(xid#0, 20), REPARTITION_BY_NUM, [plan_id=37]\n                        +- *(1) ColumnarToRow\n                           +- FileScan parquet [xid#0,action#1,date#2,website_id#3,url#4,category_id#5,zipcode#6,device#7] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/home/boucheron/Documents/IFEBY310/core/notebooks/data/webdata.pa..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<xid:string,action:string,date:timestamp,website_id:string,url:string,category_id:float,zip...\n               +- == Initial Plan ==\n                  Exchange hashpartitioning(xid#0, 20), REPARTITION_BY_NUM, [plan_id=22]\n                  +- FileScan parquet [xid#0,action#1,date#2,website_id#3,url#4,category_id#5,zipcode#6,device#7] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/home/boucheron/Documents/IFEBY310/core/notebooks/data/webdata.pa..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<xid:string,action:string,date:timestamp,website_id:string,url:string,category_id:float,zip...\n\nGenerated code:\n/* 001 */ public Object generate(Object[] references) {\n/* 002 */   return new GeneratedIteratorForCodegenStage1(references);\n/* 003 */ }\n/* 004 */\n/* 005 */ // codegenStageId=1\n/* 006 */ final class GeneratedIteratorForCodegenStage1 extends org.apache.spark.sql.execution.BufferedRowIterator {\n/* 007 */   private Object[] references;\n/* 008 */   private scala.collection.Iterator[] inputs;\n/* 009 */   private boolean sort_needToSort_0;\n/* 010 */   private org.apache.spark.sql.execution.UnsafeExternalRowSorter sort_sorter_0;\n/* 011 */   private org.apache.spark.executor.TaskMetrics sort_metrics_0;\n/* 012 */   private scala.collection.Iterator<UnsafeRow> sort_sortedIter_0;\n/* 013 */   private scala.collection.Iterator inputadapter_input_0;\n/* 014 */\n/* 015 */   public GeneratedIteratorForCodegenStage1(Object[] references) {\n/* 016 */     this.references = references;\n/* 017 */   }\n/* 018 */\n/* 019 */   public void init(int index, scala.collection.Iterator[] inputs) {\n/* 020 */     partitionIndex = index;\n/* 021 */     this.inputs = inputs;\n/* 022 */     sort_needToSort_0 = true;\n/* 023 */     sort_sorter_0 = ((org.apache.spark.sql.execution.SortExec) references[0] /* plan */).createSorter();\n/* 024 */     sort_metrics_0 = org.apache.spark.TaskContext.get().taskMetrics();\n/* 025 */\n/* 026 */     inputadapter_input_0 = inputs[0];\n/* 027 */\n/* 028 */   }\n/* 029 */\n/* 030 */   private void sort_addToSorter_0() throws java.io.IOException {\n/* 031 */     while ( inputadapter_input_0.hasNext()) {\n/* 032 */       InternalRow inputadapter_row_0 = (InternalRow) inputadapter_input_0.next();\n/* 033 */\n/* 034 */       sort_sorter_0.insertRow((UnsafeRow)inputadapter_row_0);\n/* 035 */       // shouldStop check is eliminated\n/* 036 */     }\n/* 037 */\n/* 038 */   }\n/* 039 */\n/* 040 */   protected void processNext() throws java.io.IOException {\n/* 041 */     if (sort_needToSort_0) {\n/* 042 */       long sort_spillSizeBefore_0 = sort_metrics_0.memoryBytesSpilled();\n/* 043 */       sort_addToSorter_0();\n/* 044 */       sort_sortedIter_0 = sort_sorter_0.sort();\n/* 045 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[3] /* sortTime */).add(sort_sorter_0.getSortTimeNanos() / 1000000);\n/* 046 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[1] /* peakMemory */).add(sort_sorter_0.getPeakMemoryUsage());\n/* 047 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[2] /* spillSize */).add(sort_metrics_0.memoryBytesSpilled() - sort_spillSizeBefore_0);\n/* 048 */       sort_metrics_0.incPeakExecutionMemory(sort_sorter_0.getPeakMemoryUsage());\n/* 049 */       sort_needToSort_0 = false;\n/* 050 */     }\n/* 051 */\n/* 052 */     while ( sort_sortedIter_0.hasNext()) {\n/* 053 */       UnsafeRow sort_outputRow_0 = (UnsafeRow)sort_sortedIter_0.next();\n/* 054 */\n/* 055 */       append(sort_outputRow_0);\n/* 056 */\n/* 057 */       if (shouldStop()) return;\n/* 058 */     }\n/* 059 */   }\n/* 060 */\n/* 061 */ }\n\n== Subtree 2 / 14 (maxMethodCodeSize:746; maxConstantPoolSize:210(0.32% used); numInnerClasses:0) ==\n*(2) Sort [xid#0 ASC NULLS FIRST, _w0#711 ASC NULLS FIRST], false, 0\n+- *(2) Project [xid#0, action#1, date#2, website_id#3, url#4, category_id#5, zipcode#6, device#7, hour#283, weekday#284, n_events#695L, dayofyear(cast(date#2 as date)) AS _w0#711]\n   +- Window [count(action#1) windowspecdefinition(xid#0, specifiedwindowframe(RowFrame, unboundedpreceding$(), unboundedfollowing$())) AS n_events#695L], [xid#0]\n      +- *(1) Sort [xid#0 ASC NULLS FIRST], false, 0\n         +- TableCacheQueryStage 0\n            +- InMemoryTableScan [xid#0, action#1, date#2, website_id#3, url#4, category_id#5, zipcode#6, device#7, hour#283, weekday#284]\n                  +- InMemoryRelation [xid#0, action#1, date#2, website_id#3, url#4, category_id#5, zipcode#6, device#7, hour#283, weekday#284], StorageLevel(disk, memory, deserialized, 1 replicas)\n                        +- AdaptiveSparkPlan isFinalPlan=true\n                           +- == Final Plan ==\n                              ResultQueryStage 1\n                              +- *(1) Project [xid#0, action#1, date#2, website_id#3, url#4, category_id#5, zipcode#6, device#7, hour(date#2, Some(Europe/Paris)) AS hour#283, date_format(date#2, EEEE, Some(Europe/Paris)) AS weekday#284]\n                                 +- TableCacheQueryStage 0\n                                    +- InMemoryTableScan [action#1, category_id#5, date#2, device#7, url#4, website_id#3, xid#0, zipcode#6]\n                                          +- InMemoryRelation [xid#0, action#1, date#2, website_id#3, url#4, category_id#5, zipcode#6, device#7], StorageLevel(disk, memory, deserialized, 1 replicas)\n                                                +- AdaptiveSparkPlan isFinalPlan=true\n                     +- == Final Plan ==\n                        ResultQueryStage 1\n                        +- ShuffleQueryStage 0\n                           +- Exchange hashpartitioning(xid#0, 20), REPARTITION_BY_NUM, [plan_id=37]\n                              +- *(1) ColumnarToRow\n                                 +- FileScan parquet [xid#0,action#1,date#2,website_id#3,url#4,category_id#5,zipcode#6,device#7] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/home/boucheron/Documents/IFEBY310/core/notebooks/data/webdata.pa..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<xid:string,action:string,date:timestamp,website_id:string,url:string,category_id:float,zip...\n                     +- == Initial Plan ==\n                        Exchange hashpartitioning(xid#0, 20), REPARTITION_BY_NUM, [plan_id=22]\n                        +- FileScan parquet [xid#0,action#1,date#2,website_id#3,url#4,category_id#5,zipcode#6,device#7] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/home/boucheron/Documents/IFEBY310/core/notebooks/data/webdata.pa..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<xid:string,action:string,date:timestamp,website_id:string,url:string,category_id:float,zip...\n                           +- == Initial Plan ==\n                              Project [xid#0, action#1, date#2, website_id#3, url#4, category_id#5, zipcode#6, device#7, hour(date#2, Some(Europe/Paris)) AS hour#283, date_format(date#2, EEEE, Some(Europe/Paris)) AS weekday#284]\n                              +- InMemoryTableScan [action#1, category_id#5, date#2, device#7, url#4, website_id#3, xid#0, zipcode#6]\n                                    +- InMemoryRelation [xid#0, action#1, date#2, website_id#3, url#4, category_id#5, zipcode#6, device#7], StorageLevel(disk, memory, deserialized, 1 replicas)\n                                          +- AdaptiveSparkPlan isFinalPlan=true\n               +- == Final Plan ==\n                  ResultQueryStage 1\n                  +- ShuffleQueryStage 0\n                     +- Exchange hashpartitioning(xid#0, 20), REPARTITION_BY_NUM, [plan_id=37]\n                        +- *(1) ColumnarToRow\n                           +- FileScan parquet [xid#0,action#1,date#2,website_id#3,url#4,category_id#5,zipcode#6,device#7] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/home/boucheron/Documents/IFEBY310/core/notebooks/data/webdata.pa..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<xid:string,action:string,date:timestamp,website_id:string,url:string,category_id:float,zip...\n               +- == Initial Plan ==\n                  Exchange hashpartitioning(xid#0, 20), REPARTITION_BY_NUM, [plan_id=22]\n                  +- FileScan parquet [xid#0,action#1,date#2,website_id#3,url#4,category_id#5,zipcode#6,device#7] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/home/boucheron/Documents/IFEBY310/core/notebooks/data/webdata.pa..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<xid:string,action:string,date:timestamp,website_id:string,url:string,category_id:float,zip...\n\nGenerated code:\n/* 001 */ public Object generate(Object[] references) {\n/* 002 */   return new GeneratedIteratorForCodegenStage2(references);\n/* 003 */ }\n/* 004 */\n/* 005 */ // codegenStageId=2\n/* 006 */ final class GeneratedIteratorForCodegenStage2 extends org.apache.spark.sql.execution.BufferedRowIterator {\n/* 007 */   private Object[] references;\n/* 008 */   private scala.collection.Iterator[] inputs;\n/* 009 */   private boolean sort_needToSort_0;\n/* 010 */   private org.apache.spark.sql.execution.UnsafeExternalRowSorter sort_sorter_0;\n/* 011 */   private org.apache.spark.executor.TaskMetrics sort_metrics_0;\n/* 012 */   private scala.collection.Iterator<UnsafeRow> sort_sortedIter_0;\n/* 013 */   private scala.collection.Iterator inputadapter_input_0;\n/* 014 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] project_mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];\n/* 015 */\n/* 016 */   public GeneratedIteratorForCodegenStage2(Object[] references) {\n/* 017 */     this.references = references;\n/* 018 */   }\n/* 019 */\n/* 020 */   public void init(int index, scala.collection.Iterator[] inputs) {\n/* 021 */     partitionIndex = index;\n/* 022 */     this.inputs = inputs;\n/* 023 */     sort_needToSort_0 = true;\n/* 024 */     sort_sorter_0 = ((org.apache.spark.sql.execution.SortExec) references[0] /* plan */).createSorter();\n/* 025 */     sort_metrics_0 = org.apache.spark.TaskContext.get().taskMetrics();\n/* 026 */\n/* 027 */     inputadapter_input_0 = inputs[0];\n/* 028 */     project_mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(12, 224);\n/* 029 */\n/* 030 */   }\n/* 031 */\n/* 032 */   private void sort_addToSorter_0() throws java.io.IOException {\n/* 033 */     while ( inputadapter_input_0.hasNext()) {\n/* 034 */       InternalRow inputadapter_row_0 = (InternalRow) inputadapter_input_0.next();\n/* 035 */\n/* 036 */       boolean inputadapter_isNull_2 = inputadapter_row_0.isNullAt(2);\n/* 037 */       long inputadapter_value_2 = inputadapter_isNull_2 ?\n/* 038 */       -1L : (inputadapter_row_0.getLong(2));\n/* 039 */\n/* 040 */       // common sub-expressions\n/* 041 */\n/* 042 */       boolean inputadapter_isNull_0 = inputadapter_row_0.isNullAt(0);\n/* 043 */       UTF8String inputadapter_value_0 = inputadapter_isNull_0 ?\n/* 044 */       null : (inputadapter_row_0.getUTF8String(0));\n/* 045 */       boolean inputadapter_isNull_1 = inputadapter_row_0.isNullAt(1);\n/* 046 */       UTF8String inputadapter_value_1 = inputadapter_isNull_1 ?\n/* 047 */       null : (inputadapter_row_0.getUTF8String(1));\n/* 048 */       boolean inputadapter_isNull_3 = inputadapter_row_0.isNullAt(3);\n/* 049 */       UTF8String inputadapter_value_3 = inputadapter_isNull_3 ?\n/* 050 */       null : (inputadapter_row_0.getUTF8String(3));\n/* 051 */       boolean inputadapter_isNull_4 = inputadapter_row_0.isNullAt(4);\n/* 052 */       UTF8String inputadapter_value_4 = inputadapter_isNull_4 ?\n/* 053 */       null : (inputadapter_row_0.getUTF8String(4));\n/* 054 */       boolean inputadapter_isNull_5 = inputadapter_row_0.isNullAt(5);\n/* 055 */       float inputadapter_value_5 = inputadapter_isNull_5 ?\n/* 056 */       -1.0f : (inputadapter_row_0.getFloat(5));\n/* 057 */       boolean inputadapter_isNull_6 = inputadapter_row_0.isNullAt(6);\n/* 058 */       UTF8String inputadapter_value_6 = inputadapter_isNull_6 ?\n/* 059 */       null : (inputadapter_row_0.getUTF8String(6));\n/* 060 */       boolean inputadapter_isNull_7 = inputadapter_row_0.isNullAt(7);\n/* 061 */       UTF8String inputadapter_value_7 = inputadapter_isNull_7 ?\n/* 062 */       null : (inputadapter_row_0.getUTF8String(7));\n/* 063 */       boolean inputadapter_isNull_8 = inputadapter_row_0.isNullAt(8);\n/* 064 */       int inputadapter_value_8 = inputadapter_isNull_8 ?\n/* 065 */       -1 : (inputadapter_row_0.getInt(8));\n/* 066 */       boolean inputadapter_isNull_9 = inputadapter_row_0.isNullAt(9);\n/* 067 */       UTF8String inputadapter_value_9 = inputadapter_isNull_9 ?\n/* 068 */       null : (inputadapter_row_0.getUTF8String(9));\n/* 069 */       long inputadapter_value_10 = inputadapter_row_0.getLong(10);\n/* 070 */       boolean project_isNull_12 = inputadapter_isNull_2;\n/* 071 */       int project_value_12 = -1;\n/* 072 */       if (!inputadapter_isNull_2) {\n/* 073 */         project_value_12 = org.apache.spark.sql.catalyst.util.DateTimeUtils.microsToDays(inputadapter_value_2, ((java.time.ZoneId) references[1] /* zoneId */));\n/* 074 */       }\n/* 075 */       boolean project_isNull_11 = project_isNull_12;\n/* 076 */       int project_value_11 = -1;\n/* 077 */\n/* 078 */       if (!project_isNull_12) {\n/* 079 */         project_value_11 = org.apache.spark.sql.catalyst.util.DateTimeUtils.getDayInYear(project_value_12);\n/* 080 */       }\n/* 081 */       project_mutableStateArray_0[0].reset();\n/* 082 */\n/* 083 */       project_mutableStateArray_0[0].zeroOutNullBytes();\n/* 084 */\n/* 085 */       if (inputadapter_isNull_0) {\n/* 086 */         project_mutableStateArray_0[0].setNullAt(0);\n/* 087 */       } else {\n/* 088 */         project_mutableStateArray_0[0].write(0, inputadapter_value_0);\n/* 089 */       }\n/* 090 */\n/* 091 */       if (inputadapter_isNull_1) {\n/* 092 */         project_mutableStateArray_0[0].setNullAt(1);\n/* 093 */       } else {\n/* 094 */         project_mutableStateArray_0[0].write(1, inputadapter_value_1);\n/* 095 */       }\n/* 096 */\n/* 097 */       if (inputadapter_isNull_2) {\n/* 098 */         project_mutableStateArray_0[0].setNullAt(2);\n/* 099 */       } else {\n/* 100 */         project_mutableStateArray_0[0].write(2, inputadapter_value_2);\n/* 101 */       }\n/* 102 */\n/* 103 */       if (inputadapter_isNull_3) {\n/* 104 */         project_mutableStateArray_0[0].setNullAt(3);\n/* 105 */       } else {\n/* 106 */         project_mutableStateArray_0[0].write(3, inputadapter_value_3);\n/* 107 */       }\n/* 108 */\n/* 109 */       if (inputadapter_isNull_4) {\n/* 110 */         project_mutableStateArray_0[0].setNullAt(4);\n/* 111 */       } else {\n/* 112 */         project_mutableStateArray_0[0].write(4, inputadapter_value_4);\n/* 113 */       }\n/* 114 */\n/* 115 */       if (inputadapter_isNull_5) {\n/* 116 */         project_mutableStateArray_0[0].setNullAt(5);\n/* 117 */       } else {\n/* 118 */         project_mutableStateArray_0[0].write(5, inputadapter_value_5);\n/* 119 */       }\n/* 120 */\n/* 121 */       if (inputadapter_isNull_6) {\n/* 122 */         project_mutableStateArray_0[0].setNullAt(6);\n/* 123 */       } else {\n/* 124 */         project_mutableStateArray_0[0].write(6, inputadapter_value_6);\n/* 125 */       }\n/* 126 */\n/* 127 */       if (inputadapter_isNull_7) {\n/* 128 */         project_mutableStateArray_0[0].setNullAt(7);\n/* 129 */       } else {\n/* 130 */         project_mutableStateArray_0[0].write(7, inputadapter_value_7);\n/* 131 */       }\n/* 132 */\n/* 133 */       if (inputadapter_isNull_8) {\n/* 134 */         project_mutableStateArray_0[0].setNullAt(8);\n/* 135 */       } else {\n/* 136 */         project_mutableStateArray_0[0].write(8, inputadapter_value_8);\n/* 137 */       }\n/* 138 */\n/* 139 */       if (inputadapter_isNull_9) {\n/* 140 */         project_mutableStateArray_0[0].setNullAt(9);\n/* 141 */       } else {\n/* 142 */         project_mutableStateArray_0[0].write(9, inputadapter_value_9);\n/* 143 */       }\n/* 144 */\n/* 145 */       project_mutableStateArray_0[0].write(10, inputadapter_value_10);\n/* 146 */\n/* 147 */       if (project_isNull_11) {\n/* 148 */         project_mutableStateArray_0[0].setNullAt(11);\n/* 149 */       } else {\n/* 150 */         project_mutableStateArray_0[0].write(11, project_value_11);\n/* 151 */       }\n/* 152 */       sort_sorter_0.insertRow((UnsafeRow)(project_mutableStateArray_0[0].getRow()));\n/* 153 */       // shouldStop check is eliminated\n/* 154 */     }\n/* 155 */\n/* 156 */   }\n/* 157 */\n/* 158 */   protected void processNext() throws java.io.IOException {\n/* 159 */     if (sort_needToSort_0) {\n/* 160 */       long sort_spillSizeBefore_0 = sort_metrics_0.memoryBytesSpilled();\n/* 161 */       sort_addToSorter_0();\n/* 162 */       sort_sortedIter_0 = sort_sorter_0.sort();\n/* 163 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[4] /* sortTime */).add(sort_sorter_0.getSortTimeNanos() / 1000000);\n/* 164 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[2] /* peakMemory */).add(sort_sorter_0.getPeakMemoryUsage());\n/* 165 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[3] /* spillSize */).add(sort_metrics_0.memoryBytesSpilled() - sort_spillSizeBefore_0);\n/* 166 */       sort_metrics_0.incPeakExecutionMemory(sort_sorter_0.getPeakMemoryUsage());\n/* 167 */       sort_needToSort_0 = false;\n/* 168 */     }\n/* 169 */\n/* 170 */     while ( sort_sortedIter_0.hasNext()) {\n/* 171 */       UnsafeRow sort_outputRow_0 = (UnsafeRow)sort_sortedIter_0.next();\n/* 172 */\n/* 173 */       append(sort_outputRow_0);\n/* 174 */\n/* 175 */       if (shouldStop()) return;\n/* 176 */     }\n/* 177 */   }\n/* 178 */\n/* 179 */ }\n\n== Subtree 3 / 14 (maxMethodCodeSize:698; maxConstantPoolSize:132(0.20% used); numInnerClasses:0) ==\n*(3) Project [xid#0, action#1, date#2, website_id#3, url#4, category_id#5, zipcode#6, device#7, hour#283, weekday#284, n_events#695L, _w0#710]\n+- Window [dense_rank(_w0#711) windowspecdefinition(xid#0, _w0#711 ASC NULLS FIRST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS _w0#710], [xid#0], [_w0#711 ASC NULLS FIRST]\n   +- *(2) Sort [xid#0 ASC NULLS FIRST, _w0#711 ASC NULLS FIRST], false, 0\n      +- *(2) Project [xid#0, action#1, date#2, website_id#3, url#4, category_id#5, zipcode#6, device#7, hour#283, weekday#284, n_events#695L, dayofyear(cast(date#2 as date)) AS _w0#711]\n         +- Window [count(action#1) windowspecdefinition(xid#0, specifiedwindowframe(RowFrame, unboundedpreceding$(), unboundedfollowing$())) AS n_events#695L], [xid#0]\n            +- *(1) Sort [xid#0 ASC NULLS FIRST], false, 0\n               +- TableCacheQueryStage 0\n                  +- InMemoryTableScan [xid#0, action#1, date#2, website_id#3, url#4, category_id#5, zipcode#6, device#7, hour#283, weekday#284]\n                        +- InMemoryRelation [xid#0, action#1, date#2, website_id#3, url#4, category_id#5, zipcode#6, device#7, hour#283, weekday#284], StorageLevel(disk, memory, deserialized, 1 replicas)\n                              +- AdaptiveSparkPlan isFinalPlan=true\n                                 +- == Final Plan ==\n                                    ResultQueryStage 1\n                                    +- *(1) Project [xid#0, action#1, date#2, website_id#3, url#4, category_id#5, zipcode#6, device#7, hour(date#2, Some(Europe/Paris)) AS hour#283, date_format(date#2, EEEE, Some(Europe/Paris)) AS weekday#284]\n                                       +- TableCacheQueryStage 0\n                                          +- InMemoryTableScan [action#1, category_id#5, date#2, device#7, url#4, website_id#3, xid#0, zipcode#6]\n                                                +- InMemoryRelation [xid#0, action#1, date#2, website_id#3, url#4, category_id#5, zipcode#6, device#7], StorageLevel(disk, memory, deserialized, 1 replicas)\n                                                      +- AdaptiveSparkPlan isFinalPlan=true\n                     +- == Final Plan ==\n                        ResultQueryStage 1\n                        +- ShuffleQueryStage 0\n                           +- Exchange hashpartitioning(xid#0, 20), REPARTITION_BY_NUM, [plan_id=37]\n                              +- *(1) ColumnarToRow\n                                 +- FileScan parquet [xid#0,action#1,date#2,website_id#3,url#4,category_id#5,zipcode#6,device#7] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/home/boucheron/Documents/IFEBY310/core/notebooks/data/webdata.pa..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<xid:string,action:string,date:timestamp,website_id:string,url:string,category_id:float,zip...\n                     +- == Initial Plan ==\n                        Exchange hashpartitioning(xid#0, 20), REPARTITION_BY_NUM, [plan_id=22]\n                        +- FileScan parquet [xid#0,action#1,date#2,website_id#3,url#4,category_id#5,zipcode#6,device#7] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/home/boucheron/Documents/IFEBY310/core/notebooks/data/webdata.pa..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<xid:string,action:string,date:timestamp,website_id:string,url:string,category_id:float,zip...\n                                 +- == Initial Plan ==\n                                    Project [xid#0, action#1, date#2, website_id#3, url#4, category_id#5, zipcode#6, device#7, hour(date#2, Some(Europe/Paris)) AS hour#283, date_format(date#2, EEEE, Some(Europe/Paris)) AS weekday#284]\n                                    +- InMemoryTableScan [action#1, category_id#5, date#2, device#7, url#4, website_id#3, xid#0, zipcode#6]\n                                          +- InMemoryRelation [xid#0, action#1, date#2, website_id#3, url#4, category_id#5, zipcode#6, device#7], StorageLevel(disk, memory, deserialized, 1 replicas)\n                                                +- AdaptiveSparkPlan isFinalPlan=true\n               +- == Final Plan ==\n                  ResultQueryStage 1\n                  +- ShuffleQueryStage 0\n                     +- Exchange hashpartitioning(xid#0, 20), REPARTITION_BY_NUM, [plan_id=37]\n                        +- *(1) ColumnarToRow\n                           +- FileScan parquet [xid#0,action#1,date#2,website_id#3,url#4,category_id#5,zipcode#6,device#7] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/home/boucheron/Documents/IFEBY310/core/notebooks/data/webdata.pa..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<xid:string,action:string,date:timestamp,website_id:string,url:string,category_id:float,zip...\n               +- == Initial Plan ==\n                  Exchange hashpartitioning(xid#0, 20), REPARTITION_BY_NUM, [plan_id=22]\n                  +- FileScan parquet [xid#0,action#1,date#2,website_id#3,url#4,category_id#5,zipcode#6,device#7] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/home/boucheron/Documents/IFEBY310/core/notebooks/data/webdata.pa..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<xid:string,action:string,date:timestamp,website_id:string,url:string,category_id:float,zip...\n\nGenerated code:\n/* 001 */ public Object generate(Object[] references) {\n/* 002 */   return new GeneratedIteratorForCodegenStage3(references);\n/* 003 */ }\n/* 004 */\n/* 005 */ // codegenStageId=3\n/* 006 */ final class GeneratedIteratorForCodegenStage3 extends org.apache.spark.sql.execution.BufferedRowIterator {\n/* 007 */   private Object[] references;\n/* 008 */   private scala.collection.Iterator[] inputs;\n/* 009 */   private scala.collection.Iterator inputadapter_input_0;\n/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] project_mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];\n/* 011 */\n/* 012 */   public GeneratedIteratorForCodegenStage3(Object[] references) {\n/* 013 */     this.references = references;\n/* 014 */   }\n/* 015 */\n/* 016 */   public void init(int index, scala.collection.Iterator[] inputs) {\n/* 017 */     partitionIndex = index;\n/* 018 */     this.inputs = inputs;\n/* 019 */     inputadapter_input_0 = inputs[0];\n/* 020 */     project_mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(12, 224);\n/* 021 */\n/* 022 */   }\n/* 023 */\n/* 024 */   protected void processNext() throws java.io.IOException {\n/* 025 */     while ( inputadapter_input_0.hasNext()) {\n/* 026 */       InternalRow inputadapter_row_0 = (InternalRow) inputadapter_input_0.next();\n/* 027 */\n/* 028 */       // common sub-expressions\n/* 029 */\n/* 030 */       boolean inputadapter_isNull_0 = inputadapter_row_0.isNullAt(0);\n/* 031 */       UTF8String inputadapter_value_0 = inputadapter_isNull_0 ?\n/* 032 */       null : (inputadapter_row_0.getUTF8String(0));\n/* 033 */       boolean inputadapter_isNull_1 = inputadapter_row_0.isNullAt(1);\n/* 034 */       UTF8String inputadapter_value_1 = inputadapter_isNull_1 ?\n/* 035 */       null : (inputadapter_row_0.getUTF8String(1));\n/* 036 */       boolean inputadapter_isNull_2 = inputadapter_row_0.isNullAt(2);\n/* 037 */       long inputadapter_value_2 = inputadapter_isNull_2 ?\n/* 038 */       -1L : (inputadapter_row_0.getLong(2));\n/* 039 */       boolean inputadapter_isNull_3 = inputadapter_row_0.isNullAt(3);\n/* 040 */       UTF8String inputadapter_value_3 = inputadapter_isNull_3 ?\n/* 041 */       null : (inputadapter_row_0.getUTF8String(3));\n/* 042 */       boolean inputadapter_isNull_4 = inputadapter_row_0.isNullAt(4);\n/* 043 */       UTF8String inputadapter_value_4 = inputadapter_isNull_4 ?\n/* 044 */       null : (inputadapter_row_0.getUTF8String(4));\n/* 045 */       boolean inputadapter_isNull_5 = inputadapter_row_0.isNullAt(5);\n/* 046 */       float inputadapter_value_5 = inputadapter_isNull_5 ?\n/* 047 */       -1.0f : (inputadapter_row_0.getFloat(5));\n/* 048 */       boolean inputadapter_isNull_6 = inputadapter_row_0.isNullAt(6);\n/* 049 */       UTF8String inputadapter_value_6 = inputadapter_isNull_6 ?\n/* 050 */       null : (inputadapter_row_0.getUTF8String(6));\n/* 051 */       boolean inputadapter_isNull_7 = inputadapter_row_0.isNullAt(7);\n/* 052 */       UTF8String inputadapter_value_7 = inputadapter_isNull_7 ?\n/* 053 */       null : (inputadapter_row_0.getUTF8String(7));\n/* 054 */       boolean inputadapter_isNull_8 = inputadapter_row_0.isNullAt(8);\n/* 055 */       int inputadapter_value_8 = inputadapter_isNull_8 ?\n/* 056 */       -1 : (inputadapter_row_0.getInt(8));\n/* 057 */       boolean inputadapter_isNull_9 = inputadapter_row_0.isNullAt(9);\n/* 058 */       UTF8String inputadapter_value_9 = inputadapter_isNull_9 ?\n/* 059 */       null : (inputadapter_row_0.getUTF8String(9));\n/* 060 */       long inputadapter_value_10 = inputadapter_row_0.getLong(10);\n/* 061 */       int inputadapter_value_12 = inputadapter_row_0.getInt(12);\n/* 062 */       project_mutableStateArray_0[0].reset();\n/* 063 */\n/* 064 */       project_mutableStateArray_0[0].zeroOutNullBytes();\n/* 065 */\n/* 066 */       if (inputadapter_isNull_0) {\n/* 067 */         project_mutableStateArray_0[0].setNullAt(0);\n/* 068 */       } else {\n/* 069 */         project_mutableStateArray_0[0].write(0, inputadapter_value_0);\n/* 070 */       }\n/* 071 */\n/* 072 */       if (inputadapter_isNull_1) {\n/* 073 */         project_mutableStateArray_0[0].setNullAt(1);\n/* 074 */       } else {\n/* 075 */         project_mutableStateArray_0[0].write(1, inputadapter_value_1);\n/* 076 */       }\n/* 077 */\n/* 078 */       if (inputadapter_isNull_2) {\n/* 079 */         project_mutableStateArray_0[0].setNullAt(2);\n/* 080 */       } else {\n/* 081 */         project_mutableStateArray_0[0].write(2, inputadapter_value_2);\n/* 082 */       }\n/* 083 */\n/* 084 */       if (inputadapter_isNull_3) {\n/* 085 */         project_mutableStateArray_0[0].setNullAt(3);\n/* 086 */       } else {\n/* 087 */         project_mutableStateArray_0[0].write(3, inputadapter_value_3);\n/* 088 */       }\n/* 089 */\n/* 090 */       if (inputadapter_isNull_4) {\n/* 091 */         project_mutableStateArray_0[0].setNullAt(4);\n/* 092 */       } else {\n/* 093 */         project_mutableStateArray_0[0].write(4, inputadapter_value_4);\n/* 094 */       }\n/* 095 */\n/* 096 */       if (inputadapter_isNull_5) {\n/* 097 */         project_mutableStateArray_0[0].setNullAt(5);\n/* 098 */       } else {\n/* 099 */         project_mutableStateArray_0[0].write(5, inputadapter_value_5);\n/* 100 */       }\n/* 101 */\n/* 102 */       if (inputadapter_isNull_6) {\n/* 103 */         project_mutableStateArray_0[0].setNullAt(6);\n/* 104 */       } else {\n/* 105 */         project_mutableStateArray_0[0].write(6, inputadapter_value_6);\n/* 106 */       }\n/* 107 */\n/* 108 */       if (inputadapter_isNull_7) {\n/* 109 */         project_mutableStateArray_0[0].setNullAt(7);\n/* 110 */       } else {\n/* 111 */         project_mutableStateArray_0[0].write(7, inputadapter_value_7);\n/* 112 */       }\n/* 113 */\n/* 114 */       if (inputadapter_isNull_8) {\n/* 115 */         project_mutableStateArray_0[0].setNullAt(8);\n/* 116 */       } else {\n/* 117 */         project_mutableStateArray_0[0].write(8, inputadapter_value_8);\n/* 118 */       }\n/* 119 */\n/* 120 */       if (inputadapter_isNull_9) {\n/* 121 */         project_mutableStateArray_0[0].setNullAt(9);\n/* 122 */       } else {\n/* 123 */         project_mutableStateArray_0[0].write(9, inputadapter_value_9);\n/* 124 */       }\n/* 125 */\n/* 126 */       project_mutableStateArray_0[0].write(10, inputadapter_value_10);\n/* 127 */\n/* 128 */       project_mutableStateArray_0[0].write(11, inputadapter_value_12);\n/* 129 */       append((project_mutableStateArray_0[0].getRow()));\n/* 130 */       if (shouldStop()) return;\n/* 131 */     }\n/* 132 */   }\n/* 133 */\n/* 134 */ }\n\n== Subtree 4 / 14 (maxMethodCodeSize:731; maxConstantPoolSize:199(0.30% used); numInnerClasses:0) ==\n*(4) Sort [xid#0 ASC NULLS FIRST, hour#283 ASC NULLS FIRST], false, 0\n+- *(4) Project [xid#0, action#1, date#2, website_id#3, url#4, category_id#5, zipcode#6, device#7, hour#283, weekday#284, n_events#695L, n_unique_day#700]\n   +- Window [last(_w0#710, false) windowspecdefinition(xid#0, specifiedwindowframe(RowFrame, unboundedpreceding$(), unboundedfollowing$())) AS n_unique_day#700], [xid#0]\n      +- *(3) Project [xid#0, action#1, date#2, website_id#3, url#4, category_id#5, zipcode#6, device#7, hour#283, weekday#284, n_events#695L, _w0#710]\n         +- Window [dense_rank(_w0#711) windowspecdefinition(xid#0, _w0#711 ASC NULLS FIRST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS _w0#710], [xid#0], [_w0#711 ASC NULLS FIRST]\n            +- *(2) Sort [xid#0 ASC NULLS FIRST, _w0#711 ASC NULLS FIRST], false, 0\n               +- *(2) Project [xid#0, action#1, date#2, website_id#3, url#4, category_id#5, zipcode#6, device#7, hour#283, weekday#284, n_events#695L, dayofyear(cast(date#2 as date)) AS _w0#711]\n                  +- Window [count(action#1) windowspecdefinition(xid#0, specifiedwindowframe(RowFrame, unboundedpreceding$(), unboundedfollowing$())) AS n_events#695L], [xid#0]\n                     +- *(1) Sort [xid#0 ASC NULLS FIRST], false, 0\n                        +- TableCacheQueryStage 0\n                           +- InMemoryTableScan [xid#0, action#1, date#2, website_id#3, url#4, category_id#5, zipcode#6, device#7, hour#283, weekday#284]\n                                 +- InMemoryRelation [xid#0, action#1, date#2, website_id#3, url#4, category_id#5, zipcode#6, device#7, hour#283, weekday#284], StorageLevel(disk, memory, deserialized, 1 replicas)\n                                       +- AdaptiveSparkPlan isFinalPlan=true\n                                          +- == Final Plan ==\n                                             ResultQueryStage 1\n                                             +- *(1) Project [xid#0, action#1, date#2, website_id#3, url#4, category_id#5, zipcode#6, device#7, hour(date#2, Some(Europe/Paris)) AS hour#283, date_format(date#2, EEEE, Some(Europe/Paris)) AS weekday#284]\n                                                +- TableCacheQueryStage 0\n                                                   +- InMemoryTableScan [action#1, category_id#5, date#2, device#7, url#4, website_id#3, xid#0, zipcode#6]\n                                                         +- InMemoryRelation [xid#0, action#1, date#2, website_id#3, url#4, category_id#5, zipcode#6, device#7], StorageLevel(disk, memory, deserialized, 1 replicas)\n                                                               +- AdaptiveSparkPlan isFinalPlan=true\n                     +- == Final Plan ==\n                        ResultQueryStage 1\n                        +- ShuffleQueryStage 0\n                           +- Exchange hashpartitioning(xid#0, 20), REPARTITION_BY_NUM, [plan_id=37]\n                              +- *(1) ColumnarToRow\n                                 +- FileScan parquet [xid#0,action#1,date#2,website_id#3,url#4,category_id#5,zipcode#6,device#7] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/home/boucheron/Documents/IFEBY310/core/notebooks/data/webdata.pa..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<xid:string,action:string,date:timestamp,website_id:string,url:string,category_id:float,zip...\n                     +- == Initial Plan ==\n                        Exchange hashpartitioning(xid#0, 20), REPARTITION_BY_NUM, [plan_id=22]\n                        +- FileScan parquet [xid#0,action#1,date#2,website_id#3,url#4,category_id#5,zipcode#6,device#7] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/home/boucheron/Documents/IFEBY310/core/notebooks/data/webdata.pa..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<xid:string,action:string,date:timestamp,website_id:string,url:string,category_id:float,zip...\n                                          +- == Initial Plan ==\n                                             Project [xid#0, action#1, date#2, website_id#3, url#4, category_id#5, zipcode#6, device#7, hour(date#2, Some(Europe/Paris)) AS hour#283, date_format(date#2, EEEE, Some(Europe/Paris)) AS weekday#284]\n                                             +- InMemoryTableScan [action#1, category_id#5, date#2, device#7, url#4, website_id#3, xid#0, zipcode#6]\n                                                   +- InMemoryRelation [xid#0, action#1, date#2, website_id#3, url#4, category_id#5, zipcode#6, device#7], StorageLevel(disk, memory, deserialized, 1 replicas)\n                                                         +- AdaptiveSparkPlan isFinalPlan=true\n               +- == Final Plan ==\n                  ResultQueryStage 1\n                  +- ShuffleQueryStage 0\n                     +- Exchange hashpartitioning(xid#0, 20), REPARTITION_BY_NUM, [plan_id=37]\n                        +- *(1) ColumnarToRow\n                           +- FileScan parquet [xid#0,action#1,date#2,website_id#3,url#4,category_id#5,zipcode#6,device#7] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/home/boucheron/Documents/IFEBY310/core/notebooks/data/webdata.pa..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<xid:string,action:string,date:timestamp,website_id:string,url:string,category_id:float,zip...\n               +- == Initial Plan ==\n                  Exchange hashpartitioning(xid#0, 20), REPARTITION_BY_NUM, [plan_id=22]\n                  +- FileScan parquet [xid#0,action#1,date#2,website_id#3,url#4,category_id#5,zipcode#6,device#7] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/home/boucheron/Documents/IFEBY310/core/notebooks/data/webdata.pa..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<xid:string,action:string,date:timestamp,website_id:string,url:string,category_id:float,zip...\n\nGenerated code:\n/* 001 */ public Object generate(Object[] references) {\n/* 002 */   return new GeneratedIteratorForCodegenStage4(references);\n/* 003 */ }\n/* 004 */\n/* 005 */ // codegenStageId=4\n/* 006 */ final class GeneratedIteratorForCodegenStage4 extends org.apache.spark.sql.execution.BufferedRowIterator {\n/* 007 */   private Object[] references;\n/* 008 */   private scala.collection.Iterator[] inputs;\n/* 009 */   private boolean sort_needToSort_0;\n/* 010 */   private org.apache.spark.sql.execution.UnsafeExternalRowSorter sort_sorter_0;\n/* 011 */   private org.apache.spark.executor.TaskMetrics sort_metrics_0;\n/* 012 */   private scala.collection.Iterator<UnsafeRow> sort_sortedIter_0;\n/* 013 */   private scala.collection.Iterator inputadapter_input_0;\n/* 014 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] project_mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];\n/* 015 */\n/* 016 */   public GeneratedIteratorForCodegenStage4(Object[] references) {\n/* 017 */     this.references = references;\n/* 018 */   }\n/* 019 */\n/* 020 */   public void init(int index, scala.collection.Iterator[] inputs) {\n/* 021 */     partitionIndex = index;\n/* 022 */     this.inputs = inputs;\n/* 023 */     sort_needToSort_0 = true;\n/* 024 */     sort_sorter_0 = ((org.apache.spark.sql.execution.SortExec) references[0] /* plan */).createSorter();\n/* 025 */     sort_metrics_0 = org.apache.spark.TaskContext.get().taskMetrics();\n/* 026 */\n/* 027 */     inputadapter_input_0 = inputs[0];\n/* 028 */     project_mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(12, 224);\n/* 029 */\n/* 030 */   }\n/* 031 */\n/* 032 */   private void sort_addToSorter_0() throws java.io.IOException {\n/* 033 */     while ( inputadapter_input_0.hasNext()) {\n/* 034 */       InternalRow inputadapter_row_0 = (InternalRow) inputadapter_input_0.next();\n/* 035 */\n/* 036 */       // common sub-expressions\n/* 037 */\n/* 038 */       boolean inputadapter_isNull_0 = inputadapter_row_0.isNullAt(0);\n/* 039 */       UTF8String inputadapter_value_0 = inputadapter_isNull_0 ?\n/* 040 */       null : (inputadapter_row_0.getUTF8String(0));\n/* 041 */       boolean inputadapter_isNull_1 = inputadapter_row_0.isNullAt(1);\n/* 042 */       UTF8String inputadapter_value_1 = inputadapter_isNull_1 ?\n/* 043 */       null : (inputadapter_row_0.getUTF8String(1));\n/* 044 */       boolean inputadapter_isNull_2 = inputadapter_row_0.isNullAt(2);\n/* 045 */       long inputadapter_value_2 = inputadapter_isNull_2 ?\n/* 046 */       -1L : (inputadapter_row_0.getLong(2));\n/* 047 */       boolean inputadapter_isNull_3 = inputadapter_row_0.isNullAt(3);\n/* 048 */       UTF8String inputadapter_value_3 = inputadapter_isNull_3 ?\n/* 049 */       null : (inputadapter_row_0.getUTF8String(3));\n/* 050 */       boolean inputadapter_isNull_4 = inputadapter_row_0.isNullAt(4);\n/* 051 */       UTF8String inputadapter_value_4 = inputadapter_isNull_4 ?\n/* 052 */       null : (inputadapter_row_0.getUTF8String(4));\n/* 053 */       boolean inputadapter_isNull_5 = inputadapter_row_0.isNullAt(5);\n/* 054 */       float inputadapter_value_5 = inputadapter_isNull_5 ?\n/* 055 */       -1.0f : (inputadapter_row_0.getFloat(5));\n/* 056 */       boolean inputadapter_isNull_6 = inputadapter_row_0.isNullAt(6);\n/* 057 */       UTF8String inputadapter_value_6 = inputadapter_isNull_6 ?\n/* 058 */       null : (inputadapter_row_0.getUTF8String(6));\n/* 059 */       boolean inputadapter_isNull_7 = inputadapter_row_0.isNullAt(7);\n/* 060 */       UTF8String inputadapter_value_7 = inputadapter_isNull_7 ?\n/* 061 */       null : (inputadapter_row_0.getUTF8String(7));\n/* 062 */       boolean inputadapter_isNull_8 = inputadapter_row_0.isNullAt(8);\n/* 063 */       int inputadapter_value_8 = inputadapter_isNull_8 ?\n/* 064 */       -1 : (inputadapter_row_0.getInt(8));\n/* 065 */       boolean inputadapter_isNull_9 = inputadapter_row_0.isNullAt(9);\n/* 066 */       UTF8String inputadapter_value_9 = inputadapter_isNull_9 ?\n/* 067 */       null : (inputadapter_row_0.getUTF8String(9));\n/* 068 */       long inputadapter_value_10 = inputadapter_row_0.getLong(10);\n/* 069 */       boolean inputadapter_isNull_12 = inputadapter_row_0.isNullAt(12);\n/* 070 */       int inputadapter_value_12 = inputadapter_isNull_12 ?\n/* 071 */       -1 : (inputadapter_row_0.getInt(12));\n/* 072 */       project_mutableStateArray_0[0].reset();\n/* 073 */\n/* 074 */       project_mutableStateArray_0[0].zeroOutNullBytes();\n/* 075 */\n/* 076 */       if (inputadapter_isNull_0) {\n/* 077 */         project_mutableStateArray_0[0].setNullAt(0);\n/* 078 */       } else {\n/* 079 */         project_mutableStateArray_0[0].write(0, inputadapter_value_0);\n/* 080 */       }\n/* 081 */\n/* 082 */       if (inputadapter_isNull_1) {\n/* 083 */         project_mutableStateArray_0[0].setNullAt(1);\n/* 084 */       } else {\n/* 085 */         project_mutableStateArray_0[0].write(1, inputadapter_value_1);\n/* 086 */       }\n/* 087 */\n/* 088 */       if (inputadapter_isNull_2) {\n/* 089 */         project_mutableStateArray_0[0].setNullAt(2);\n/* 090 */       } else {\n/* 091 */         project_mutableStateArray_0[0].write(2, inputadapter_value_2);\n/* 092 */       }\n/* 093 */\n/* 094 */       if (inputadapter_isNull_3) {\n/* 095 */         project_mutableStateArray_0[0].setNullAt(3);\n/* 096 */       } else {\n/* 097 */         project_mutableStateArray_0[0].write(3, inputadapter_value_3);\n/* 098 */       }\n/* 099 */\n/* 100 */       if (inputadapter_isNull_4) {\n/* 101 */         project_mutableStateArray_0[0].setNullAt(4);\n/* 102 */       } else {\n/* 103 */         project_mutableStateArray_0[0].write(4, inputadapter_value_4);\n/* 104 */       }\n/* 105 */\n/* 106 */       if (inputadapter_isNull_5) {\n/* 107 */         project_mutableStateArray_0[0].setNullAt(5);\n/* 108 */       } else {\n/* 109 */         project_mutableStateArray_0[0].write(5, inputadapter_value_5);\n/* 110 */       }\n/* 111 */\n/* 112 */       if (inputadapter_isNull_6) {\n/* 113 */         project_mutableStateArray_0[0].setNullAt(6);\n/* 114 */       } else {\n/* 115 */         project_mutableStateArray_0[0].write(6, inputadapter_value_6);\n/* 116 */       }\n/* 117 */\n/* 118 */       if (inputadapter_isNull_7) {\n/* 119 */         project_mutableStateArray_0[0].setNullAt(7);\n/* 120 */       } else {\n/* 121 */         project_mutableStateArray_0[0].write(7, inputadapter_value_7);\n/* 122 */       }\n/* 123 */\n/* 124 */       if (inputadapter_isNull_8) {\n/* 125 */         project_mutableStateArray_0[0].setNullAt(8);\n/* 126 */       } else {\n/* 127 */         project_mutableStateArray_0[0].write(8, inputadapter_value_8);\n/* 128 */       }\n/* 129 */\n/* 130 */       if (inputadapter_isNull_9) {\n/* 131 */         project_mutableStateArray_0[0].setNullAt(9);\n/* 132 */       } else {\n/* 133 */         project_mutableStateArray_0[0].write(9, inputadapter_value_9);\n/* 134 */       }\n/* 135 */\n/* 136 */       project_mutableStateArray_0[0].write(10, inputadapter_value_10);\n/* 137 */\n/* 138 */       if (inputadapter_isNull_12) {\n/* 139 */         project_mutableStateArray_0[0].setNullAt(11);\n/* 140 */       } else {\n/* 141 */         project_mutableStateArray_0[0].write(11, inputadapter_value_12);\n/* 142 */       }\n/* 143 */       sort_sorter_0.insertRow((UnsafeRow)(project_mutableStateArray_0[0].getRow()));\n/* 144 */       // shouldStop check is eliminated\n/* 145 */     }\n/* 146 */\n/* 147 */   }\n/* 148 */\n/* 149 */   protected void processNext() throws java.io.IOException {\n/* 150 */     if (sort_needToSort_0) {\n/* 151 */       long sort_spillSizeBefore_0 = sort_metrics_0.memoryBytesSpilled();\n/* 152 */       sort_addToSorter_0();\n/* 153 */       sort_sortedIter_0 = sort_sorter_0.sort();\n/* 154 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[3] /* sortTime */).add(sort_sorter_0.getSortTimeNanos() / 1000000);\n/* 155 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[1] /* peakMemory */).add(sort_sorter_0.getPeakMemoryUsage());\n/* 156 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[2] /* spillSize */).add(sort_metrics_0.memoryBytesSpilled() - sort_spillSizeBefore_0);\n/* 157 */       sort_metrics_0.incPeakExecutionMemory(sort_sorter_0.getPeakMemoryUsage());\n/* 158 */       sort_needToSort_0 = false;\n/* 159 */     }\n/* 160 */\n/* 161 */     while ( sort_sortedIter_0.hasNext()) {\n/* 162 */       UnsafeRow sort_outputRow_0 = (UnsafeRow)sort_sortedIter_0.next();\n/* 163 */\n/* 164 */       append(sort_outputRow_0);\n/* 165 */\n/* 166 */       if (shouldStop()) return;\n/* 167 */     }\n/* 168 */   }\n/* 169 */\n/* 170 */ }\n\n== Subtree 5 / 14 (maxMethodCodeSize:792; maxConstantPoolSize:199(0.30% used); numInnerClasses:0) ==\n*(5) Sort [xid#0 ASC NULLS FIRST, device#7 ASC NULLS FIRST], false, 0\n+- *(5) Project [xid#0, action#1, date#2, website_id#3, url#4, category_id#5, zipcode#6, device#7, hour#283, weekday#284, n_events#695L, n_unique_day#700, n_unique_hour#718]\n   +- Window [last(_w0#728, false) windowspecdefinition(xid#0, specifiedwindowframe(RowFrame, unboundedpreceding$(), unboundedfollowing$())) AS n_unique_hour#718], [xid#0]\n      +- Window [dense_rank(hour#283) windowspecdefinition(xid#0, hour#283 ASC NULLS FIRST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS _w0#728], [xid#0], [hour#283 ASC NULLS FIRST]\n         +- *(4) Sort [xid#0 ASC NULLS FIRST, hour#283 ASC NULLS FIRST], false, 0\n            +- *(4) Project [xid#0, action#1, date#2, website_id#3, url#4, category_id#5, zipcode#6, device#7, hour#283, weekday#284, n_events#695L, n_unique_day#700]\n               +- Window [last(_w0#710, false) windowspecdefinition(xid#0, specifiedwindowframe(RowFrame, unboundedpreceding$(), unboundedfollowing$())) AS n_unique_day#700], [xid#0]\n                  +- *(3) Project [xid#0, action#1, date#2, website_id#3, url#4, category_id#5, zipcode#6, device#7, hour#283, weekday#284, n_events#695L, _w0#710]\n                     +- Window [dense_rank(_w0#711) windowspecdefinition(xid#0, _w0#711 ASC NULLS FIRST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS _w0#710], [xid#0], [_w0#711 ASC NULLS FIRST]\n                        +- *(2) Sort [xid#0 ASC NULLS FIRST, _w0#711 ASC NULLS FIRST], false, 0\n                           +- *(2) Project [xid#0, action#1, date#2, website_id#3, url#4, category_id#5, zipcode#6, device#7, hour#283, weekday#284, n_events#695L, dayofyear(cast(date#2 as date)) AS _w0#711]\n                              +- Window [count(action#1) windowspecdefinition(xid#0, specifiedwindowframe(RowFrame, unboundedpreceding$(), unboundedfollowing$())) AS n_events#695L], [xid#0]\n                                 +- *(1) Sort [xid#0 ASC NULLS FIRST], false, 0\n                                    +- TableCacheQueryStage 0\n                                       +- InMemoryTableScan [xid#0, action#1, date#2, website_id#3, url#4, category_id#5, zipcode#6, device#7, hour#283, weekday#284]\n                                             +- InMemoryRelation [xid#0, action#1, date#2, website_id#3, url#4, category_id#5, zipcode#6, device#7, hour#283, weekday#284], StorageLevel(disk, memory, deserialized, 1 replicas)\n                                                   +- AdaptiveSparkPlan isFinalPlan=true\n                                                      +- == Final Plan ==\n                                                         ResultQueryStage 1\n                                                         +- *(1) Project [xid#0, action#1, date#2, website_id#3, url#4, category_id#5, zipcode#6, device#7, hour(date#2, Some(Europe/Paris)) AS hour#283, date_format(date#2, EEEE, Some(Europe/Paris)) AS weekday#284]\n                                                            +- TableCacheQueryStage 0\n                                                               +- InMemoryTableScan [action#1, category_id#5, date#2, device#7, url#4, website_id#3, xid#0, zipcode#6]\n                                                                     +- InMemoryRelation [xid#0, action#1, date#2, website_id#3, url#4, category_id#5, zipcode#6, device#7], StorageLevel(disk, memory, deserialized, 1 replicas)\n                                                                           +- AdaptiveSparkPlan isFinalPlan=true\n                     +- == Final Plan ==\n                        ResultQueryStage 1\n                        +- ShuffleQueryStage 0\n                           +- Exchange hashpartitioning(xid#0, 20), REPARTITION_BY_NUM, [plan_id=37]\n                              +- *(1) ColumnarToRow\n                                 +- FileScan parquet [xid#0,action#1,date#2,website_id#3,url#4,category_id#5,zipcode#6,device#7] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/home/boucheron/Documents/IFEBY310/core/notebooks/data/webdata.pa..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<xid:string,action:string,date:timestamp,website_id:string,url:string,category_id:float,zip...\n                     +- == Initial Plan ==\n                        Exchange hashpartitioning(xid#0, 20), REPARTITION_BY_NUM, [plan_id=22]\n                        +- FileScan parquet [xid#0,action#1,date#2,website_id#3,url#4,category_id#5,zipcode#6,device#7] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/home/boucheron/Documents/IFEBY310/core/notebooks/data/webdata.pa..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<xid:string,action:string,date:timestamp,website_id:string,url:string,category_id:float,zip...\n                                                      +- == Initial Plan ==\n                                                         Project [xid#0, action#1, date#2, website_id#3, url#4, category_id#5, zipcode#6, device#7, hour(date#2, Some(Europe/Paris)) AS hour#283, date_format(date#2, EEEE, Some(Europe/Paris)) AS weekday#284]\n                                                         +- InMemoryTableScan [action#1, category_id#5, date#2, device#7, url#4, website_id#3, xid#0, zipcode#6]\n                                                               +- InMemoryRelation [xid#0, action#1, date#2, website_id#3, url#4, category_id#5, zipcode#6, device#7], StorageLevel(disk, memory, deserialized, 1 replicas)\n                                                                     +- AdaptiveSparkPlan isFinalPlan=true\n               +- == Final Plan ==\n                  ResultQueryStage 1\n                  +- ShuffleQueryStage 0\n                     +- Exchange hashpartitioning(xid#0, 20), REPARTITION_BY_NUM, [plan_id=37]\n                        +- *(1) ColumnarToRow\n                           +- FileScan parquet [xid#0,action#1,date#2,website_id#3,url#4,category_id#5,zipcode#6,device#7] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/home/boucheron/Documents/IFEBY310/core/notebooks/data/webdata.pa..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<xid:string,action:string,date:timestamp,website_id:string,url:string,category_id:float,zip...\n               +- == Initial Plan ==\n                  Exchange hashpartitioning(xid#0, 20), REPARTITION_BY_NUM, [plan_id=22]\n                  +- FileScan parquet [xid#0,action#1,date#2,website_id#3,url#4,category_id#5,zipcode#6,device#7] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/home/boucheron/Documents/IFEBY310/core/notebooks/data/webdata.pa..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<xid:string,action:string,date:timestamp,website_id:string,url:string,category_id:float,zip...\n\nGenerated code:\n/* 001 */ public Object generate(Object[] references) {\n/* 002 */   return new GeneratedIteratorForCodegenStage5(references);\n/* 003 */ }\n/* 004 */\n/* 005 */ // codegenStageId=5\n/* 006 */ final class GeneratedIteratorForCodegenStage5 extends org.apache.spark.sql.execution.BufferedRowIterator {\n/* 007 */   private Object[] references;\n/* 008 */   private scala.collection.Iterator[] inputs;\n/* 009 */   private boolean sort_needToSort_0;\n/* 010 */   private org.apache.spark.sql.execution.UnsafeExternalRowSorter sort_sorter_0;\n/* 011 */   private org.apache.spark.executor.TaskMetrics sort_metrics_0;\n/* 012 */   private scala.collection.Iterator<UnsafeRow> sort_sortedIter_0;\n/* 013 */   private scala.collection.Iterator inputadapter_input_0;\n/* 014 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] project_mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];\n/* 015 */\n/* 016 */   public GeneratedIteratorForCodegenStage5(Object[] references) {\n/* 017 */     this.references = references;\n/* 018 */   }\n/* 019 */\n/* 020 */   public void init(int index, scala.collection.Iterator[] inputs) {\n/* 021 */     partitionIndex = index;\n/* 022 */     this.inputs = inputs;\n/* 023 */     sort_needToSort_0 = true;\n/* 024 */     sort_sorter_0 = ((org.apache.spark.sql.execution.SortExec) references[0] /* plan */).createSorter();\n/* 025 */     sort_metrics_0 = org.apache.spark.TaskContext.get().taskMetrics();\n/* 026 */\n/* 027 */     inputadapter_input_0 = inputs[0];\n/* 028 */     project_mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(13, 224);\n/* 029 */\n/* 030 */   }\n/* 031 */\n/* 032 */   private void sort_addToSorter_0() throws java.io.IOException {\n/* 033 */     while ( inputadapter_input_0.hasNext()) {\n/* 034 */       InternalRow inputadapter_row_0 = (InternalRow) inputadapter_input_0.next();\n/* 035 */\n/* 036 */       // common sub-expressions\n/* 037 */\n/* 038 */       boolean inputadapter_isNull_0 = inputadapter_row_0.isNullAt(0);\n/* 039 */       UTF8String inputadapter_value_0 = inputadapter_isNull_0 ?\n/* 040 */       null : (inputadapter_row_0.getUTF8String(0));\n/* 041 */       boolean inputadapter_isNull_1 = inputadapter_row_0.isNullAt(1);\n/* 042 */       UTF8String inputadapter_value_1 = inputadapter_isNull_1 ?\n/* 043 */       null : (inputadapter_row_0.getUTF8String(1));\n/* 044 */       boolean inputadapter_isNull_2 = inputadapter_row_0.isNullAt(2);\n/* 045 */       long inputadapter_value_2 = inputadapter_isNull_2 ?\n/* 046 */       -1L : (inputadapter_row_0.getLong(2));\n/* 047 */       boolean inputadapter_isNull_3 = inputadapter_row_0.isNullAt(3);\n/* 048 */       UTF8String inputadapter_value_3 = inputadapter_isNull_3 ?\n/* 049 */       null : (inputadapter_row_0.getUTF8String(3));\n/* 050 */       boolean inputadapter_isNull_4 = inputadapter_row_0.isNullAt(4);\n/* 051 */       UTF8String inputadapter_value_4 = inputadapter_isNull_4 ?\n/* 052 */       null : (inputadapter_row_0.getUTF8String(4));\n/* 053 */       boolean inputadapter_isNull_5 = inputadapter_row_0.isNullAt(5);\n/* 054 */       float inputadapter_value_5 = inputadapter_isNull_5 ?\n/* 055 */       -1.0f : (inputadapter_row_0.getFloat(5));\n/* 056 */       boolean inputadapter_isNull_6 = inputadapter_row_0.isNullAt(6);\n/* 057 */       UTF8String inputadapter_value_6 = inputadapter_isNull_6 ?\n/* 058 */       null : (inputadapter_row_0.getUTF8String(6));\n/* 059 */       boolean inputadapter_isNull_7 = inputadapter_row_0.isNullAt(7);\n/* 060 */       UTF8String inputadapter_value_7 = inputadapter_isNull_7 ?\n/* 061 */       null : (inputadapter_row_0.getUTF8String(7));\n/* 062 */       boolean inputadapter_isNull_8 = inputadapter_row_0.isNullAt(8);\n/* 063 */       int inputadapter_value_8 = inputadapter_isNull_8 ?\n/* 064 */       -1 : (inputadapter_row_0.getInt(8));\n/* 065 */       boolean inputadapter_isNull_9 = inputadapter_row_0.isNullAt(9);\n/* 066 */       UTF8String inputadapter_value_9 = inputadapter_isNull_9 ?\n/* 067 */       null : (inputadapter_row_0.getUTF8String(9));\n/* 068 */       long inputadapter_value_10 = inputadapter_row_0.getLong(10);\n/* 069 */       boolean inputadapter_isNull_11 = inputadapter_row_0.isNullAt(11);\n/* 070 */       int inputadapter_value_11 = inputadapter_isNull_11 ?\n/* 071 */       -1 : (inputadapter_row_0.getInt(11));\n/* 072 */       boolean inputadapter_isNull_13 = inputadapter_row_0.isNullAt(13);\n/* 073 */       int inputadapter_value_13 = inputadapter_isNull_13 ?\n/* 074 */       -1 : (inputadapter_row_0.getInt(13));\n/* 075 */       project_mutableStateArray_0[0].reset();\n/* 076 */\n/* 077 */       project_mutableStateArray_0[0].zeroOutNullBytes();\n/* 078 */\n/* 079 */       if (inputadapter_isNull_0) {\n/* 080 */         project_mutableStateArray_0[0].setNullAt(0);\n/* 081 */       } else {\n/* 082 */         project_mutableStateArray_0[0].write(0, inputadapter_value_0);\n/* 083 */       }\n/* 084 */\n/* 085 */       if (inputadapter_isNull_1) {\n/* 086 */         project_mutableStateArray_0[0].setNullAt(1);\n/* 087 */       } else {\n/* 088 */         project_mutableStateArray_0[0].write(1, inputadapter_value_1);\n/* 089 */       }\n/* 090 */\n/* 091 */       if (inputadapter_isNull_2) {\n/* 092 */         project_mutableStateArray_0[0].setNullAt(2);\n/* 093 */       } else {\n/* 094 */         project_mutableStateArray_0[0].write(2, inputadapter_value_2);\n/* 095 */       }\n/* 096 */\n/* 097 */       if (inputadapter_isNull_3) {\n/* 098 */         project_mutableStateArray_0[0].setNullAt(3);\n/* 099 */       } else {\n/* 100 */         project_mutableStateArray_0[0].write(3, inputadapter_value_3);\n/* 101 */       }\n/* 102 */\n/* 103 */       if (inputadapter_isNull_4) {\n/* 104 */         project_mutableStateArray_0[0].setNullAt(4);\n/* 105 */       } else {\n/* 106 */         project_mutableStateArray_0[0].write(4, inputadapter_value_4);\n/* 107 */       }\n/* 108 */\n/* 109 */       if (inputadapter_isNull_5) {\n/* 110 */         project_mutableStateArray_0[0].setNullAt(5);\n/* 111 */       } else {\n/* 112 */         project_mutableStateArray_0[0].write(5, inputadapter_value_5);\n/* 113 */       }\n/* 114 */\n/* 115 */       if (inputadapter_isNull_6) {\n/* 116 */         project_mutableStateArray_0[0].setNullAt(6);\n/* 117 */       } else {\n/* 118 */         project_mutableStateArray_0[0].write(6, inputadapter_value_6);\n/* 119 */       }\n/* 120 */\n/* 121 */       if (inputadapter_isNull_7) {\n/* 122 */         project_mutableStateArray_0[0].setNullAt(7);\n/* 123 */       } else {\n/* 124 */         project_mutableStateArray_0[0].write(7, inputadapter_value_7);\n/* 125 */       }\n/* 126 */\n/* 127 */       if (inputadapter_isNull_8) {\n/* 128 */         project_mutableStateArray_0[0].setNullAt(8);\n/* 129 */       } else {\n/* 130 */         project_mutableStateArray_0[0].write(8, inputadapter_value_8);\n/* 131 */       }\n/* 132 */\n/* 133 */       if (inputadapter_isNull_9) {\n/* 134 */         project_mutableStateArray_0[0].setNullAt(9);\n/* 135 */       } else {\n/* 136 */         project_mutableStateArray_0[0].write(9, inputadapter_value_9);\n/* 137 */       }\n/* 138 */\n/* 139 */       project_mutableStateArray_0[0].write(10, inputadapter_value_10);\n/* 140 */\n/* 141 */       if (inputadapter_isNull_11) {\n/* 142 */         project_mutableStateArray_0[0].setNullAt(11);\n/* 143 */       } else {\n/* 144 */         project_mutableStateArray_0[0].write(11, inputadapter_value_11);\n/* 145 */       }\n/* 146 */\n/* 147 */       if (inputadapter_isNull_13) {\n/* 148 */         project_mutableStateArray_0[0].setNullAt(12);\n/* 149 */       } else {\n/* 150 */         project_mutableStateArray_0[0].write(12, inputadapter_value_13);\n/* 151 */       }\n/* 152 */       sort_sorter_0.insertRow((UnsafeRow)(project_mutableStateArray_0[0].getRow()));\n/* 153 */       // shouldStop check is eliminated\n/* 154 */     }\n/* 155 */\n/* 156 */   }\n/* 157 */\n/* 158 */   protected void processNext() throws java.io.IOException {\n/* 159 */     if (sort_needToSort_0) {\n/* 160 */       long sort_spillSizeBefore_0 = sort_metrics_0.memoryBytesSpilled();\n/* 161 */       sort_addToSorter_0();\n/* 162 */       sort_sortedIter_0 = sort_sorter_0.sort();\n/* 163 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[3] /* sortTime */).add(sort_sorter_0.getSortTimeNanos() / 1000000);\n/* 164 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[1] /* peakMemory */).add(sort_sorter_0.getPeakMemoryUsage());\n/* 165 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[2] /* spillSize */).add(sort_metrics_0.memoryBytesSpilled() - sort_spillSizeBefore_0);\n/* 166 */       sort_metrics_0.incPeakExecutionMemory(sort_sorter_0.getPeakMemoryUsage());\n/* 167 */       sort_needToSort_0 = false;\n/* 168 */     }\n/* 169 */\n/* 170 */     while ( sort_sortedIter_0.hasNext()) {\n/* 171 */       UnsafeRow sort_outputRow_0 = (UnsafeRow)sort_sortedIter_0.next();\n/* 172 */\n/* 173 */       append(sort_outputRow_0);\n/* 174 */\n/* 175 */       if (shouldStop()) return;\n/* 176 */     }\n/* 177 */   }\n/* 178 */\n/* 179 */ }\n\n== Subtree 6 / 14 (maxMethodCodeSize:853; maxConstantPoolSize:199(0.30% used); numInnerClasses:0) ==\n*(6) Sort [xid#0 ASC NULLS FIRST, category_id#5 ASC NULLS FIRST], false, 0\n+- *(6) Project [xid#0, action#1, date#2, website_id#3, url#4, category_id#5, zipcode#6, device#7, hour#283, weekday#284, n_events#695L, n_unique_day#700, n_unique_hour#718, n_device#732]\n   +- Window [last(_w0#742, false) windowspecdefinition(xid#0, specifiedwindowframe(RowFrame, unboundedpreceding$(), unboundedfollowing$())) AS n_device#732], [xid#0]\n      +- Window [dense_rank(device#7) windowspecdefinition(xid#0, device#7 ASC NULLS FIRST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS _w0#742], [xid#0], [device#7 ASC NULLS FIRST]\n         +- *(5) Sort [xid#0 ASC NULLS FIRST, device#7 ASC NULLS FIRST], false, 0\n            +- *(5) Project [xid#0, action#1, date#2, website_id#3, url#4, category_id#5, zipcode#6, device#7, hour#283, weekday#284, n_events#695L, n_unique_day#700, n_unique_hour#718]\n               +- Window [last(_w0#728, false) windowspecdefinition(xid#0, specifiedwindowframe(RowFrame, unboundedpreceding$(), unboundedfollowing$())) AS n_unique_hour#718], [xid#0]\n                  +- Window [dense_rank(hour#283) windowspecdefinition(xid#0, hour#283 ASC NULLS FIRST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS _w0#728], [xid#0], [hour#283 ASC NULLS FIRST]\n                     +- *(4) Sort [xid#0 ASC NULLS FIRST, hour#283 ASC NULLS FIRST], false, 0\n                        +- *(4) Project [xid#0, action#1, date#2, website_id#3, url#4, category_id#5, zipcode#6, device#7, hour#283, weekday#284, n_events#695L, n_unique_day#700]\n                           +- Window [last(_w0#710, false) windowspecdefinition(xid#0, specifiedwindowframe(RowFrame, unboundedpreceding$(), unboundedfollowing$())) AS n_unique_day#700], [xid#0]\n                              +- *(3) Project [xid#0, action#1, date#2, website_id#3, url#4, category_id#5, zipcode#6, device#7, hour#283, weekday#284, n_events#695L, _w0#710]\n                                 +- Window [dense_rank(_w0#711) windowspecdefinition(xid#0, _w0#711 ASC NULLS FIRST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS _w0#710], [xid#0], [_w0#711 ASC NULLS FIRST]\n                                    +- *(2) Sort [xid#0 ASC NULLS FIRST, _w0#711 ASC NULLS FIRST], false, 0\n                                       +- *(2) Project [xid#0, action#1, date#2, website_id#3, url#4, category_id#5, zipcode#6, device#7, hour#283, weekday#284, n_events#695L, dayofyear(cast(date#2 as date)) AS _w0#711]\n                                          +- Window [count(action#1) windowspecdefinition(xid#0, specifiedwindowframe(RowFrame, unboundedpreceding$(), unboundedfollowing$())) AS n_events#695L], [xid#0]\n                                             +- *(1) Sort [xid#0 ASC NULLS FIRST], false, 0\n                                                +- TableCacheQueryStage 0\n                                                   +- InMemoryTableScan [xid#0, action#1, date#2, website_id#3, url#4, category_id#5, zipcode#6, device#7, hour#283, weekday#284]\n                                                         +- InMemoryRelation [xid#0, action#1, date#2, website_id#3, url#4, category_id#5, zipcode#6, device#7, hour#283, weekday#284], StorageLevel(disk, memory, deserialized, 1 replicas)\n                                                               +- AdaptiveSparkPlan isFinalPlan=true\n                                                                  +- == Final Plan ==\n                                                                     ResultQueryStage 1\n                                                                     +- *(1) Project [xid#0, action#1, date#2, website_id#3, url#4, category_id#5, zipcode#6, device#7, hour(date#2, Some(Europe/Paris)) AS hour#283, date_format(date#2, EEEE, Some(Europe/Paris)) AS weekday#284]\n                                                                        +- TableCacheQueryStage 0\n                                                                           +- InMemoryTableScan [action#1, category_id#5, date#2, device#7, url#4, website_id#3, xid#0, zipcode#6]\n                                                                                 +- InMemoryRelation [xid#0, action#1, date#2, website_id#3, url#4, category_id#5, zipcode#6, device#7], StorageLevel(disk, memory, deserialized, 1 replicas)\n                                                                                       +- AdaptiveSparkPlan isFinalPlan=true\n                     +- == Final Plan ==\n                        ResultQueryStage 1\n                        +- ShuffleQueryStage 0\n                           +- Exchange hashpartitioning(xid#0, 20), REPARTITION_BY_NUM, [plan_id=37]\n                              +- *(1) ColumnarToRow\n                                 +- FileScan parquet [xid#0,action#1,date#2,website_id#3,url#4,category_id#5,zipcode#6,device#7] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/home/boucheron/Documents/IFEBY310/core/notebooks/data/webdata.pa..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<xid:string,action:string,date:timestamp,website_id:string,url:string,category_id:float,zip...\n                     +- == Initial Plan ==\n                        Exchange hashpartitioning(xid#0, 20), REPARTITION_BY_NUM, [plan_id=22]\n                        +- FileScan parquet [xid#0,action#1,date#2,website_id#3,url#4,category_id#5,zipcode#6,device#7] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/home/boucheron/Documents/IFEBY310/core/notebooks/data/webdata.pa..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<xid:string,action:string,date:timestamp,website_id:string,url:string,category_id:float,zip...\n                                                                  +- == Initial Plan ==\n                                                                     Project [xid#0, action#1, date#2, website_id#3, url#4, category_id#5, zipcode#6, device#7, hour(date#2, Some(Europe/Paris)) AS hour#283, date_format(date#2, EEEE, Some(Europe/Paris)) AS weekday#284]\n                                                                     +- InMemoryTableScan [action#1, category_id#5, date#2, device#7, url#4, website_id#3, xid#0, zipcode#6]\n                                                                           +- InMemoryRelation [xid#0, action#1, date#2, website_id#3, url#4, category_id#5, zipcode#6, device#7], StorageLevel(disk, memory, deserialized, 1 replicas)\n                                                                                 +- AdaptiveSparkPlan isFinalPlan=true\n               +- == Final Plan ==\n                  ResultQueryStage 1\n                  +- ShuffleQueryStage 0\n                     +- Exchange hashpartitioning(xid#0, 20), REPARTITION_BY_NUM, [plan_id=37]\n                        +- *(1) ColumnarToRow\n                           +- FileScan parquet [xid#0,action#1,date#2,website_id#3,url#4,category_id#5,zipcode#6,device#7] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/home/boucheron/Documents/IFEBY310/core/notebooks/data/webdata.pa..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<xid:string,action:string,date:timestamp,website_id:string,url:string,category_id:float,zip...\n               +- == Initial Plan ==\n                  Exchange hashpartitioning(xid#0, 20), REPARTITION_BY_NUM, [plan_id=22]\n                  +- FileScan parquet [xid#0,action#1,date#2,website_id#3,url#4,category_id#5,zipcode#6,device#7] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/home/boucheron/Documents/IFEBY310/core/notebooks/data/webdata.pa..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<xid:string,action:string,date:timestamp,website_id:string,url:string,category_id:float,zip...\n\nGenerated code:\n/* 001 */ public Object generate(Object[] references) {\n/* 002 */   return new GeneratedIteratorForCodegenStage6(references);\n/* 003 */ }\n/* 004 */\n/* 005 */ // codegenStageId=6\n/* 006 */ final class GeneratedIteratorForCodegenStage6 extends org.apache.spark.sql.execution.BufferedRowIterator {\n/* 007 */   private Object[] references;\n/* 008 */   private scala.collection.Iterator[] inputs;\n/* 009 */   private boolean sort_needToSort_0;\n/* 010 */   private org.apache.spark.sql.execution.UnsafeExternalRowSorter sort_sorter_0;\n/* 011 */   private org.apache.spark.executor.TaskMetrics sort_metrics_0;\n/* 012 */   private scala.collection.Iterator<UnsafeRow> sort_sortedIter_0;\n/* 013 */   private scala.collection.Iterator inputadapter_input_0;\n/* 014 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] project_mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];\n/* 015 */\n/* 016 */   public GeneratedIteratorForCodegenStage6(Object[] references) {\n/* 017 */     this.references = references;\n/* 018 */   }\n/* 019 */\n/* 020 */   public void init(int index, scala.collection.Iterator[] inputs) {\n/* 021 */     partitionIndex = index;\n/* 022 */     this.inputs = inputs;\n/* 023 */     sort_needToSort_0 = true;\n/* 024 */     sort_sorter_0 = ((org.apache.spark.sql.execution.SortExec) references[0] /* plan */).createSorter();\n/* 025 */     sort_metrics_0 = org.apache.spark.TaskContext.get().taskMetrics();\n/* 026 */\n/* 027 */     inputadapter_input_0 = inputs[0];\n/* 028 */     project_mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(14, 224);\n/* 029 */\n/* 030 */   }\n/* 031 */\n/* 032 */   private void sort_addToSorter_0() throws java.io.IOException {\n/* 033 */     while ( inputadapter_input_0.hasNext()) {\n/* 034 */       InternalRow inputadapter_row_0 = (InternalRow) inputadapter_input_0.next();\n/* 035 */\n/* 036 */       // common sub-expressions\n/* 037 */\n/* 038 */       boolean inputadapter_isNull_0 = inputadapter_row_0.isNullAt(0);\n/* 039 */       UTF8String inputadapter_value_0 = inputadapter_isNull_0 ?\n/* 040 */       null : (inputadapter_row_0.getUTF8String(0));\n/* 041 */       boolean inputadapter_isNull_1 = inputadapter_row_0.isNullAt(1);\n/* 042 */       UTF8String inputadapter_value_1 = inputadapter_isNull_1 ?\n/* 043 */       null : (inputadapter_row_0.getUTF8String(1));\n/* 044 */       boolean inputadapter_isNull_2 = inputadapter_row_0.isNullAt(2);\n/* 045 */       long inputadapter_value_2 = inputadapter_isNull_2 ?\n/* 046 */       -1L : (inputadapter_row_0.getLong(2));\n/* 047 */       boolean inputadapter_isNull_3 = inputadapter_row_0.isNullAt(3);\n/* 048 */       UTF8String inputadapter_value_3 = inputadapter_isNull_3 ?\n/* 049 */       null : (inputadapter_row_0.getUTF8String(3));\n/* 050 */       boolean inputadapter_isNull_4 = inputadapter_row_0.isNullAt(4);\n/* 051 */       UTF8String inputadapter_value_4 = inputadapter_isNull_4 ?\n/* 052 */       null : (inputadapter_row_0.getUTF8String(4));\n/* 053 */       boolean inputadapter_isNull_5 = inputadapter_row_0.isNullAt(5);\n/* 054 */       float inputadapter_value_5 = inputadapter_isNull_5 ?\n/* 055 */       -1.0f : (inputadapter_row_0.getFloat(5));\n/* 056 */       boolean inputadapter_isNull_6 = inputadapter_row_0.isNullAt(6);\n/* 057 */       UTF8String inputadapter_value_6 = inputadapter_isNull_6 ?\n/* 058 */       null : (inputadapter_row_0.getUTF8String(6));\n/* 059 */       boolean inputadapter_isNull_7 = inputadapter_row_0.isNullAt(7);\n/* 060 */       UTF8String inputadapter_value_7 = inputadapter_isNull_7 ?\n/* 061 */       null : (inputadapter_row_0.getUTF8String(7));\n/* 062 */       boolean inputadapter_isNull_8 = inputadapter_row_0.isNullAt(8);\n/* 063 */       int inputadapter_value_8 = inputadapter_isNull_8 ?\n/* 064 */       -1 : (inputadapter_row_0.getInt(8));\n/* 065 */       boolean inputadapter_isNull_9 = inputadapter_row_0.isNullAt(9);\n/* 066 */       UTF8String inputadapter_value_9 = inputadapter_isNull_9 ?\n/* 067 */       null : (inputadapter_row_0.getUTF8String(9));\n/* 068 */       long inputadapter_value_10 = inputadapter_row_0.getLong(10);\n/* 069 */       boolean inputadapter_isNull_11 = inputadapter_row_0.isNullAt(11);\n/* 070 */       int inputadapter_value_11 = inputadapter_isNull_11 ?\n/* 071 */       -1 : (inputadapter_row_0.getInt(11));\n/* 072 */       boolean inputadapter_isNull_12 = inputadapter_row_0.isNullAt(12);\n/* 073 */       int inputadapter_value_12 = inputadapter_isNull_12 ?\n/* 074 */       -1 : (inputadapter_row_0.getInt(12));\n/* 075 */       boolean inputadapter_isNull_14 = inputadapter_row_0.isNullAt(14);\n/* 076 */       int inputadapter_value_14 = inputadapter_isNull_14 ?\n/* 077 */       -1 : (inputadapter_row_0.getInt(14));\n/* 078 */       project_mutableStateArray_0[0].reset();\n/* 079 */\n/* 080 */       project_mutableStateArray_0[0].zeroOutNullBytes();\n/* 081 */\n/* 082 */       if (inputadapter_isNull_0) {\n/* 083 */         project_mutableStateArray_0[0].setNullAt(0);\n/* 084 */       } else {\n/* 085 */         project_mutableStateArray_0[0].write(0, inputadapter_value_0);\n/* 086 */       }\n/* 087 */\n/* 088 */       if (inputadapter_isNull_1) {\n/* 089 */         project_mutableStateArray_0[0].setNullAt(1);\n/* 090 */       } else {\n/* 091 */         project_mutableStateArray_0[0].write(1, inputadapter_value_1);\n/* 092 */       }\n/* 093 */\n/* 094 */       if (inputadapter_isNull_2) {\n/* 095 */         project_mutableStateArray_0[0].setNullAt(2);\n/* 096 */       } else {\n/* 097 */         project_mutableStateArray_0[0].write(2, inputadapter_value_2);\n/* 098 */       }\n/* 099 */\n/* 100 */       if (inputadapter_isNull_3) {\n/* 101 */         project_mutableStateArray_0[0].setNullAt(3);\n/* 102 */       } else {\n/* 103 */         project_mutableStateArray_0[0].write(3, inputadapter_value_3);\n/* 104 */       }\n/* 105 */\n/* 106 */       if (inputadapter_isNull_4) {\n/* 107 */         project_mutableStateArray_0[0].setNullAt(4);\n/* 108 */       } else {\n/* 109 */         project_mutableStateArray_0[0].write(4, inputadapter_value_4);\n/* 110 */       }\n/* 111 */\n/* 112 */       if (inputadapter_isNull_5) {\n/* 113 */         project_mutableStateArray_0[0].setNullAt(5);\n/* 114 */       } else {\n/* 115 */         project_mutableStateArray_0[0].write(5, inputadapter_value_5);\n/* 116 */       }\n/* 117 */\n/* 118 */       if (inputadapter_isNull_6) {\n/* 119 */         project_mutableStateArray_0[0].setNullAt(6);\n/* 120 */       } else {\n/* 121 */         project_mutableStateArray_0[0].write(6, inputadapter_value_6);\n/* 122 */       }\n/* 123 */\n/* 124 */       if (inputadapter_isNull_7) {\n/* 125 */         project_mutableStateArray_0[0].setNullAt(7);\n/* 126 */       } else {\n/* 127 */         project_mutableStateArray_0[0].write(7, inputadapter_value_7);\n/* 128 */       }\n/* 129 */\n/* 130 */       if (inputadapter_isNull_8) {\n/* 131 */         project_mutableStateArray_0[0].setNullAt(8);\n/* 132 */       } else {\n/* 133 */         project_mutableStateArray_0[0].write(8, inputadapter_value_8);\n/* 134 */       }\n/* 135 */\n/* 136 */       if (inputadapter_isNull_9) {\n/* 137 */         project_mutableStateArray_0[0].setNullAt(9);\n/* 138 */       } else {\n/* 139 */         project_mutableStateArray_0[0].write(9, inputadapter_value_9);\n/* 140 */       }\n/* 141 */\n/* 142 */       project_mutableStateArray_0[0].write(10, inputadapter_value_10);\n/* 143 */\n/* 144 */       if (inputadapter_isNull_11) {\n/* 145 */         project_mutableStateArray_0[0].setNullAt(11);\n/* 146 */       } else {\n/* 147 */         project_mutableStateArray_0[0].write(11, inputadapter_value_11);\n/* 148 */       }\n/* 149 */\n/* 150 */       if (inputadapter_isNull_12) {\n/* 151 */         project_mutableStateArray_0[0].setNullAt(12);\n/* 152 */       } else {\n/* 153 */         project_mutableStateArray_0[0].write(12, inputadapter_value_12);\n/* 154 */       }\n/* 155 */\n/* 156 */       if (inputadapter_isNull_14) {\n/* 157 */         project_mutableStateArray_0[0].setNullAt(13);\n/* 158 */       } else {\n/* 159 */         project_mutableStateArray_0[0].write(13, inputadapter_value_14);\n/* 160 */       }\n/* 161 */       sort_sorter_0.insertRow((UnsafeRow)(project_mutableStateArray_0[0].getRow()));\n/* 162 */       // shouldStop check is eliminated\n/* 163 */     }\n/* 164 */\n/* 165 */   }\n/* 166 */\n/* 167 */   protected void processNext() throws java.io.IOException {\n/* 168 */     if (sort_needToSort_0) {\n/* 169 */       long sort_spillSizeBefore_0 = sort_metrics_0.memoryBytesSpilled();\n/* 170 */       sort_addToSorter_0();\n/* 171 */       sort_sortedIter_0 = sort_sorter_0.sort();\n/* 172 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[3] /* sortTime */).add(sort_sorter_0.getSortTimeNanos() / 1000000);\n/* 173 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[1] /* peakMemory */).add(sort_sorter_0.getPeakMemoryUsage());\n/* 174 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[2] /* spillSize */).add(sort_metrics_0.memoryBytesSpilled() - sort_spillSizeBefore_0);\n/* 175 */       sort_metrics_0.incPeakExecutionMemory(sort_sorter_0.getPeakMemoryUsage());\n/* 176 */       sort_needToSort_0 = false;\n/* 177 */     }\n/* 178 */\n/* 179 */     while ( sort_sortedIter_0.hasNext()) {\n/* 180 */       UnsafeRow sort_outputRow_0 = (UnsafeRow)sort_sortedIter_0.next();\n/* 181 */\n/* 182 */       append(sort_outputRow_0);\n/* 183 */\n/* 184 */       if (shouldStop()) return;\n/* 185 */     }\n/* 186 */   }\n/* 187 */\n/* 188 */ }\n\n== Subtree 7 / 14 (maxMethodCodeSize:977; maxConstantPoolSize:199(0.30% used); numInnerClasses:0) ==\n*(7) Sort [xid#0 ASC NULLS FIRST, action#1 ASC NULLS FIRST], false, 0\n+- *(7) Project [xid#0, action#1, date#2, website_id#3, url#4, category_id#5, zipcode#6, device#7, hour#283, weekday#284, n_events#695L, n_unique_day#700, n_unique_hour#718, n_device#732, n_unique_category_id#746, _we0#1612]\n   +- Window [last(_w0#756, false) windowspecdefinition(xid#0, specifiedwindowframe(RowFrame, unboundedpreceding$(), unboundedfollowing$())) AS n_unique_category_id#746, max(date#2) windowspecdefinition(xid#0, specifiedwindowframe(RowFrame, unboundedpreceding$(), unboundedfollowing$())) AS _we0#1612], [xid#0]\n      +- Window [dense_rank(category_id#5) windowspecdefinition(xid#0, category_id#5 ASC NULLS FIRST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS _w0#756], [xid#0], [category_id#5 ASC NULLS FIRST]\n         +- *(6) Sort [xid#0 ASC NULLS FIRST, category_id#5 ASC NULLS FIRST], false, 0\n            +- *(6) Project [xid#0, action#1, date#2, website_id#3, url#4, category_id#5, zipcode#6, device#7, hour#283, weekday#284, n_events#695L, n_unique_day#700, n_unique_hour#718, n_device#732]\n               +- Window [last(_w0#742, false) windowspecdefinition(xid#0, specifiedwindowframe(RowFrame, unboundedpreceding$(), unboundedfollowing$())) AS n_device#732], [xid#0]\n                  +- Window [dense_rank(device#7) windowspecdefinition(xid#0, device#7 ASC NULLS FIRST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS _w0#742], [xid#0], [device#7 ASC NULLS FIRST]\n                     +- *(5) Sort [xid#0 ASC NULLS FIRST, device#7 ASC NULLS FIRST], false, 0\n                        +- *(5) Project [xid#0, action#1, date#2, website_id#3, url#4, category_id#5, zipcode#6, device#7, hour#283, weekday#284, n_events#695L, n_unique_day#700, n_unique_hour#718]\n                           +- Window [last(_w0#728, false) windowspecdefinition(xid#0, specifiedwindowframe(RowFrame, unboundedpreceding$(), unboundedfollowing$())) AS n_unique_hour#718], [xid#0]\n                              +- Window [dense_rank(hour#283) windowspecdefinition(xid#0, hour#283 ASC NULLS FIRST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS _w0#728], [xid#0], [hour#283 ASC NULLS FIRST]\n                                 +- *(4) Sort [xid#0 ASC NULLS FIRST, hour#283 ASC NULLS FIRST], false, 0\n                                    +- *(4) Project [xid#0, action#1, date#2, website_id#3, url#4, category_id#5, zipcode#6, device#7, hour#283, weekday#284, n_events#695L, n_unique_day#700]\n                                       +- Window [last(_w0#710, false) windowspecdefinition(xid#0, specifiedwindowframe(RowFrame, unboundedpreceding$(), unboundedfollowing$())) AS n_unique_day#700], [xid#0]\n                                          +- *(3) Project [xid#0, action#1, date#2, website_id#3, url#4, category_id#5, zipcode#6, device#7, hour#283, weekday#284, n_events#695L, _w0#710]\n                                             +- Window [dense_rank(_w0#711) windowspecdefinition(xid#0, _w0#711 ASC NULLS FIRST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS _w0#710], [xid#0], [_w0#711 ASC NULLS FIRST]\n                                                +- *(2) Sort [xid#0 ASC NULLS FIRST, _w0#711 ASC NULLS FIRST], false, 0\n                                                   +- *(2) Project [xid#0, action#1, date#2, website_id#3, url#4, category_id#5, zipcode#6, device#7, hour#283, weekday#284, n_events#695L, dayofyear(cast(date#2 as date)) AS _w0#711]\n                                                      +- Window [count(action#1) windowspecdefinition(xid#0, specifiedwindowframe(RowFrame, unboundedpreceding$(), unboundedfollowing$())) AS n_events#695L], [xid#0]\n                                                         +- *(1) Sort [xid#0 ASC NULLS FIRST], false, 0\n                                                            +- TableCacheQueryStage 0\n                                                               +- InMemoryTableScan [xid#0, action#1, date#2, website_id#3, url#4, category_id#5, zipcode#6, device#7, hour#283, weekday#284]\n                                                                     +- InMemoryRelation [xid#0, action#1, date#2, website_id#3, url#4, category_id#5, zipcode#6, device#7, hour#283, weekday#284], StorageLevel(disk, memory, deserialized, 1 replicas)\n                                                                           +- AdaptiveSparkPlan isFinalPlan=true\n                                                                              +- == Final Plan ==\n                                                                                 ResultQueryStage 1\n                                                                                 +- *(1) Project [xid#0, action#1, date#2, website_id#3, url#4, category_id#5, zipcode#6, device#7, hour(date#2, Some(Europe/Paris)) AS hour#283, date_format(date#2, EEEE, Some(Europe/Paris)) AS weekday#284]\n                                                                                    +- TableCacheQueryStage 0\n                                                                                       +- InMemoryTableScan [action#1, category_id#5, date#2, device#7, url#4, website_id#3, xid#0, zipcode#6]\n                                                                                             +- InMemoryRelation [xid#0, action#1, date#2, website_id#3, url#4, category_id#5, zipcode#6, device#7], StorageLevel(disk, memory, deserialized, 1 replicas)\n                                                                                                   +- AdaptiveSparkPlan isFinalPlan=true\n                     +- == Final Plan ==\n                        ResultQueryStage 1\n                        +- ShuffleQueryStage 0\n                           +- Exchange hashpartitioning(xid#0, 20), REPARTITION_BY_NUM, [plan_id=37]\n                              +- *(1) ColumnarToRow\n                                 +- FileScan parquet [xid#0,action#1,date#2,website_id#3,url#4,category_id#5,zipcode#6,device#7] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/home/boucheron/Documents/IFEBY310/core/notebooks/data/webdata.pa..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<xid:string,action:string,date:timestamp,website_id:string,url:string,category_id:float,zip...\n                     +- == Initial Plan ==\n                        Exchange hashpartitioning(xid#0, 20), REPARTITION_BY_NUM, [plan_id=22]\n                        +- FileScan parquet [xid#0,action#1,date#2,website_id#3,url#4,category_id#5,zipcode#6,device#7] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/home/boucheron/Documents/IFEBY310/core/notebooks/data/webdata.pa..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<xid:string,action:string,date:timestamp,website_id:string,url:string,category_id:float,zip...\n                                                                              +- == Initial Plan ==\n                                                                                 Project [xid#0, action#1, date#2, website_id#3, url#4, category_id#5, zipcode#6, device#7, hour(date#2, Some(Europe/Paris)) AS hour#283, date_format(date#2, EEEE, Some(Europe/Paris)) AS weekday#284]\n                                                                                 +- InMemoryTableScan [action#1, category_id#5, date#2, device#7, url#4, website_id#3, xid#0, zipcode#6]\n                                                                                       +- InMemoryRelation [xid#0, action#1, date#2, website_id#3, url#4, category_id#5, zipcode#6, device#7], StorageLevel(disk, memory, deserialized, 1 replicas)\n                                                                                             +- AdaptiveSparkPlan isFinalPlan=true\n               +- == Final Plan ==\n                  ResultQueryStage 1\n                  +- ShuffleQueryStage 0\n                     +- Exchange hashpartitioning(xid#0, 20), REPARTITION_BY_NUM, [plan_id=37]\n                        +- *(1) ColumnarToRow\n                           +- FileScan parquet [xid#0,action#1,date#2,website_id#3,url#4,category_id#5,zipcode#6,device#7] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/home/boucheron/Documents/IFEBY310/core/notebooks/data/webdata.pa..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<xid:string,action:string,date:timestamp,website_id:string,url:string,category_id:float,zip...\n               +- == Initial Plan ==\n                  Exchange hashpartitioning(xid#0, 20), REPARTITION_BY_NUM, [plan_id=22]\n                  +- FileScan parquet [xid#0,action#1,date#2,website_id#3,url#4,category_id#5,zipcode#6,device#7] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/home/boucheron/Documents/IFEBY310/core/notebooks/data/webdata.pa..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<xid:string,action:string,date:timestamp,website_id:string,url:string,category_id:float,zip...\n\nGenerated code:\n/* 001 */ public Object generate(Object[] references) {\n/* 002 */   return new GeneratedIteratorForCodegenStage7(references);\n/* 003 */ }\n/* 004 */\n/* 005 */ // codegenStageId=7\n/* 006 */ final class GeneratedIteratorForCodegenStage7 extends org.apache.spark.sql.execution.BufferedRowIterator {\n/* 007 */   private Object[] references;\n/* 008 */   private scala.collection.Iterator[] inputs;\n/* 009 */   private boolean sort_needToSort_0;\n/* 010 */   private org.apache.spark.sql.execution.UnsafeExternalRowSorter sort_sorter_0;\n/* 011 */   private org.apache.spark.executor.TaskMetrics sort_metrics_0;\n/* 012 */   private scala.collection.Iterator<UnsafeRow> sort_sortedIter_0;\n/* 013 */   private scala.collection.Iterator inputadapter_input_0;\n/* 014 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] project_mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];\n/* 015 */\n/* 016 */   public GeneratedIteratorForCodegenStage7(Object[] references) {\n/* 017 */     this.references = references;\n/* 018 */   }\n/* 019 */\n/* 020 */   public void init(int index, scala.collection.Iterator[] inputs) {\n/* 021 */     partitionIndex = index;\n/* 022 */     this.inputs = inputs;\n/* 023 */     sort_needToSort_0 = true;\n/* 024 */     sort_sorter_0 = ((org.apache.spark.sql.execution.SortExec) references[0] /* plan */).createSorter();\n/* 025 */     sort_metrics_0 = org.apache.spark.TaskContext.get().taskMetrics();\n/* 026 */\n/* 027 */     inputadapter_input_0 = inputs[0];\n/* 028 */     project_mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(16, 224);\n/* 029 */\n/* 030 */   }\n/* 031 */\n/* 032 */   private void sort_addToSorter_0() throws java.io.IOException {\n/* 033 */     while ( inputadapter_input_0.hasNext()) {\n/* 034 */       InternalRow inputadapter_row_0 = (InternalRow) inputadapter_input_0.next();\n/* 035 */\n/* 036 */       // common sub-expressions\n/* 037 */\n/* 038 */       boolean inputadapter_isNull_0 = inputadapter_row_0.isNullAt(0);\n/* 039 */       UTF8String inputadapter_value_0 = inputadapter_isNull_0 ?\n/* 040 */       null : (inputadapter_row_0.getUTF8String(0));\n/* 041 */       boolean inputadapter_isNull_1 = inputadapter_row_0.isNullAt(1);\n/* 042 */       UTF8String inputadapter_value_1 = inputadapter_isNull_1 ?\n/* 043 */       null : (inputadapter_row_0.getUTF8String(1));\n/* 044 */       boolean inputadapter_isNull_2 = inputadapter_row_0.isNullAt(2);\n/* 045 */       long inputadapter_value_2 = inputadapter_isNull_2 ?\n/* 046 */       -1L : (inputadapter_row_0.getLong(2));\n/* 047 */       boolean inputadapter_isNull_3 = inputadapter_row_0.isNullAt(3);\n/* 048 */       UTF8String inputadapter_value_3 = inputadapter_isNull_3 ?\n/* 049 */       null : (inputadapter_row_0.getUTF8String(3));\n/* 050 */       boolean inputadapter_isNull_4 = inputadapter_row_0.isNullAt(4);\n/* 051 */       UTF8String inputadapter_value_4 = inputadapter_isNull_4 ?\n/* 052 */       null : (inputadapter_row_0.getUTF8String(4));\n/* 053 */       boolean inputadapter_isNull_5 = inputadapter_row_0.isNullAt(5);\n/* 054 */       float inputadapter_value_5 = inputadapter_isNull_5 ?\n/* 055 */       -1.0f : (inputadapter_row_0.getFloat(5));\n/* 056 */       boolean inputadapter_isNull_6 = inputadapter_row_0.isNullAt(6);\n/* 057 */       UTF8String inputadapter_value_6 = inputadapter_isNull_6 ?\n/* 058 */       null : (inputadapter_row_0.getUTF8String(6));\n/* 059 */       boolean inputadapter_isNull_7 = inputadapter_row_0.isNullAt(7);\n/* 060 */       UTF8String inputadapter_value_7 = inputadapter_isNull_7 ?\n/* 061 */       null : (inputadapter_row_0.getUTF8String(7));\n/* 062 */       boolean inputadapter_isNull_8 = inputadapter_row_0.isNullAt(8);\n/* 063 */       int inputadapter_value_8 = inputadapter_isNull_8 ?\n/* 064 */       -1 : (inputadapter_row_0.getInt(8));\n/* 065 */       boolean inputadapter_isNull_9 = inputadapter_row_0.isNullAt(9);\n/* 066 */       UTF8String inputadapter_value_9 = inputadapter_isNull_9 ?\n/* 067 */       null : (inputadapter_row_0.getUTF8String(9));\n/* 068 */       long inputadapter_value_10 = inputadapter_row_0.getLong(10);\n/* 069 */       boolean inputadapter_isNull_11 = inputadapter_row_0.isNullAt(11);\n/* 070 */       int inputadapter_value_11 = inputadapter_isNull_11 ?\n/* 071 */       -1 : (inputadapter_row_0.getInt(11));\n/* 072 */       boolean inputadapter_isNull_12 = inputadapter_row_0.isNullAt(12);\n/* 073 */       int inputadapter_value_12 = inputadapter_isNull_12 ?\n/* 074 */       -1 : (inputadapter_row_0.getInt(12));\n/* 075 */       boolean inputadapter_isNull_13 = inputadapter_row_0.isNullAt(13);\n/* 076 */       int inputadapter_value_13 = inputadapter_isNull_13 ?\n/* 077 */       -1 : (inputadapter_row_0.getInt(13));\n/* 078 */       boolean inputadapter_isNull_15 = inputadapter_row_0.isNullAt(15);\n/* 079 */       int inputadapter_value_15 = inputadapter_isNull_15 ?\n/* 080 */       -1 : (inputadapter_row_0.getInt(15));\n/* 081 */       boolean inputadapter_isNull_16 = inputadapter_row_0.isNullAt(16);\n/* 082 */       long inputadapter_value_16 = inputadapter_isNull_16 ?\n/* 083 */       -1L : (inputadapter_row_0.getLong(16));\n/* 084 */       project_mutableStateArray_0[0].reset();\n/* 085 */\n/* 086 */       project_mutableStateArray_0[0].zeroOutNullBytes();\n/* 087 */\n/* 088 */       if (inputadapter_isNull_0) {\n/* 089 */         project_mutableStateArray_0[0].setNullAt(0);\n/* 090 */       } else {\n/* 091 */         project_mutableStateArray_0[0].write(0, inputadapter_value_0);\n/* 092 */       }\n/* 093 */\n/* 094 */       if (inputadapter_isNull_1) {\n/* 095 */         project_mutableStateArray_0[0].setNullAt(1);\n/* 096 */       } else {\n/* 097 */         project_mutableStateArray_0[0].write(1, inputadapter_value_1);\n/* 098 */       }\n/* 099 */\n/* 100 */       if (inputadapter_isNull_2) {\n/* 101 */         project_mutableStateArray_0[0].setNullAt(2);\n/* 102 */       } else {\n/* 103 */         project_mutableStateArray_0[0].write(2, inputadapter_value_2);\n/* 104 */       }\n/* 105 */\n/* 106 */       if (inputadapter_isNull_3) {\n/* 107 */         project_mutableStateArray_0[0].setNullAt(3);\n/* 108 */       } else {\n/* 109 */         project_mutableStateArray_0[0].write(3, inputadapter_value_3);\n/* 110 */       }\n/* 111 */\n/* 112 */       if (inputadapter_isNull_4) {\n/* 113 */         project_mutableStateArray_0[0].setNullAt(4);\n/* 114 */       } else {\n/* 115 */         project_mutableStateArray_0[0].write(4, inputadapter_value_4);\n/* 116 */       }\n/* 117 */\n/* 118 */       if (inputadapter_isNull_5) {\n/* 119 */         project_mutableStateArray_0[0].setNullAt(5);\n/* 120 */       } else {\n/* 121 */         project_mutableStateArray_0[0].write(5, inputadapter_value_5);\n/* 122 */       }\n/* 123 */\n/* 124 */       if (inputadapter_isNull_6) {\n/* 125 */         project_mutableStateArray_0[0].setNullAt(6);\n/* 126 */       } else {\n/* 127 */         project_mutableStateArray_0[0].write(6, inputadapter_value_6);\n/* 128 */       }\n/* 129 */\n/* 130 */       if (inputadapter_isNull_7) {\n/* 131 */         project_mutableStateArray_0[0].setNullAt(7);\n/* 132 */       } else {\n/* 133 */         project_mutableStateArray_0[0].write(7, inputadapter_value_7);\n/* 134 */       }\n/* 135 */\n/* 136 */       if (inputadapter_isNull_8) {\n/* 137 */         project_mutableStateArray_0[0].setNullAt(8);\n/* 138 */       } else {\n/* 139 */         project_mutableStateArray_0[0].write(8, inputadapter_value_8);\n/* 140 */       }\n/* 141 */\n/* 142 */       if (inputadapter_isNull_9) {\n/* 143 */         project_mutableStateArray_0[0].setNullAt(9);\n/* 144 */       } else {\n/* 145 */         project_mutableStateArray_0[0].write(9, inputadapter_value_9);\n/* 146 */       }\n/* 147 */\n/* 148 */       project_mutableStateArray_0[0].write(10, inputadapter_value_10);\n/* 149 */\n/* 150 */       if (inputadapter_isNull_11) {\n/* 151 */         project_mutableStateArray_0[0].setNullAt(11);\n/* 152 */       } else {\n/* 153 */         project_mutableStateArray_0[0].write(11, inputadapter_value_11);\n/* 154 */       }\n/* 155 */\n/* 156 */       if (inputadapter_isNull_12) {\n/* 157 */         project_mutableStateArray_0[0].setNullAt(12);\n/* 158 */       } else {\n/* 159 */         project_mutableStateArray_0[0].write(12, inputadapter_value_12);\n/* 160 */       }\n/* 161 */\n/* 162 */       if (inputadapter_isNull_13) {\n/* 163 */         project_mutableStateArray_0[0].setNullAt(13);\n/* 164 */       } else {\n/* 165 */         project_mutableStateArray_0[0].write(13, inputadapter_value_13);\n/* 166 */       }\n/* 167 */\n/* 168 */       if (inputadapter_isNull_15) {\n/* 169 */         project_mutableStateArray_0[0].setNullAt(14);\n/* 170 */       } else {\n/* 171 */         project_mutableStateArray_0[0].write(14, inputadapter_value_15);\n/* 172 */       }\n/* 173 */\n/* 174 */       if (inputadapter_isNull_16) {\n/* 175 */         project_mutableStateArray_0[0].setNullAt(15);\n/* 176 */       } else {\n/* 177 */         project_mutableStateArray_0[0].write(15, inputadapter_value_16);\n/* 178 */       }\n/* 179 */       sort_sorter_0.insertRow((UnsafeRow)(project_mutableStateArray_0[0].getRow()));\n/* 180 */       // shouldStop check is eliminated\n/* 181 */     }\n/* 182 */\n/* 183 */   }\n/* 184 */\n/* 185 */   protected void processNext() throws java.io.IOException {\n/* 186 */     if (sort_needToSort_0) {\n/* 187 */       long sort_spillSizeBefore_0 = sort_metrics_0.memoryBytesSpilled();\n/* 188 */       sort_addToSorter_0();\n/* 189 */       sort_sortedIter_0 = sort_sorter_0.sort();\n/* 190 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[3] /* sortTime */).add(sort_sorter_0.getSortTimeNanos() / 1000000);\n/* 191 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[1] /* peakMemory */).add(sort_sorter_0.getPeakMemoryUsage());\n/* 192 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[2] /* spillSize */).add(sort_metrics_0.memoryBytesSpilled() - sort_spillSizeBefore_0);\n/* 193 */       sort_metrics_0.incPeakExecutionMemory(sort_sorter_0.getPeakMemoryUsage());\n/* 194 */       sort_needToSort_0 = false;\n/* 195 */     }\n/* 196 */\n/* 197 */     while ( sort_sortedIter_0.hasNext()) {\n/* 198 */       UnsafeRow sort_outputRow_0 = (UnsafeRow)sort_sortedIter_0.next();\n/* 199 */\n/* 200 */       append(sort_outputRow_0);\n/* 201 */\n/* 202 */       if (shouldStop()) return;\n/* 203 */     }\n/* 204 */   }\n/* 205 */\n/* 206 */ }\n\n== Subtree 8 / 14 (maxMethodCodeSize:1000; maxConstantPoolSize:199(0.30% used); numInnerClasses:0) ==\n*(8) Sort [xid#0 ASC NULLS FIRST, hour#283 ASC NULLS FIRST], false, 0\n+- *(8) Project [xid#0, action#1, date#2, website_id#3, url#4, category_id#5, zipcode#6, device#7, hour#283, weekday#284, n_events#695L, n_unique_day#700, n_unique_hour#718, n_device#732, n_unique_category_id#746, n_events_per_action#1172L, _we0#1612]\n   +- Window [count(action#1) windowspecdefinition(xid#0, action#1, specifiedwindowframe(RowFrame, unboundedpreceding$(), unboundedfollowing$())) AS n_events_per_action#1172L], [xid#0, action#1]\n      +- *(7) Sort [xid#0 ASC NULLS FIRST, action#1 ASC NULLS FIRST], false, 0\n         +- *(7) Project [xid#0, action#1, date#2, website_id#3, url#4, category_id#5, zipcode#6, device#7, hour#283, weekday#284, n_events#695L, n_unique_day#700, n_unique_hour#718, n_device#732, n_unique_category_id#746, _we0#1612]\n            +- Window [last(_w0#756, false) windowspecdefinition(xid#0, specifiedwindowframe(RowFrame, unboundedpreceding$(), unboundedfollowing$())) AS n_unique_category_id#746, max(date#2) windowspecdefinition(xid#0, specifiedwindowframe(RowFrame, unboundedpreceding$(), unboundedfollowing$())) AS _we0#1612], [xid#0]\n               +- Window [dense_rank(category_id#5) windowspecdefinition(xid#0, category_id#5 ASC NULLS FIRST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS _w0#756], [xid#0], [category_id#5 ASC NULLS FIRST]\n                  +- *(6) Sort [xid#0 ASC NULLS FIRST, category_id#5 ASC NULLS FIRST], false, 0\n                     +- *(6) Project [xid#0, action#1, date#2, website_id#3, url#4, category_id#5, zipcode#6, device#7, hour#283, weekday#284, n_events#695L, n_unique_day#700, n_unique_hour#718, n_device#732]\n                        +- Window [last(_w0#742, false) windowspecdefinition(xid#0, specifiedwindowframe(RowFrame, unboundedpreceding$(), unboundedfollowing$())) AS n_device#732], [xid#0]\n                           +- Window [dense_rank(device#7) windowspecdefinition(xid#0, device#7 ASC NULLS FIRST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS _w0#742], [xid#0], [device#7 ASC NULLS FIRST]\n                              +- *(5) Sort [xid#0 ASC NULLS FIRST, device#7 ASC NULLS FIRST], false, 0\n                                 +- *(5) Project [xid#0, action#1, date#2, website_id#3, url#4, category_id#5, zipcode#6, device#7, hour#283, weekday#284, n_events#695L, n_unique_day#700, n_unique_hour#718]\n                                    +- Window [last(_w0#728, false) windowspecdefinition(xid#0, specifiedwindowframe(RowFrame, unboundedpreceding$(), unboundedfollowing$())) AS n_unique_hour#718], [xid#0]\n                                       +- Window [dense_rank(hour#283) windowspecdefinition(xid#0, hour#283 ASC NULLS FIRST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS _w0#728], [xid#0], [hour#283 ASC NULLS FIRST]\n                                          +- *(4) Sort [xid#0 ASC NULLS FIRST, hour#283 ASC NULLS FIRST], false, 0\n                                             +- *(4) Project [xid#0, action#1, date#2, website_id#3, url#4, category_id#5, zipcode#6, device#7, hour#283, weekday#284, n_events#695L, n_unique_day#700]\n                                                +- Window [last(_w0#710, false) windowspecdefinition(xid#0, specifiedwindowframe(RowFrame, unboundedpreceding$(), unboundedfollowing$())) AS n_unique_day#700], [xid#0]\n                                                   +- *(3) Project [xid#0, action#1, date#2, website_id#3, url#4, category_id#5, zipcode#6, device#7, hour#283, weekday#284, n_events#695L, _w0#710]\n                                                      +- Window [dense_rank(_w0#711) windowspecdefinition(xid#0, _w0#711 ASC NULLS FIRST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS _w0#710], [xid#0], [_w0#711 ASC NULLS FIRST]\n                                                         +- *(2) Sort [xid#0 ASC NULLS FIRST, _w0#711 ASC NULLS FIRST], false, 0\n                                                            +- *(2) Project [xid#0, action#1, date#2, website_id#3, url#4, category_id#5, zipcode#6, device#7, hour#283, weekday#284, n_events#695L, dayofyear(cast(date#2 as date)) AS _w0#711]\n                                                               +- Window [count(action#1) windowspecdefinition(xid#0, specifiedwindowframe(RowFrame, unboundedpreceding$(), unboundedfollowing$())) AS n_events#695L], [xid#0]\n                                                                  +- *(1) Sort [xid#0 ASC NULLS FIRST], false, 0\n                                                                     +- TableCacheQueryStage 0\n                                                                        +- InMemoryTableScan [xid#0, action#1, date#2, website_id#3, url#4, category_id#5, zipcode#6, device#7, hour#283, weekday#284]\n                                                                              +- InMemoryRelation [xid#0, action#1, date#2, website_id#3, url#4, category_id#5, zipcode#6, device#7, hour#283, weekday#284], StorageLevel(disk, memory, deserialized, 1 replicas)\n                                                                                    +- AdaptiveSparkPlan isFinalPlan=true\n                                                                                       +- == Final Plan ==\n                                                                                          ResultQueryStage 1\n                                                                                          +- *(1) Project [xid#0, action#1, date#2, website_id#3, url#4, category_id#5, zipcode#6, device#7, hour(date#2, Some(Europe/Paris)) AS hour#283, date_format(date#2, EEEE, Some(Europe/Paris)) AS weekday#284]\n                                                                                             +- TableCacheQueryStage 0\n                                                                                                +- InMemoryTableScan [action#1, category_id#5, date#2, device#7, url#4, website_id#3, xid#0, zipcode#6]\n                                                                                                      +- InMemoryRelation [xid#0, action#1, date#2, website_id#3, url#4, category_id#5, zipcode#6, device#7], StorageLevel(disk, memory, deserialized, 1 replicas)\n                                                                                                            +- AdaptiveSparkPlan isFinalPlan=true\n                     +- == Final Plan ==\n                        ResultQueryStage 1\n                        +- ShuffleQueryStage 0\n                           +- Exchange hashpartitioning(xid#0, 20), REPARTITION_BY_NUM, [plan_id=37]\n                              +- *(1) ColumnarToRow\n                                 +- FileScan parquet [xid#0,action#1,date#2,website_id#3,url#4,category_id#5,zipcode#6,device#7] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/home/boucheron/Documents/IFEBY310/core/notebooks/data/webdata.pa..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<xid:string,action:string,date:timestamp,website_id:string,url:string,category_id:float,zip...\n                     +- == Initial Plan ==\n                        Exchange hashpartitioning(xid#0, 20), REPARTITION_BY_NUM, [plan_id=22]\n                        +- FileScan parquet [xid#0,action#1,date#2,website_id#3,url#4,category_id#5,zipcode#6,device#7] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/home/boucheron/Documents/IFEBY310/core/notebooks/data/webdata.pa..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<xid:string,action:string,date:timestamp,website_id:string,url:string,category_id:float,zip...\n                                                                                       +- == Initial Plan ==\n                                                                                          Project [xid#0, action#1, date#2, website_id#3, url#4, category_id#5, zipcode#6, device#7, hour(date#2, Some(Europe/Paris)) AS hour#283, date_format(date#2, EEEE, Some(Europe/Paris)) AS weekday#284]\n                                                                                          +- InMemoryTableScan [action#1, category_id#5, date#2, device#7, url#4, website_id#3, xid#0, zipcode#6]\n                                                                                                +- InMemoryRelation [xid#0, action#1, date#2, website_id#3, url#4, category_id#5, zipcode#6, device#7], StorageLevel(disk, memory, deserialized, 1 replicas)\n                                                                                                      +- AdaptiveSparkPlan isFinalPlan=true\n               +- == Final Plan ==\n                  ResultQueryStage 1\n                  +- ShuffleQueryStage 0\n                     +- Exchange hashpartitioning(xid#0, 20), REPARTITION_BY_NUM, [plan_id=37]\n                        +- *(1) ColumnarToRow\n                           +- FileScan parquet [xid#0,action#1,date#2,website_id#3,url#4,category_id#5,zipcode#6,device#7] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/home/boucheron/Documents/IFEBY310/core/notebooks/data/webdata.pa..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<xid:string,action:string,date:timestamp,website_id:string,url:string,category_id:float,zip...\n               +- == Initial Plan ==\n                  Exchange hashpartitioning(xid#0, 20), REPARTITION_BY_NUM, [plan_id=22]\n                  +- FileScan parquet [xid#0,action#1,date#2,website_id#3,url#4,category_id#5,zipcode#6,device#7] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/home/boucheron/Documents/IFEBY310/core/notebooks/data/webdata.pa..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<xid:string,action:string,date:timestamp,website_id:string,url:string,category_id:float,zip...\n\nGenerated code:\n/* 001 */ public Object generate(Object[] references) {\n/* 002 */   return new GeneratedIteratorForCodegenStage8(references);\n/* 003 */ }\n/* 004 */\n/* 005 */ // codegenStageId=8\n/* 006 */ final class GeneratedIteratorForCodegenStage8 extends org.apache.spark.sql.execution.BufferedRowIterator {\n/* 007 */   private Object[] references;\n/* 008 */   private scala.collection.Iterator[] inputs;\n/* 009 */   private boolean sort_needToSort_0;\n/* 010 */   private org.apache.spark.sql.execution.UnsafeExternalRowSorter sort_sorter_0;\n/* 011 */   private org.apache.spark.executor.TaskMetrics sort_metrics_0;\n/* 012 */   private scala.collection.Iterator<UnsafeRow> sort_sortedIter_0;\n/* 013 */   private scala.collection.Iterator inputadapter_input_0;\n/* 014 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] project_mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];\n/* 015 */\n/* 016 */   public GeneratedIteratorForCodegenStage8(Object[] references) {\n/* 017 */     this.references = references;\n/* 018 */   }\n/* 019 */\n/* 020 */   public void init(int index, scala.collection.Iterator[] inputs) {\n/* 021 */     partitionIndex = index;\n/* 022 */     this.inputs = inputs;\n/* 023 */     sort_needToSort_0 = true;\n/* 024 */     sort_sorter_0 = ((org.apache.spark.sql.execution.SortExec) references[0] /* plan */).createSorter();\n/* 025 */     sort_metrics_0 = org.apache.spark.TaskContext.get().taskMetrics();\n/* 026 */\n/* 027 */     inputadapter_input_0 = inputs[0];\n/* 028 */     project_mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(17, 224);\n/* 029 */\n/* 030 */   }\n/* 031 */\n/* 032 */   private void sort_addToSorter_0() throws java.io.IOException {\n/* 033 */     while ( inputadapter_input_0.hasNext()) {\n/* 034 */       InternalRow inputadapter_row_0 = (InternalRow) inputadapter_input_0.next();\n/* 035 */\n/* 036 */       // common sub-expressions\n/* 037 */\n/* 038 */       boolean inputadapter_isNull_0 = inputadapter_row_0.isNullAt(0);\n/* 039 */       UTF8String inputadapter_value_0 = inputadapter_isNull_0 ?\n/* 040 */       null : (inputadapter_row_0.getUTF8String(0));\n/* 041 */       boolean inputadapter_isNull_1 = inputadapter_row_0.isNullAt(1);\n/* 042 */       UTF8String inputadapter_value_1 = inputadapter_isNull_1 ?\n/* 043 */       null : (inputadapter_row_0.getUTF8String(1));\n/* 044 */       boolean inputadapter_isNull_2 = inputadapter_row_0.isNullAt(2);\n/* 045 */       long inputadapter_value_2 = inputadapter_isNull_2 ?\n/* 046 */       -1L : (inputadapter_row_0.getLong(2));\n/* 047 */       boolean inputadapter_isNull_3 = inputadapter_row_0.isNullAt(3);\n/* 048 */       UTF8String inputadapter_value_3 = inputadapter_isNull_3 ?\n/* 049 */       null : (inputadapter_row_0.getUTF8String(3));\n/* 050 */       boolean inputadapter_isNull_4 = inputadapter_row_0.isNullAt(4);\n/* 051 */       UTF8String inputadapter_value_4 = inputadapter_isNull_4 ?\n/* 052 */       null : (inputadapter_row_0.getUTF8String(4));\n/* 053 */       boolean inputadapter_isNull_5 = inputadapter_row_0.isNullAt(5);\n/* 054 */       float inputadapter_value_5 = inputadapter_isNull_5 ?\n/* 055 */       -1.0f : (inputadapter_row_0.getFloat(5));\n/* 056 */       boolean inputadapter_isNull_6 = inputadapter_row_0.isNullAt(6);\n/* 057 */       UTF8String inputadapter_value_6 = inputadapter_isNull_6 ?\n/* 058 */       null : (inputadapter_row_0.getUTF8String(6));\n/* 059 */       boolean inputadapter_isNull_7 = inputadapter_row_0.isNullAt(7);\n/* 060 */       UTF8String inputadapter_value_7 = inputadapter_isNull_7 ?\n/* 061 */       null : (inputadapter_row_0.getUTF8String(7));\n/* 062 */       boolean inputadapter_isNull_8 = inputadapter_row_0.isNullAt(8);\n/* 063 */       int inputadapter_value_8 = inputadapter_isNull_8 ?\n/* 064 */       -1 : (inputadapter_row_0.getInt(8));\n/* 065 */       boolean inputadapter_isNull_9 = inputadapter_row_0.isNullAt(9);\n/* 066 */       UTF8String inputadapter_value_9 = inputadapter_isNull_9 ?\n/* 067 */       null : (inputadapter_row_0.getUTF8String(9));\n/* 068 */       long inputadapter_value_10 = inputadapter_row_0.getLong(10);\n/* 069 */       boolean inputadapter_isNull_11 = inputadapter_row_0.isNullAt(11);\n/* 070 */       int inputadapter_value_11 = inputadapter_isNull_11 ?\n/* 071 */       -1 : (inputadapter_row_0.getInt(11));\n/* 072 */       boolean inputadapter_isNull_12 = inputadapter_row_0.isNullAt(12);\n/* 073 */       int inputadapter_value_12 = inputadapter_isNull_12 ?\n/* 074 */       -1 : (inputadapter_row_0.getInt(12));\n/* 075 */       boolean inputadapter_isNull_13 = inputadapter_row_0.isNullAt(13);\n/* 076 */       int inputadapter_value_13 = inputadapter_isNull_13 ?\n/* 077 */       -1 : (inputadapter_row_0.getInt(13));\n/* 078 */       boolean inputadapter_isNull_14 = inputadapter_row_0.isNullAt(14);\n/* 079 */       int inputadapter_value_14 = inputadapter_isNull_14 ?\n/* 080 */       -1 : (inputadapter_row_0.getInt(14));\n/* 081 */       long inputadapter_value_16 = inputadapter_row_0.getLong(16);\n/* 082 */       boolean inputadapter_isNull_15 = inputadapter_row_0.isNullAt(15);\n/* 083 */       long inputadapter_value_15 = inputadapter_isNull_15 ?\n/* 084 */       -1L : (inputadapter_row_0.getLong(15));\n/* 085 */       project_mutableStateArray_0[0].reset();\n/* 086 */\n/* 087 */       project_mutableStateArray_0[0].zeroOutNullBytes();\n/* 088 */\n/* 089 */       if (inputadapter_isNull_0) {\n/* 090 */         project_mutableStateArray_0[0].setNullAt(0);\n/* 091 */       } else {\n/* 092 */         project_mutableStateArray_0[0].write(0, inputadapter_value_0);\n/* 093 */       }\n/* 094 */\n/* 095 */       if (inputadapter_isNull_1) {\n/* 096 */         project_mutableStateArray_0[0].setNullAt(1);\n/* 097 */       } else {\n/* 098 */         project_mutableStateArray_0[0].write(1, inputadapter_value_1);\n/* 099 */       }\n/* 100 */\n/* 101 */       if (inputadapter_isNull_2) {\n/* 102 */         project_mutableStateArray_0[0].setNullAt(2);\n/* 103 */       } else {\n/* 104 */         project_mutableStateArray_0[0].write(2, inputadapter_value_2);\n/* 105 */       }\n/* 106 */\n/* 107 */       if (inputadapter_isNull_3) {\n/* 108 */         project_mutableStateArray_0[0].setNullAt(3);\n/* 109 */       } else {\n/* 110 */         project_mutableStateArray_0[0].write(3, inputadapter_value_3);\n/* 111 */       }\n/* 112 */\n/* 113 */       if (inputadapter_isNull_4) {\n/* 114 */         project_mutableStateArray_0[0].setNullAt(4);\n/* 115 */       } else {\n/* 116 */         project_mutableStateArray_0[0].write(4, inputadapter_value_4);\n/* 117 */       }\n/* 118 */\n/* 119 */       if (inputadapter_isNull_5) {\n/* 120 */         project_mutableStateArray_0[0].setNullAt(5);\n/* 121 */       } else {\n/* 122 */         project_mutableStateArray_0[0].write(5, inputadapter_value_5);\n/* 123 */       }\n/* 124 */\n/* 125 */       if (inputadapter_isNull_6) {\n/* 126 */         project_mutableStateArray_0[0].setNullAt(6);\n/* 127 */       } else {\n/* 128 */         project_mutableStateArray_0[0].write(6, inputadapter_value_6);\n/* 129 */       }\n/* 130 */\n/* 131 */       if (inputadapter_isNull_7) {\n/* 132 */         project_mutableStateArray_0[0].setNullAt(7);\n/* 133 */       } else {\n/* 134 */         project_mutableStateArray_0[0].write(7, inputadapter_value_7);\n/* 135 */       }\n/* 136 */\n/* 137 */       if (inputadapter_isNull_8) {\n/* 138 */         project_mutableStateArray_0[0].setNullAt(8);\n/* 139 */       } else {\n/* 140 */         project_mutableStateArray_0[0].write(8, inputadapter_value_8);\n/* 141 */       }\n/* 142 */\n/* 143 */       if (inputadapter_isNull_9) {\n/* 144 */         project_mutableStateArray_0[0].setNullAt(9);\n/* 145 */       } else {\n/* 146 */         project_mutableStateArray_0[0].write(9, inputadapter_value_9);\n/* 147 */       }\n/* 148 */\n/* 149 */       project_mutableStateArray_0[0].write(10, inputadapter_value_10);\n/* 150 */\n/* 151 */       if (inputadapter_isNull_11) {\n/* 152 */         project_mutableStateArray_0[0].setNullAt(11);\n/* 153 */       } else {\n/* 154 */         project_mutableStateArray_0[0].write(11, inputadapter_value_11);\n/* 155 */       }\n/* 156 */\n/* 157 */       if (inputadapter_isNull_12) {\n/* 158 */         project_mutableStateArray_0[0].setNullAt(12);\n/* 159 */       } else {\n/* 160 */         project_mutableStateArray_0[0].write(12, inputadapter_value_12);\n/* 161 */       }\n/* 162 */\n/* 163 */       if (inputadapter_isNull_13) {\n/* 164 */         project_mutableStateArray_0[0].setNullAt(13);\n/* 165 */       } else {\n/* 166 */         project_mutableStateArray_0[0].write(13, inputadapter_value_13);\n/* 167 */       }\n/* 168 */\n/* 169 */       if (inputadapter_isNull_14) {\n/* 170 */         project_mutableStateArray_0[0].setNullAt(14);\n/* 171 */       } else {\n/* 172 */         project_mutableStateArray_0[0].write(14, inputadapter_value_14);\n/* 173 */       }\n/* 174 */\n/* 175 */       project_mutableStateArray_0[0].write(15, inputadapter_value_16);\n/* 176 */\n/* 177 */       if (inputadapter_isNull_15) {\n/* 178 */         project_mutableStateArray_0[0].setNullAt(16);\n/* 179 */       } else {\n/* 180 */         project_mutableStateArray_0[0].write(16, inputadapter_value_15);\n/* 181 */       }\n/* 182 */       sort_sorter_0.insertRow((UnsafeRow)(project_mutableStateArray_0[0].getRow()));\n/* 183 */       // shouldStop check is eliminated\n/* 184 */     }\n/* 185 */\n/* 186 */   }\n/* 187 */\n/* 188 */   protected void processNext() throws java.io.IOException {\n/* 189 */     if (sort_needToSort_0) {\n/* 190 */       long sort_spillSizeBefore_0 = sort_metrics_0.memoryBytesSpilled();\n/* 191 */       sort_addToSorter_0();\n/* 192 */       sort_sortedIter_0 = sort_sorter_0.sort();\n/* 193 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[3] /* sortTime */).add(sort_sorter_0.getSortTimeNanos() / 1000000);\n/* 194 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[1] /* peakMemory */).add(sort_sorter_0.getPeakMemoryUsage());\n/* 195 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[2] /* spillSize */).add(sort_metrics_0.memoryBytesSpilled() - sort_spillSizeBefore_0);\n/* 196 */       sort_metrics_0.incPeakExecutionMemory(sort_sorter_0.getPeakMemoryUsage());\n/* 197 */       sort_needToSort_0 = false;\n/* 198 */     }\n/* 199 */\n/* 200 */     while ( sort_sortedIter_0.hasNext()) {\n/* 201 */       UnsafeRow sort_outputRow_0 = (UnsafeRow)sort_sortedIter_0.next();\n/* 202 */\n/* 203 */       append(sort_outputRow_0);\n/* 204 */\n/* 205 */       if (shouldStop()) return;\n/* 206 */     }\n/* 207 */   }\n/* 208 */\n/* 209 */ }\n\n== Subtree 9 / 14 (maxMethodCodeSize:1023; maxConstantPoolSize:199(0.30% used); numInnerClasses:0) ==\n*(9) Sort [xid#0 ASC NULLS FIRST, weekday#284 ASC NULLS FIRST], false, 0\n+- *(9) Project [xid#0, action#1, date#2, website_id#3, url#4, category_id#5, zipcode#6, device#7, hour#283, weekday#284, n_events#695L, n_unique_day#700, n_unique_hour#718, n_device#732, n_unique_category_id#746, n_events_per_action#1172L, n_events_per_hour#1606L, _we0#1612]\n   +- Window [count(action#1) windowspecdefinition(xid#0, hour#283, specifiedwindowframe(RowFrame, unboundedpreceding$(), unboundedfollowing$())) AS n_events_per_hour#1606L], [xid#0, hour#283]\n      +- *(8) Sort [xid#0 ASC NULLS FIRST, hour#283 ASC NULLS FIRST], false, 0\n         +- *(8) Project [xid#0, action#1, date#2, website_id#3, url#4, category_id#5, zipcode#6, device#7, hour#283, weekday#284, n_events#695L, n_unique_day#700, n_unique_hour#718, n_device#732, n_unique_category_id#746, n_events_per_action#1172L, _we0#1612]\n            +- Window [count(action#1) windowspecdefinition(xid#0, action#1, specifiedwindowframe(RowFrame, unboundedpreceding$(), unboundedfollowing$())) AS n_events_per_action#1172L], [xid#0, action#1]\n               +- *(7) Sort [xid#0 ASC NULLS FIRST, action#1 ASC NULLS FIRST], false, 0\n                  +- *(7) Project [xid#0, action#1, date#2, website_id#3, url#4, category_id#5, zipcode#6, device#7, hour#283, weekday#284, n_events#695L, n_unique_day#700, n_unique_hour#718, n_device#732, n_unique_category_id#746, _we0#1612]\n                     +- Window [last(_w0#756, false) windowspecdefinition(xid#0, specifiedwindowframe(RowFrame, unboundedpreceding$(), unboundedfollowing$())) AS n_unique_category_id#746, max(date#2) windowspecdefinition(xid#0, specifiedwindowframe(RowFrame, unboundedpreceding$(), unboundedfollowing$())) AS _we0#1612], [xid#0]\n                        +- Window [dense_rank(category_id#5) windowspecdefinition(xid#0, category_id#5 ASC NULLS FIRST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS _w0#756], [xid#0], [category_id#5 ASC NULLS FIRST]\n                           +- *(6) Sort [xid#0 ASC NULLS FIRST, category_id#5 ASC NULLS FIRST], false, 0\n                              +- *(6) Project [xid#0, action#1, date#2, website_id#3, url#4, category_id#5, zipcode#6, device#7, hour#283, weekday#284, n_events#695L, n_unique_day#700, n_unique_hour#718, n_device#732]\n                                 +- Window [last(_w0#742, false) windowspecdefinition(xid#0, specifiedwindowframe(RowFrame, unboundedpreceding$(), unboundedfollowing$())) AS n_device#732], [xid#0]\n                                    +- Window [dense_rank(device#7) windowspecdefinition(xid#0, device#7 ASC NULLS FIRST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS _w0#742], [xid#0], [device#7 ASC NULLS FIRST]\n                                       +- *(5) Sort [xid#0 ASC NULLS FIRST, device#7 ASC NULLS FIRST], false, 0\n                                          +- *(5) Project [xid#0, action#1, date#2, website_id#3, url#4, category_id#5, zipcode#6, device#7, hour#283, weekday#284, n_events#695L, n_unique_day#700, n_unique_hour#718]\n                                             +- Window [last(_w0#728, false) windowspecdefinition(xid#0, specifiedwindowframe(RowFrame, unboundedpreceding$(), unboundedfollowing$())) AS n_unique_hour#718], [xid#0]\n                                                +- Window [dense_rank(hour#283) windowspecdefinition(xid#0, hour#283 ASC NULLS FIRST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS _w0#728], [xid#0], [hour#283 ASC NULLS FIRST]\n                                                   +- *(4) Sort [xid#0 ASC NULLS FIRST, hour#283 ASC NULLS FIRST], false, 0\n                                                      +- *(4) Project [xid#0, action#1, date#2, website_id#3, url#4, category_id#5, zipcode#6, device#7, hour#283, weekday#284, n_events#695L, n_unique_day#700]\n                                                         +- Window [last(_w0#710, false) windowspecdefinition(xid#0, specifiedwindowframe(RowFrame, unboundedpreceding$(), unboundedfollowing$())) AS n_unique_day#700], [xid#0]\n                                                            +- *(3) Project [xid#0, action#1, date#2, website_id#3, url#4, category_id#5, zipcode#6, device#7, hour#283, weekday#284, n_events#695L, _w0#710]\n                                                               +- Window [dense_rank(_w0#711) windowspecdefinition(xid#0, _w0#711 ASC NULLS FIRST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS _w0#710], [xid#0], [_w0#711 ASC NULLS FIRST]\n                                                                  +- *(2) Sort [xid#0 ASC NULLS FIRST, _w0#711 ASC NULLS FIRST], false, 0\n                                                                     +- *(2) Project [xid#0, action#1, date#2, website_id#3, url#4, category_id#5, zipcode#6, device#7, hour#283, weekday#284, n_events#695L, dayofyear(cast(date#2 as date)) AS _w0#711]\n                                                                        +- Window [count(action#1) windowspecdefinition(xid#0, specifiedwindowframe(RowFrame, unboundedpreceding$(), unboundedfollowing$())) AS n_events#695L], [xid#0]\n                                                                           +- *(1) Sort [xid#0 ASC NULLS FIRST], false, 0\n                                                                              +- TableCacheQueryStage 0\n                                                                                 +- InMemoryTableScan [xid#0, action#1, date#2, website_id#3, url#4, category_id#5, zipcode#6, device#7, hour#283, weekday#284]\n                                                                                       +- InMemoryRelation [xid#0, action#1, date#2, website_id#3, url#4, category_id#5, zipcode#6, device#7, hour#283, weekday#284], StorageLevel(disk, memory, deserialized, 1 replicas)\n                                                                                             +- AdaptiveSparkPlan isFinalPlan=true\n                                                                                                +- == Final Plan ==\n                                                                                                   ResultQueryStage 1\n                                                                                                   +- *(1) Project [xid#0, action#1, date#2, website_id#3, url#4, category_id#5, zipcode#6, device#7, hour(date#2, Some(Europe/Paris)) AS hour#283, date_format(date#2, EEEE, Some(Europe/Paris)) AS weekday#284]\n                                                                                                      +- TableCacheQueryStage 0\n                                                                                                         +- InMemoryTableScan [action#1, category_id#5, date#2, device#7, url#4, website_id#3, xid#0, zipcode#6]\n                                                                                                               +- InMemoryRelation [xid#0, action#1, date#2, website_id#3, url#4, category_id#5, zipcode#6, device#7], StorageLevel(disk, memory, deserialized, 1 replicas)\n                                                                                                                     +- AdaptiveSparkPlan isFinalPlan=true\n                     +- == Final Plan ==\n                        ResultQueryStage 1\n                        +- ShuffleQueryStage 0\n                           +- Exchange hashpartitioning(xid#0, 20), REPARTITION_BY_NUM, [plan_id=37]\n                              +- *(1) ColumnarToRow\n                                 +- FileScan parquet [xid#0,action#1,date#2,website_id#3,url#4,category_id#5,zipcode#6,device#7] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/home/boucheron/Documents/IFEBY310/core/notebooks/data/webdata.pa..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<xid:string,action:string,date:timestamp,website_id:string,url:string,category_id:float,zip...\n                     +- == Initial Plan ==\n                        Exchange hashpartitioning(xid#0, 20), REPARTITION_BY_NUM, [plan_id=22]\n                        +- FileScan parquet [xid#0,action#1,date#2,website_id#3,url#4,category_id#5,zipcode#6,device#7] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/home/boucheron/Documents/IFEBY310/core/notebooks/data/webdata.pa..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<xid:string,action:string,date:timestamp,website_id:string,url:string,category_id:float,zip...\n                                                                                                +- == Initial Plan ==\n                                                                                                   Project [xid#0, action#1, date#2, website_id#3, url#4, category_id#5, zipcode#6, device#7, hour(date#2, Some(Europe/Paris)) AS hour#283, date_format(date#2, EEEE, Some(Europe/Paris)) AS weekday#284]\n                                                                                                   +- InMemoryTableScan [action#1, category_id#5, date#2, device#7, url#4, website_id#3, xid#0, zipcode#6]\n                                                                                                         +- InMemoryRelation [xid#0, action#1, date#2, website_id#3, url#4, category_id#5, zipcode#6, device#7], StorageLevel(disk, memory, deserialized, 1 replicas)\n                                                                                                               +- AdaptiveSparkPlan isFinalPlan=true\n               +- == Final Plan ==\n                  ResultQueryStage 1\n                  +- ShuffleQueryStage 0\n                     +- Exchange hashpartitioning(xid#0, 20), REPARTITION_BY_NUM, [plan_id=37]\n                        +- *(1) ColumnarToRow\n                           +- FileScan parquet [xid#0,action#1,date#2,website_id#3,url#4,category_id#5,zipcode#6,device#7] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/home/boucheron/Documents/IFEBY310/core/notebooks/data/webdata.pa..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<xid:string,action:string,date:timestamp,website_id:string,url:string,category_id:float,zip...\n               +- == Initial Plan ==\n                  Exchange hashpartitioning(xid#0, 20), REPARTITION_BY_NUM, [plan_id=22]\n                  +- FileScan parquet [xid#0,action#1,date#2,website_id#3,url#4,category_id#5,zipcode#6,device#7] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/home/boucheron/Documents/IFEBY310/core/notebooks/data/webdata.pa..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<xid:string,action:string,date:timestamp,website_id:string,url:string,category_id:float,zip...\n\nGenerated code:\n/* 001 */ public Object generate(Object[] references) {\n/* 002 */   return new GeneratedIteratorForCodegenStage9(references);\n/* 003 */ }\n/* 004 */\n/* 005 */ // codegenStageId=9\n/* 006 */ final class GeneratedIteratorForCodegenStage9 extends org.apache.spark.sql.execution.BufferedRowIterator {\n/* 007 */   private Object[] references;\n/* 008 */   private scala.collection.Iterator[] inputs;\n/* 009 */   private boolean sort_needToSort_0;\n/* 010 */   private org.apache.spark.sql.execution.UnsafeExternalRowSorter sort_sorter_0;\n/* 011 */   private org.apache.spark.executor.TaskMetrics sort_metrics_0;\n/* 012 */   private scala.collection.Iterator<UnsafeRow> sort_sortedIter_0;\n/* 013 */   private scala.collection.Iterator inputadapter_input_0;\n/* 014 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] project_mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];\n/* 015 */\n/* 016 */   public GeneratedIteratorForCodegenStage9(Object[] references) {\n/* 017 */     this.references = references;\n/* 018 */   }\n/* 019 */\n/* 020 */   public void init(int index, scala.collection.Iterator[] inputs) {\n/* 021 */     partitionIndex = index;\n/* 022 */     this.inputs = inputs;\n/* 023 */     sort_needToSort_0 = true;\n/* 024 */     sort_sorter_0 = ((org.apache.spark.sql.execution.SortExec) references[0] /* plan */).createSorter();\n/* 025 */     sort_metrics_0 = org.apache.spark.TaskContext.get().taskMetrics();\n/* 026 */\n/* 027 */     inputadapter_input_0 = inputs[0];\n/* 028 */     project_mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(18, 224);\n/* 029 */\n/* 030 */   }\n/* 031 */\n/* 032 */   private void sort_addToSorter_0() throws java.io.IOException {\n/* 033 */     while ( inputadapter_input_0.hasNext()) {\n/* 034 */       InternalRow inputadapter_row_0 = (InternalRow) inputadapter_input_0.next();\n/* 035 */\n/* 036 */       // common sub-expressions\n/* 037 */\n/* 038 */       boolean inputadapter_isNull_0 = inputadapter_row_0.isNullAt(0);\n/* 039 */       UTF8String inputadapter_value_0 = inputadapter_isNull_0 ?\n/* 040 */       null : (inputadapter_row_0.getUTF8String(0));\n/* 041 */       boolean inputadapter_isNull_1 = inputadapter_row_0.isNullAt(1);\n/* 042 */       UTF8String inputadapter_value_1 = inputadapter_isNull_1 ?\n/* 043 */       null : (inputadapter_row_0.getUTF8String(1));\n/* 044 */       boolean inputadapter_isNull_2 = inputadapter_row_0.isNullAt(2);\n/* 045 */       long inputadapter_value_2 = inputadapter_isNull_2 ?\n/* 046 */       -1L : (inputadapter_row_0.getLong(2));\n/* 047 */       boolean inputadapter_isNull_3 = inputadapter_row_0.isNullAt(3);\n/* 048 */       UTF8String inputadapter_value_3 = inputadapter_isNull_3 ?\n/* 049 */       null : (inputadapter_row_0.getUTF8String(3));\n/* 050 */       boolean inputadapter_isNull_4 = inputadapter_row_0.isNullAt(4);\n/* 051 */       UTF8String inputadapter_value_4 = inputadapter_isNull_4 ?\n/* 052 */       null : (inputadapter_row_0.getUTF8String(4));\n/* 053 */       boolean inputadapter_isNull_5 = inputadapter_row_0.isNullAt(5);\n/* 054 */       float inputadapter_value_5 = inputadapter_isNull_5 ?\n/* 055 */       -1.0f : (inputadapter_row_0.getFloat(5));\n/* 056 */       boolean inputadapter_isNull_6 = inputadapter_row_0.isNullAt(6);\n/* 057 */       UTF8String inputadapter_value_6 = inputadapter_isNull_6 ?\n/* 058 */       null : (inputadapter_row_0.getUTF8String(6));\n/* 059 */       boolean inputadapter_isNull_7 = inputadapter_row_0.isNullAt(7);\n/* 060 */       UTF8String inputadapter_value_7 = inputadapter_isNull_7 ?\n/* 061 */       null : (inputadapter_row_0.getUTF8String(7));\n/* 062 */       boolean inputadapter_isNull_8 = inputadapter_row_0.isNullAt(8);\n/* 063 */       int inputadapter_value_8 = inputadapter_isNull_8 ?\n/* 064 */       -1 : (inputadapter_row_0.getInt(8));\n/* 065 */       boolean inputadapter_isNull_9 = inputadapter_row_0.isNullAt(9);\n/* 066 */       UTF8String inputadapter_value_9 = inputadapter_isNull_9 ?\n/* 067 */       null : (inputadapter_row_0.getUTF8String(9));\n/* 068 */       long inputadapter_value_10 = inputadapter_row_0.getLong(10);\n/* 069 */       boolean inputadapter_isNull_11 = inputadapter_row_0.isNullAt(11);\n/* 070 */       int inputadapter_value_11 = inputadapter_isNull_11 ?\n/* 071 */       -1 : (inputadapter_row_0.getInt(11));\n/* 072 */       boolean inputadapter_isNull_12 = inputadapter_row_0.isNullAt(12);\n/* 073 */       int inputadapter_value_12 = inputadapter_isNull_12 ?\n/* 074 */       -1 : (inputadapter_row_0.getInt(12));\n/* 075 */       boolean inputadapter_isNull_13 = inputadapter_row_0.isNullAt(13);\n/* 076 */       int inputadapter_value_13 = inputadapter_isNull_13 ?\n/* 077 */       -1 : (inputadapter_row_0.getInt(13));\n/* 078 */       boolean inputadapter_isNull_14 = inputadapter_row_0.isNullAt(14);\n/* 079 */       int inputadapter_value_14 = inputadapter_isNull_14 ?\n/* 080 */       -1 : (inputadapter_row_0.getInt(14));\n/* 081 */       long inputadapter_value_15 = inputadapter_row_0.getLong(15);\n/* 082 */       long inputadapter_value_17 = inputadapter_row_0.getLong(17);\n/* 083 */       boolean inputadapter_isNull_16 = inputadapter_row_0.isNullAt(16);\n/* 084 */       long inputadapter_value_16 = inputadapter_isNull_16 ?\n/* 085 */       -1L : (inputadapter_row_0.getLong(16));\n/* 086 */       project_mutableStateArray_0[0].reset();\n/* 087 */\n/* 088 */       project_mutableStateArray_0[0].zeroOutNullBytes();\n/* 089 */\n/* 090 */       if (inputadapter_isNull_0) {\n/* 091 */         project_mutableStateArray_0[0].setNullAt(0);\n/* 092 */       } else {\n/* 093 */         project_mutableStateArray_0[0].write(0, inputadapter_value_0);\n/* 094 */       }\n/* 095 */\n/* 096 */       if (inputadapter_isNull_1) {\n/* 097 */         project_mutableStateArray_0[0].setNullAt(1);\n/* 098 */       } else {\n/* 099 */         project_mutableStateArray_0[0].write(1, inputadapter_value_1);\n/* 100 */       }\n/* 101 */\n/* 102 */       if (inputadapter_isNull_2) {\n/* 103 */         project_mutableStateArray_0[0].setNullAt(2);\n/* 104 */       } else {\n/* 105 */         project_mutableStateArray_0[0].write(2, inputadapter_value_2);\n/* 106 */       }\n/* 107 */\n/* 108 */       if (inputadapter_isNull_3) {\n/* 109 */         project_mutableStateArray_0[0].setNullAt(3);\n/* 110 */       } else {\n/* 111 */         project_mutableStateArray_0[0].write(3, inputadapter_value_3);\n/* 112 */       }\n/* 113 */\n/* 114 */       if (inputadapter_isNull_4) {\n/* 115 */         project_mutableStateArray_0[0].setNullAt(4);\n/* 116 */       } else {\n/* 117 */         project_mutableStateArray_0[0].write(4, inputadapter_value_4);\n/* 118 */       }\n/* 119 */\n/* 120 */       if (inputadapter_isNull_5) {\n/* 121 */         project_mutableStateArray_0[0].setNullAt(5);\n/* 122 */       } else {\n/* 123 */         project_mutableStateArray_0[0].write(5, inputadapter_value_5);\n/* 124 */       }\n/* 125 */\n/* 126 */       if (inputadapter_isNull_6) {\n/* 127 */         project_mutableStateArray_0[0].setNullAt(6);\n/* 128 */       } else {\n/* 129 */         project_mutableStateArray_0[0].write(6, inputadapter_value_6);\n/* 130 */       }\n/* 131 */\n/* 132 */       if (inputadapter_isNull_7) {\n/* 133 */         project_mutableStateArray_0[0].setNullAt(7);\n/* 134 */       } else {\n/* 135 */         project_mutableStateArray_0[0].write(7, inputadapter_value_7);\n/* 136 */       }\n/* 137 */\n/* 138 */       if (inputadapter_isNull_8) {\n/* 139 */         project_mutableStateArray_0[0].setNullAt(8);\n/* 140 */       } else {\n/* 141 */         project_mutableStateArray_0[0].write(8, inputadapter_value_8);\n/* 142 */       }\n/* 143 */\n/* 144 */       if (inputadapter_isNull_9) {\n/* 145 */         project_mutableStateArray_0[0].setNullAt(9);\n/* 146 */       } else {\n/* 147 */         project_mutableStateArray_0[0].write(9, inputadapter_value_9);\n/* 148 */       }\n/* 149 */\n/* 150 */       project_mutableStateArray_0[0].write(10, inputadapter_value_10);\n/* 151 */\n/* 152 */       if (inputadapter_isNull_11) {\n/* 153 */         project_mutableStateArray_0[0].setNullAt(11);\n/* 154 */       } else {\n/* 155 */         project_mutableStateArray_0[0].write(11, inputadapter_value_11);\n/* 156 */       }\n/* 157 */\n/* 158 */       if (inputadapter_isNull_12) {\n/* 159 */         project_mutableStateArray_0[0].setNullAt(12);\n/* 160 */       } else {\n/* 161 */         project_mutableStateArray_0[0].write(12, inputadapter_value_12);\n/* 162 */       }\n/* 163 */\n/* 164 */       if (inputadapter_isNull_13) {\n/* 165 */         project_mutableStateArray_0[0].setNullAt(13);\n/* 166 */       } else {\n/* 167 */         project_mutableStateArray_0[0].write(13, inputadapter_value_13);\n/* 168 */       }\n/* 169 */\n/* 170 */       if (inputadapter_isNull_14) {\n/* 171 */         project_mutableStateArray_0[0].setNullAt(14);\n/* 172 */       } else {\n/* 173 */         project_mutableStateArray_0[0].write(14, inputadapter_value_14);\n/* 174 */       }\n/* 175 */\n/* 176 */       project_mutableStateArray_0[0].write(15, inputadapter_value_15);\n/* 177 */\n/* 178 */       project_mutableStateArray_0[0].write(16, inputadapter_value_17);\n/* 179 */\n/* 180 */       if (inputadapter_isNull_16) {\n/* 181 */         project_mutableStateArray_0[0].setNullAt(17);\n/* 182 */       } else {\n/* 183 */         project_mutableStateArray_0[0].write(17, inputadapter_value_16);\n/* 184 */       }\n/* 185 */       sort_sorter_0.insertRow((UnsafeRow)(project_mutableStateArray_0[0].getRow()));\n/* 186 */       // shouldStop check is eliminated\n/* 187 */     }\n/* 188 */\n/* 189 */   }\n/* 190 */\n/* 191 */   protected void processNext() throws java.io.IOException {\n/* 192 */     if (sort_needToSort_0) {\n/* 193 */       long sort_spillSizeBefore_0 = sort_metrics_0.memoryBytesSpilled();\n/* 194 */       sort_addToSorter_0();\n/* 195 */       sort_sortedIter_0 = sort_sorter_0.sort();\n/* 196 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[3] /* sortTime */).add(sort_sorter_0.getSortTimeNanos() / 1000000);\n/* 197 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[1] /* peakMemory */).add(sort_sorter_0.getPeakMemoryUsage());\n/* 198 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[2] /* spillSize */).add(sort_metrics_0.memoryBytesSpilled() - sort_spillSizeBefore_0);\n/* 199 */       sort_metrics_0.incPeakExecutionMemory(sort_sorter_0.getPeakMemoryUsage());\n/* 200 */       sort_needToSort_0 = false;\n/* 201 */     }\n/* 202 */\n/* 203 */     while ( sort_sortedIter_0.hasNext()) {\n/* 204 */       UnsafeRow sort_outputRow_0 = (UnsafeRow)sort_sortedIter_0.next();\n/* 205 */\n/* 206 */       append(sort_outputRow_0);\n/* 207 */\n/* 208 */       if (shouldStop()) return;\n/* 209 */     }\n/* 210 */   }\n/* 211 */\n/* 212 */ }\n\n== Subtree 10 / 14 (maxMethodCodeSize:1139; maxConstantPoolSize:214(0.33% used); numInnerClasses:0) ==\n*(10) Sort [xid#0 ASC NULLS FIRST, action#1 ASC NULLS FIRST], false, 0\n+- *(10) Project [xid#0, action#1, date#2, website_id#3, url#4, category_id#5, zipcode#6, device#7, hour#283, weekday#284, n_events#695L, (cast(datediff(2026-02-16, cast(_we0#1612 as date)) as double) + 0.1) AS n_days_since_last_event#1610, n_unique_day#700, n_unique_hour#718, n_device#732, n_unique_category_id#746, n_events_per_action#1172L, n_events_per_hour#1606L, n_events_per_weekday#1608L]\n   +- Window [count(action#1) windowspecdefinition(xid#0, weekday#284, specifiedwindowframe(RowFrame, unboundedpreceding$(), unboundedfollowing$())) AS n_events_per_weekday#1608L], [xid#0, weekday#284]\n      +- *(9) Sort [xid#0 ASC NULLS FIRST, weekday#284 ASC NULLS FIRST], false, 0\n         +- *(9) Project [xid#0, action#1, date#2, website_id#3, url#4, category_id#5, zipcode#6, device#7, hour#283, weekday#284, n_events#695L, n_unique_day#700, n_unique_hour#718, n_device#732, n_unique_category_id#746, n_events_per_action#1172L, n_events_per_hour#1606L, _we0#1612]\n            +- Window [count(action#1) windowspecdefinition(xid#0, hour#283, specifiedwindowframe(RowFrame, unboundedpreceding$(), unboundedfollowing$())) AS n_events_per_hour#1606L], [xid#0, hour#283]\n               +- *(8) Sort [xid#0 ASC NULLS FIRST, hour#283 ASC NULLS FIRST], false, 0\n                  +- *(8) Project [xid#0, action#1, date#2, website_id#3, url#4, category_id#5, zipcode#6, device#7, hour#283, weekday#284, n_events#695L, n_unique_day#700, n_unique_hour#718, n_device#732, n_unique_category_id#746, n_events_per_action#1172L, _we0#1612]\n                     +- Window [count(action#1) windowspecdefinition(xid#0, action#1, specifiedwindowframe(RowFrame, unboundedpreceding$(), unboundedfollowing$())) AS n_events_per_action#1172L], [xid#0, action#1]\n                        +- *(7) Sort [xid#0 ASC NULLS FIRST, action#1 ASC NULLS FIRST], false, 0\n                           +- *(7) Project [xid#0, action#1, date#2, website_id#3, url#4, category_id#5, zipcode#6, device#7, hour#283, weekday#284, n_events#695L, n_unique_day#700, n_unique_hour#718, n_device#732, n_unique_category_id#746, _we0#1612]\n                              +- Window [last(_w0#756, false) windowspecdefinition(xid#0, specifiedwindowframe(RowFrame, unboundedpreceding$(), unboundedfollowing$())) AS n_unique_category_id#746, max(date#2) windowspecdefinition(xid#0, specifiedwindowframe(RowFrame, unboundedpreceding$(), unboundedfollowing$())) AS _we0#1612], [xid#0]\n                                 +- Window [dense_rank(category_id#5) windowspecdefinition(xid#0, category_id#5 ASC NULLS FIRST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS _w0#756], [xid#0], [category_id#5 ASC NULLS FIRST]\n                                    +- *(6) Sort [xid#0 ASC NULLS FIRST, category_id#5 ASC NULLS FIRST], false, 0\n                                       +- *(6) Project [xid#0, action#1, date#2, website_id#3, url#4, category_id#5, zipcode#6, device#7, hour#283, weekday#284, n_events#695L, n_unique_day#700, n_unique_hour#718, n_device#732]\n                                          +- Window [last(_w0#742, false) windowspecdefinition(xid#0, specifiedwindowframe(RowFrame, unboundedpreceding$(), unboundedfollowing$())) AS n_device#732], [xid#0]\n                                             +- Window [dense_rank(device#7) windowspecdefinition(xid#0, device#7 ASC NULLS FIRST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS _w0#742], [xid#0], [device#7 ASC NULLS FIRST]\n                                                +- *(5) Sort [xid#0 ASC NULLS FIRST, device#7 ASC NULLS FIRST], false, 0\n                                                   +- *(5) Project [xid#0, action#1, date#2, website_id#3, url#4, category_id#5, zipcode#6, device#7, hour#283, weekday#284, n_events#695L, n_unique_day#700, n_unique_hour#718]\n                                                      +- Window [last(_w0#728, false) windowspecdefinition(xid#0, specifiedwindowframe(RowFrame, unboundedpreceding$(), unboundedfollowing$())) AS n_unique_hour#718], [xid#0]\n                                                         +- Window [dense_rank(hour#283) windowspecdefinition(xid#0, hour#283 ASC NULLS FIRST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS _w0#728], [xid#0], [hour#283 ASC NULLS FIRST]\n                                                            +- *(4) Sort [xid#0 ASC NULLS FIRST, hour#283 ASC NULLS FIRST], false, 0\n                                                               +- *(4) Project [xid#0, action#1, date#2, website_id#3, url#4, category_id#5, zipcode#6, device#7, hour#283, weekday#284, n_events#695L, n_unique_day#700]\n                                                                  +- Window [last(_w0#710, false) windowspecdefinition(xid#0, specifiedwindowframe(RowFrame, unboundedpreceding$(), unboundedfollowing$())) AS n_unique_day#700], [xid#0]\n                                                                     +- *(3) Project [xid#0, action#1, date#2, website_id#3, url#4, category_id#5, zipcode#6, device#7, hour#283, weekday#284, n_events#695L, _w0#710]\n                                                                        +- Window [dense_rank(_w0#711) windowspecdefinition(xid#0, _w0#711 ASC NULLS FIRST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS _w0#710], [xid#0], [_w0#711 ASC NULLS FIRST]\n                                                                           +- *(2) Sort [xid#0 ASC NULLS FIRST, _w0#711 ASC NULLS FIRST], false, 0\n                                                                              +- *(2) Project [xid#0, action#1, date#2, website_id#3, url#4, category_id#5, zipcode#6, device#7, hour#283, weekday#284, n_events#695L, dayofyear(cast(date#2 as date)) AS _w0#711]\n                                                                                 +- Window [count(action#1) windowspecdefinition(xid#0, specifiedwindowframe(RowFrame, unboundedpreceding$(), unboundedfollowing$())) AS n_events#695L], [xid#0]\n                                                                                    +- *(1) Sort [xid#0 ASC NULLS FIRST], false, 0\n                                                                                       +- TableCacheQueryStage 0\n                                                                                          +- InMemoryTableScan [xid#0, action#1, date#2, website_id#3, url#4, category_id#5, zipcode#6, device#7, hour#283, weekday#284]\n                                                                                                +- InMemoryRelation [xid#0, action#1, date#2, website_id#3, url#4, category_id#5, zipcode#6, device#7, hour#283, weekday#284], StorageLevel(disk, memory, deserialized, 1 replicas)\n                                                                                                      +- AdaptiveSparkPlan isFinalPlan=true\n                                                                                                         +- == Final Plan ==\n                                                                                                            ResultQueryStage 1\n                                                                                                            +- *(1) Project [xid#0, action#1, date#2, website_id#3, url#4, category_id#5, zipcode#6, device#7, hour(date#2, Some(Europe/Paris)) AS hour#283, date_format(date#2, EEEE, Some(Europe/Paris)) AS weekday#284]\n                                                                                                               +- TableCacheQueryStage 0\n                                                                                                                  +- InMemoryTableScan [action#1, category_id#5, date#2, device#7, url#4, website_id#3, xid#0, zipcode#6]\n                                                                                                                        +- InMemoryRelation [xid#0, action#1, date#2, website_id#3, url#4, category_id#5, zipcode#6, device#7], StorageLevel(disk, memory, deserialized, 1 replicas)\n                                                                                                                              +- AdaptiveSparkPlan isFinalPlan=true\n                     +- == Final Plan ==\n                        ResultQueryStage 1\n                        +- ShuffleQueryStage 0\n                           +- Exchange hashpartitioning(xid#0, 20), REPARTITION_BY_NUM, [plan_id=37]\n                              +- *(1) ColumnarToRow\n                                 +- FileScan parquet [xid#0,action#1,date#2,website_id#3,url#4,category_id#5,zipcode#6,device#7] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/home/boucheron/Documents/IFEBY310/core/notebooks/data/webdata.pa..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<xid:string,action:string,date:timestamp,website_id:string,url:string,category_id:float,zip...\n                     +- == Initial Plan ==\n                        Exchange hashpartitioning(xid#0, 20), REPARTITION_BY_NUM, [plan_id=22]\n                        +- FileScan parquet [xid#0,action#1,date#2,website_id#3,url#4,category_id#5,zipcode#6,device#7] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/home/boucheron/Documents/IFEBY310/core/notebooks/data/webdata.pa..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<xid:string,action:string,date:timestamp,website_id:string,url:string,category_id:float,zip...\n                                                                                                         +- == Initial Plan ==\n                                                                                                            Project [xid#0, action#1, date#2, website_id#3, url#4, category_id#5, zipcode#6, device#7, hour(date#2, Some(Europe/Paris)) AS hour#283, date_format(date#2, EEEE, Some(Europe/Paris)) AS weekday#284]\n                                                                                                            +- InMemoryTableScan [action#1, category_id#5, date#2, device#7, url#4, website_id#3, xid#0, zipcode#6]\n                                                                                                                  +- InMemoryRelation [xid#0, action#1, date#2, website_id#3, url#4, category_id#5, zipcode#6, device#7], StorageLevel(disk, memory, deserialized, 1 replicas)\n                                                                                                                        +- AdaptiveSparkPlan isFinalPlan=true\n               +- == Final Plan ==\n                  ResultQueryStage 1\n                  +- ShuffleQueryStage 0\n                     +- Exchange hashpartitioning(xid#0, 20), REPARTITION_BY_NUM, [plan_id=37]\n                        +- *(1) ColumnarToRow\n                           +- FileScan parquet [xid#0,action#1,date#2,website_id#3,url#4,category_id#5,zipcode#6,device#7] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/home/boucheron/Documents/IFEBY310/core/notebooks/data/webdata.pa..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<xid:string,action:string,date:timestamp,website_id:string,url:string,category_id:float,zip...\n               +- == Initial Plan ==\n                  Exchange hashpartitioning(xid#0, 20), REPARTITION_BY_NUM, [plan_id=22]\n                  +- FileScan parquet [xid#0,action#1,date#2,website_id#3,url#4,category_id#5,zipcode#6,device#7] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/home/boucheron/Documents/IFEBY310/core/notebooks/data/webdata.pa..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<xid:string,action:string,date:timestamp,website_id:string,url:string,category_id:float,zip...\n\nGenerated code:\n/* 001 */ public Object generate(Object[] references) {\n/* 002 */   return new GeneratedIteratorForCodegenStage10(references);\n/* 003 */ }\n/* 004 */\n/* 005 */ // codegenStageId=10\n/* 006 */ final class GeneratedIteratorForCodegenStage10 extends org.apache.spark.sql.execution.BufferedRowIterator {\n/* 007 */   private Object[] references;\n/* 008 */   private scala.collection.Iterator[] inputs;\n/* 009 */   private boolean sort_needToSort_0;\n/* 010 */   private org.apache.spark.sql.execution.UnsafeExternalRowSorter sort_sorter_0;\n/* 011 */   private org.apache.spark.executor.TaskMetrics sort_metrics_0;\n/* 012 */   private scala.collection.Iterator<UnsafeRow> sort_sortedIter_0;\n/* 013 */   private scala.collection.Iterator inputadapter_input_0;\n/* 014 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] project_mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];\n/* 015 */\n/* 016 */   public GeneratedIteratorForCodegenStage10(Object[] references) {\n/* 017 */     this.references = references;\n/* 018 */   }\n/* 019 */\n/* 020 */   public void init(int index, scala.collection.Iterator[] inputs) {\n/* 021 */     partitionIndex = index;\n/* 022 */     this.inputs = inputs;\n/* 023 */     sort_needToSort_0 = true;\n/* 024 */     sort_sorter_0 = ((org.apache.spark.sql.execution.SortExec) references[0] /* plan */).createSorter();\n/* 025 */     sort_metrics_0 = org.apache.spark.TaskContext.get().taskMetrics();\n/* 026 */\n/* 027 */     inputadapter_input_0 = inputs[0];\n/* 028 */     project_mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(19, 224);\n/* 029 */\n/* 030 */   }\n/* 031 */\n/* 032 */   private void sort_addToSorter_0() throws java.io.IOException {\n/* 033 */     while ( inputadapter_input_0.hasNext()) {\n/* 034 */       InternalRow inputadapter_row_0 = (InternalRow) inputadapter_input_0.next();\n/* 035 */\n/* 036 */       // common sub-expressions\n/* 037 */\n/* 038 */       boolean inputadapter_isNull_0 = inputadapter_row_0.isNullAt(0);\n/* 039 */       UTF8String inputadapter_value_0 = inputadapter_isNull_0 ?\n/* 040 */       null : (inputadapter_row_0.getUTF8String(0));\n/* 041 */       boolean inputadapter_isNull_1 = inputadapter_row_0.isNullAt(1);\n/* 042 */       UTF8String inputadapter_value_1 = inputadapter_isNull_1 ?\n/* 043 */       null : (inputadapter_row_0.getUTF8String(1));\n/* 044 */       boolean inputadapter_isNull_2 = inputadapter_row_0.isNullAt(2);\n/* 045 */       long inputadapter_value_2 = inputadapter_isNull_2 ?\n/* 046 */       -1L : (inputadapter_row_0.getLong(2));\n/* 047 */       boolean inputadapter_isNull_3 = inputadapter_row_0.isNullAt(3);\n/* 048 */       UTF8String inputadapter_value_3 = inputadapter_isNull_3 ?\n/* 049 */       null : (inputadapter_row_0.getUTF8String(3));\n/* 050 */       boolean inputadapter_isNull_4 = inputadapter_row_0.isNullAt(4);\n/* 051 */       UTF8String inputadapter_value_4 = inputadapter_isNull_4 ?\n/* 052 */       null : (inputadapter_row_0.getUTF8String(4));\n/* 053 */       boolean inputadapter_isNull_5 = inputadapter_row_0.isNullAt(5);\n/* 054 */       float inputadapter_value_5 = inputadapter_isNull_5 ?\n/* 055 */       -1.0f : (inputadapter_row_0.getFloat(5));\n/* 056 */       boolean inputadapter_isNull_6 = inputadapter_row_0.isNullAt(6);\n/* 057 */       UTF8String inputadapter_value_6 = inputadapter_isNull_6 ?\n/* 058 */       null : (inputadapter_row_0.getUTF8String(6));\n/* 059 */       boolean inputadapter_isNull_7 = inputadapter_row_0.isNullAt(7);\n/* 060 */       UTF8String inputadapter_value_7 = inputadapter_isNull_7 ?\n/* 061 */       null : (inputadapter_row_0.getUTF8String(7));\n/* 062 */       boolean inputadapter_isNull_8 = inputadapter_row_0.isNullAt(8);\n/* 063 */       int inputadapter_value_8 = inputadapter_isNull_8 ?\n/* 064 */       -1 : (inputadapter_row_0.getInt(8));\n/* 065 */       boolean inputadapter_isNull_9 = inputadapter_row_0.isNullAt(9);\n/* 066 */       UTF8String inputadapter_value_9 = inputadapter_isNull_9 ?\n/* 067 */       null : (inputadapter_row_0.getUTF8String(9));\n/* 068 */       long inputadapter_value_10 = inputadapter_row_0.getLong(10);\n/* 069 */       boolean project_isNull_11 = true;\n/* 070 */       double project_value_11 = -1.0;\n/* 071 */       boolean project_isNull_13 = true;\n/* 072 */       int project_value_13 = -1;\n/* 073 */\n/* 074 */       boolean inputadapter_isNull_17 = inputadapter_row_0.isNullAt(17);\n/* 075 */       long inputadapter_value_17 = inputadapter_isNull_17 ?\n/* 076 */       -1L : (inputadapter_row_0.getLong(17));\n/* 077 */       boolean project_isNull_15 = inputadapter_isNull_17;\n/* 078 */       int project_value_15 = -1;\n/* 079 */       if (!inputadapter_isNull_17) {\n/* 080 */         project_value_15 = org.apache.spark.sql.catalyst.util.DateTimeUtils.microsToDays(inputadapter_value_17, ((java.time.ZoneId) references[1] /* zoneId */));\n/* 081 */       }\n/* 082 */       if (!project_isNull_15) {\n/* 083 */         project_isNull_13 = false; // resultCode could change nullability.\n/* 084 */         project_value_13 = 20500 - project_value_15;\n/* 085 */\n/* 086 */       }\n/* 087 */       boolean project_isNull_12 = project_isNull_13;\n/* 088 */       double project_value_12 = -1.0;\n/* 089 */       if (!project_isNull_13) {\n/* 090 */         project_value_12 = (double) project_value_13;\n/* 091 */       }\n/* 092 */       if (!project_isNull_12) {\n/* 093 */         project_isNull_11 = false; // resultCode could change nullability.\n/* 094 */\n/* 095 */         project_value_11 = project_value_12 + 0.1D;\n/* 096 */\n/* 097 */       }\n/* 098 */       boolean inputadapter_isNull_11 = inputadapter_row_0.isNullAt(11);\n/* 099 */       int inputadapter_value_11 = inputadapter_isNull_11 ?\n/* 100 */       -1 : (inputadapter_row_0.getInt(11));\n/* 101 */       boolean inputadapter_isNull_12 = inputadapter_row_0.isNullAt(12);\n/* 102 */       int inputadapter_value_12 = inputadapter_isNull_12 ?\n/* 103 */       -1 : (inputadapter_row_0.getInt(12));\n/* 104 */       boolean inputadapter_isNull_13 = inputadapter_row_0.isNullAt(13);\n/* 105 */       int inputadapter_value_13 = inputadapter_isNull_13 ?\n/* 106 */       -1 : (inputadapter_row_0.getInt(13));\n/* 107 */       boolean inputadapter_isNull_14 = inputadapter_row_0.isNullAt(14);\n/* 108 */       int inputadapter_value_14 = inputadapter_isNull_14 ?\n/* 109 */       -1 : (inputadapter_row_0.getInt(14));\n/* 110 */       long inputadapter_value_15 = inputadapter_row_0.getLong(15);\n/* 111 */       long inputadapter_value_16 = inputadapter_row_0.getLong(16);\n/* 112 */       long inputadapter_value_18 = inputadapter_row_0.getLong(18);\n/* 113 */       project_mutableStateArray_0[0].reset();\n/* 114 */\n/* 115 */       project_mutableStateArray_0[0].zeroOutNullBytes();\n/* 116 */\n/* 117 */       if (inputadapter_isNull_0) {\n/* 118 */         project_mutableStateArray_0[0].setNullAt(0);\n/* 119 */       } else {\n/* 120 */         project_mutableStateArray_0[0].write(0, inputadapter_value_0);\n/* 121 */       }\n/* 122 */\n/* 123 */       if (inputadapter_isNull_1) {\n/* 124 */         project_mutableStateArray_0[0].setNullAt(1);\n/* 125 */       } else {\n/* 126 */         project_mutableStateArray_0[0].write(1, inputadapter_value_1);\n/* 127 */       }\n/* 128 */\n/* 129 */       if (inputadapter_isNull_2) {\n/* 130 */         project_mutableStateArray_0[0].setNullAt(2);\n/* 131 */       } else {\n/* 132 */         project_mutableStateArray_0[0].write(2, inputadapter_value_2);\n/* 133 */       }\n/* 134 */\n/* 135 */       if (inputadapter_isNull_3) {\n/* 136 */         project_mutableStateArray_0[0].setNullAt(3);\n/* 137 */       } else {\n/* 138 */         project_mutableStateArray_0[0].write(3, inputadapter_value_3);\n/* 139 */       }\n/* 140 */\n/* 141 */       if (inputadapter_isNull_4) {\n/* 142 */         project_mutableStateArray_0[0].setNullAt(4);\n/* 143 */       } else {\n/* 144 */         project_mutableStateArray_0[0].write(4, inputadapter_value_4);\n/* 145 */       }\n/* 146 */\n/* 147 */       if (inputadapter_isNull_5) {\n/* 148 */         project_mutableStateArray_0[0].setNullAt(5);\n/* 149 */       } else {\n/* 150 */         project_mutableStateArray_0[0].write(5, inputadapter_value_5);\n/* 151 */       }\n/* 152 */\n/* 153 */       if (inputadapter_isNull_6) {\n/* 154 */         project_mutableStateArray_0[0].setNullAt(6);\n/* 155 */       } else {\n/* 156 */         project_mutableStateArray_0[0].write(6, inputadapter_value_6);\n/* 157 */       }\n/* 158 */\n/* 159 */       if (inputadapter_isNull_7) {\n/* 160 */         project_mutableStateArray_0[0].setNullAt(7);\n/* 161 */       } else {\n/* 162 */         project_mutableStateArray_0[0].write(7, inputadapter_value_7);\n/* 163 */       }\n/* 164 */\n/* 165 */       if (inputadapter_isNull_8) {\n/* 166 */         project_mutableStateArray_0[0].setNullAt(8);\n/* 167 */       } else {\n/* 168 */         project_mutableStateArray_0[0].write(8, inputadapter_value_8);\n/* 169 */       }\n/* 170 */\n/* 171 */       if (inputadapter_isNull_9) {\n/* 172 */         project_mutableStateArray_0[0].setNullAt(9);\n/* 173 */       } else {\n/* 174 */         project_mutableStateArray_0[0].write(9, inputadapter_value_9);\n/* 175 */       }\n/* 176 */\n/* 177 */       project_mutableStateArray_0[0].write(10, inputadapter_value_10);\n/* 178 */\n/* 179 */       if (project_isNull_11) {\n/* 180 */         project_mutableStateArray_0[0].setNullAt(11);\n/* 181 */       } else {\n/* 182 */         project_mutableStateArray_0[0].write(11, project_value_11);\n/* 183 */       }\n/* 184 */\n/* 185 */       if (inputadapter_isNull_11) {\n/* 186 */         project_mutableStateArray_0[0].setNullAt(12);\n/* 187 */       } else {\n/* 188 */         project_mutableStateArray_0[0].write(12, inputadapter_value_11);\n/* 189 */       }\n/* 190 */\n/* 191 */       if (inputadapter_isNull_12) {\n/* 192 */         project_mutableStateArray_0[0].setNullAt(13);\n/* 193 */       } else {\n/* 194 */         project_mutableStateArray_0[0].write(13, inputadapter_value_12);\n/* 195 */       }\n/* 196 */\n/* 197 */       if (inputadapter_isNull_13) {\n/* 198 */         project_mutableStateArray_0[0].setNullAt(14);\n/* 199 */       } else {\n/* 200 */         project_mutableStateArray_0[0].write(14, inputadapter_value_13);\n/* 201 */       }\n/* 202 */\n/* 203 */       if (inputadapter_isNull_14) {\n/* 204 */         project_mutableStateArray_0[0].setNullAt(15);\n/* 205 */       } else {\n/* 206 */         project_mutableStateArray_0[0].write(15, inputadapter_value_14);\n/* 207 */       }\n/* 208 */\n/* 209 */       project_mutableStateArray_0[0].write(16, inputadapter_value_15);\n/* 210 */\n/* 211 */       project_mutableStateArray_0[0].write(17, inputadapter_value_16);\n/* 212 */\n/* 213 */       project_mutableStateArray_0[0].write(18, inputadapter_value_18);\n/* 214 */       sort_sorter_0.insertRow((UnsafeRow)(project_mutableStateArray_0[0].getRow()));\n/* 215 */       // shouldStop check is eliminated\n/* 216 */     }\n/* 217 */\n/* 218 */   }\n/* 219 */\n/* 220 */   protected void processNext() throws java.io.IOException {\n/* 221 */     if (sort_needToSort_0) {\n/* 222 */       long sort_spillSizeBefore_0 = sort_metrics_0.memoryBytesSpilled();\n/* 223 */       sort_addToSorter_0();\n/* 224 */       sort_sortedIter_0 = sort_sorter_0.sort();\n/* 225 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[4] /* sortTime */).add(sort_sorter_0.getSortTimeNanos() / 1000000);\n/* 226 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[2] /* peakMemory */).add(sort_sorter_0.getPeakMemoryUsage());\n/* 227 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[3] /* spillSize */).add(sort_metrics_0.memoryBytesSpilled() - sort_spillSizeBefore_0);\n/* 228 */       sort_metrics_0.incPeakExecutionMemory(sort_sorter_0.getPeakMemoryUsage());\n/* 229 */       sort_needToSort_0 = false;\n/* 230 */     }\n/* 231 */\n/* 232 */     while ( sort_sortedIter_0.hasNext()) {\n/* 233 */       UnsafeRow sort_outputRow_0 = (UnsafeRow)sort_sortedIter_0.next();\n/* 234 */\n/* 235 */       append(sort_outputRow_0);\n/* 236 */\n/* 237 */       if (shouldStop()) return;\n/* 238 */     }\n/* 239 */   }\n/* 240 */\n/* 241 */ }\n\n== Subtree 11 / 14 (maxMethodCodeSize:1202; maxConstantPoolSize:218(0.33% used); numInnerClasses:0) ==\n*(11) Sort [xid#0 ASC NULLS FIRST, device#7 ASC NULLS FIRST], false, 0\n+- *(11) Project [xid#0, action#1, date#2, website_id#3, url#4, category_id#5, zipcode#6, device#7, hour#283, weekday#284, n_events#695L, n_days_since_last_event#1610, n_unique_day#700, n_unique_hour#718, n_device#732, n_unique_category_id#746, n_events_per_action#1172L, n_events_per_hour#1606L, n_events_per_weekday#1608L, (cast(datediff(2026-02-16, cast(_we0#1615 as date)) as double) + 0.1) AS n_days_since_last_action#1613]\n   +- Window [max(date#2) windowspecdefinition(xid#0, action#1, specifiedwindowframe(RowFrame, unboundedpreceding$(), unboundedfollowing$())) AS _we0#1615], [xid#0, action#1]\n      +- *(10) Sort [xid#0 ASC NULLS FIRST, action#1 ASC NULLS FIRST], false, 0\n         +- *(10) Project [xid#0, action#1, date#2, website_id#3, url#4, category_id#5, zipcode#6, device#7, hour#283, weekday#284, n_events#695L, (cast(datediff(2026-02-16, cast(_we0#1612 as date)) as double) + 0.1) AS n_days_since_last_event#1610, n_unique_day#700, n_unique_hour#718, n_device#732, n_unique_category_id#746, n_events_per_action#1172L, n_events_per_hour#1606L, n_events_per_weekday#1608L]\n            +- Window [count(action#1) windowspecdefinition(xid#0, weekday#284, specifiedwindowframe(RowFrame, unboundedpreceding$(), unboundedfollowing$())) AS n_events_per_weekday#1608L], [xid#0, weekday#284]\n               +- *(9) Sort [xid#0 ASC NULLS FIRST, weekday#284 ASC NULLS FIRST], false, 0\n                  +- *(9) Project [xid#0, action#1, date#2, website_id#3, url#4, category_id#5, zipcode#6, device#7, hour#283, weekday#284, n_events#695L, n_unique_day#700, n_unique_hour#718, n_device#732, n_unique_category_id#746, n_events_per_action#1172L, n_events_per_hour#1606L, _we0#1612]\n                     +- Window [count(action#1) windowspecdefinition(xid#0, hour#283, specifiedwindowframe(RowFrame, unboundedpreceding$(), unboundedfollowing$())) AS n_events_per_hour#1606L], [xid#0, hour#283]\n                        +- *(8) Sort [xid#0 ASC NULLS FIRST, hour#283 ASC NULLS FIRST], false, 0\n                           +- *(8) Project [xid#0, action#1, date#2, website_id#3, url#4, category_id#5, zipcode#6, device#7, hour#283, weekday#284, n_events#695L, n_unique_day#700, n_unique_hour#718, n_device#732, n_unique_category_id#746, n_events_per_action#1172L, _we0#1612]\n                              +- Window [count(action#1) windowspecdefinition(xid#0, action#1, specifiedwindowframe(RowFrame, unboundedpreceding$(), unboundedfollowing$())) AS n_events_per_action#1172L], [xid#0, action#1]\n                                 +- *(7) Sort [xid#0 ASC NULLS FIRST, action#1 ASC NULLS FIRST], false, 0\n                                    +- *(7) Project [xid#0, action#1, date#2, website_id#3, url#4, category_id#5, zipcode#6, device#7, hour#283, weekday#284, n_events#695L, n_unique_day#700, n_unique_hour#718, n_device#732, n_unique_category_id#746, _we0#1612]\n                                       +- Window [last(_w0#756, false) windowspecdefinition(xid#0, specifiedwindowframe(RowFrame, unboundedpreceding$(), unboundedfollowing$())) AS n_unique_category_id#746, max(date#2) windowspecdefinition(xid#0, specifiedwindowframe(RowFrame, unboundedpreceding$(), unboundedfollowing$())) AS _we0#1612], [xid#0]\n                                          +- Window [dense_rank(category_id#5) windowspecdefinition(xid#0, category_id#5 ASC NULLS FIRST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS _w0#756], [xid#0], [category_id#5 ASC NULLS FIRST]\n                                             +- *(6) Sort [xid#0 ASC NULLS FIRST, category_id#5 ASC NULLS FIRST], false, 0\n                                                +- *(6) Project [xid#0, action#1, date#2, website_id#3, url#4, category_id#5, zipcode#6, device#7, hour#283, weekday#284, n_events#695L, n_unique_day#700, n_unique_hour#718, n_device#732]\n                                                   +- Window [last(_w0#742, false) windowspecdefinition(xid#0, specifiedwindowframe(RowFrame, unboundedpreceding$(), unboundedfollowing$())) AS n_device#732], [xid#0]\n                                                      +- Window [dense_rank(device#7) windowspecdefinition(xid#0, device#7 ASC NULLS FIRST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS _w0#742], [xid#0], [device#7 ASC NULLS FIRST]\n                                                         +- *(5) Sort [xid#0 ASC NULLS FIRST, device#7 ASC NULLS FIRST], false, 0\n                                                            +- *(5) Project [xid#0, action#1, date#2, website_id#3, url#4, category_id#5, zipcode#6, device#7, hour#283, weekday#284, n_events#695L, n_unique_day#700, n_unique_hour#718]\n                                                               +- Window [last(_w0#728, false) windowspecdefinition(xid#0, specifiedwindowframe(RowFrame, unboundedpreceding$(), unboundedfollowing$())) AS n_unique_hour#718], [xid#0]\n                                                                  +- Window [dense_rank(hour#283) windowspecdefinition(xid#0, hour#283 ASC NULLS FIRST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS _w0#728], [xid#0], [hour#283 ASC NULLS FIRST]\n                                                                     +- *(4) Sort [xid#0 ASC NULLS FIRST, hour#283 ASC NULLS FIRST], false, 0\n                                                                        +- *(4) Project [xid#0, action#1, date#2, website_id#3, url#4, category_id#5, zipcode#6, device#7, hour#283, weekday#284, n_events#695L, n_unique_day#700]\n                                                                           +- Window [last(_w0#710, false) windowspecdefinition(xid#0, specifiedwindowframe(RowFrame, unboundedpreceding$(), unboundedfollowing$())) AS n_unique_day#700], [xid#0]\n                                                                              +- *(3) Project [xid#0, action#1, date#2, website_id#3, url#4, category_id#5, zipcode#6, device#7, hour#283, weekday#284, n_events#695L, _w0#710]\n                                                                                 +- Window [dense_rank(_w0#711) windowspecdefinition(xid#0, _w0#711 ASC NULLS FIRST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS _w0#710], [xid#0], [_w0#711 ASC NULLS FIRST]\n                                                                                    +- *(2) Sort [xid#0 ASC NULLS FIRST, _w0#711 ASC NULLS FIRST], false, 0\n                                                                                       +- *(2) Project [xid#0, action#1, date#2, website_id#3, url#4, category_id#5, zipcode#6, device#7, hour#283, weekday#284, n_events#695L, dayofyear(cast(date#2 as date)) AS _w0#711]\n                                                                                          +- Window [count(action#1) windowspecdefinition(xid#0, specifiedwindowframe(RowFrame, unboundedpreceding$(), unboundedfollowing$())) AS n_events#695L], [xid#0]\n                                                                                             +- *(1) Sort [xid#0 ASC NULLS FIRST], false, 0\n                                                                                                +- TableCacheQueryStage 0\n                                                                                                   +- InMemoryTableScan [xid#0, action#1, date#2, website_id#3, url#4, category_id#5, zipcode#6, device#7, hour#283, weekday#284]\n                                                                                                         +- InMemoryRelation [xid#0, action#1, date#2, website_id#3, url#4, category_id#5, zipcode#6, device#7, hour#283, weekday#284], StorageLevel(disk, memory, deserialized, 1 replicas)\n                                                                                                               +- AdaptiveSparkPlan isFinalPlan=true\n                                                                                                                  +- == Final Plan ==\n                                                                                                                     ResultQueryStage 1\n                                                                                                                     +- *(1) Project [xid#0, action#1, date#2, website_id#3, url#4, category_id#5, zipcode#6, device#7, hour(date#2, Some(Europe/Paris)) AS hour#283, date_format(date#2, EEEE, Some(Europe/Paris)) AS weekday#284]\n                                                                                                                        +- TableCacheQueryStage 0\n                                                                                                                           +- InMemoryTableScan [action#1, category_id#5, date#2, device#7, url#4, website_id#3, xid#0, zipcode#6]\n                                                                                                                                 +- InMemoryRelation [xid#0, action#1, date#2, website_id#3, url#4, category_id#5, zipcode#6, device#7], StorageLevel(disk, memory, deserialized, 1 replicas)\n                                                                                                                                       +- AdaptiveSparkPlan isFinalPlan=true\n                     +- == Final Plan ==\n                        ResultQueryStage 1\n                        +- ShuffleQueryStage 0\n                           +- Exchange hashpartitioning(xid#0, 20), REPARTITION_BY_NUM, [plan_id=37]\n                              +- *(1) ColumnarToRow\n                                 +- FileScan parquet [xid#0,action#1,date#2,website_id#3,url#4,category_id#5,zipcode#6,device#7] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/home/boucheron/Documents/IFEBY310/core/notebooks/data/webdata.pa..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<xid:string,action:string,date:timestamp,website_id:string,url:string,category_id:float,zip...\n                     +- == Initial Plan ==\n                        Exchange hashpartitioning(xid#0, 20), REPARTITION_BY_NUM, [plan_id=22]\n                        +- FileScan parquet [xid#0,action#1,date#2,website_id#3,url#4,category_id#5,zipcode#6,device#7] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/home/boucheron/Documents/IFEBY310/core/notebooks/data/webdata.pa..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<xid:string,action:string,date:timestamp,website_id:string,url:string,category_id:float,zip...\n                                                                                                                  +- == Initial Plan ==\n                                                                                                                     Project [xid#0, action#1, date#2, website_id#3, url#4, category_id#5, zipcode#6, device#7, hour(date#2, Some(Europe/Paris)) AS hour#283, date_format(date#2, EEEE, Some(Europe/Paris)) AS weekday#284]\n                                                                                                                     +- InMemoryTableScan [action#1, category_id#5, date#2, device#7, url#4, website_id#3, xid#0, zipcode#6]\n                                                                                                                           +- InMemoryRelation [xid#0, action#1, date#2, website_id#3, url#4, category_id#5, zipcode#6, device#7], StorageLevel(disk, memory, deserialized, 1 replicas)\n                                                                                                                                 +- AdaptiveSparkPlan isFinalPlan=true\n               +- == Final Plan ==\n                  ResultQueryStage 1\n                  +- ShuffleQueryStage 0\n                     +- Exchange hashpartitioning(xid#0, 20), REPARTITION_BY_NUM, [plan_id=37]\n                        +- *(1) ColumnarToRow\n                           +- FileScan parquet [xid#0,action#1,date#2,website_id#3,url#4,category_id#5,zipcode#6,device#7] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/home/boucheron/Documents/IFEBY310/core/notebooks/data/webdata.pa..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<xid:string,action:string,date:timestamp,website_id:string,url:string,category_id:float,zip...\n               +- == Initial Plan ==\n                  Exchange hashpartitioning(xid#0, 20), REPARTITION_BY_NUM, [plan_id=22]\n                  +- FileScan parquet [xid#0,action#1,date#2,website_id#3,url#4,category_id#5,zipcode#6,device#7] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/home/boucheron/Documents/IFEBY310/core/notebooks/data/webdata.pa..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<xid:string,action:string,date:timestamp,website_id:string,url:string,category_id:float,zip...\n\nGenerated code:\n/* 001 */ public Object generate(Object[] references) {\n/* 002 */   return new GeneratedIteratorForCodegenStage11(references);\n/* 003 */ }\n/* 004 */\n/* 005 */ // codegenStageId=11\n/* 006 */ final class GeneratedIteratorForCodegenStage11 extends org.apache.spark.sql.execution.BufferedRowIterator {\n/* 007 */   private Object[] references;\n/* 008 */   private scala.collection.Iterator[] inputs;\n/* 009 */   private boolean sort_needToSort_0;\n/* 010 */   private org.apache.spark.sql.execution.UnsafeExternalRowSorter sort_sorter_0;\n/* 011 */   private org.apache.spark.executor.TaskMetrics sort_metrics_0;\n/* 012 */   private scala.collection.Iterator<UnsafeRow> sort_sortedIter_0;\n/* 013 */   private scala.collection.Iterator inputadapter_input_0;\n/* 014 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] project_mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];\n/* 015 */\n/* 016 */   public GeneratedIteratorForCodegenStage11(Object[] references) {\n/* 017 */     this.references = references;\n/* 018 */   }\n/* 019 */\n/* 020 */   public void init(int index, scala.collection.Iterator[] inputs) {\n/* 021 */     partitionIndex = index;\n/* 022 */     this.inputs = inputs;\n/* 023 */     sort_needToSort_0 = true;\n/* 024 */     sort_sorter_0 = ((org.apache.spark.sql.execution.SortExec) references[0] /* plan */).createSorter();\n/* 025 */     sort_metrics_0 = org.apache.spark.TaskContext.get().taskMetrics();\n/* 026 */\n/* 027 */     inputadapter_input_0 = inputs[0];\n/* 028 */     project_mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(20, 224);\n/* 029 */\n/* 030 */   }\n/* 031 */\n/* 032 */   private void sort_addToSorter_0() throws java.io.IOException {\n/* 033 */     while ( inputadapter_input_0.hasNext()) {\n/* 034 */       InternalRow inputadapter_row_0 = (InternalRow) inputadapter_input_0.next();\n/* 035 */\n/* 036 */       // common sub-expressions\n/* 037 */\n/* 038 */       boolean inputadapter_isNull_0 = inputadapter_row_0.isNullAt(0);\n/* 039 */       UTF8String inputadapter_value_0 = inputadapter_isNull_0 ?\n/* 040 */       null : (inputadapter_row_0.getUTF8String(0));\n/* 041 */       boolean inputadapter_isNull_1 = inputadapter_row_0.isNullAt(1);\n/* 042 */       UTF8String inputadapter_value_1 = inputadapter_isNull_1 ?\n/* 043 */       null : (inputadapter_row_0.getUTF8String(1));\n/* 044 */       boolean inputadapter_isNull_2 = inputadapter_row_0.isNullAt(2);\n/* 045 */       long inputadapter_value_2 = inputadapter_isNull_2 ?\n/* 046 */       -1L : (inputadapter_row_0.getLong(2));\n/* 047 */       boolean inputadapter_isNull_3 = inputadapter_row_0.isNullAt(3);\n/* 048 */       UTF8String inputadapter_value_3 = inputadapter_isNull_3 ?\n/* 049 */       null : (inputadapter_row_0.getUTF8String(3));\n/* 050 */       boolean inputadapter_isNull_4 = inputadapter_row_0.isNullAt(4);\n/* 051 */       UTF8String inputadapter_value_4 = inputadapter_isNull_4 ?\n/* 052 */       null : (inputadapter_row_0.getUTF8String(4));\n/* 053 */       boolean inputadapter_isNull_5 = inputadapter_row_0.isNullAt(5);\n/* 054 */       float inputadapter_value_5 = inputadapter_isNull_5 ?\n/* 055 */       -1.0f : (inputadapter_row_0.getFloat(5));\n/* 056 */       boolean inputadapter_isNull_6 = inputadapter_row_0.isNullAt(6);\n/* 057 */       UTF8String inputadapter_value_6 = inputadapter_isNull_6 ?\n/* 058 */       null : (inputadapter_row_0.getUTF8String(6));\n/* 059 */       boolean inputadapter_isNull_7 = inputadapter_row_0.isNullAt(7);\n/* 060 */       UTF8String inputadapter_value_7 = inputadapter_isNull_7 ?\n/* 061 */       null : (inputadapter_row_0.getUTF8String(7));\n/* 062 */       boolean inputadapter_isNull_8 = inputadapter_row_0.isNullAt(8);\n/* 063 */       int inputadapter_value_8 = inputadapter_isNull_8 ?\n/* 064 */       -1 : (inputadapter_row_0.getInt(8));\n/* 065 */       boolean inputadapter_isNull_9 = inputadapter_row_0.isNullAt(9);\n/* 066 */       UTF8String inputadapter_value_9 = inputadapter_isNull_9 ?\n/* 067 */       null : (inputadapter_row_0.getUTF8String(9));\n/* 068 */       long inputadapter_value_10 = inputadapter_row_0.getLong(10);\n/* 069 */       boolean inputadapter_isNull_11 = inputadapter_row_0.isNullAt(11);\n/* 070 */       double inputadapter_value_11 = inputadapter_isNull_11 ?\n/* 071 */       -1.0 : (inputadapter_row_0.getDouble(11));\n/* 072 */       boolean inputadapter_isNull_12 = inputadapter_row_0.isNullAt(12);\n/* 073 */       int inputadapter_value_12 = inputadapter_isNull_12 ?\n/* 074 */       -1 : (inputadapter_row_0.getInt(12));\n/* 075 */       boolean inputadapter_isNull_13 = inputadapter_row_0.isNullAt(13);\n/* 076 */       int inputadapter_value_13 = inputadapter_isNull_13 ?\n/* 077 */       -1 : (inputadapter_row_0.getInt(13));\n/* 078 */       boolean inputadapter_isNull_14 = inputadapter_row_0.isNullAt(14);\n/* 079 */       int inputadapter_value_14 = inputadapter_isNull_14 ?\n/* 080 */       -1 : (inputadapter_row_0.getInt(14));\n/* 081 */       boolean inputadapter_isNull_15 = inputadapter_row_0.isNullAt(15);\n/* 082 */       int inputadapter_value_15 = inputadapter_isNull_15 ?\n/* 083 */       -1 : (inputadapter_row_0.getInt(15));\n/* 084 */       long inputadapter_value_16 = inputadapter_row_0.getLong(16);\n/* 085 */       long inputadapter_value_17 = inputadapter_row_0.getLong(17);\n/* 086 */       long inputadapter_value_18 = inputadapter_row_0.getLong(18);\n/* 087 */       boolean project_isNull_19 = true;\n/* 088 */       double project_value_19 = -1.0;\n/* 089 */       boolean project_isNull_21 = true;\n/* 090 */       int project_value_21 = -1;\n/* 091 */\n/* 092 */       boolean inputadapter_isNull_19 = inputadapter_row_0.isNullAt(19);\n/* 093 */       long inputadapter_value_19 = inputadapter_isNull_19 ?\n/* 094 */       -1L : (inputadapter_row_0.getLong(19));\n/* 095 */       boolean project_isNull_23 = inputadapter_isNull_19;\n/* 096 */       int project_value_23 = -1;\n/* 097 */       if (!inputadapter_isNull_19) {\n/* 098 */         project_value_23 = org.apache.spark.sql.catalyst.util.DateTimeUtils.microsToDays(inputadapter_value_19, ((java.time.ZoneId) references[1] /* zoneId */));\n/* 099 */       }\n/* 100 */       if (!project_isNull_23) {\n/* 101 */         project_isNull_21 = false; // resultCode could change nullability.\n/* 102 */         project_value_21 = 20500 - project_value_23;\n/* 103 */\n/* 104 */       }\n/* 105 */       boolean project_isNull_20 = project_isNull_21;\n/* 106 */       double project_value_20 = -1.0;\n/* 107 */       if (!project_isNull_21) {\n/* 108 */         project_value_20 = (double) project_value_21;\n/* 109 */       }\n/* 110 */       if (!project_isNull_20) {\n/* 111 */         project_isNull_19 = false; // resultCode could change nullability.\n/* 112 */\n/* 113 */         project_value_19 = project_value_20 + 0.1D;\n/* 114 */\n/* 115 */       }\n/* 116 */       project_mutableStateArray_0[0].reset();\n/* 117 */\n/* 118 */       project_mutableStateArray_0[0].zeroOutNullBytes();\n/* 119 */\n/* 120 */       if (inputadapter_isNull_0) {\n/* 121 */         project_mutableStateArray_0[0].setNullAt(0);\n/* 122 */       } else {\n/* 123 */         project_mutableStateArray_0[0].write(0, inputadapter_value_0);\n/* 124 */       }\n/* 125 */\n/* 126 */       if (inputadapter_isNull_1) {\n/* 127 */         project_mutableStateArray_0[0].setNullAt(1);\n/* 128 */       } else {\n/* 129 */         project_mutableStateArray_0[0].write(1, inputadapter_value_1);\n/* 130 */       }\n/* 131 */\n/* 132 */       if (inputadapter_isNull_2) {\n/* 133 */         project_mutableStateArray_0[0].setNullAt(2);\n/* 134 */       } else {\n/* 135 */         project_mutableStateArray_0[0].write(2, inputadapter_value_2);\n/* 136 */       }\n/* 137 */\n/* 138 */       if (inputadapter_isNull_3) {\n/* 139 */         project_mutableStateArray_0[0].setNullAt(3);\n/* 140 */       } else {\n/* 141 */         project_mutableStateArray_0[0].write(3, inputadapter_value_3);\n/* 142 */       }\n/* 143 */\n/* 144 */       if (inputadapter_isNull_4) {\n/* 145 */         project_mutableStateArray_0[0].setNullAt(4);\n/* 146 */       } else {\n/* 147 */         project_mutableStateArray_0[0].write(4, inputadapter_value_4);\n/* 148 */       }\n/* 149 */\n/* 150 */       if (inputadapter_isNull_5) {\n/* 151 */         project_mutableStateArray_0[0].setNullAt(5);\n/* 152 */       } else {\n/* 153 */         project_mutableStateArray_0[0].write(5, inputadapter_value_5);\n/* 154 */       }\n/* 155 */\n/* 156 */       if (inputadapter_isNull_6) {\n/* 157 */         project_mutableStateArray_0[0].setNullAt(6);\n/* 158 */       } else {\n/* 159 */         project_mutableStateArray_0[0].write(6, inputadapter_value_6);\n/* 160 */       }\n/* 161 */\n/* 162 */       if (inputadapter_isNull_7) {\n/* 163 */         project_mutableStateArray_0[0].setNullAt(7);\n/* 164 */       } else {\n/* 165 */         project_mutableStateArray_0[0].write(7, inputadapter_value_7);\n/* 166 */       }\n/* 167 */\n/* 168 */       if (inputadapter_isNull_8) {\n/* 169 */         project_mutableStateArray_0[0].setNullAt(8);\n/* 170 */       } else {\n/* 171 */         project_mutableStateArray_0[0].write(8, inputadapter_value_8);\n/* 172 */       }\n/* 173 */\n/* 174 */       if (inputadapter_isNull_9) {\n/* 175 */         project_mutableStateArray_0[0].setNullAt(9);\n/* 176 */       } else {\n/* 177 */         project_mutableStateArray_0[0].write(9, inputadapter_value_9);\n/* 178 */       }\n/* 179 */\n/* 180 */       project_mutableStateArray_0[0].write(10, inputadapter_value_10);\n/* 181 */\n/* 182 */       if (inputadapter_isNull_11) {\n/* 183 */         project_mutableStateArray_0[0].setNullAt(11);\n/* 184 */       } else {\n/* 185 */         project_mutableStateArray_0[0].write(11, inputadapter_value_11);\n/* 186 */       }\n/* 187 */\n/* 188 */       if (inputadapter_isNull_12) {\n/* 189 */         project_mutableStateArray_0[0].setNullAt(12);\n/* 190 */       } else {\n/* 191 */         project_mutableStateArray_0[0].write(12, inputadapter_value_12);\n/* 192 */       }\n/* 193 */\n/* 194 */       if (inputadapter_isNull_13) {\n/* 195 */         project_mutableStateArray_0[0].setNullAt(13);\n/* 196 */       } else {\n/* 197 */         project_mutableStateArray_0[0].write(13, inputadapter_value_13);\n/* 198 */       }\n/* 199 */\n/* 200 */       if (inputadapter_isNull_14) {\n/* 201 */         project_mutableStateArray_0[0].setNullAt(14);\n/* 202 */       } else {\n/* 203 */         project_mutableStateArray_0[0].write(14, inputadapter_value_14);\n/* 204 */       }\n/* 205 */\n/* 206 */       if (inputadapter_isNull_15) {\n/* 207 */         project_mutableStateArray_0[0].setNullAt(15);\n/* 208 */       } else {\n/* 209 */         project_mutableStateArray_0[0].write(15, inputadapter_value_15);\n/* 210 */       }\n/* 211 */\n/* 212 */       project_mutableStateArray_0[0].write(16, inputadapter_value_16);\n/* 213 */\n/* 214 */       project_mutableStateArray_0[0].write(17, inputadapter_value_17);\n/* 215 */\n/* 216 */       project_mutableStateArray_0[0].write(18, inputadapter_value_18);\n/* 217 */\n/* 218 */       if (project_isNull_19) {\n/* 219 */         project_mutableStateArray_0[0].setNullAt(19);\n/* 220 */       } else {\n/* 221 */         project_mutableStateArray_0[0].write(19, project_value_19);\n/* 222 */       }\n/* 223 */       sort_sorter_0.insertRow((UnsafeRow)(project_mutableStateArray_0[0].getRow()));\n/* 224 */       // shouldStop check is eliminated\n/* 225 */     }\n/* 226 */\n/* 227 */   }\n/* 228 */\n/* 229 */   protected void processNext() throws java.io.IOException {\n/* 230 */     if (sort_needToSort_0) {\n/* 231 */       long sort_spillSizeBefore_0 = sort_metrics_0.memoryBytesSpilled();\n/* 232 */       sort_addToSorter_0();\n/* 233 */       sort_sortedIter_0 = sort_sorter_0.sort();\n/* 234 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[4] /* sortTime */).add(sort_sorter_0.getSortTimeNanos() / 1000000);\n/* 235 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[2] /* peakMemory */).add(sort_sorter_0.getPeakMemoryUsage());\n/* 236 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[3] /* spillSize */).add(sort_metrics_0.memoryBytesSpilled() - sort_spillSizeBefore_0);\n/* 237 */       sort_metrics_0.incPeakExecutionMemory(sort_sorter_0.getPeakMemoryUsage());\n/* 238 */       sort_needToSort_0 = false;\n/* 239 */     }\n/* 240 */\n/* 241 */     while ( sort_sortedIter_0.hasNext()) {\n/* 242 */       UnsafeRow sort_outputRow_0 = (UnsafeRow)sort_sortedIter_0.next();\n/* 243 */\n/* 244 */       append(sort_outputRow_0);\n/* 245 */\n/* 246 */       if (shouldStop()) return;\n/* 247 */     }\n/* 248 */   }\n/* 249 */\n/* 250 */ }\n\n== Subtree 12 / 14 (maxMethodCodeSize:154; maxConstantPoolSize:134(0.20% used); numInnerClasses:0) ==\n*(12) Sort [xid#0 ASC NULLS FIRST, knownfloatingpointnormalized(normalizenanandzero(category_id#5)) ASC NULLS FIRST], false, 0\n+- Window [count(device#7) windowspecdefinition(xid#0, device#7, specifiedwindowframe(RowFrame, unboundedpreceding$(), unboundedfollowing$())) AS n_events_per_device#1616L], [xid#0, device#7]\n   +- *(11) Sort [xid#0 ASC NULLS FIRST, device#7 ASC NULLS FIRST], false, 0\n      +- *(11) Project [xid#0, action#1, date#2, website_id#3, url#4, category_id#5, zipcode#6, device#7, hour#283, weekday#284, n_events#695L, n_days_since_last_event#1610, n_unique_day#700, n_unique_hour#718, n_device#732, n_unique_category_id#746, n_events_per_action#1172L, n_events_per_hour#1606L, n_events_per_weekday#1608L, (cast(datediff(2026-02-16, cast(_we0#1615 as date)) as double) + 0.1) AS n_days_since_last_action#1613]\n         +- Window [max(date#2) windowspecdefinition(xid#0, action#1, specifiedwindowframe(RowFrame, unboundedpreceding$(), unboundedfollowing$())) AS _we0#1615], [xid#0, action#1]\n            +- *(10) Sort [xid#0 ASC NULLS FIRST, action#1 ASC NULLS FIRST], false, 0\n               +- *(10) Project [xid#0, action#1, date#2, website_id#3, url#4, category_id#5, zipcode#6, device#7, hour#283, weekday#284, n_events#695L, (cast(datediff(2026-02-16, cast(_we0#1612 as date)) as double) + 0.1) AS n_days_since_last_event#1610, n_unique_day#700, n_unique_hour#718, n_device#732, n_unique_category_id#746, n_events_per_action#1172L, n_events_per_hour#1606L, n_events_per_weekday#1608L]\n                  +- Window [count(action#1) windowspecdefinition(xid#0, weekday#284, specifiedwindowframe(RowFrame, unboundedpreceding$(), unboundedfollowing$())) AS n_events_per_weekday#1608L], [xid#0, weekday#284]\n                     +- *(9) Sort [xid#0 ASC NULLS FIRST, weekday#284 ASC NULLS FIRST], false, 0\n                        +- *(9) Project [xid#0, action#1, date#2, website_id#3, url#4, category_id#5, zipcode#6, device#7, hour#283, weekday#284, n_events#695L, n_unique_day#700, n_unique_hour#718, n_device#732, n_unique_category_id#746, n_events_per_action#1172L, n_events_per_hour#1606L, _we0#1612]\n                           +- Window [count(action#1) windowspecdefinition(xid#0, hour#283, specifiedwindowframe(RowFrame, unboundedpreceding$(), unboundedfollowing$())) AS n_events_per_hour#1606L], [xid#0, hour#283]\n                              +- *(8) Sort [xid#0 ASC NULLS FIRST, hour#283 ASC NULLS FIRST], false, 0\n                                 +- *(8) Project [xid#0, action#1, date#2, website_id#3, url#4, category_id#5, zipcode#6, device#7, hour#283, weekday#284, n_events#695L, n_unique_day#700, n_unique_hour#718, n_device#732, n_unique_category_id#746, n_events_per_action#1172L, _we0#1612]\n                                    +- Window [count(action#1) windowspecdefinition(xid#0, action#1, specifiedwindowframe(RowFrame, unboundedpreceding$(), unboundedfollowing$())) AS n_events_per_action#1172L], [xid#0, action#1]\n                                       +- *(7) Sort [xid#0 ASC NULLS FIRST, action#1 ASC NULLS FIRST], false, 0\n                                          +- *(7) Project [xid#0, action#1, date#2, website_id#3, url#4, category_id#5, zipcode#6, device#7, hour#283, weekday#284, n_events#695L, n_unique_day#700, n_unique_hour#718, n_device#732, n_unique_category_id#746, _we0#1612]\n                                             +- Window [last(_w0#756, false) windowspecdefinition(xid#0, specifiedwindowframe(RowFrame, unboundedpreceding$(), unboundedfollowing$())) AS n_unique_category_id#746, max(date#2) windowspecdefinition(xid#0, specifiedwindowframe(RowFrame, unboundedpreceding$(), unboundedfollowing$())) AS _we0#1612], [xid#0]\n                                                +- Window [dense_rank(category_id#5) windowspecdefinition(xid#0, category_id#5 ASC NULLS FIRST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS _w0#756], [xid#0], [category_id#5 ASC NULLS FIRST]\n                                                   +- *(6) Sort [xid#0 ASC NULLS FIRST, category_id#5 ASC NULLS FIRST], false, 0\n                                                      +- *(6) Project [xid#0, action#1, date#2, website_id#3, url#4, category_id#5, zipcode#6, device#7, hour#283, weekday#284, n_events#695L, n_unique_day#700, n_unique_hour#718, n_device#732]\n                                                         +- Window [last(_w0#742, false) windowspecdefinition(xid#0, specifiedwindowframe(RowFrame, unboundedpreceding$(), unboundedfollowing$())) AS n_device#732], [xid#0]\n                                                            +- Window [dense_rank(device#7) windowspecdefinition(xid#0, device#7 ASC NULLS FIRST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS _w0#742], [xid#0], [device#7 ASC NULLS FIRST]\n                                                               +- *(5) Sort [xid#0 ASC NULLS FIRST, device#7 ASC NULLS FIRST], false, 0\n                                                                  +- *(5) Project [xid#0, action#1, date#2, website_id#3, url#4, category_id#5, zipcode#6, device#7, hour#283, weekday#284, n_events#695L, n_unique_day#700, n_unique_hour#718]\n                                                                     +- Window [last(_w0#728, false) windowspecdefinition(xid#0, specifiedwindowframe(RowFrame, unboundedpreceding$(), unboundedfollowing$())) AS n_unique_hour#718], [xid#0]\n                                                                        +- Window [dense_rank(hour#283) windowspecdefinition(xid#0, hour#283 ASC NULLS FIRST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS _w0#728], [xid#0], [hour#283 ASC NULLS FIRST]\n                                                                           +- *(4) Sort [xid#0 ASC NULLS FIRST, hour#283 ASC NULLS FIRST], false, 0\n                                                                              +- *(4) Project [xid#0, action#1, date#2, website_id#3, url#4, category_id#5, zipcode#6, device#7, hour#283, weekday#284, n_events#695L, n_unique_day#700]\n                                                                                 +- Window [last(_w0#710, false) windowspecdefinition(xid#0, specifiedwindowframe(RowFrame, unboundedpreceding$(), unboundedfollowing$())) AS n_unique_day#700], [xid#0]\n                                                                                    +- *(3) Project [xid#0, action#1, date#2, website_id#3, url#4, category_id#5, zipcode#6, device#7, hour#283, weekday#284, n_events#695L, _w0#710]\n                                                                                       +- Window [dense_rank(_w0#711) windowspecdefinition(xid#0, _w0#711 ASC NULLS FIRST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS _w0#710], [xid#0], [_w0#711 ASC NULLS FIRST]\n                                                                                          +- *(2) Sort [xid#0 ASC NULLS FIRST, _w0#711 ASC NULLS FIRST], false, 0\n                                                                                             +- *(2) Project [xid#0, action#1, date#2, website_id#3, url#4, category_id#5, zipcode#6, device#7, hour#283, weekday#284, n_events#695L, dayofyear(cast(date#2 as date)) AS _w0#711]\n                                                                                                +- Window [count(action#1) windowspecdefinition(xid#0, specifiedwindowframe(RowFrame, unboundedpreceding$(), unboundedfollowing$())) AS n_events#695L], [xid#0]\n                                                                                                   +- *(1) Sort [xid#0 ASC NULLS FIRST], false, 0\n                                                                                                      +- TableCacheQueryStage 0\n                                                                                                         +- InMemoryTableScan [xid#0, action#1, date#2, website_id#3, url#4, category_id#5, zipcode#6, device#7, hour#283, weekday#284]\n                                                                                                               +- InMemoryRelation [xid#0, action#1, date#2, website_id#3, url#4, category_id#5, zipcode#6, device#7, hour#283, weekday#284], StorageLevel(disk, memory, deserialized, 1 replicas)\n                                                                                                                     +- AdaptiveSparkPlan isFinalPlan=true\n                                                                                                                        +- == Final Plan ==\n                                                                                                                           ResultQueryStage 1\n                                                                                                                           +- *(1) Project [xid#0, action#1, date#2, website_id#3, url#4, category_id#5, zipcode#6, device#7, hour(date#2, Some(Europe/Paris)) AS hour#283, date_format(date#2, EEEE, Some(Europe/Paris)) AS weekday#284]\n                                                                                                                              +- TableCacheQueryStage 0\n                                                                                                                                 +- InMemoryTableScan [action#1, category_id#5, date#2, device#7, url#4, website_id#3, xid#0, zipcode#6]\n                                                                                                                                       +- InMemoryRelation [xid#0, action#1, date#2, website_id#3, url#4, category_id#5, zipcode#6, device#7], StorageLevel(disk, memory, deserialized, 1 replicas)\n                                                                                                                                             +- AdaptiveSparkPlan isFinalPlan=true\n                     +- == Final Plan ==\n                        ResultQueryStage 1\n                        +- ShuffleQueryStage 0\n                           +- Exchange hashpartitioning(xid#0, 20), REPARTITION_BY_NUM, [plan_id=37]\n                              +- *(1) ColumnarToRow\n                                 +- FileScan parquet [xid#0,action#1,date#2,website_id#3,url#4,category_id#5,zipcode#6,device#7] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/home/boucheron/Documents/IFEBY310/core/notebooks/data/webdata.pa..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<xid:string,action:string,date:timestamp,website_id:string,url:string,category_id:float,zip...\n                     +- == Initial Plan ==\n                        Exchange hashpartitioning(xid#0, 20), REPARTITION_BY_NUM, [plan_id=22]\n                        +- FileScan parquet [xid#0,action#1,date#2,website_id#3,url#4,category_id#5,zipcode#6,device#7] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/home/boucheron/Documents/IFEBY310/core/notebooks/data/webdata.pa..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<xid:string,action:string,date:timestamp,website_id:string,url:string,category_id:float,zip...\n                                                                                                                        +- == Initial Plan ==\n                                                                                                                           Project [xid#0, action#1, date#2, website_id#3, url#4, category_id#5, zipcode#6, device#7, hour(date#2, Some(Europe/Paris)) AS hour#283, date_format(date#2, EEEE, Some(Europe/Paris)) AS weekday#284]\n                                                                                                                           +- InMemoryTableScan [action#1, category_id#5, date#2, device#7, url#4, website_id#3, xid#0, zipcode#6]\n                                                                                                                                 +- InMemoryRelation [xid#0, action#1, date#2, website_id#3, url#4, category_id#5, zipcode#6, device#7], StorageLevel(disk, memory, deserialized, 1 replicas)\n                                                                                                                                       +- AdaptiveSparkPlan isFinalPlan=true\n               +- == Final Plan ==\n                  ResultQueryStage 1\n                  +- ShuffleQueryStage 0\n                     +- Exchange hashpartitioning(xid#0, 20), REPARTITION_BY_NUM, [plan_id=37]\n                        +- *(1) ColumnarToRow\n                           +- FileScan parquet [xid#0,action#1,date#2,website_id#3,url#4,category_id#5,zipcode#6,device#7] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/home/boucheron/Documents/IFEBY310/core/notebooks/data/webdata.pa..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<xid:string,action:string,date:timestamp,website_id:string,url:string,category_id:float,zip...\n               +- == Initial Plan ==\n                  Exchange hashpartitioning(xid#0, 20), REPARTITION_BY_NUM, [plan_id=22]\n                  +- FileScan parquet [xid#0,action#1,date#2,website_id#3,url#4,category_id#5,zipcode#6,device#7] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/home/boucheron/Documents/IFEBY310/core/notebooks/data/webdata.pa..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<xid:string,action:string,date:timestamp,website_id:string,url:string,category_id:float,zip...\n\nGenerated code:\n/* 001 */ public Object generate(Object[] references) {\n/* 002 */   return new GeneratedIteratorForCodegenStage12(references);\n/* 003 */ }\n/* 004 */\n/* 005 */ // codegenStageId=12\n/* 006 */ final class GeneratedIteratorForCodegenStage12 extends org.apache.spark.sql.execution.BufferedRowIterator {\n/* 007 */   private Object[] references;\n/* 008 */   private scala.collection.Iterator[] inputs;\n/* 009 */   private boolean sort_needToSort_0;\n/* 010 */   private org.apache.spark.sql.execution.UnsafeExternalRowSorter sort_sorter_0;\n/* 011 */   private org.apache.spark.executor.TaskMetrics sort_metrics_0;\n/* 012 */   private scala.collection.Iterator<UnsafeRow> sort_sortedIter_0;\n/* 013 */   private scala.collection.Iterator inputadapter_input_0;\n/* 014 */\n/* 015 */   public GeneratedIteratorForCodegenStage12(Object[] references) {\n/* 016 */     this.references = references;\n/* 017 */   }\n/* 018 */\n/* 019 */   public void init(int index, scala.collection.Iterator[] inputs) {\n/* 020 */     partitionIndex = index;\n/* 021 */     this.inputs = inputs;\n/* 022 */     sort_needToSort_0 = true;\n/* 023 */     sort_sorter_0 = ((org.apache.spark.sql.execution.SortExec) references[0] /* plan */).createSorter();\n/* 024 */     sort_metrics_0 = org.apache.spark.TaskContext.get().taskMetrics();\n/* 025 */\n/* 026 */     inputadapter_input_0 = inputs[0];\n/* 027 */\n/* 028 */   }\n/* 029 */\n/* 030 */   private void sort_addToSorter_0() throws java.io.IOException {\n/* 031 */     while ( inputadapter_input_0.hasNext()) {\n/* 032 */       InternalRow inputadapter_row_0 = (InternalRow) inputadapter_input_0.next();\n/* 033 */\n/* 034 */       sort_sorter_0.insertRow((UnsafeRow)inputadapter_row_0);\n/* 035 */       // shouldStop check is eliminated\n/* 036 */     }\n/* 037 */\n/* 038 */   }\n/* 039 */\n/* 040 */   protected void processNext() throws java.io.IOException {\n/* 041 */     if (sort_needToSort_0) {\n/* 042 */       long sort_spillSizeBefore_0 = sort_metrics_0.memoryBytesSpilled();\n/* 043 */       sort_addToSorter_0();\n/* 044 */       sort_sortedIter_0 = sort_sorter_0.sort();\n/* 045 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[3] /* sortTime */).add(sort_sorter_0.getSortTimeNanos() / 1000000);\n/* 046 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[1] /* peakMemory */).add(sort_sorter_0.getPeakMemoryUsage());\n/* 047 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[2] /* spillSize */).add(sort_metrics_0.memoryBytesSpilled() - sort_spillSizeBefore_0);\n/* 048 */       sort_metrics_0.incPeakExecutionMemory(sort_sorter_0.getPeakMemoryUsage());\n/* 049 */       sort_needToSort_0 = false;\n/* 050 */     }\n/* 051 */\n/* 052 */     while ( sort_sortedIter_0.hasNext()) {\n/* 053 */       UnsafeRow sort_outputRow_0 = (UnsafeRow)sort_sortedIter_0.next();\n/* 054 */\n/* 055 */       append(sort_outputRow_0);\n/* 056 */\n/* 057 */       if (shouldStop()) return;\n/* 058 */     }\n/* 059 */   }\n/* 060 */\n/* 061 */ }\n\n== Subtree 13 / 14 (maxMethodCodeSize:154; maxConstantPoolSize:134(0.20% used); numInnerClasses:0) ==\n*(13) Sort [xid#0 ASC NULLS FIRST, knownfloatingpointnormalized(normalizenanandzero(category_id#5)) ASC NULLS FIRST, action#1 ASC NULLS FIRST], false, 0\n+- Window [count(action#1) windowspecdefinition(xid#0, category_id#5, specifiedwindowframe(RowFrame, unboundedpreceding$(), unboundedfollowing$())) AS n_events_per_category_id#1620L], [xid#0, knownfloatingpointnormalized(normalizenanandzero(category_id#5))]\n   +- *(12) Sort [xid#0 ASC NULLS FIRST, knownfloatingpointnormalized(normalizenanandzero(category_id#5)) ASC NULLS FIRST], false, 0\n      +- Window [count(device#7) windowspecdefinition(xid#0, device#7, specifiedwindowframe(RowFrame, unboundedpreceding$(), unboundedfollowing$())) AS n_events_per_device#1616L], [xid#0, device#7]\n         +- *(11) Sort [xid#0 ASC NULLS FIRST, device#7 ASC NULLS FIRST], false, 0\n            +- *(11) Project [xid#0, action#1, date#2, website_id#3, url#4, category_id#5, zipcode#6, device#7, hour#283, weekday#284, n_events#695L, n_days_since_last_event#1610, n_unique_day#700, n_unique_hour#718, n_device#732, n_unique_category_id#746, n_events_per_action#1172L, n_events_per_hour#1606L, n_events_per_weekday#1608L, (cast(datediff(2026-02-16, cast(_we0#1615 as date)) as double) + 0.1) AS n_days_since_last_action#1613]\n               +- Window [max(date#2) windowspecdefinition(xid#0, action#1, specifiedwindowframe(RowFrame, unboundedpreceding$(), unboundedfollowing$())) AS _we0#1615], [xid#0, action#1]\n                  +- *(10) Sort [xid#0 ASC NULLS FIRST, action#1 ASC NULLS FIRST], false, 0\n                     +- *(10) Project [xid#0, action#1, date#2, website_id#3, url#4, category_id#5, zipcode#6, device#7, hour#283, weekday#284, n_events#695L, (cast(datediff(2026-02-16, cast(_we0#1612 as date)) as double) + 0.1) AS n_days_since_last_event#1610, n_unique_day#700, n_unique_hour#718, n_device#732, n_unique_category_id#746, n_events_per_action#1172L, n_events_per_hour#1606L, n_events_per_weekday#1608L]\n                        +- Window [count(action#1) windowspecdefinition(xid#0, weekday#284, specifiedwindowframe(RowFrame, unboundedpreceding$(), unboundedfollowing$())) AS n_events_per_weekday#1608L], [xid#0, weekday#284]\n                           +- *(9) Sort [xid#0 ASC NULLS FIRST, weekday#284 ASC NULLS FIRST], false, 0\n                              +- *(9) Project [xid#0, action#1, date#2, website_id#3, url#4, category_id#5, zipcode#6, device#7, hour#283, weekday#284, n_events#695L, n_unique_day#700, n_unique_hour#718, n_device#732, n_unique_category_id#746, n_events_per_action#1172L, n_events_per_hour#1606L, _we0#1612]\n                                 +- Window [count(action#1) windowspecdefinition(xid#0, hour#283, specifiedwindowframe(RowFrame, unboundedpreceding$(), unboundedfollowing$())) AS n_events_per_hour#1606L], [xid#0, hour#283]\n                                    +- *(8) Sort [xid#0 ASC NULLS FIRST, hour#283 ASC NULLS FIRST], false, 0\n                                       +- *(8) Project [xid#0, action#1, date#2, website_id#3, url#4, category_id#5, zipcode#6, device#7, hour#283, weekday#284, n_events#695L, n_unique_day#700, n_unique_hour#718, n_device#732, n_unique_category_id#746, n_events_per_action#1172L, _we0#1612]\n                                          +- Window [count(action#1) windowspecdefinition(xid#0, action#1, specifiedwindowframe(RowFrame, unboundedpreceding$(), unboundedfollowing$())) AS n_events_per_action#1172L], [xid#0, action#1]\n                                             +- *(7) Sort [xid#0 ASC NULLS FIRST, action#1 ASC NULLS FIRST], false, 0\n                                                +- *(7) Project [xid#0, action#1, date#2, website_id#3, url#4, category_id#5, zipcode#6, device#7, hour#283, weekday#284, n_events#695L, n_unique_day#700, n_unique_hour#718, n_device#732, n_unique_category_id#746, _we0#1612]\n                                                   +- Window [last(_w0#756, false) windowspecdefinition(xid#0, specifiedwindowframe(RowFrame, unboundedpreceding$(), unboundedfollowing$())) AS n_unique_category_id#746, max(date#2) windowspecdefinition(xid#0, specifiedwindowframe(RowFrame, unboundedpreceding$(), unboundedfollowing$())) AS _we0#1612], [xid#0]\n                                                      +- Window [dense_rank(category_id#5) windowspecdefinition(xid#0, category_id#5 ASC NULLS FIRST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS _w0#756], [xid#0], [category_id#5 ASC NULLS FIRST]\n                                                         +- *(6) Sort [xid#0 ASC NULLS FIRST, category_id#5 ASC NULLS FIRST], false, 0\n                                                            +- *(6) Project [xid#0, action#1, date#2, website_id#3, url#4, category_id#5, zipcode#6, device#7, hour#283, weekday#284, n_events#695L, n_unique_day#700, n_unique_hour#718, n_device#732]\n                                                               +- Window [last(_w0#742, false) windowspecdefinition(xid#0, specifiedwindowframe(RowFrame, unboundedpreceding$(), unboundedfollowing$())) AS n_device#732], [xid#0]\n                                                                  +- Window [dense_rank(device#7) windowspecdefinition(xid#0, device#7 ASC NULLS FIRST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS _w0#742], [xid#0], [device#7 ASC NULLS FIRST]\n                                                                     +- *(5) Sort [xid#0 ASC NULLS FIRST, device#7 ASC NULLS FIRST], false, 0\n                                                                        +- *(5) Project [xid#0, action#1, date#2, website_id#3, url#4, category_id#5, zipcode#6, device#7, hour#283, weekday#284, n_events#695L, n_unique_day#700, n_unique_hour#718]\n                                                                           +- Window [last(_w0#728, false) windowspecdefinition(xid#0, specifiedwindowframe(RowFrame, unboundedpreceding$(), unboundedfollowing$())) AS n_unique_hour#718], [xid#0]\n                                                                              +- Window [dense_rank(hour#283) windowspecdefinition(xid#0, hour#283 ASC NULLS FIRST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS _w0#728], [xid#0], [hour#283 ASC NULLS FIRST]\n                                                                                 +- *(4) Sort [xid#0 ASC NULLS FIRST, hour#283 ASC NULLS FIRST], false, 0\n                                                                                    +- *(4) Project [xid#0, action#1, date#2, website_id#3, url#4, category_id#5, zipcode#6, device#7, hour#283, weekday#284, n_events#695L, n_unique_day#700]\n                                                                                       +- Window [last(_w0#710, false) windowspecdefinition(xid#0, specifiedwindowframe(RowFrame, unboundedpreceding$(), unboundedfollowing$())) AS n_unique_day#700], [xid#0]\n                                                                                          +- *(3) Project [xid#0, action#1, date#2, website_id#3, url#4, category_id#5, zipcode#6, device#7, hour#283, weekday#284, n_events#695L, _w0#710]\n                                                                                             +- Window [dense_rank(_w0#711) windowspecdefinition(xid#0, _w0#711 ASC NULLS FIRST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS _w0#710], [xid#0], [_w0#711 ASC NULLS FIRST]\n                                                                                                +- *(2) Sort [xid#0 ASC NULLS FIRST, _w0#711 ASC NULLS FIRST], false, 0\n                                                                                                   +- *(2) Project [xid#0, action#1, date#2, website_id#3, url#4, category_id#5, zipcode#6, device#7, hour#283, weekday#284, n_events#695L, dayofyear(cast(date#2 as date)) AS _w0#711]\n                                                                                                      +- Window [count(action#1) windowspecdefinition(xid#0, specifiedwindowframe(RowFrame, unboundedpreceding$(), unboundedfollowing$())) AS n_events#695L], [xid#0]\n                                                                                                         +- *(1) Sort [xid#0 ASC NULLS FIRST], false, 0\n                                                                                                            +- TableCacheQueryStage 0\n                                                                                                               +- InMemoryTableScan [xid#0, action#1, date#2, website_id#3, url#4, category_id#5, zipcode#6, device#7, hour#283, weekday#284]\n                                                                                                                     +- InMemoryRelation [xid#0, action#1, date#2, website_id#3, url#4, category_id#5, zipcode#6, device#7, hour#283, weekday#284], StorageLevel(disk, memory, deserialized, 1 replicas)\n                                                                                                                           +- AdaptiveSparkPlan isFinalPlan=true\n                                                                                                                              +- == Final Plan ==\n                                                                                                                                 ResultQueryStage 1\n                                                                                                                                 +- *(1) Project [xid#0, action#1, date#2, website_id#3, url#4, category_id#5, zipcode#6, device#7, hour(date#2, Some(Europe/Paris)) AS hour#283, date_format(date#2, EEEE, Some(Europe/Paris)) AS weekday#284]\n                                                                                                                                    +- TableCacheQueryStage 0\n                                                                                                                                       +- InMemoryTableScan [action#1, category_id#5, date#2, device#7, url#4, website_id#3, xid#0, zipcode#6]\n                                                                                                                                             +- InMemoryRelation [xid#0, action#1, date#2, website_id#3, url#4, category_id#5, zipcode#6, device#7], StorageLevel(disk, memory, deserialized, 1 replicas)\n                                                                                                                                                   +- AdaptiveSparkPlan isFinalPlan=true\n                     +- == Final Plan ==\n                        ResultQueryStage 1\n                        +- ShuffleQueryStage 0\n                           +- Exchange hashpartitioning(xid#0, 20), REPARTITION_BY_NUM, [plan_id=37]\n                              +- *(1) ColumnarToRow\n                                 +- FileScan parquet [xid#0,action#1,date#2,website_id#3,url#4,category_id#5,zipcode#6,device#7] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/home/boucheron/Documents/IFEBY310/core/notebooks/data/webdata.pa..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<xid:string,action:string,date:timestamp,website_id:string,url:string,category_id:float,zip...\n                     +- == Initial Plan ==\n                        Exchange hashpartitioning(xid#0, 20), REPARTITION_BY_NUM, [plan_id=22]\n                        +- FileScan parquet [xid#0,action#1,date#2,website_id#3,url#4,category_id#5,zipcode#6,device#7] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/home/boucheron/Documents/IFEBY310/core/notebooks/data/webdata.pa..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<xid:string,action:string,date:timestamp,website_id:string,url:string,category_id:float,zip...\n                                                                                                                              +- == Initial Plan ==\n                                                                                                                                 Project [xid#0, action#1, date#2, website_id#3, url#4, category_id#5, zipcode#6, device#7, hour(date#2, Some(Europe/Paris)) AS hour#283, date_format(date#2, EEEE, Some(Europe/Paris)) AS weekday#284]\n                                                                                                                                 +- InMemoryTableScan [action#1, category_id#5, date#2, device#7, url#4, website_id#3, xid#0, zipcode#6]\n                                                                                                                                       +- InMemoryRelation [xid#0, action#1, date#2, website_id#3, url#4, category_id#5, zipcode#6, device#7], StorageLevel(disk, memory, deserialized, 1 replicas)\n                                                                                                                                             +- AdaptiveSparkPlan isFinalPlan=true\n               +- == Final Plan ==\n                  ResultQueryStage 1\n                  +- ShuffleQueryStage 0\n                     +- Exchange hashpartitioning(xid#0, 20), REPARTITION_BY_NUM, [plan_id=37]\n                        +- *(1) ColumnarToRow\n                           +- FileScan parquet [xid#0,action#1,date#2,website_id#3,url#4,category_id#5,zipcode#6,device#7] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/home/boucheron/Documents/IFEBY310/core/notebooks/data/webdata.pa..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<xid:string,action:string,date:timestamp,website_id:string,url:string,category_id:float,zip...\n               +- == Initial Plan ==\n                  Exchange hashpartitioning(xid#0, 20), REPARTITION_BY_NUM, [plan_id=22]\n                  +- FileScan parquet [xid#0,action#1,date#2,website_id#3,url#4,category_id#5,zipcode#6,device#7] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/home/boucheron/Documents/IFEBY310/core/notebooks/data/webdata.pa..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<xid:string,action:string,date:timestamp,website_id:string,url:string,category_id:float,zip...\n\nGenerated code:\n/* 001 */ public Object generate(Object[] references) {\n/* 002 */   return new GeneratedIteratorForCodegenStage13(references);\n/* 003 */ }\n/* 004 */\n/* 005 */ // codegenStageId=13\n/* 006 */ final class GeneratedIteratorForCodegenStage13 extends org.apache.spark.sql.execution.BufferedRowIterator {\n/* 007 */   private Object[] references;\n/* 008 */   private scala.collection.Iterator[] inputs;\n/* 009 */   private boolean sort_needToSort_0;\n/* 010 */   private org.apache.spark.sql.execution.UnsafeExternalRowSorter sort_sorter_0;\n/* 011 */   private org.apache.spark.executor.TaskMetrics sort_metrics_0;\n/* 012 */   private scala.collection.Iterator<UnsafeRow> sort_sortedIter_0;\n/* 013 */   private scala.collection.Iterator inputadapter_input_0;\n/* 014 */\n/* 015 */   public GeneratedIteratorForCodegenStage13(Object[] references) {\n/* 016 */     this.references = references;\n/* 017 */   }\n/* 018 */\n/* 019 */   public void init(int index, scala.collection.Iterator[] inputs) {\n/* 020 */     partitionIndex = index;\n/* 021 */     this.inputs = inputs;\n/* 022 */     sort_needToSort_0 = true;\n/* 023 */     sort_sorter_0 = ((org.apache.spark.sql.execution.SortExec) references[0] /* plan */).createSorter();\n/* 024 */     sort_metrics_0 = org.apache.spark.TaskContext.get().taskMetrics();\n/* 025 */\n/* 026 */     inputadapter_input_0 = inputs[0];\n/* 027 */\n/* 028 */   }\n/* 029 */\n/* 030 */   private void sort_addToSorter_0() throws java.io.IOException {\n/* 031 */     while ( inputadapter_input_0.hasNext()) {\n/* 032 */       InternalRow inputadapter_row_0 = (InternalRow) inputadapter_input_0.next();\n/* 033 */\n/* 034 */       sort_sorter_0.insertRow((UnsafeRow)inputadapter_row_0);\n/* 035 */       // shouldStop check is eliminated\n/* 036 */     }\n/* 037 */\n/* 038 */   }\n/* 039 */\n/* 040 */   protected void processNext() throws java.io.IOException {\n/* 041 */     if (sort_needToSort_0) {\n/* 042 */       long sort_spillSizeBefore_0 = sort_metrics_0.memoryBytesSpilled();\n/* 043 */       sort_addToSorter_0();\n/* 044 */       sort_sortedIter_0 = sort_sorter_0.sort();\n/* 045 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[3] /* sortTime */).add(sort_sorter_0.getSortTimeNanos() / 1000000);\n/* 046 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[1] /* peakMemory */).add(sort_sorter_0.getPeakMemoryUsage());\n/* 047 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[2] /* spillSize */).add(sort_metrics_0.memoryBytesSpilled() - sort_spillSizeBefore_0);\n/* 048 */       sort_metrics_0.incPeakExecutionMemory(sort_sorter_0.getPeakMemoryUsage());\n/* 049 */       sort_needToSort_0 = false;\n/* 050 */     }\n/* 051 */\n/* 052 */     while ( sort_sortedIter_0.hasNext()) {\n/* 053 */       UnsafeRow sort_outputRow_0 = (UnsafeRow)sort_sortedIter_0.next();\n/* 054 */\n/* 055 */       append(sort_outputRow_0);\n/* 056 */\n/* 057 */       if (shouldStop()) return;\n/* 058 */     }\n/* 059 */   }\n/* 060 */\n/* 061 */ }\n\n== Subtree 14 / 14 (maxMethodCodeSize:1178; maxConstantPoolSize:208(0.32% used); numInnerClasses:0) ==\n*(14) Sort [xid#0 ASC NULLS FIRST, website_id#3 ASC NULLS FIRST], false, 0\n+- *(14) Project [xid#0, action#1, date#2, website_id#3, url#4, category_id#5, zipcode#6, device#7, hour#283, weekday#284, n_events#695L, n_days_since_last_event#1610, n_unique_day#700, n_unique_hour#718, n_device#732, n_unique_category_id#746, n_events_per_action#1172L, n_events_per_hour#1606L, n_events_per_weekday#1608L, n_days_since_last_action#1613, n_events_per_device#1616L, n_actions_per_category_id#1618L, n_events_per_category_id#1620L]\n   +- Window [count(action#1) windowspecdefinition(xid#0, category_id#5, action#1, specifiedwindowframe(RowFrame, unboundedpreceding$(), unboundedfollowing$())) AS n_actions_per_category_id#1618L], [xid#0, knownfloatingpointnormalized(normalizenanandzero(category_id#5)), action#1]\n      +- *(13) Sort [xid#0 ASC NULLS FIRST, knownfloatingpointnormalized(normalizenanandzero(category_id#5)) ASC NULLS FIRST, action#1 ASC NULLS FIRST], false, 0\n         +- Window [count(action#1) windowspecdefinition(xid#0, category_id#5, specifiedwindowframe(RowFrame, unboundedpreceding$(), unboundedfollowing$())) AS n_events_per_category_id#1620L], [xid#0, knownfloatingpointnormalized(normalizenanandzero(category_id#5))]\n            +- *(12) Sort [xid#0 ASC NULLS FIRST, knownfloatingpointnormalized(normalizenanandzero(category_id#5)) ASC NULLS FIRST], false, 0\n               +- Window [count(device#7) windowspecdefinition(xid#0, device#7, specifiedwindowframe(RowFrame, unboundedpreceding$(), unboundedfollowing$())) AS n_events_per_device#1616L], [xid#0, device#7]\n                  +- *(11) Sort [xid#0 ASC NULLS FIRST, device#7 ASC NULLS FIRST], false, 0\n                     +- *(11) Project [xid#0, action#1, date#2, website_id#3, url#4, category_id#5, zipcode#6, device#7, hour#283, weekday#284, n_events#695L, n_days_since_last_event#1610, n_unique_day#700, n_unique_hour#718, n_device#732, n_unique_category_id#746, n_events_per_action#1172L, n_events_per_hour#1606L, n_events_per_weekday#1608L, (cast(datediff(2026-02-16, cast(_we0#1615 as date)) as double) + 0.1) AS n_days_since_last_action#1613]\n                        +- Window [max(date#2) windowspecdefinition(xid#0, action#1, specifiedwindowframe(RowFrame, unboundedpreceding$(), unboundedfollowing$())) AS _we0#1615], [xid#0, action#1]\n                           +- *(10) Sort [xid#0 ASC NULLS FIRST, action#1 ASC NULLS FIRST], false, 0\n                              +- *(10) Project [xid#0, action#1, date#2, website_id#3, url#4, category_id#5, zipcode#6, device#7, hour#283, weekday#284, n_events#695L, (cast(datediff(2026-02-16, cast(_we0#1612 as date)) as double) + 0.1) AS n_days_since_last_event#1610, n_unique_day#700, n_unique_hour#718, n_device#732, n_unique_category_id#746, n_events_per_action#1172L, n_events_per_hour#1606L, n_events_per_weekday#1608L]\n                                 +- Window [count(action#1) windowspecdefinition(xid#0, weekday#284, specifiedwindowframe(RowFrame, unboundedpreceding$(), unboundedfollowing$())) AS n_events_per_weekday#1608L], [xid#0, weekday#284]\n                                    +- *(9) Sort [xid#0 ASC NULLS FIRST, weekday#284 ASC NULLS FIRST], false, 0\n                                       +- *(9) Project [xid#0, action#1, date#2, website_id#3, url#4, category_id#5, zipcode#6, device#7, hour#283, weekday#284, n_events#695L, n_unique_day#700, n_unique_hour#718, n_device#732, n_unique_category_id#746, n_events_per_action#1172L, n_events_per_hour#1606L, _we0#1612]\n                                          +- Window [count(action#1) windowspecdefinition(xid#0, hour#283, specifiedwindowframe(RowFrame, unboundedpreceding$(), unboundedfollowing$())) AS n_events_per_hour#1606L], [xid#0, hour#283]\n                                             +- *(8) Sort [xid#0 ASC NULLS FIRST, hour#283 ASC NULLS FIRST], false, 0\n                                                +- *(8) Project [xid#0, action#1, date#2, website_id#3, url#4, category_id#5, zipcode#6, device#7, hour#283, weekday#284, n_events#695L, n_unique_day#700, n_unique_hour#718, n_device#732, n_unique_category_id#746, n_events_per_action#1172L, _we0#1612]\n                                                   +- Window [count(action#1) windowspecdefinition(xid#0, action#1, specifiedwindowframe(RowFrame, unboundedpreceding$(), unboundedfollowing$())) AS n_events_per_action#1172L], [xid#0, action#1]\n                                                      +- *(7) Sort [xid#0 ASC NULLS FIRST, action#1 ASC NULLS FIRST], false, 0\n                                                         +- *(7) Project [xid#0, action#1, date#2, website_id#3, url#4, category_id#5, zipcode#6, device#7, hour#283, weekday#284, n_events#695L, n_unique_day#700, n_unique_hour#718, n_device#732, n_unique_category_id#746, _we0#1612]\n                                                            +- Window [last(_w0#756, false) windowspecdefinition(xid#0, specifiedwindowframe(RowFrame, unboundedpreceding$(), unboundedfollowing$())) AS n_unique_category_id#746, max(date#2) windowspecdefinition(xid#0, specifiedwindowframe(RowFrame, unboundedpreceding$(), unboundedfollowing$())) AS _we0#1612], [xid#0]\n                                                               +- Window [dense_rank(category_id#5) windowspecdefinition(xid#0, category_id#5 ASC NULLS FIRST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS _w0#756], [xid#0], [category_id#5 ASC NULLS FIRST]\n                                                                  +- *(6) Sort [xid#0 ASC NULLS FIRST, category_id#5 ASC NULLS FIRST], false, 0\n                                                                     +- *(6) Project [xid#0, action#1, date#2, website_id#3, url#4, category_id#5, zipcode#6, device#7, hour#283, weekday#284, n_events#695L, n_unique_day#700, n_unique_hour#718, n_device#732]\n                                                                        +- Window [last(_w0#742, false) windowspecdefinition(xid#0, specifiedwindowframe(RowFrame, unboundedpreceding$(), unboundedfollowing$())) AS n_device#732], [xid#0]\n                                                                           +- Window [dense_rank(device#7) windowspecdefinition(xid#0, device#7 ASC NULLS FIRST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS _w0#742], [xid#0], [device#7 ASC NULLS FIRST]\n                                                                              +- *(5) Sort [xid#0 ASC NULLS FIRST, device#7 ASC NULLS FIRST], false, 0\n                                                                                 +- *(5) Project [xid#0, action#1, date#2, website_id#3, url#4, category_id#5, zipcode#6, device#7, hour#283, weekday#284, n_events#695L, n_unique_day#700, n_unique_hour#718]\n                                                                                    +- Window [last(_w0#728, false) windowspecdefinition(xid#0, specifiedwindowframe(RowFrame, unboundedpreceding$(), unboundedfollowing$())) AS n_unique_hour#718], [xid#0]\n                                                                                       +- Window [dense_rank(hour#283) windowspecdefinition(xid#0, hour#283 ASC NULLS FIRST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS _w0#728], [xid#0], [hour#283 ASC NULLS FIRST]\n                                                                                          +- *(4) Sort [xid#0 ASC NULLS FIRST, hour#283 ASC NULLS FIRST], false, 0\n                                                                                             +- *(4) Project [xid#0, action#1, date#2, website_id#3, url#4, category_id#5, zipcode#6, device#7, hour#283, weekday#284, n_events#695L, n_unique_day#700]\n                                                                                                +- Window [last(_w0#710, false) windowspecdefinition(xid#0, specifiedwindowframe(RowFrame, unboundedpreceding$(), unboundedfollowing$())) AS n_unique_day#700], [xid#0]\n                                                                                                   +- *(3) Project [xid#0, action#1, date#2, website_id#3, url#4, category_id#5, zipcode#6, device#7, hour#283, weekday#284, n_events#695L, _w0#710]\n                                                                                                      +- Window [dense_rank(_w0#711) windowspecdefinition(xid#0, _w0#711 ASC NULLS FIRST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS _w0#710], [xid#0], [_w0#711 ASC NULLS FIRST]\n                                                                                                         +- *(2) Sort [xid#0 ASC NULLS FIRST, _w0#711 ASC NULLS FIRST], false, 0\n                                                                                                            +- *(2) Project [xid#0, action#1, date#2, website_id#3, url#4, category_id#5, zipcode#6, device#7, hour#283, weekday#284, n_events#695L, dayofyear(cast(date#2 as date)) AS _w0#711]\n                                                                                                               +- Window [count(action#1) windowspecdefinition(xid#0, specifiedwindowframe(RowFrame, unboundedpreceding$(), unboundedfollowing$())) AS n_events#695L], [xid#0]\n                                                                                                                  +- *(1) Sort [xid#0 ASC NULLS FIRST], false, 0\n                                                                                                                     +- TableCacheQueryStage 0\n                                                                                                                        +- InMemoryTableScan [xid#0, action#1, date#2, website_id#3, url#4, category_id#5, zipcode#6, device#7, hour#283, weekday#284]\n                                                                                                                              +- InMemoryRelation [xid#0, action#1, date#2, website_id#3, url#4, category_id#5, zipcode#6, device#7, hour#283, weekday#284], StorageLevel(disk, memory, deserialized, 1 replicas)\n                                                                                                                                    +- AdaptiveSparkPlan isFinalPlan=true\n                                                                                                                                       +- == Final Plan ==\n                                                                                                                                          ResultQueryStage 1\n                                                                                                                                          +- *(1) Project [xid#0, action#1, date#2, website_id#3, url#4, category_id#5, zipcode#6, device#7, hour(date#2, Some(Europe/Paris)) AS hour#283, date_format(date#2, EEEE, Some(Europe/Paris)) AS weekday#284]\n                                                                                                                                             +- TableCacheQueryStage 0\n                                                                                                                                                +- InMemoryTableScan [action#1, category_id#5, date#2, device#7, url#4, website_id#3, xid#0, zipcode#6]\n                                                                                                                                                      +- InMemoryRelation [xid#0, action#1, date#2, website_id#3, url#4, category_id#5, zipcode#6, device#7], StorageLevel(disk, memory, deserialized, 1 replicas)\n                                                                                                                                                            +- AdaptiveSparkPlan isFinalPlan=true\n                     +- == Final Plan ==\n                        ResultQueryStage 1\n                        +- ShuffleQueryStage 0\n                           +- Exchange hashpartitioning(xid#0, 20), REPARTITION_BY_NUM, [plan_id=37]\n                              +- *(1) ColumnarToRow\n                                 +- FileScan parquet [xid#0,action#1,date#2,website_id#3,url#4,category_id#5,zipcode#6,device#7] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/home/boucheron/Documents/IFEBY310/core/notebooks/data/webdata.pa..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<xid:string,action:string,date:timestamp,website_id:string,url:string,category_id:float,zip...\n                     +- == Initial Plan ==\n                        Exchange hashpartitioning(xid#0, 20), REPARTITION_BY_NUM, [plan_id=22]\n                        +- FileScan parquet [xid#0,action#1,date#2,website_id#3,url#4,category_id#5,zipcode#6,device#7] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/home/boucheron/Documents/IFEBY310/core/notebooks/data/webdata.pa..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<xid:string,action:string,date:timestamp,website_id:string,url:string,category_id:float,zip...\n                                                                                                                                       +- == Initial Plan ==\n                                                                                                                                          Project [xid#0, action#1, date#2, website_id#3, url#4, category_id#5, zipcode#6, device#7, hour(date#2, Some(Europe/Paris)) AS hour#283, date_format(date#2, EEEE, Some(Europe/Paris)) AS weekday#284]\n                                                                                                                                          +- InMemoryTableScan [action#1, category_id#5, date#2, device#7, url#4, website_id#3, xid#0, zipcode#6]\n                                                                                                                                                +- InMemoryRelation [xid#0, action#1, date#2, website_id#3, url#4, category_id#5, zipcode#6, device#7], StorageLevel(disk, memory, deserialized, 1 replicas)\n                                                                                                                                                      +- AdaptiveSparkPlan isFinalPlan=true\n               +- == Final Plan ==\n                  ResultQueryStage 1\n                  +- ShuffleQueryStage 0\n                     +- Exchange hashpartitioning(xid#0, 20), REPARTITION_BY_NUM, [plan_id=37]\n                        +- *(1) ColumnarToRow\n                           +- FileScan parquet [xid#0,action#1,date#2,website_id#3,url#4,category_id#5,zipcode#6,device#7] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/home/boucheron/Documents/IFEBY310/core/notebooks/data/webdata.pa..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<xid:string,action:string,date:timestamp,website_id:string,url:string,category_id:float,zip...\n               +- == Initial Plan ==\n                  Exchange hashpartitioning(xid#0, 20), REPARTITION_BY_NUM, [plan_id=22]\n                  +- FileScan parquet [xid#0,action#1,date#2,website_id#3,url#4,category_id#5,zipcode#6,device#7] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/home/boucheron/Documents/IFEBY310/core/notebooks/data/webdata.pa..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<xid:string,action:string,date:timestamp,website_id:string,url:string,category_id:float,zip...\n\nGenerated code:\n/* 001 */ public Object generate(Object[] references) {\n/* 002 */   return new GeneratedIteratorForCodegenStage14(references);\n/* 003 */ }\n/* 004 */\n/* 005 */ // codegenStageId=14\n/* 006 */ final class GeneratedIteratorForCodegenStage14 extends org.apache.spark.sql.execution.BufferedRowIterator {\n/* 007 */   private Object[] references;\n/* 008 */   private scala.collection.Iterator[] inputs;\n/* 009 */   private boolean sort_needToSort_0;\n/* 010 */   private org.apache.spark.sql.execution.UnsafeExternalRowSorter sort_sorter_0;\n/* 011 */   private org.apache.spark.executor.TaskMetrics sort_metrics_0;\n/* 012 */   private scala.collection.Iterator<UnsafeRow> sort_sortedIter_0;\n/* 013 */   private scala.collection.Iterator inputadapter_input_0;\n/* 014 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] project_mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];\n/* 015 */\n/* 016 */   public GeneratedIteratorForCodegenStage14(Object[] references) {\n/* 017 */     this.references = references;\n/* 018 */   }\n/* 019 */\n/* 020 */   public void init(int index, scala.collection.Iterator[] inputs) {\n/* 021 */     partitionIndex = index;\n/* 022 */     this.inputs = inputs;\n/* 023 */     sort_needToSort_0 = true;\n/* 024 */     sort_sorter_0 = ((org.apache.spark.sql.execution.SortExec) references[0] /* plan */).createSorter();\n/* 025 */     sort_metrics_0 = org.apache.spark.TaskContext.get().taskMetrics();\n/* 026 */\n/* 027 */     inputadapter_input_0 = inputs[0];\n/* 028 */     project_mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(23, 224);\n/* 029 */\n/* 030 */   }\n/* 031 */\n/* 032 */   private void sort_addToSorter_0() throws java.io.IOException {\n/* 033 */     while ( inputadapter_input_0.hasNext()) {\n/* 034 */       InternalRow inputadapter_row_0 = (InternalRow) inputadapter_input_0.next();\n/* 035 */\n/* 036 */       // common sub-expressions\n/* 037 */\n/* 038 */       boolean inputadapter_isNull_0 = inputadapter_row_0.isNullAt(0);\n/* 039 */       UTF8String inputadapter_value_0 = inputadapter_isNull_0 ?\n/* 040 */       null : (inputadapter_row_0.getUTF8String(0));\n/* 041 */       boolean inputadapter_isNull_1 = inputadapter_row_0.isNullAt(1);\n/* 042 */       UTF8String inputadapter_value_1 = inputadapter_isNull_1 ?\n/* 043 */       null : (inputadapter_row_0.getUTF8String(1));\n/* 044 */       boolean inputadapter_isNull_2 = inputadapter_row_0.isNullAt(2);\n/* 045 */       long inputadapter_value_2 = inputadapter_isNull_2 ?\n/* 046 */       -1L : (inputadapter_row_0.getLong(2));\n/* 047 */       boolean inputadapter_isNull_3 = inputadapter_row_0.isNullAt(3);\n/* 048 */       UTF8String inputadapter_value_3 = inputadapter_isNull_3 ?\n/* 049 */       null : (inputadapter_row_0.getUTF8String(3));\n/* 050 */       boolean inputadapter_isNull_4 = inputadapter_row_0.isNullAt(4);\n/* 051 */       UTF8String inputadapter_value_4 = inputadapter_isNull_4 ?\n/* 052 */       null : (inputadapter_row_0.getUTF8String(4));\n/* 053 */       boolean inputadapter_isNull_5 = inputadapter_row_0.isNullAt(5);\n/* 054 */       float inputadapter_value_5 = inputadapter_isNull_5 ?\n/* 055 */       -1.0f : (inputadapter_row_0.getFloat(5));\n/* 056 */       boolean inputadapter_isNull_6 = inputadapter_row_0.isNullAt(6);\n/* 057 */       UTF8String inputadapter_value_6 = inputadapter_isNull_6 ?\n/* 058 */       null : (inputadapter_row_0.getUTF8String(6));\n/* 059 */       boolean inputadapter_isNull_7 = inputadapter_row_0.isNullAt(7);\n/* 060 */       UTF8String inputadapter_value_7 = inputadapter_isNull_7 ?\n/* 061 */       null : (inputadapter_row_0.getUTF8String(7));\n/* 062 */       boolean inputadapter_isNull_8 = inputadapter_row_0.isNullAt(8);\n/* 063 */       int inputadapter_value_8 = inputadapter_isNull_8 ?\n/* 064 */       -1 : (inputadapter_row_0.getInt(8));\n/* 065 */       boolean inputadapter_isNull_9 = inputadapter_row_0.isNullAt(9);\n/* 066 */       UTF8String inputadapter_value_9 = inputadapter_isNull_9 ?\n/* 067 */       null : (inputadapter_row_0.getUTF8String(9));\n/* 068 */       long inputadapter_value_10 = inputadapter_row_0.getLong(10);\n/* 069 */       boolean inputadapter_isNull_11 = inputadapter_row_0.isNullAt(11);\n/* 070 */       double inputadapter_value_11 = inputadapter_isNull_11 ?\n/* 071 */       -1.0 : (inputadapter_row_0.getDouble(11));\n/* 072 */       boolean inputadapter_isNull_12 = inputadapter_row_0.isNullAt(12);\n/* 073 */       int inputadapter_value_12 = inputadapter_isNull_12 ?\n/* 074 */       -1 : (inputadapter_row_0.getInt(12));\n/* 075 */       boolean inputadapter_isNull_13 = inputadapter_row_0.isNullAt(13);\n/* 076 */       int inputadapter_value_13 = inputadapter_isNull_13 ?\n/* 077 */       -1 : (inputadapter_row_0.getInt(13));\n/* 078 */       boolean inputadapter_isNull_14 = inputadapter_row_0.isNullAt(14);\n/* 079 */       int inputadapter_value_14 = inputadapter_isNull_14 ?\n/* 080 */       -1 : (inputadapter_row_0.getInt(14));\n/* 081 */       boolean inputadapter_isNull_15 = inputadapter_row_0.isNullAt(15);\n/* 082 */       int inputadapter_value_15 = inputadapter_isNull_15 ?\n/* 083 */       -1 : (inputadapter_row_0.getInt(15));\n/* 084 */       long inputadapter_value_16 = inputadapter_row_0.getLong(16);\n/* 085 */       long inputadapter_value_17 = inputadapter_row_0.getLong(17);\n/* 086 */       long inputadapter_value_18 = inputadapter_row_0.getLong(18);\n/* 087 */       boolean inputadapter_isNull_19 = inputadapter_row_0.isNullAt(19);\n/* 088 */       double inputadapter_value_19 = inputadapter_isNull_19 ?\n/* 089 */       -1.0 : (inputadapter_row_0.getDouble(19));\n/* 090 */       long inputadapter_value_20 = inputadapter_row_0.getLong(20);\n/* 091 */       long inputadapter_value_22 = inputadapter_row_0.getLong(22);\n/* 092 */       long inputadapter_value_21 = inputadapter_row_0.getLong(21);\n/* 093 */       project_mutableStateArray_0[0].reset();\n/* 094 */\n/* 095 */       project_mutableStateArray_0[0].zeroOutNullBytes();\n/* 096 */\n/* 097 */       if (inputadapter_isNull_0) {\n/* 098 */         project_mutableStateArray_0[0].setNullAt(0);\n/* 099 */       } else {\n/* 100 */         project_mutableStateArray_0[0].write(0, inputadapter_value_0);\n/* 101 */       }\n/* 102 */\n/* 103 */       if (inputadapter_isNull_1) {\n/* 104 */         project_mutableStateArray_0[0].setNullAt(1);\n/* 105 */       } else {\n/* 106 */         project_mutableStateArray_0[0].write(1, inputadapter_value_1);\n/* 107 */       }\n/* 108 */\n/* 109 */       if (inputadapter_isNull_2) {\n/* 110 */         project_mutableStateArray_0[0].setNullAt(2);\n/* 111 */       } else {\n/* 112 */         project_mutableStateArray_0[0].write(2, inputadapter_value_2);\n/* 113 */       }\n/* 114 */\n/* 115 */       if (inputadapter_isNull_3) {\n/* 116 */         project_mutableStateArray_0[0].setNullAt(3);\n/* 117 */       } else {\n/* 118 */         project_mutableStateArray_0[0].write(3, inputadapter_value_3);\n/* 119 */       }\n/* 120 */\n/* 121 */       if (inputadapter_isNull_4) {\n/* 122 */         project_mutableStateArray_0[0].setNullAt(4);\n/* 123 */       } else {\n/* 124 */         project_mutableStateArray_0[0].write(4, inputadapter_value_4);\n/* 125 */       }\n/* 126 */\n/* 127 */       if (inputadapter_isNull_5) {\n/* 128 */         project_mutableStateArray_0[0].setNullAt(5);\n/* 129 */       } else {\n/* 130 */         project_mutableStateArray_0[0].write(5, inputadapter_value_5);\n/* 131 */       }\n/* 132 */\n/* 133 */       if (inputadapter_isNull_6) {\n/* 134 */         project_mutableStateArray_0[0].setNullAt(6);\n/* 135 */       } else {\n/* 136 */         project_mutableStateArray_0[0].write(6, inputadapter_value_6);\n/* 137 */       }\n/* 138 */\n/* 139 */       if (inputadapter_isNull_7) {\n/* 140 */         project_mutableStateArray_0[0].setNullAt(7);\n/* 141 */       } else {\n/* 142 */         project_mutableStateArray_0[0].write(7, inputadapter_value_7);\n/* 143 */       }\n/* 144 */\n/* 145 */       if (inputadapter_isNull_8) {\n/* 146 */         project_mutableStateArray_0[0].setNullAt(8);\n/* 147 */       } else {\n/* 148 */         project_mutableStateArray_0[0].write(8, inputadapter_value_8);\n/* 149 */       }\n/* 150 */\n/* 151 */       if (inputadapter_isNull_9) {\n/* 152 */         project_mutableStateArray_0[0].setNullAt(9);\n/* 153 */       } else {\n/* 154 */         project_mutableStateArray_0[0].write(9, inputadapter_value_9);\n/* 155 */       }\n/* 156 */\n/* 157 */       project_mutableStateArray_0[0].write(10, inputadapter_value_10);\n/* 158 */\n/* 159 */       if (inputadapter_isNull_11) {\n/* 160 */         project_mutableStateArray_0[0].setNullAt(11);\n/* 161 */       } else {\n/* 162 */         project_mutableStateArray_0[0].write(11, inputadapter_value_11);\n/* 163 */       }\n/* 164 */\n/* 165 */       if (inputadapter_isNull_12) {\n/* 166 */         project_mutableStateArray_0[0].setNullAt(12);\n/* 167 */       } else {\n/* 168 */         project_mutableStateArray_0[0].write(12, inputadapter_value_12);\n/* 169 */       }\n/* 170 */\n/* 171 */       if (inputadapter_isNull_13) {\n/* 172 */         project_mutableStateArray_0[0].setNullAt(13);\n/* 173 */       } else {\n/* 174 */         project_mutableStateArray_0[0].write(13, inputadapter_value_13);\n/* 175 */       }\n/* 176 */\n/* 177 */       if (inputadapter_isNull_14) {\n/* 178 */         project_mutableStateArray_0[0].setNullAt(14);\n/* 179 */       } else {\n/* 180 */         project_mutableStateArray_0[0].write(14, inputadapter_value_14);\n/* 181 */       }\n/* 182 */\n/* 183 */       if (inputadapter_isNull_15) {\n/* 184 */         project_mutableStateArray_0[0].setNullAt(15);\n/* 185 */       } else {\n/* 186 */         project_mutableStateArray_0[0].write(15, inputadapter_value_15);\n/* 187 */       }\n/* 188 */\n/* 189 */       project_mutableStateArray_0[0].write(16, inputadapter_value_16);\n/* 190 */\n/* 191 */       project_mutableStateArray_0[0].write(17, inputadapter_value_17);\n/* 192 */\n/* 193 */       project_mutableStateArray_0[0].write(18, inputadapter_value_18);\n/* 194 */\n/* 195 */       if (inputadapter_isNull_19) {\n/* 196 */         project_mutableStateArray_0[0].setNullAt(19);\n/* 197 */       } else {\n/* 198 */         project_mutableStateArray_0[0].write(19, inputadapter_value_19);\n/* 199 */       }\n/* 200 */\n/* 201 */       project_mutableStateArray_0[0].write(20, inputadapter_value_20);\n/* 202 */\n/* 203 */       project_mutableStateArray_0[0].write(21, inputadapter_value_22);\n/* 204 */\n/* 205 */       project_mutableStateArray_0[0].write(22, inputadapter_value_21);\n/* 206 */       sort_sorter_0.insertRow((UnsafeRow)(project_mutableStateArray_0[0].getRow()));\n/* 207 */       // shouldStop check is eliminated\n/* 208 */     }\n/* 209 */\n/* 210 */   }\n/* 211 */\n/* 212 */   protected void processNext() throws java.io.IOException {\n/* 213 */     if (sort_needToSort_0) {\n/* 214 */       long sort_spillSizeBefore_0 = sort_metrics_0.memoryBytesSpilled();\n/* 215 */       sort_addToSorter_0();\n/* 216 */       sort_sortedIter_0 = sort_sorter_0.sort();\n/* 217 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[3] /* sortTime */).add(sort_sorter_0.getSortTimeNanos() / 1000000);\n/* 218 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[1] /* peakMemory */).add(sort_sorter_0.getPeakMemoryUsage());\n/* 219 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[2] /* spillSize */).add(sort_metrics_0.memoryBytesSpilled() - sort_spillSizeBefore_0);\n/* 220 */       sort_metrics_0.incPeakExecutionMemory(sort_sorter_0.getPeakMemoryUsage());\n/* 221 */       sort_needToSort_0 = false;\n/* 222 */     }\n/* 223 */\n/* 224 */     while ( sort_sortedIter_0.hasNext()) {\n/* 225 */       UnsafeRow sort_outputRow_0 = (UnsafeRow)sort_sortedIter_0.next();\n/* 226 */\n/* 227 */       append(sort_outputRow_0);\n/* 228 */\n/* 229 */       if (shouldStop()) return;\n/* 230 */     }\n/* 231 */   }\n/* 232 */\n/* 233 */ }\n\n\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n\r[Stage 18:===============================================>        (17 + 3) / 20]\r\r                                                                                \r\n```\n:::\n:::\n\n\n::: {.callout-note}\n\nAgain: \n\n- How many jobs were triggered by `checkpoint()`?\n- How many stages? How many tasks?\n- Could you spot shuffle reads and writes?\n- Have a look at detailed plans on Spark UI.  \n- In the physical plan, can you spot `WholeStageCodegen`? What is it for?\n- In the physical plan, can you spot `ShuffleQueryStage`? What does it stand for?\n   \n:::\n\n::: {#e3280fea .cell execution_count=33}\n``` {.python .cell-code}\nN = 10000\n```\n:::\n\n\n::: {#275df8f0 .cell execution_count=34}\n``` {.python .cell-code}\nsample_df = (\n    df\n      .sample(withReplacement=False, fraction=.05)\n      .repartition('xid')\n)\n```\n:::\n\n\n::: {#d28b5b1f .cell execution_count=35}\n``` {.python .cell-code}\nsample_df.count()\n```\n\n::: {.cell-output .cell-output-display execution_count=32}\n```\n58872\n```\n:::\n:::\n\n\n::: {#ebd97c19 .cell execution_count=36}\n``` {.python .cell-code}\nfor transformer in transformers:\n    sample_df = transformer(sample_df)\n\n# sample_df.show(n=1)\n```\n:::\n\n\n::: {#c7f22556 .cell execution_count=37}\n``` {.python .cell-code}\nsample_df.printSchema()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nroot\n |-- xid: string (nullable = true)\n |-- action: string (nullable = true)\n |-- date: timestamp (nullable = true)\n |-- website_id: string (nullable = true)\n |-- url: string (nullable = true)\n |-- category_id: float (nullable = true)\n |-- zipcode: string (nullable = true)\n |-- device: string (nullable = true)\n |-- hour: integer (nullable = true)\n |-- weekday: string (nullable = true)\n |-- n_events: long (nullable = false)\n |-- n_days_since_last_event: double (nullable = true)\n |-- n_unique_day: integer (nullable = true)\n |-- n_unique_hour: integer (nullable = true)\n |-- n_device: integer (nullable = true)\n |-- n_unique_category_id: integer (nullable = true)\n |-- n_events_per_action: long (nullable = false)\n |-- n_events_per_hour: long (nullable = false)\n |-- n_events_per_weekday: long (nullable = false)\n |-- n_days_since_last_action: double (nullable = true)\n |-- n_events_per_device: long (nullable = false)\n |-- n_actions_per_category_id: long (nullable = false)\n |-- n_events_per_category_id: long (nullable = false)\n |-- n_events_per_website_id: long (nullable = false)\n\n```\n:::\n:::\n\n\n::: {.callout-note}\n\nCall  `explain(mode=\"codegen\")` on the resulting dataframe. \n\n:::\n\n::: {#e7ea8b5e .cell execution_count=38}\n``` {.python .cell-code}\nsample_df.explain(mode=\"codegen\")\n```\n:::\n\n\n::: {#apply-transformers .cell execution_count=39}\n``` {.python .cell-code}\nt0 =  perf_counter()\nfor transformer in transformers:\n    df = transformer(df)\n\n( \n    df\n        .sample(withReplacement=False, fraction=.05)\n        .show(n=1)\n)\n\nprint(perf_counter() -t0)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n\r[Stage 29:>                                                         (0 + 1) / 1]\r\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n+--------------------+------+-------------------+----------+--------------------+-----------+-------+------+----+-------+--------+-----------------------+------------+-------------+--------+--------------------+-------------------+-----------------+--------------------+------------------------+-------------------+-------------------------+------------------------+-----------------------+\n|                 xid|action|               date|website_id|                 url|category_id|zipcode|device|hour|weekday|n_events|n_days_since_last_event|n_unique_day|n_unique_hour|n_device|n_unique_category_id|n_events_per_action|n_events_per_hour|n_events_per_weekday|n_days_since_last_action|n_events_per_device|n_actions_per_category_id|n_events_per_category_id|n_events_per_website_id|\n+--------------------+------+-------------------+----------+--------------------+-----------+-------+------+----+-------+--------+-----------------------+------------+-------------+--------+--------------------+-------------------+-----------------+--------------------+------------------------+-------------------+-------------------------+------------------------+-----------------------+\n|00037ace-b22f-480...|     O|2017-01-22 18:57:34|        74|http://www.realit...|     1002.0|   NULL|   DSK|  18| Sunday|       6|                 3312.1|           1|            2|       1|                   2|                  6|                2|                   6|                  3312.1|                  6|                        4|                       4|                      6|\n+--------------------+------+-------------------+----------+--------------------+-----------+-------+------+----+-------+--------+-----------------------+------------+-------------+--------+--------------------+-------------------+-----------------+--------------------+------------------------+-------------------+-------------------------+------------------------+-----------------------+\nonly showing top 1 row\n1.3951444809999884\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n\r                                                                                \r\n```\n:::\n:::\n\n\n::: {#33a44cce .cell execution_count=40}\n``` {.python .cell-code}\n# df = df.repartition(20, 'xid')\ndf.checkpoint()\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n\r[Stage 31:>                                                       (0 + 20) / 20]\r\r[Stage 31:==>                                                     (1 + 19) / 20]\r\r[Stage 31:=========================>                              (9 + 11) / 20]\r\r[Stage 31:============================================>           (16 + 4) / 20]\r\r                                                                                \r\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=36}\n```\nDataFrame[xid: string, action: string, date: timestamp, website_id: string, url: string, category_id: float, zipcode: string, device: string, hour: int, weekday: string, n_events: bigint, n_days_since_last_event: double, n_unique_day: int, n_unique_hour: int, n_device: int, n_unique_category_id: int, n_events_per_action: bigint, n_events_per_hour: bigint, n_events_per_weekday: bigint, n_days_since_last_action: double, n_events_per_device: bigint, n_actions_per_category_id: bigint, n_events_per_category_id: bigint, n_events_per_website_id: bigint]\n```\n:::\n:::\n\n\n::: {#d3cf829a .cell execution_count=41}\n``` {.python .cell-code}\ndf.write.parquet('file:///home/boucheron/Documents/IFEBY310/core/notebooks/data/webdata-transformed.parquet', mode='overwrite')\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n\r[Stage 33:>                                                       (0 + 20) / 20]\r\r[Stage 33:=========================>                              (9 + 11) / 20]\r\r                                                                                \r\n```\n:::\n:::\n\n\n::: {#dbf6b9d2 .cell execution_count=42}\n``` {.python .cell-code}\n!ls -l data/webdata-transformed.parquet/\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntotal 42452\r\n-rw-r--r-- 1 boucheron boucheron 2029216 fvr. 16 10:03 part-00000-fb0a5b92-5452-4a31-85b4-74b62147e995-c000.snappy.parquet\r\n-rw-r--r-- 1 boucheron boucheron 2571583 fvr. 16 10:03 part-00001-fb0a5b92-5452-4a31-85b4-74b62147e995-c000.snappy.parquet\r\n-rw-r--r-- 1 boucheron boucheron 2016855 fvr. 16 10:03 part-00002-fb0a5b92-5452-4a31-85b4-74b62147e995-c000.snappy.parquet\r\n-rw-r--r-- 1 boucheron boucheron 1213194 fvr. 16 10:03 part-00003-fb0a5b92-5452-4a31-85b4-74b62147e995-c000.snappy.parquet\r\n-rw-r--r-- 1 boucheron boucheron 2605526 fvr. 16 10:03 part-00004-fb0a5b92-5452-4a31-85b4-74b62147e995-c000.snappy.parquet\r\n-rw-r--r-- 1 boucheron boucheron 1739664 fvr. 16 10:03 part-00005-fb0a5b92-5452-4a31-85b4-74b62147e995-c000.snappy.parquet\r\n-rw-r--r-- 1 boucheron boucheron 2325113 fvr. 16 10:03 part-00006-fb0a5b92-5452-4a31-85b4-74b62147e995-c000.snappy.parquet\r\n-rw-r--r-- 1 boucheron boucheron 2596096 fvr. 16 10:03 part-00007-fb0a5b92-5452-4a31-85b4-74b62147e995-c000.snappy.parquet\r\n-rw-r--r-- 1 boucheron boucheron 2266491 fvr. 16 10:03 part-00008-fb0a5b92-5452-4a31-85b4-74b62147e995-c000.snappy.parquet\r\n-rw-r--r-- 1 boucheron boucheron 2313765 fvr. 16 10:03 part-00009-fb0a5b92-5452-4a31-85b4-74b62147e995-c000.snappy.parquet\r\n-rw-r--r-- 1 boucheron boucheron 2330433 fvr. 16 10:03 part-00010-fb0a5b92-5452-4a31-85b4-74b62147e995-c000.snappy.parquet\r\n-rw-r--r-- 1 boucheron boucheron 2846406 fvr. 16 10:03 part-00011-fb0a5b92-5452-4a31-85b4-74b62147e995-c000.snappy.parquet\r\n-rw-r--r-- 1 boucheron boucheron 1721613 fvr. 16 10:03 part-00012-fb0a5b92-5452-4a31-85b4-74b62147e995-c000.snappy.parquet\r\n-rw-r--r-- 1 boucheron boucheron 2299943 fvr. 16 10:03 part-00013-fb0a5b92-5452-4a31-85b4-74b62147e995-c000.snappy.parquet\r\n-rw-r--r-- 1 boucheron boucheron 1758838 fvr. 16 10:03 part-00014-fb0a5b92-5452-4a31-85b4-74b62147e995-c000.snappy.parquet\r\n-rw-r--r-- 1 boucheron boucheron 1758969 fvr. 16 10:03 part-00015-fb0a5b92-5452-4a31-85b4-74b62147e995-c000.snappy.parquet\r\n-rw-r--r-- 1 boucheron boucheron 1770399 fvr. 16 10:03 part-00016-fb0a5b92-5452-4a31-85b4-74b62147e995-c000.snappy.parquet\r\n-rw-r--r-- 1 boucheron boucheron 2638662 fvr. 16 10:03 part-00017-fb0a5b92-5452-4a31-85b4-74b62147e995-c000.snappy.parquet\r\n-rw-r--r-- 1 boucheron boucheron 1771383 fvr. 16 10:03 part-00018-fb0a5b92-5452-4a31-85b4-74b62147e995-c000.snappy.parquet\r\n-rw-r--r-- 1 boucheron boucheron 2855105 fvr. 16 10:03 part-00019-fb0a5b92-5452-4a31-85b4-74b62147e995-c000.snappy.parquet\r\n-rw-r--r-- 1 boucheron boucheron       0 fvr. 16 10:03 _SUCCESS\r\n```\n:::\n:::\n\n\n::: {.callout-note}\n\nIf things go wrong with the next message \n\nIn Spark SQL `local` mode, the heap size is controlled by the driver JVM, because in local mode the driver and executor run in the same process. \n\nWe can increase Java heap by increasing the driver memory.\n\n```{.python}\nspark = (\n    SparkSession\n        .builder\n        .appName(\"Taming Webdata\")\n        .config(\"spark.driver.memory\", \"16G\") # The default is 5G\n        .config(\"spark.driver.maxResultSize\", \"0\")\n        .getOrCreate()\n)\n\n```\n\n:::\n\n::: {#459eb9db .cell execution_count=43}\n``` {.python .cell-code}\nsorted(df.columns)\n```\n\n::: {.cell-output .cell-output-display execution_count=39}\n```\n['action',\n 'category_id',\n 'date',\n 'device',\n 'hour',\n 'n_actions_per_category_id',\n 'n_days_since_last_action',\n 'n_days_since_last_event',\n 'n_device',\n 'n_events',\n 'n_events_per_action',\n 'n_events_per_category_id',\n 'n_events_per_device',\n 'n_events_per_hour',\n 'n_events_per_website_id',\n 'n_events_per_weekday',\n 'n_unique_category_id',\n 'n_unique_day',\n 'n_unique_hour',\n 'url',\n 'website_id',\n 'weekday',\n 'xid',\n 'zipcode']\n```\n:::\n:::\n\n\n::: {.callout-note}\n\n- How DRY-compliant is this code? \n- Should we dry it?\n- Why would we dry it (readability, maintainability, ...)? \n- Rewrite the construction of the transformed dataframe using SQL. Compare the (final) execution plans. \n\n:::\n\n\n## Load step\n\nHere, we use all the previous computations (saved in the columns of the dataframe) to compute aggregated informations about each user.\n\n::: {#115b5051 .cell execution_count=44}\n``` {.python .cell-code}\ndf = spark.read.parquet(\n    'file:///home/boucheron/Documents/IFEBY310/core/notebooks/data/webdata-transformed.parquet')\n```\n:::\n\n\n::: {.callout-note}\n\nAll functions in the next chunk have one argument: a dataframe with schema\n\n```\nroot\n |-- xid: string (nullable = true)\n |-- action: string (nullable = true)\n |-- date: timestamp (nullable = true)\n |-- website_id: string (nullable = true)\n |-- url: string (nullable = true)\n |-- category_id: float (nullable = true)\n |-- zipcode: string (nullable = true)\n |-- device: string (nullable = true)\n |-- hour: integer (nullable = true)\n |-- weekday: string (nullable = true)\n |-- n_events_per_hour: long (nullable = false)\n |-- n_events_per_weekday: long (nullable = false)\n |-- n_days_since_last_event: double (nullable = true)\n |-- n_days_since_last_action: double (nullable = true)\n |-- n_unique_day: integer (nullable = true)\n |-- n_unique_hour: integer (nullable = true)\n |-- n_events_per_device: long (nullable = false)\n |-- n_device: integer (nullable = true)\n |-- n_actions_per_category_id: long (nullable = false)\n |-- n_events_per_category_id: long (nullable = false)\n |-- n_events_per_website_id: long (nullable = false)\n```\n\nAll functions in the next chunk return a dataframe with three columns: \n`xid`, `value`, `feature_name`.  Column `value` is obtained by renaming \na column `n_...` (built by one of the previous transformers). Column `feature_name`   \n\n\n:::\n\n::: {#9f82e526 .cell execution_count=45}\n``` {.python .cell-code}\ndef n_events_per_hour_loader(df):\n    csr = df\\\n        .select('xid', 'hour', 'n_events_per_hour')\\\n        .withColumnRenamed('n_events_per_hour', 'value')\\\n        .distinct()     # action\n    feature_name = func.concat(lit('n_events_per_hour#'), col('hour'))\n    csr = csr\\\n        .withColumn('feature_name', feature_name)\\\n        .drop('hour')\n    return csr\n\ndef n_events_per_website_id_loader(df):\n    csr = df.select('xid', 'website_id', 'n_events_per_website_id')\\\n        .withColumnRenamed('n_events_per_website_id', 'value')\\\n        .distinct()\n    feature_name = func.concat(lit('n_events_per_website_id#'),\n                               col('website_id'))\n    csr = csr\\\n        .withColumn('feature_name', feature_name)\\\n        .drop('website_id')\n    return csr\n\n\n\ndef n_events_per_weekday_loader(df):\n    csr = df\\\n        .select('xid', 'weekday', 'n_events_per_weekday')\\\n        .withColumnRenamed('n_events_per_weekday', 'value')\\\n        .distinct()\n    feature_name = func.concat(lit('n_events_per_weekday#'), col('weekday'))\n    csr = csr\\\n        .withColumn('feature_name', feature_name)\\\n        .drop('weekday')\n    return csr\n\ndef n_days_since_last_event_loader(df):\n    csr = df.select('xid',  'n_days_since_last_event')\\\n        .withColumnRenamed('n_days_since_last_event#', 'value')\\\n        .distinct()\n    feature_name = lit('n_days_since_last_event')\n    csr = csr\\\n        .withColumn('feature_name', feature_name)\n    return csr\n\ndef n_days_since_last_action_loader(df):\n    csr = df.select('xid', 'action', 'n_days_since_last_action')\\\n        .withColumnRenamed('n_days_since_last_action', 'value')\\\n        .distinct()\n    feature_name = func.concat(lit('n_days_since_last_action#'), col('action'))\n    csr = csr\\\n        .withColumn('feature_name', feature_name)\\\n        .drop('action')\n    return csr\n\ndef n_unique_day_loader(df):\n    csr = df.select('xid', 'n_unique_day')\\\n        .withColumnRenamed('n_unique_day', 'value')\\\n        .distinct()\n    feature_name = lit('n_unique_day')\n    csr = csr\\\n        .withColumn('feature_name', feature_name)\n    return csr\n\ndef n_unique_hour_loader(df):\n    csr = df.select('xid', 'n_unique_hour')\\\n        .withColumnRenamed('n_unique_hour', 'value')\\\n        .distinct()\n    feature_name = lit('n_unique_hour')\n    csr = csr\\\n        .withColumn('feature_name', feature_name)\n    return csr\n\ndef n_events_per_device_loader(df):\n    csr = df\\\n        .select('xid', 'device', 'n_events_per_device')\\\n        .withColumnRenamed('n_events_per_device', 'value')\\\n        .distinct()\n    feature_name = func.concat(lit('n_events_per_device#'), col('device'))\n    csr = csr\\\n        .withColumn('feature_name', feature_name)\\\n        .drop('device')\n    return csr\n\ndef n_unique_device_loader(df):\n    csr = df.select('xid', 'n_device')\\\n        .withColumnRenamed('n_device', 'value')\\\n        .distinct()\n    feature_name = lit('n_device')\n    csr = csr\\\n        .withColumn('feature_name', feature_name)\n    return csr\n\ndef n_events_per_category_id_loader(df):\n    csr = df.select('xid', 'category_id', 'n_events_per_category_id')\\\n        .withColumnRenamed('n_events_per_category_id', 'value')\\\n        .distinct()\n    feature_name = func.concat(lit('n_events_per_category_id#'),\n                               col('category_id'))\n    csr = csr\\\n        .withColumn('feature_name', feature_name)\\\n        .drop('category_id')\n    return csr\n\ndef n_actions_per_category_id_loader(df):\n    csr = df.select('xid', 'category_id', 'action', 'n_actions_per_category_id')\\\n        .withColumnRenamed('n_actions_per_category_id', 'value')\\\n        .distinct()\n    feature_name = func.concat(lit('n_actions_per_category_id#'),\n                               col('action'), lit('#'), \n                               col('category_id'))\n    csr = csr\\\n        .withColumn('feature_name', feature_name)\\\n        .drop('category_id')\\\n        .drop('action')\n    return csr\n\ndef n_events_per_website_id_loader(df):\n    csr = df.select('xid', 'website_id', 'n_events_per_website_id')\\\n        .withColumnRenamed('n_events_per_website_id', 'value')\\\n        .distinct()\n    feature_name = func.concat(lit('n_events_per_website_id#'),\n                               col('website_id'))\n    csr = csr\\\n        .withColumn('feature_name', feature_name)\\\n        .drop('website_id')\n    return csr\n```\n:::\n\n\n::: {.callout-note}\n\n- How DRY-compliant is this code (bis)? \n- Should we dry it?\n- Why would we dry it (readability, maintainability, ...)? \n\n\n:::\n\n::: {#d85dd6ac .cell execution_count=46}\n``` {.python .cell-code}\n(\n    df\n        .select('xid', \n            'n_unique_day', \n            'n_unique_hour', \n            'n_days_since_last_event',\n            'n_device')\n        .distinct()\n        .unpivot(ids='xid', \n                 values = ['n_unique_day', \n                           'n_unique_hour', \n                           'n_days_since_last_event', \n                           'n_device'],\n                 variableColumnName='feature_name',\n                 valueColumnName='value')\n        .show(20)\n\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n+--------------------+--------------------+------+\n|                 xid|        feature_name| value|\n+--------------------+--------------------+------+\n|00037ace-b22f-480...|        n_unique_day|   1.0|\n|00037ace-b22f-480...|       n_unique_hour|   2.0|\n|00037ace-b22f-480...|n_days_since_last...|3312.1|\n|00037ace-b22f-480...|            n_device|   1.0|\n|00043755-8e16-42b...|        n_unique_day|   1.0|\n|00043755-8e16-42b...|       n_unique_hour|   1.0|\n|00043755-8e16-42b...|n_days_since_last...|3318.1|\n|00043755-8e16-42b...|            n_device|   1.0|\n|000676b2-dd4b-4d4...|        n_unique_day|   1.0|\n|000676b2-dd4b-4d4...|       n_unique_hour|   1.0|\n|000676b2-dd4b-4d4...|n_days_since_last...|3301.1|\n|000676b2-dd4b-4d4...|            n_device|   1.0|\n|0008c5d2-c263-4b5...|        n_unique_day|   1.0|\n|0008c5d2-c263-4b5...|       n_unique_hour|   1.0|\n|0008c5d2-c263-4b5...|n_days_since_last...|3318.1|\n|0008c5d2-c263-4b5...|            n_device|   1.0|\n|000a421d-371f-492...|        n_unique_day|   1.0|\n|000a421d-371f-492...|       n_unique_hour|   1.0|\n|000a421d-371f-492...|n_days_since_last...|3334.1|\n|000a421d-371f-492...|            n_device|   1.0|\n+--------------------+--------------------+------+\nonly showing top 20 rows\n```\n:::\n:::\n\n\n::: {#eb0d794f .cell execution_count=47}\n``` {.python .cell-code}\n(\n\n)\n```\n\n::: {.cell-output .cell-output-display execution_count=42}\n```\n()\n```\n:::\n:::\n\n\n::: {#b1a2bc0c .cell execution_count=48}\n``` {.python .cell-code}\nloaders = [\n    n_events_per_hour_loader,\n    n_events_per_website_id_loader,\n    n_events_per_weekday_loader,\n    n_days_since_last_event_loader,\n    n_days_since_last_action_loader,\n    n_unique_day_loader,\n    n_unique_hour_loader,\n    n_events_per_device_loader,\n    n_unique_device_loader,\n    n_events_per_category_id_loader,\n    n_actions_per_category_id_loader,\n    n_events_per_website_id_loader,\n]\n```\n:::\n\n\n::: {#7c35da89 .cell execution_count=49}\n``` {.python .cell-code}\ndef union(df, other):\n    return df.union(other)\n```\n:::\n\n\n::: {.callout-caution title=\"About DataFrame.union()\"}\n\nThis method performs a SQL-style set union of the rows from both DataFrame objects, with no automatic deduplication of elements.\n\nUse the `distinct()` method to perform deduplication of rows.\n\nThe method resolves columns by position (not by name), following the standard behavior in SQL.\n\n:::\n\n::: {#50abe7bd .cell execution_count=50}\n``` {.python .cell-code}\nt0 = perf_counter()\nspam = [loader(df) for loader in loaders]\nperf_counter() - t0\n```\n\n::: {.cell-output .cell-output-display execution_count=45}\n```\n0.13093846700030554\n```\n:::\n:::\n\n\n::: {#c12b05ef .cell execution_count=51}\n``` {.python .cell-code}\nspam[0].printSchema()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nroot\n |-- xid: string (nullable = true)\n |-- value: long (nullable = false)\n |-- feature_name: string (nullable = true)\n\n```\n:::\n:::\n\n\n::: {#56a441f7 .cell execution_count=52}\n``` {.python .cell-code}\nlen(spam)\n```\n\n::: {.cell-output .cell-output-display execution_count=47}\n```\n12\n```\n:::\n:::\n\n\n::: {#97cdf6c5 .cell execution_count=53}\n``` {.python .cell-code}\nt0 = perf_counter()\ncsr = reduce(\n    lambda df1, df2: df1.union(df2),\n    spam\n)\n\ncsr.show(n=3)\nperf_counter() - t0\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n+--------------------+-----+--------------------+\n|                 xid|value|        feature_name|\n+--------------------+-----+--------------------+\n|00037ace-b22f-480...|  2.0|n_events_per_hour#18|\n|00037ace-b22f-480...|  4.0|n_events_per_hour#19|\n|00043755-8e16-42b...|  1.0|n_events_per_hour#12|\n+--------------------+-----+--------------------+\nonly showing top 3 rows\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=48}\n```\n1.0603654340002322\n```\n:::\n:::\n\n\n::: {#033ae6a3 .cell execution_count=54}\n``` {.python .cell-code}\ncsr.columns\n```\n\n::: {.cell-output .cell-output-display execution_count=49}\n```\n['xid', 'value', 'feature_name']\n```\n:::\n:::\n\n\n::: {#cc0b04ae .cell execution_count=55}\n``` {.python .cell-code}\ncsr.show(5)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n+--------------------+-----+--------------------+\n|                 xid|value|        feature_name|\n+--------------------+-----+--------------------+\n|00037ace-b22f-480...|  2.0|n_events_per_hour#18|\n|00037ace-b22f-480...|  4.0|n_events_per_hour#19|\n|00043755-8e16-42b...|  1.0|n_events_per_hour#12|\n|000676b2-dd4b-4d4...|  1.0|n_events_per_hour#17|\n|0008c5d2-c263-4b5...|  1.0| n_events_per_hour#4|\n+--------------------+-----+--------------------+\nonly showing top 5 rows\n```\n:::\n:::\n\n\n::: {#986635bb .cell execution_count=56}\n``` {.python .cell-code}\ncsr.rdd.getNumPartitions()\n```\n\n::: {.cell-output .cell-output-display execution_count=51}\n```\n20\n```\n:::\n:::\n\n\n::: {#bfe0ec9b .cell execution_count=57}\n``` {.python .cell-code}\nfeature_name_partition = Window().orderBy('feature_name')\n\nfn_idx = (\n    csr\n        .select('feature_name')\n        .distinct()\n        .withColumn('col', func.row_number().over(feature_name_partition))\n)\n```\n:::\n\n\n::: {#5f1c33c4 .cell execution_count=58}\n``` {.python .cell-code}\nfn_idx.cache()\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n26/02/16 10:03:06 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n26/02/16 10:03:06 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n26/02/16 10:03:06 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=53}\n```\nDataFrame[feature_name: string, col: int]\n```\n:::\n:::\n\n\n::: {#82a1d2bb .cell execution_count=59}\n``` {.python .cell-code}\ncsr = ( \n    csr\n        .join(fn_idx, 'feature_name')\n)\n\ncsr.show(10)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n26/02/16 10:03:07 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n26/02/16 10:03:07 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n\r[Stage 41:=====>                                                (25 + 20) / 240]\r\r[Stage 41:=========>                                            (41 + 20) / 240]\r\r[Stage 41:============>                                         (55 + 20) / 240]\r\r[Stage 41:=================>                                    (79 + 20) / 240]\r\r[Stage 41:==================>                                   (80 + 20) / 240]\r\r[Stage 41:======================>                              (100 + 20) / 240]\r\r[Stage 41:======================>                              (103 + 21) / 240]\r\r[Stage 41:==========================>                          (120 + 20) / 240]\r\r[Stage 41:==============================>                      (140 + 20) / 240]\r\r[Stage 41:===============================>                     (143 + 21) / 240]\r\r[Stage 41:===================================>                 (160 + 20) / 240]\r\r[Stage 41:=======================================>             (180 + 20) / 240]\r\r[Stage 41:========================================>            (182 + 20) / 240]\r\r[Stage 41:============================================>        (201 + 20) / 240]\r\r[Stage 41:================================================>    (218 + 21) / 240]\r26/02/16 10:03:11 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n26/02/16 10:03:11 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n26/02/16 10:03:11 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n26/02/16 10:03:11 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n\r                                                                                \r\r[Stage 54:>                                                       (0 + 20) / 20]\r\r[Stage 54:==>                                                     (1 + 19) / 20]\r\r[Stage 54:===============================================>        (17 + 3) / 20]\r\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n+--------------------+--------------------+-----+---+\n|        feature_name|                 xid|value|col|\n+--------------------+--------------------+-----+---+\n|n_events_per_webs...|00b0dd52-ba6f-4c9...|  2.0| 57|\n|n_events_per_webs...|012eae8a-3ca2-428...|  1.0| 57|\n|n_events_per_webs...|02410328-85b2-4b4...|  1.0| 57|\n|n_events_per_webs...|03412f95-946c-4b2...|  1.0| 57|\n|n_events_per_webs...|03d5202b-f64b-42e...|  1.0| 57|\n|n_events_per_webs...|0465a601-3171-435...|  1.0| 57|\n|n_events_per_webs...|0516e7da-7c78-41f...|  1.0| 57|\n|n_events_per_webs...|056189c9-f926-460...|  1.0| 57|\n|n_events_per_webs...|05d8404b-c83b-48a...|  1.0| 57|\n|n_events_per_webs...|064b6d87-25b1-4f5...|  1.0| 57|\n+--------------------+--------------------+-----+---+\nonly showing top 10 rows\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n\r                                                                                \r\n```\n:::\n:::\n\n\n::: {#3bdcc70c .cell execution_count=60}\n``` {.python .cell-code}\n# Replace features names and xid by a unique number\nfeature_name_partition = Window().orderBy('feature_name')\ncol_idx = func.dense_rank().over(feature_name_partition)\n```\n:::\n\n\n::: {#dd0efb6e .cell execution_count=61}\n``` {.python .cell-code}\nxid_partition = Window().orderBy('xid')\nrow_idx = func.dense_rank().over(xid_partition)\n```\n:::\n\n\n::: {#13ff1a60 .cell execution_count=62}\n``` {.python .cell-code}\ncsr = (\n    csr\n#        .withColumn('col', col_idx)\n        .withColumn('row', row_idx)\n)\n```\n:::\n\n\n::: {#756540ce .cell execution_count=63}\n``` {.python .cell-code}\ncsr = csr.na.drop('any')\n```\n:::\n\n\n::: {#d2986537 .cell execution_count=64}\n``` {.python .cell-code}\ncsr.show(n=5)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n26/02/16 10:03:16 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n26/02/16 10:03:16 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n26/02/16 10:03:16 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n26/02/16 10:03:16 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n26/02/16 10:03:16 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n26/02/16 10:03:16 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n26/02/16 10:03:16 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n\r[Stage 63:>                                                       (0 + 20) / 20]\r26/02/16 10:03:20 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n26/02/16 10:03:20 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n\r[Stage 66:>                                                         (0 + 1) / 1]\r\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n+--------------------+--------------------+------+---+---+\n|        feature_name|                 xid| value|col|row|\n+--------------------+--------------------+------+---+---+\n|n_events_per_hour#17|00008f69-9f2f-445...|   1.0| 26|  1|\n|n_events_per_webs...|00008f69-9f2f-445...|   1.0| 61|  1|\n|n_events_per_week...|00008f69-9f2f-445...|   1.0|101|  1|\n|n_days_since_last...|00008f69-9f2f-445...|3308.1|  8|  1|\n|n_days_since_last...|00008f69-9f2f-445...|3308.1|  7|  1|\n+--------------------+--------------------+------+---+---+\nonly showing top 5 rows\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n\r                                                                                \r\n```\n:::\n:::\n\n\n::: {#be9b4cf4 .cell execution_count=65}\n``` {.python .cell-code}\n# Let's save the result of our hard work into a new parquet file (in the local filesystem)\noutput_path = os.path.abspath('./data')\noutput_file = 'file://' + output_path + '/' + 'csr.parquet'\ncsr.write.parquet(output_file, mode='overwrite')\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n26/02/16 10:03:22 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n26/02/16 10:03:22 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n26/02/16 10:03:22 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n26/02/16 10:03:22 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n26/02/16 10:03:22 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n26/02/16 10:03:22 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n26/02/16 10:03:22 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n26/02/16 10:03:22 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n26/02/16 10:03:22 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n\r[Stage 72:>                                                       (0 + 20) / 20]\r\r[Stage 72:=====================================================>  (19 + 1) / 20]\r26/02/16 10:03:26 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n26/02/16 10:03:26 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n\r[Stage 75:>                                                         (0 + 1) / 1]\r\r                                                                                \r\n```\n:::\n:::\n\n\n::: {#4ab45ba1 .cell execution_count=66}\n``` {.python .cell-code}\n# Save  the result  to hdfs\ncsr.write.parquet('csr.parquet', mode='overwrite')\n```\n:::\n\n\n# Preparation of the training dataset\n\n::: {#3e78e0df .cell execution_count=67}\n``` {.python .cell-code}\ninput_path = os.path.abspath('./data')\ncsr_file = 'file://' + input_path + '/' + 'csr.parquet'\n\ndf = spark.read.parquet(csr_file)\ndf.head(n=5)\n```\n\n::: {.cell-output .cell-output-display execution_count=60}\n```\n[Row(feature_name='n_events_per_hour#17', xid='00008f69-9f2f-4453-80ec-98b4ae8a3085', value=1.0, col=26, row=1),\n Row(feature_name='n_events_per_website_id#3', xid='00008f69-9f2f-4453-80ec-98b4ae8a3085', value=1.0, col=61, row=1),\n Row(feature_name='n_events_per_weekday#Thursday', xid='00008f69-9f2f-4453-80ec-98b4ae8a3085', value=1.0, col=101, row=1),\n Row(feature_name='n_days_since_last_event', xid='00008f69-9f2f-4453-80ec-98b4ae8a3085', value=3308.1, col=8, row=1),\n Row(feature_name='n_days_since_last_action#O', xid='00008f69-9f2f-4453-80ec-98b4ae8a3085', value=3308.1, col=7, row=1)]\n```\n:::\n:::\n\n\n::: {#14358e7a .cell execution_count=68}\n``` {.python .cell-code}\ndf.count()\n```\n\n::: {.cell-output .cell-output-display execution_count=61}\n```\n5860593\n```\n:::\n:::\n\n\n::: {#b8c2c472 .cell execution_count=69}\n``` {.python .cell-code}\n# What are the features related to category_id 1204 ?\nfeatures_names = \\\n    df.select('feature_name')\\\n    .distinct()\\\n    .toPandas()['feature_name']\n```\n:::\n\n\n::: {#502ec60f .cell execution_count=70}\n``` {.python .cell-code}\nfeatures_names\n```\n\n::: {.cell-output .cell-output-display execution_count=63}\n```\n0       n_events_per_website_id#25\n1      n_events_per_weekday#Sunday\n2       n_events_per_website_id#42\n3              n_events_per_hour#0\n4       n_events_per_website_id#22\n                  ...             \n99      n_events_per_website_id#29\n100     n_events_per_website_id#60\n101    n_events_per_weekday#Monday\n102     n_events_per_website_id#47\n103     n_events_per_website_id#32\nName: feature_name, Length: 104, dtype: str\n```\n:::\n:::\n\n\n::: {#3e099e7e .cell execution_count=71}\n``` {.python .cell-code}\n[feature_name for feature_name in features_names if '1204' in feature_name]\n```\n\n::: {.cell-output .cell-output-display execution_count=64}\n```\n['n_events_per_category_id#1204.0',\n 'n_actions_per_category_id#C#1204.0',\n 'n_actions_per_category_id#O#1204.0']\n```\n:::\n:::\n\n\n::: {#d636cb2d .cell execution_count=72}\n``` {.python .cell-code}\n# Look for the xid that have at least one exposure to campaign 1204\nkeep = func.when(\n    (col('feature_name') == 'n_actions_per_category_id#C#1204.0') |\n    (col('feature_name') == 'n_actions_per_category_id#O#1204.0'),\n    1).otherwise(0)\ndf = df.withColumn('keep', keep)\n\ndf.where(col('keep') > 0).count()\n```\n\n::: {.cell-output .cell-output-display execution_count=65}\n```\n153545\n```\n:::\n:::\n\n\n::: {#b5614e2f .cell execution_count=73}\n``` {.python .cell-code}\n# Sum of the keeps :)\nxid_partition = Window.partitionBy('xid')\nsum_keep = func.sum(col('keep')).over(xid_partition)\ndf = df.withColumn('sum_keep', sum_keep)\n```\n:::\n\n\n::: {#0919ff87 .cell execution_count=74}\n``` {.python .cell-code}\n# Let's keep the xid exposed to 1204\ndf = df.where(col('sum_keep') > 0)\n```\n:::\n\n\n::: {#b51a8df8 .cell execution_count=75}\n``` {.python .cell-code}\ndf.count()\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n\r[Stage 88:===================================================>     (9 + 1) / 10]\r\r                                                                                \r\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=68}\n```\n1937035\n```\n:::\n:::\n\n\n::: {#aaea53ef .cell execution_count=76}\n``` {.python .cell-code}\ndf.select('xid').distinct().count()\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n\r[Stage 94:===================================================>     (9 + 1) / 10]\r\r                                                                                \r\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=69}\n```\n152347\n```\n:::\n:::\n\n\n::: {#7f92d92b .cell execution_count=77}\n``` {.python .cell-code}\nrow_partition = Window().orderBy('row')\ncol_partition = Window().orderBy('col')\nrow_new = func.dense_rank().over(row_partition)\ncol_new = func.dense_rank().over(col_partition)\ndf = df.withColumn('row_new', row_new)\ndf = df.withColumn('col_new', col_new)\ncsr_data = df.select('row_new', 'col_new', 'value').toPandas()\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n26/02/16 10:03:37 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n26/02/16 10:03:37 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n26/02/16 10:03:37 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n26/02/16 10:03:37 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n26/02/16 10:03:37 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n\r[Stage 100:==================================================>     (9 + 1) / 10]\r26/02/16 10:03:39 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n26/02/16 10:03:39 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n26/02/16 10:03:39 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n26/02/16 10:03:39 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n\r[Stage 102:====================================================>  (21 + 1) / 22]\r26/02/16 10:03:39 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n26/02/16 10:03:39 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n26/02/16 10:03:39 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n26/02/16 10:03:39 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n\r[Stage 105:>                                                        (0 + 1) / 1]\r\r                                                                                \r\n```\n:::\n:::\n\n\n::: {#3e583550 .cell execution_count=78}\n``` {.python .cell-code}\ncsr_data.head()\n```\n\n::: {.cell-output .cell-output-display execution_count=71}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>row_new</th>\n      <th>col_new</th>\n      <th>value</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>221</td>\n      <td>1</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>998</td>\n      <td>1</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2079</td>\n      <td>1</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2712</td>\n      <td>1</td>\n      <td>9.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2961</td>\n      <td>1</td>\n      <td>4.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n::: {#2fdba9eb .cell execution_count=79}\n``` {.python .cell-code}\nfeatures_names = (\n    df\n        .select('feature_name', 'col_new')\n        .distinct()\n)\n\n(\n    features_names\n        .where(col('feature_name') == 'n_actions_per_category_id#C#1204.0')\n        .head()\n)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n26/02/16 10:03:45 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n26/02/16 10:03:45 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n26/02/16 10:03:45 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n\r[Stage 106:==================================================>     (9 + 1) / 10]\r26/02/16 10:03:47 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n26/02/16 10:03:47 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n26/02/16 10:03:48 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n26/02/16 10:03:48 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n\r                                                                                \r\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=72}\n```\nRow(feature_name='n_actions_per_category_id#C#1204.0', col_new=2)\n```\n:::\n:::\n\n\n::: {#e6429f2f .cell execution_count=80}\n``` {.python .cell-code}\n(\n    features_names\n        .where(col('feature_name') == 'n_actions_per_category_id#O#1204.0')\n        .head()\n)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n26/02/16 10:03:48 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n26/02/16 10:03:48 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n26/02/16 10:03:48 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n\r[Stage 112:==================================================>     (9 + 1) / 10]\r26/02/16 10:03:50 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n26/02/16 10:03:50 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n26/02/16 10:03:50 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n26/02/16 10:03:50 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n\r                                                                                \r\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=73}\n```\nRow(feature_name='n_actions_per_category_id#O#1204.0', col_new=4)\n```\n:::\n:::\n\n\n::: {#0acc0347 .cell execution_count=81}\n``` {.python .cell-code}\nrows = csr_data['row_new'].values - 1\ncols = csr_data['col_new'].values - 1\nvals = csr_data['value'].values\n\nX_csr = csr_matrix((vals, (rows, cols)))\n```\n:::\n\n\n::: {#9715f56e .cell execution_count=82}\n``` {.python .cell-code}\nX_csr.shape\n```\n\n::: {.cell-output .cell-output-display execution_count=75}\n```\n(152347, 92)\n```\n:::\n:::\n\n\n::: {#7c83a675 .cell execution_count=83}\n``` {.python .cell-code}\nX_csr.shape, X_csr.nnz\n```\n\n::: {.cell-output .cell-output-display execution_count=76}\n```\n((152347, 92), 1783847)\n```\n:::\n:::\n\n\n::: {#a21be78b .cell execution_count=84}\n``` {.python .cell-code}\nX_csr.nnz / (X_csr.shape[0]* X_csr.shape[1])   # 0152347 * 92)\n```\n\n::: {.cell-output .cell-output-display execution_count=77}\n```\n0.12727287904814552\n```\n:::\n:::\n\n\n::: {#7be31850 .cell execution_count=85}\n``` {.python .cell-code}\n# The label vector. Let's make it dense, flat and binary\ny = np.array(X_csr[:, 1].todense()).ravel()\ny = np.array(y > 0, dtype=np.int64)\n```\n:::\n\n\n::: {#3292220e .cell execution_count=86}\n``` {.python .cell-code}\nX_csr.shape\n```\n\n::: {.cell-output .cell-output-display execution_count=79}\n```\n(152347, 92)\n```\n:::\n:::\n\n\n::: {#3ba3b295 .cell execution_count=87}\n``` {.python .cell-code}\n# We remove the second and fourth column. \n# It actually contain the label we'll want to predict.\nkept_cols = list(range(X_csr.shape[1]))\nkept_cols.pop(1)\nkept_cols.pop(2)\nX = X_csr[:, kept_cols]\n```\n:::\n\n\n::: {#d349386e .cell execution_count=88}\n``` {.python .cell-code}\nlen(kept_cols)\n```\n\n::: {.cell-output .cell-output-display execution_count=81}\n```\n90\n```\n:::\n:::\n\n\n::: {#592f8c44 .cell execution_count=89}\n``` {.python .cell-code}\nX_csr.shape, X.shape\n```\n\n::: {.cell-output .cell-output-display execution_count=82}\n```\n((152347, 92), (152347, 90))\n```\n:::\n:::\n\n\n## Finally !!\n\nWow ! That was a lot of work. Now we have a features matrix $X$ and a vector of labels $y$.\n\n::: {#6e9e7a56 .cell execution_count=90}\n``` {.python .cell-code}\nX.indices\n```\n\n::: {.cell-output .cell-output-display execution_count=83}\n```\narray([ 3,  4,  5, ..., 85, 88, 89], shape=(1630302,), dtype=int32)\n```\n:::\n:::\n\n\n::: {#b9432695 .cell execution_count=91}\n``` {.python .cell-code}\nX.indptr\n```\n\n::: {.cell-output .cell-output-display execution_count=84}\n```\narray([      0,      10,      20, ..., 1630282, 1630292, 1630302],\n      shape=(152348,), dtype=int32)\n```\n:::\n:::\n\n\n::: {#e38cf695 .cell execution_count=92}\n``` {.python .cell-code}\nX.shape, X.nnz\n```\n\n::: {.cell-output .cell-output-display execution_count=85}\n```\n((152347, 90), 1630302)\n```\n:::\n:::\n\n\n::: {#f6395f73 .cell execution_count=93}\n``` {.python .cell-code}\ny.shape, y.sum()\n```\n\n::: {.cell-output .cell-output-display execution_count=86}\n```\n((152347,), np.int64(1594))\n```\n:::\n:::\n\n\n# Some learning for/from this data\n\n::: {#7f5d8e05 .cell execution_count=94}\n``` {.python .cell-code}\n# Normalize the features\nX = MaxAbsScaler().fit_transform(X)\nX_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.3)\n\nclf = LogisticRegression(\n    penalty='l2',\n    C=1e3,\n    solver='lbfgs',\n    class_weight='balanced'\n)\n\nclf.fit(X_train, y_train)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n/home/boucheron/Documents/IFEBY310/.nlp-venv/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:1135: FutureWarning:\n\n'penalty' was deprecated in version 1.8 and will be removed in 1.10. To avoid this warning, leave 'penalty' set to its default value and use 'l1_ratio' or 'C' instead. Use l1_ratio=0 instead of penalty='l2', l1_ratio=1 instead of penalty='l1', and C=np.inf instead of penalty=None.\n\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=87}\n``````{=html}\n<style>#sk-container-id-1 {\n  /* Definition of color scheme common for light and dark mode */\n  --sklearn-color-text: #000;\n  --sklearn-color-text-muted: #666;\n  --sklearn-color-line: gray;\n  /* Definition of color scheme for unfitted estimators */\n  --sklearn-color-unfitted-level-0: #fff5e6;\n  --sklearn-color-unfitted-level-1: #f6e4d2;\n  --sklearn-color-unfitted-level-2: #ffe0b3;\n  --sklearn-color-unfitted-level-3: chocolate;\n  /* Definition of color scheme for fitted estimators */\n  --sklearn-color-fitted-level-0: #f0f8ff;\n  --sklearn-color-fitted-level-1: #d4ebff;\n  --sklearn-color-fitted-level-2: #b3dbfd;\n  --sklearn-color-fitted-level-3: cornflowerblue;\n}\n\n#sk-container-id-1.light {\n  /* Specific color for light theme */\n  --sklearn-color-text-on-default-background: black;\n  --sklearn-color-background: white;\n  --sklearn-color-border-box: black;\n  --sklearn-color-icon: #696969;\n}\n\n#sk-container-id-1.dark {\n  --sklearn-color-text-on-default-background: white;\n  --sklearn-color-background: #111;\n  --sklearn-color-border-box: white;\n  --sklearn-color-icon: #878787;\n}\n\n#sk-container-id-1 {\n  color: var(--sklearn-color-text);\n}\n\n#sk-container-id-1 pre {\n  padding: 0;\n}\n\n#sk-container-id-1 input.sk-hidden--visually {\n  border: 0;\n  clip: rect(1px 1px 1px 1px);\n  clip: rect(1px, 1px, 1px, 1px);\n  height: 1px;\n  margin: -1px;\n  overflow: hidden;\n  padding: 0;\n  position: absolute;\n  width: 1px;\n}\n\n#sk-container-id-1 div.sk-dashed-wrapped {\n  border: 1px dashed var(--sklearn-color-line);\n  margin: 0 0.4em 0.5em 0.4em;\n  box-sizing: border-box;\n  padding-bottom: 0.4em;\n  background-color: var(--sklearn-color-background);\n}\n\n#sk-container-id-1 div.sk-container {\n  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n     but bootstrap.min.css set `[hidden] { display: none !important; }`\n     so we also need the `!important` here to be able to override the\n     default hidden behavior on the sphinx rendered scikit-learn.org.\n     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n  display: inline-block !important;\n  position: relative;\n}\n\n#sk-container-id-1 div.sk-text-repr-fallback {\n  display: none;\n}\n\ndiv.sk-parallel-item,\ndiv.sk-serial,\ndiv.sk-item {\n  /* draw centered vertical line to link estimators */\n  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n  background-size: 2px 100%;\n  background-repeat: no-repeat;\n  background-position: center center;\n}\n\n/* Parallel-specific style estimator block */\n\n#sk-container-id-1 div.sk-parallel-item::after {\n  content: \"\";\n  width: 100%;\n  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n  flex-grow: 1;\n}\n\n#sk-container-id-1 div.sk-parallel {\n  display: flex;\n  align-items: stretch;\n  justify-content: center;\n  background-color: var(--sklearn-color-background);\n  position: relative;\n}\n\n#sk-container-id-1 div.sk-parallel-item {\n  display: flex;\n  flex-direction: column;\n}\n\n#sk-container-id-1 div.sk-parallel-item:first-child::after {\n  align-self: flex-end;\n  width: 50%;\n}\n\n#sk-container-id-1 div.sk-parallel-item:last-child::after {\n  align-self: flex-start;\n  width: 50%;\n}\n\n#sk-container-id-1 div.sk-parallel-item:only-child::after {\n  width: 0;\n}\n\n/* Serial-specific style estimator block */\n\n#sk-container-id-1 div.sk-serial {\n  display: flex;\n  flex-direction: column;\n  align-items: center;\n  background-color: var(--sklearn-color-background);\n  padding-right: 1em;\n  padding-left: 1em;\n}\n\n\n/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\nclickable and can be expanded/collapsed.\n- Pipeline and ColumnTransformer use this feature and define the default style\n- Estimators will overwrite some part of the style using the `sk-estimator` class\n*/\n\n/* Pipeline and ColumnTransformer style (default) */\n\n#sk-container-id-1 div.sk-toggleable {\n  /* Default theme specific background. It is overwritten whether we have a\n  specific estimator or a Pipeline/ColumnTransformer */\n  background-color: var(--sklearn-color-background);\n}\n\n/* Toggleable label */\n#sk-container-id-1 label.sk-toggleable__label {\n  cursor: pointer;\n  display: flex;\n  width: 100%;\n  margin-bottom: 0;\n  padding: 0.5em;\n  box-sizing: border-box;\n  text-align: center;\n  align-items: center;\n  justify-content: center;\n  gap: 0.5em;\n}\n\n#sk-container-id-1 label.sk-toggleable__label .caption {\n  font-size: 0.6rem;\n  font-weight: lighter;\n  color: var(--sklearn-color-text-muted);\n}\n\n#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n  /* Arrow on the left of the label */\n  content: \"\";\n  float: left;\n  margin-right: 0.25em;\n  color: var(--sklearn-color-icon);\n}\n\n#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n  color: var(--sklearn-color-text);\n}\n\n/* Toggleable content - dropdown */\n\n#sk-container-id-1 div.sk-toggleable__content {\n  display: none;\n  text-align: left;\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-0);\n}\n\n#sk-container-id-1 div.sk-toggleable__content.fitted {\n  /* fitted */\n  background-color: var(--sklearn-color-fitted-level-0);\n}\n\n#sk-container-id-1 div.sk-toggleable__content pre {\n  margin: 0.2em;\n  border-radius: 0.25em;\n  color: var(--sklearn-color-text);\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-0);\n}\n\n#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n  /* unfitted */\n  background-color: var(--sklearn-color-fitted-level-0);\n}\n\n#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n  /* Expand drop-down */\n  display: block;\n  width: 100%;\n  overflow: visible;\n}\n\n#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n  content: \"\";\n}\n\n/* Pipeline/ColumnTransformer-specific style */\n\n#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n  color: var(--sklearn-color-text);\n  background-color: var(--sklearn-color-unfitted-level-2);\n}\n\n#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n  background-color: var(--sklearn-color-fitted-level-2);\n}\n\n/* Estimator-specific style */\n\n/* Colorize estimator box */\n#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-2);\n}\n\n#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n  /* fitted */\n  background-color: var(--sklearn-color-fitted-level-2);\n}\n\n#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n#sk-container-id-1 div.sk-label label {\n  /* The background is the default theme color */\n  color: var(--sklearn-color-text-on-default-background);\n}\n\n/* On hover, darken the color of the background */\n#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n  color: var(--sklearn-color-text);\n  background-color: var(--sklearn-color-unfitted-level-2);\n}\n\n/* Label box, darken color on hover, fitted */\n#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n  color: var(--sklearn-color-text);\n  background-color: var(--sklearn-color-fitted-level-2);\n}\n\n/* Estimator label */\n\n#sk-container-id-1 div.sk-label label {\n  font-family: monospace;\n  font-weight: bold;\n  line-height: 1.2em;\n}\n\n#sk-container-id-1 div.sk-label-container {\n  text-align: center;\n}\n\n/* Estimator-specific */\n#sk-container-id-1 div.sk-estimator {\n  font-family: monospace;\n  border: 1px dotted var(--sklearn-color-border-box);\n  border-radius: 0.25em;\n  box-sizing: border-box;\n  margin-bottom: 0.5em;\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-0);\n}\n\n#sk-container-id-1 div.sk-estimator.fitted {\n  /* fitted */\n  background-color: var(--sklearn-color-fitted-level-0);\n}\n\n/* on hover */\n#sk-container-id-1 div.sk-estimator:hover {\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-2);\n}\n\n#sk-container-id-1 div.sk-estimator.fitted:hover {\n  /* fitted */\n  background-color: var(--sklearn-color-fitted-level-2);\n}\n\n/* Specification for estimator info (e.g. \"i\" and \"?\") */\n\n/* Common style for \"i\" and \"?\" */\n\n.sk-estimator-doc-link,\na:link.sk-estimator-doc-link,\na:visited.sk-estimator-doc-link {\n  float: right;\n  font-size: smaller;\n  line-height: 1em;\n  font-family: monospace;\n  background-color: var(--sklearn-color-unfitted-level-0);\n  border-radius: 1em;\n  height: 1em;\n  width: 1em;\n  text-decoration: none !important;\n  margin-left: 0.5em;\n  text-align: center;\n  /* unfitted */\n  border: var(--sklearn-color-unfitted-level-3) 1pt solid;\n  color: var(--sklearn-color-unfitted-level-3);\n}\n\n.sk-estimator-doc-link.fitted,\na:link.sk-estimator-doc-link.fitted,\na:visited.sk-estimator-doc-link.fitted {\n  /* fitted */\n  background-color: var(--sklearn-color-fitted-level-0);\n  border: var(--sklearn-color-fitted-level-3) 1pt solid;\n  color: var(--sklearn-color-fitted-level-3);\n}\n\n/* On hover */\ndiv.sk-estimator:hover .sk-estimator-doc-link:hover,\n.sk-estimator-doc-link:hover,\ndiv.sk-label-container:hover .sk-estimator-doc-link:hover,\n.sk-estimator-doc-link:hover {\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-3);\n  border: var(--sklearn-color-fitted-level-0) 1pt solid;\n  color: var(--sklearn-color-unfitted-level-0);\n  text-decoration: none;\n}\n\ndiv.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n.sk-estimator-doc-link.fitted:hover,\ndiv.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n.sk-estimator-doc-link.fitted:hover {\n  /* fitted */\n  background-color: var(--sklearn-color-fitted-level-3);\n  border: var(--sklearn-color-fitted-level-0) 1pt solid;\n  color: var(--sklearn-color-fitted-level-0);\n  text-decoration: none;\n}\n\n/* Span, style for the box shown on hovering the info icon */\n.sk-estimator-doc-link span {\n  display: none;\n  z-index: 9999;\n  position: relative;\n  font-weight: normal;\n  right: .2ex;\n  padding: .5ex;\n  margin: .5ex;\n  width: min-content;\n  min-width: 20ex;\n  max-width: 50ex;\n  color: var(--sklearn-color-text);\n  box-shadow: 2pt 2pt 4pt #999;\n  /* unfitted */\n  background: var(--sklearn-color-unfitted-level-0);\n  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n}\n\n.sk-estimator-doc-link.fitted span {\n  /* fitted */\n  background: var(--sklearn-color-fitted-level-0);\n  border: var(--sklearn-color-fitted-level-3);\n}\n\n.sk-estimator-doc-link:hover span {\n  display: block;\n}\n\n/* \"?\"-specific style due to the `<a>` HTML tag */\n\n#sk-container-id-1 a.estimator_doc_link {\n  float: right;\n  font-size: 1rem;\n  line-height: 1em;\n  font-family: monospace;\n  background-color: var(--sklearn-color-unfitted-level-0);\n  border-radius: 1rem;\n  height: 1rem;\n  width: 1rem;\n  text-decoration: none;\n  /* unfitted */\n  color: var(--sklearn-color-unfitted-level-1);\n  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n}\n\n#sk-container-id-1 a.estimator_doc_link.fitted {\n  /* fitted */\n  background-color: var(--sklearn-color-fitted-level-0);\n  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n  color: var(--sklearn-color-fitted-level-1);\n}\n\n/* On hover */\n#sk-container-id-1 a.estimator_doc_link:hover {\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-3);\n  color: var(--sklearn-color-background);\n  text-decoration: none;\n}\n\n#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n  /* fitted */\n  background-color: var(--sklearn-color-fitted-level-3);\n}\n\n.estimator-table {\n    font-family: monospace;\n}\n\n.estimator-table summary {\n    padding: .5rem;\n    cursor: pointer;\n}\n\n.estimator-table summary::marker {\n    font-size: 0.7rem;\n}\n\n.estimator-table details[open] {\n    padding-left: 0.1rem;\n    padding-right: 0.1rem;\n    padding-bottom: 0.3rem;\n}\n\n.estimator-table .parameters-table {\n    margin-left: auto !important;\n    margin-right: auto !important;\n    margin-top: 0;\n}\n\n.estimator-table .parameters-table tr:nth-child(odd) {\n    background-color: #fff;\n}\n\n.estimator-table .parameters-table tr:nth-child(even) {\n    background-color: #f6f6f6;\n}\n\n.estimator-table .parameters-table tr:hover {\n    background-color: #e0e0e0;\n}\n\n.estimator-table table td {\n    border: 1px solid rgba(106, 105, 104, 0.232);\n}\n\n/*\n    `table td`is set in notebook with right text-align.\n    We need to overwrite it.\n*/\n.estimator-table table td.param {\n    text-align: left;\n    position: relative;\n    padding: 0;\n}\n\n.user-set td {\n    color:rgb(255, 94, 0);\n    text-align: left !important;\n}\n\n.user-set td.value {\n    color:rgb(255, 94, 0);\n    background-color: transparent;\n}\n\n.default td {\n    color: black;\n    text-align: left !important;\n}\n\n.user-set td i,\n.default td i {\n    color: black;\n}\n\n/*\n    Styles for parameter documentation links\n    We need styling for visited so jupyter doesn't overwrite it\n*/\na.param-doc-link,\na.param-doc-link:link,\na.param-doc-link:visited {\n    text-decoration: underline dashed;\n    text-underline-offset: .3em;\n    color: inherit;\n    display: block;\n    padding: .5em;\n}\n\n/* \"hack\" to make the entire area of the cell containing the link clickable */\na.param-doc-link::before {\n    position: absolute;\n    content: \"\";\n    inset: 0;\n}\n\n.param-doc-description {\n    display: none;\n    position: absolute;\n    z-index: 9999;\n    left: 0;\n    padding: .5ex;\n    margin-left: 1.5em;\n    color: var(--sklearn-color-text);\n    box-shadow: .3em .3em .4em #999;\n    width: max-content;\n    text-align: left;\n    max-height: 10em;\n    overflow-y: auto;\n\n    /* unfitted */\n    background: var(--sklearn-color-unfitted-level-0);\n    border: thin solid var(--sklearn-color-unfitted-level-3);\n}\n\n/* Fitted state for parameter tooltips */\n.fitted .param-doc-description {\n    /* fitted */\n    background: var(--sklearn-color-fitted-level-0);\n    border: thin solid var(--sklearn-color-fitted-level-3);\n}\n\n.param-doc-link:hover .param-doc-description {\n    display: block;\n}\n\n.copy-paste-icon {\n    background-image: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHZpZXdCb3g9IjAgMCA0NDggNTEyIj48IS0tIUZvbnQgQXdlc29tZSBGcmVlIDYuNy4yIGJ5IEBmb250YXdlc29tZSAtIGh0dHBzOi8vZm9udGF3ZXNvbWUuY29tIExpY2Vuc2UgLSBodHRwczovL2ZvbnRhd2Vzb21lLmNvbS9saWNlbnNlL2ZyZWUgQ29weXJpZ2h0IDIwMjUgRm9udGljb25zLCBJbmMuLS0+PHBhdGggZD0iTTIwOCAwTDMzMi4xIDBjMTIuNyAwIDI0LjkgNS4xIDMzLjkgMTQuMWw2Ny45IDY3LjljOSA5IDE0LjEgMjEuMiAxNC4xIDMzLjlMNDQ4IDMzNmMwIDI2LjUtMjEuNSA0OC00OCA0OGwtMTkyIDBjLTI2LjUgMC00OC0yMS41LTQ4LTQ4bDAtMjg4YzAtMjYuNSAyMS41LTQ4IDQ4LTQ4ek00OCAxMjhsODAgMCAwIDY0LTY0IDAgMCAyNTYgMTkyIDAgMC0zMiA2NCAwIDAgNDhjMCAyNi41LTIxLjUgNDgtNDggNDhMNDggNTEyYy0yNi41IDAtNDgtMjEuNS00OC00OEwwIDE3NmMwLTI2LjUgMjEuNS00OCA0OC00OHoiLz48L3N2Zz4=);\n    background-repeat: no-repeat;\n    background-size: 14px 14px;\n    background-position: 0;\n    display: inline-block;\n    width: 14px;\n    height: 14px;\n    cursor: pointer;\n}\n</style><body><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(C=1000.0, class_weight=&#x27;balanced&#x27;, penalty=&#x27;l2&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>LogisticRegression</div></div><div><a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.8/modules/generated/sklearn.linear_model.LogisticRegression.html\">?<span>Documentation for LogisticRegression</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></div></label><div class=\"sk-toggleable__content fitted\" data-param-prefix=\"\">\n        <div class=\"estimator-table\">\n            <details>\n                <summary>Parameters</summary>\n                <table class=\"parameters-table\">\n                  <tbody>\n                    \n        <tr class=\"user-set\">\n            <td><i class=\"copy-paste-icon\"\n                 onclick=\"copyToClipboard('penalty',\n                          this.parentElement.nextElementSibling)\"\n            ></i></td>\n            <td class=\"param\">\n        <a class=\"param-doc-link\"\n            rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.8/modules/generated/sklearn.linear_model.LogisticRegression.html#:~:text=penalty,-%7B%27l1%27%2C%20%27l2%27%2C%20%27elasticnet%27%2C%20None%7D%2C%20default%3D%27l2%27\">\n            penalty\n            <span class=\"param-doc-description\">penalty: {'l1', 'l2', 'elasticnet', None}, default='l2'<br><br>Specify the norm of the penalty:<br><br>- `None`: no penalty is added;<br>- `'l2'`: add a L2 penalty term and it is the default choice;<br>- `'l1'`: add a L1 penalty term;<br>- `'elasticnet'`: both L1 and L2 penalty terms are added.<br><br>.. warning::<br>   Some penalties may not work with some solvers. See the parameter<br>   `solver` below, to know the compatibility between the penalty and<br>   solver.<br><br>.. versionadded:: 0.19<br>   l1 penalty with SAGA solver (allowing 'multinomial' + L1)<br><br>.. deprecated:: 1.8<br>   `penalty` was deprecated in version 1.8 and will be removed in 1.10.<br>   Use `l1_ratio` instead. `l1_ratio=0` for `penalty='l2'`, `l1_ratio=1` for<br>   `penalty='l1'` and `l1_ratio` set to any float between 0 and 1 for<br>   `'penalty='elasticnet'`.</span>\n        </a>\n    </td>\n            <td class=\"value\">&#x27;l2&#x27;</td>\n        </tr>\n    \n\n        <tr class=\"user-set\">\n            <td><i class=\"copy-paste-icon\"\n                 onclick=\"copyToClipboard('C',\n                          this.parentElement.nextElementSibling)\"\n            ></i></td>\n            <td class=\"param\">\n        <a class=\"param-doc-link\"\n            rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.8/modules/generated/sklearn.linear_model.LogisticRegression.html#:~:text=C,-float%2C%20default%3D1.0\">\n            C\n            <span class=\"param-doc-description\">C: float, default=1.0<br><br>Inverse of regularization strength; must be a positive float.<br>Like in support vector machines, smaller values specify stronger<br>regularization. `C=np.inf` results in unpenalized logistic regression.<br>For a visual example on the effect of tuning the `C` parameter<br>with an L1 penalty, see:<br>:ref:`sphx_glr_auto_examples_linear_model_plot_logistic_path.py`.</span>\n        </a>\n    </td>\n            <td class=\"value\">1000.0</td>\n        </tr>\n    \n\n        <tr class=\"default\">\n            <td><i class=\"copy-paste-icon\"\n                 onclick=\"copyToClipboard('l1_ratio',\n                          this.parentElement.nextElementSibling)\"\n            ></i></td>\n            <td class=\"param\">\n        <a class=\"param-doc-link\"\n            rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.8/modules/generated/sklearn.linear_model.LogisticRegression.html#:~:text=l1_ratio,-float%2C%20default%3D0.0\">\n            l1_ratio\n            <span class=\"param-doc-description\">l1_ratio: float, default=0.0<br><br>The Elastic-Net mixing parameter, with `0 <= l1_ratio <= 1`. Setting<br>`l1_ratio=1` gives a pure L1-penalty, setting `l1_ratio=0` a pure L2-penalty.<br>Any value between 0 and 1 gives an Elastic-Net penalty of the form<br>`l1_ratio * L1 + (1 - l1_ratio) * L2`.<br><br>.. warning::<br>   Certain values of `l1_ratio`, i.e. some penalties, may not work with some<br>   solvers. See the parameter `solver` below, to know the compatibility between<br>   the penalty and solver.<br><br>.. versionchanged:: 1.8<br>    Default value changed from None to 0.0.<br><br>.. deprecated:: 1.8<br>    `None` is deprecated and will be removed in version 1.10. Always use<br>    `l1_ratio` to specify the penalty type.</span>\n        </a>\n    </td>\n            <td class=\"value\">0.0</td>\n        </tr>\n    \n\n        <tr class=\"default\">\n            <td><i class=\"copy-paste-icon\"\n                 onclick=\"copyToClipboard('dual',\n                          this.parentElement.nextElementSibling)\"\n            ></i></td>\n            <td class=\"param\">\n        <a class=\"param-doc-link\"\n            rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.8/modules/generated/sklearn.linear_model.LogisticRegression.html#:~:text=dual,-bool%2C%20default%3DFalse\">\n            dual\n            <span class=\"param-doc-description\">dual: bool, default=False<br><br>Dual (constrained) or primal (regularized, see also<br>:ref:`this equation <regularized-logistic-loss>`) formulation. Dual formulation<br>is only implemented for l2 penalty with liblinear solver. Prefer `dual=False`<br>when n_samples > n_features.</span>\n        </a>\n    </td>\n            <td class=\"value\">False</td>\n        </tr>\n    \n\n        <tr class=\"default\">\n            <td><i class=\"copy-paste-icon\"\n                 onclick=\"copyToClipboard('tol',\n                          this.parentElement.nextElementSibling)\"\n            ></i></td>\n            <td class=\"param\">\n        <a class=\"param-doc-link\"\n            rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.8/modules/generated/sklearn.linear_model.LogisticRegression.html#:~:text=tol,-float%2C%20default%3D1e-4\">\n            tol\n            <span class=\"param-doc-description\">tol: float, default=1e-4<br><br>Tolerance for stopping criteria.</span>\n        </a>\n    </td>\n            <td class=\"value\">0.0001</td>\n        </tr>\n    \n\n        <tr class=\"default\">\n            <td><i class=\"copy-paste-icon\"\n                 onclick=\"copyToClipboard('fit_intercept',\n                          this.parentElement.nextElementSibling)\"\n            ></i></td>\n            <td class=\"param\">\n        <a class=\"param-doc-link\"\n            rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.8/modules/generated/sklearn.linear_model.LogisticRegression.html#:~:text=fit_intercept,-bool%2C%20default%3DTrue\">\n            fit_intercept\n            <span class=\"param-doc-description\">fit_intercept: bool, default=True<br><br>Specifies if a constant (a.k.a. bias or intercept) should be<br>added to the decision function.</span>\n        </a>\n    </td>\n            <td class=\"value\">True</td>\n        </tr>\n    \n\n        <tr class=\"default\">\n            <td><i class=\"copy-paste-icon\"\n                 onclick=\"copyToClipboard('intercept_scaling',\n                          this.parentElement.nextElementSibling)\"\n            ></i></td>\n            <td class=\"param\">\n        <a class=\"param-doc-link\"\n            rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.8/modules/generated/sklearn.linear_model.LogisticRegression.html#:~:text=intercept_scaling,-float%2C%20default%3D1\">\n            intercept_scaling\n            <span class=\"param-doc-description\">intercept_scaling: float, default=1<br><br>Useful only when the solver `liblinear` is used<br>and `self.fit_intercept` is set to `True`. In this case, `x` becomes<br>`[x, self.intercept_scaling]`,<br>i.e. a \"synthetic\" feature with constant value equal to<br>`intercept_scaling` is appended to the instance vector.<br>The intercept becomes<br>``intercept_scaling * synthetic_feature_weight``.<br><br>.. note::<br>    The synthetic feature weight is subject to L1 or L2<br>    regularization as all other features.<br>    To lessen the effect of regularization on synthetic feature weight<br>    (and therefore on the intercept) `intercept_scaling` has to be increased.</span>\n        </a>\n    </td>\n            <td class=\"value\">1</td>\n        </tr>\n    \n\n        <tr class=\"user-set\">\n            <td><i class=\"copy-paste-icon\"\n                 onclick=\"copyToClipboard('class_weight',\n                          this.parentElement.nextElementSibling)\"\n            ></i></td>\n            <td class=\"param\">\n        <a class=\"param-doc-link\"\n            rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.8/modules/generated/sklearn.linear_model.LogisticRegression.html#:~:text=class_weight,-dict%20or%20%27balanced%27%2C%20default%3DNone\">\n            class_weight\n            <span class=\"param-doc-description\">class_weight: dict or 'balanced', default=None<br><br>Weights associated with classes in the form ``{class_label: weight}``.<br>If not given, all classes are supposed to have weight one.<br><br>The \"balanced\" mode uses the values of y to automatically adjust<br>weights inversely proportional to class frequencies in the input data<br>as ``n_samples / (n_classes * np.bincount(y))``.<br><br>Note that these weights will be multiplied with sample_weight (passed<br>through the fit method) if sample_weight is specified.<br><br>.. versionadded:: 0.17<br>   *class_weight='balanced'*</span>\n        </a>\n    </td>\n            <td class=\"value\">&#x27;balanced&#x27;</td>\n        </tr>\n    \n\n        <tr class=\"default\">\n            <td><i class=\"copy-paste-icon\"\n                 onclick=\"copyToClipboard('random_state',\n                          this.parentElement.nextElementSibling)\"\n            ></i></td>\n            <td class=\"param\">\n        <a class=\"param-doc-link\"\n            rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.8/modules/generated/sklearn.linear_model.LogisticRegression.html#:~:text=random_state,-int%2C%20RandomState%20instance%2C%20default%3DNone\">\n            random_state\n            <span class=\"param-doc-description\">random_state: int, RandomState instance, default=None<br><br>Used when ``solver`` == 'sag', 'saga' or 'liblinear' to shuffle the<br>data. See :term:`Glossary <random_state>` for details.</span>\n        </a>\n    </td>\n            <td class=\"value\">None</td>\n        </tr>\n    \n\n        <tr class=\"default\">\n            <td><i class=\"copy-paste-icon\"\n                 onclick=\"copyToClipboard('solver',\n                          this.parentElement.nextElementSibling)\"\n            ></i></td>\n            <td class=\"param\">\n        <a class=\"param-doc-link\"\n            rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.8/modules/generated/sklearn.linear_model.LogisticRegression.html#:~:text=solver,-%7B%27lbfgs%27%2C%20%27liblinear%27%2C%20%27newton-cg%27%2C%20%27newton-cholesky%27%2C%20%27sag%27%2C%20%27saga%27%7D%2C%20%20%20%20%20%20%20%20%20%20%20%20%20default%3D%27lbfgs%27\">\n            solver\n            <span class=\"param-doc-description\">solver: {'lbfgs', 'liblinear', 'newton-cg', 'newton-cholesky', 'sag', 'saga'},             default='lbfgs'<br><br>Algorithm to use in the optimization problem. Default is 'lbfgs'.<br>To choose a solver, you might want to consider the following aspects:<br><br>- 'lbfgs' is a good default solver because it works reasonably well for a wide<br>  class of problems.<br>- For :term:`multiclass` problems (`n_classes >= 3`), all solvers except<br>  'liblinear' minimize the full multinomial loss, 'liblinear' will raise an<br>  error.<br>- 'newton-cholesky' is a good choice for<br>  `n_samples` >> `n_features * n_classes`, especially with one-hot encoded<br>  categorical features with rare categories. Be aware that the memory usage<br>  of this solver has a quadratic dependency on `n_features * n_classes`<br>  because it explicitly computes the full Hessian matrix.<br>- For small datasets, 'liblinear' is a good choice, whereas 'sag'<br>  and 'saga' are faster for large ones;<br>- 'liblinear' can only handle binary classification by default. To apply a<br>  one-versus-rest scheme for the multiclass setting one can wrap it with the<br>  :class:`~sklearn.multiclass.OneVsRestClassifier`.<br><br>.. warning::<br>   The choice of the algorithm depends on the penalty chosen (`l1_ratio=0`<br>   for L2-penalty, `l1_ratio=1` for L1-penalty and `0 < l1_ratio < 1` for<br>   Elastic-Net) and on (multinomial) multiclass support:<br><br>   ================= ======================== ======================<br>   solver            l1_ratio                 multinomial multiclass<br>   ================= ======================== ======================<br>   'lbfgs'           l1_ratio=0               yes<br>   'liblinear'       l1_ratio=1 or l1_ratio=0 no<br>   'newton-cg'       l1_ratio=0               yes<br>   'newton-cholesky' l1_ratio=0               yes<br>   'sag'             l1_ratio=0               yes<br>   'saga'            0<=l1_ratio<=1           yes<br>   ================= ======================== ======================<br><br>.. note::<br>   'sag' and 'saga' fast convergence is only guaranteed on features<br>   with approximately the same scale. You can preprocess the data with<br>   a scaler from :mod:`sklearn.preprocessing`.<br><br>.. seealso::<br>   Refer to the :ref:`User Guide <Logistic_regression>` for more<br>   information regarding :class:`LogisticRegression` and more specifically the<br>   :ref:`Table <logistic_regression_solvers>`<br>   summarizing solver/penalty supports.<br><br>.. versionadded:: 0.17<br>   Stochastic Average Gradient (SAG) descent solver. Multinomial support in<br>   version 0.18.<br>.. versionadded:: 0.19<br>   SAGA solver.<br>.. versionchanged:: 0.22<br>   The default solver changed from 'liblinear' to 'lbfgs' in 0.22.<br>.. versionadded:: 1.2<br>   newton-cholesky solver. Multinomial support in version 1.6.</span>\n        </a>\n    </td>\n            <td class=\"value\">&#x27;lbfgs&#x27;</td>\n        </tr>\n    \n\n        <tr class=\"default\">\n            <td><i class=\"copy-paste-icon\"\n                 onclick=\"copyToClipboard('max_iter',\n                          this.parentElement.nextElementSibling)\"\n            ></i></td>\n            <td class=\"param\">\n        <a class=\"param-doc-link\"\n            rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.8/modules/generated/sklearn.linear_model.LogisticRegression.html#:~:text=max_iter,-int%2C%20default%3D100\">\n            max_iter\n            <span class=\"param-doc-description\">max_iter: int, default=100<br><br>Maximum number of iterations taken for the solvers to converge.</span>\n        </a>\n    </td>\n            <td class=\"value\">100</td>\n        </tr>\n    \n\n        <tr class=\"default\">\n            <td><i class=\"copy-paste-icon\"\n                 onclick=\"copyToClipboard('verbose',\n                          this.parentElement.nextElementSibling)\"\n            ></i></td>\n            <td class=\"param\">\n        <a class=\"param-doc-link\"\n            rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.8/modules/generated/sklearn.linear_model.LogisticRegression.html#:~:text=verbose,-int%2C%20default%3D0\">\n            verbose\n            <span class=\"param-doc-description\">verbose: int, default=0<br><br>For the liblinear and lbfgs solvers set verbose to any positive<br>number for verbosity.</span>\n        </a>\n    </td>\n            <td class=\"value\">0</td>\n        </tr>\n    \n\n        <tr class=\"default\">\n            <td><i class=\"copy-paste-icon\"\n                 onclick=\"copyToClipboard('warm_start',\n                          this.parentElement.nextElementSibling)\"\n            ></i></td>\n            <td class=\"param\">\n        <a class=\"param-doc-link\"\n            rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.8/modules/generated/sklearn.linear_model.LogisticRegression.html#:~:text=warm_start,-bool%2C%20default%3DFalse\">\n            warm_start\n            <span class=\"param-doc-description\">warm_start: bool, default=False<br><br>When set to True, reuse the solution of the previous call to fit as<br>initialization, otherwise, just erase the previous solution.<br>Useless for liblinear solver. See :term:`the Glossary <warm_start>`.<br><br>.. versionadded:: 0.17<br>   *warm_start* to support *lbfgs*, *newton-cg*, *sag*, *saga* solvers.</span>\n        </a>\n    </td>\n            <td class=\"value\">False</td>\n        </tr>\n    \n\n        <tr class=\"default\">\n            <td><i class=\"copy-paste-icon\"\n                 onclick=\"copyToClipboard('n_jobs',\n                          this.parentElement.nextElementSibling)\"\n            ></i></td>\n            <td class=\"param\">\n        <a class=\"param-doc-link\"\n            rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.8/modules/generated/sklearn.linear_model.LogisticRegression.html#:~:text=n_jobs,-int%2C%20default%3DNone\">\n            n_jobs\n            <span class=\"param-doc-description\">n_jobs: int, default=None<br><br>Does not have any effect.<br><br>.. deprecated:: 1.8<br>   `n_jobs` is deprecated in version 1.8 and will be removed in 1.10.</span>\n        </a>\n    </td>\n            <td class=\"value\">None</td>\n        </tr>\n    \n                  </tbody>\n                </table>\n            </details>\n        </div>\n    </div></div></div></div></div><script>function copyToClipboard(text, element) {\n    // Get the parameter prefix from the closest toggleable content\n    const toggleableContent = element.closest('.sk-toggleable__content');\n    const paramPrefix = toggleableContent ? toggleableContent.dataset.paramPrefix : '';\n    const fullParamName = paramPrefix ? `${paramPrefix}${text}` : text;\n\n    const originalStyle = element.style;\n    const computedStyle = window.getComputedStyle(element);\n    const originalWidth = computedStyle.width;\n    const originalHTML = element.innerHTML.replace('Copied!', '');\n\n    navigator.clipboard.writeText(fullParamName)\n        .then(() => {\n            element.style.width = originalWidth;\n            element.style.color = 'green';\n            element.innerHTML = \"Copied!\";\n\n            setTimeout(() => {\n                element.innerHTML = originalHTML;\n                element.style = originalStyle;\n            }, 2000);\n        })\n        .catch(err => {\n            console.error('Failed to copy:', err);\n            element.style.color = 'red';\n            element.innerHTML = \"Failed!\";\n            setTimeout(() => {\n                element.innerHTML = originalHTML;\n                element.style = originalStyle;\n            }, 2000);\n        });\n    return false;\n}\n\ndocument.querySelectorAll('.copy-paste-icon').forEach(function(element) {\n    const toggleableContent = element.closest('.sk-toggleable__content');\n    const paramPrefix = toggleableContent ? toggleableContent.dataset.paramPrefix : '';\n    const paramName = element.parentElement.nextElementSibling\n        .textContent.trim().split(' ')[0];\n    const fullParamName = paramPrefix ? `${paramPrefix}${paramName}` : paramName;\n\n    element.setAttribute('title', fullParamName);\n});\n\n\n/**\n * Adapted from Skrub\n * https://github.com/skrub-data/skrub/blob/403466d1d5d4dc76a7ef569b3f8228db59a31dc3/skrub/_reporting/_data/templates/report.js#L789\n * @returns \"light\" or \"dark\"\n */\nfunction detectTheme(element) {\n    const body = document.querySelector('body');\n\n    // Check VSCode theme\n    const themeKindAttr = body.getAttribute('data-vscode-theme-kind');\n    const themeNameAttr = body.getAttribute('data-vscode-theme-name');\n\n    if (themeKindAttr && themeNameAttr) {\n        const themeKind = themeKindAttr.toLowerCase();\n        const themeName = themeNameAttr.toLowerCase();\n\n        if (themeKind.includes(\"dark\") || themeName.includes(\"dark\")) {\n            return \"dark\";\n        }\n        if (themeKind.includes(\"light\") || themeName.includes(\"light\")) {\n            return \"light\";\n        }\n    }\n\n    // Check Jupyter theme\n    if (body.getAttribute('data-jp-theme-light') === 'false') {\n        return 'dark';\n    } else if (body.getAttribute('data-jp-theme-light') === 'true') {\n        return 'light';\n    }\n\n    // Guess based on a parent element's color\n    const color = window.getComputedStyle(element.parentNode, null).getPropertyValue('color');\n    const match = color.match(/^rgb\\s*\\(\\s*(\\d+)\\s*,\\s*(\\d+)\\s*,\\s*(\\d+)\\s*\\)\\s*$/i);\n    if (match) {\n        const [r, g, b] = [\n            parseFloat(match[1]),\n            parseFloat(match[2]),\n            parseFloat(match[3])\n        ];\n\n        // https://en.wikipedia.org/wiki/HSL_and_HSV#Lightness\n        const luma = 0.299 * r + 0.587 * g + 0.114 * b;\n\n        if (luma > 180) {\n            // If the text is very bright we have a dark theme\n            return 'dark';\n        }\n        if (luma < 75) {\n            // If the text is very dark we have a light theme\n            return 'light';\n        }\n        // Otherwise fall back to the next heuristic.\n    }\n\n    // Fallback to system preference\n    return window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'light';\n}\n\n\nfunction forceTheme(elementId) {\n    const estimatorElement = document.querySelector(`#${elementId}`);\n    if (estimatorElement === null) {\n        console.error(`Element with id ${elementId} not found.`);\n    } else {\n        const theme = detectTheme(estimatorElement);\n        estimatorElement.classList.add(theme);\n    }\n}\n\nforceTheme('sk-container-id-1');</script></body>\n``````\n:::\n:::\n\n\n::: {#fd02ece9 .cell execution_count=95}\n``` {.python .cell-code}\nfeatures_names = features_names.toPandas()['feature_name']\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n26/02/16 10:03:51 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n26/02/16 10:03:51 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n\r[Stage 118:==================================================>     (9 + 1) / 10]\r26/02/16 10:03:53 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n26/02/16 10:03:53 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n26/02/16 10:03:53 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n26/02/16 10:03:53 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n\r[Stage 123:>                                                        (0 + 1) / 1]\r\r                                                                                \r\n```\n:::\n:::\n\n\n::: {#80504834 .cell execution_count=96}\n``` {.python .cell-code}\nfeatures_names[range(6)]\n```\n\n::: {.cell-output .cell-output-display execution_count=89}\n```\n0    n_actions_per_category_id#C#1002.0\n1    n_actions_per_category_id#C#1204.0\n2    n_actions_per_category_id#O#1002.0\n3    n_actions_per_category_id#O#1204.0\n4            n_days_since_last_action#C\n5            n_days_since_last_action#O\nName: feature_name, dtype: str\n```\n:::\n:::\n\n\n::: {#8c826c4b .cell execution_count=97}\n``` {.python .cell-code}\nimport matplotlib.pyplot as plt\n%matplotlib inline\n```\n:::\n\n\n::: {#ed1beefb .cell execution_count=98}\n``` {.python .cell-code}\nplt.figure(figsize=(16, 5))\nplt.stem(clf.coef_[0]) # , use_line_collection=True)\nplt.title('Logistic regression coefficients', fontsize=18)\n```\n\n::: {.cell-output .cell-output-display execution_count=91}\n```\nText(0.5, 1.0, 'Logistic regression coefficients')\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](notebook08_webdata-III_files/figure-html/cell-99-output-2.png){width=1251 height=438}\n:::\n:::\n\n\n::: {#c3a2861a .cell execution_count=99}\n``` {.python .cell-code}\nclf.coef_[0].shape[0]\n```\n\n::: {.cell-output .cell-output-display execution_count=92}\n```\n90\n```\n:::\n:::\n\n\n::: {#cc8b9b6a .cell execution_count=100}\n``` {.python .cell-code}\nlen(features_names)\n```\n\n::: {.cell-output .cell-output-display execution_count=93}\n```\n92\n```\n:::\n:::\n\n\n::: {#52b944d2 .cell execution_count=101}\n``` {.python .cell-code}\n# We change the fontsize of minor ticks label\n_ = plt.xticks(np.arange(clf.coef_[0].shape[0]), features_names, \n           rotation='vertical', fontsize=8)\n```\n\n::: {.cell-output .cell-output-error}\n\n::: {.ansi-escaped-output}\n```{=html}\n<pre><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">ValueError</span>                                Traceback (most recent call last)\n<span class=\"ansi-cyan-fg\">Cell</span><span class=\"ansi-cyan-fg\"> </span><span class=\"ansi-green-fg\">In[94]</span><span class=\"ansi-green-fg\">, line 2</span>\n<span class=\"ansi-green-fg\">      1</span> <span style=\"font-style:italic;color:rgb(95,135,135)\"># We change the fontsize of minor ticks label</span>\n<span class=\"ansi-green-fg\">----&gt; </span><span class=\"ansi-green-fg\">2</span> _ = <span class=\"ansi-yellow-bg\">plt</span><span class=\"ansi-yellow-bg\">.</span><span class=\"ansi-yellow-bg\">xticks</span><span class=\"ansi-yellow-bg\">(</span><span class=\"ansi-yellow-bg\">np</span><span class=\"ansi-yellow-bg\">.</span><span class=\"ansi-yellow-bg\">arange</span><span class=\"ansi-yellow-bg\">(</span><span class=\"ansi-yellow-bg\">clf</span><span class=\"ansi-yellow-bg\">.</span><span class=\"ansi-yellow-bg\">coef_</span><span class=\"ansi-yellow-bg\">[</span><span class=\"ansi-green-fg ansi-yellow-bg\">0</span><span class=\"ansi-yellow-bg\">]</span><span class=\"ansi-yellow-bg\">.</span><span class=\"ansi-yellow-bg\">shape</span><span class=\"ansi-yellow-bg\">[</span><span class=\"ansi-green-fg ansi-yellow-bg\">0</span><span class=\"ansi-yellow-bg\">]</span><span class=\"ansi-yellow-bg\">)</span><span class=\"ansi-yellow-bg\">,</span><span class=\"ansi-yellow-bg\"> </span><span class=\"ansi-yellow-bg\">features_names</span><span class=\"ansi-yellow-bg\">,</span><span class=\"ansi-yellow-bg\"> </span>\n<span class=\"ansi-green-fg\">      3</span> <span class=\"ansi-yellow-bg\">           </span><span class=\"ansi-yellow-bg\">rotation</span><span class=\"ansi-yellow-bg\">=</span><span class=\"ansi-yellow-fg ansi-yellow-bg\">'</span><span class=\"ansi-yellow-fg ansi-yellow-bg\">vertical</span><span class=\"ansi-yellow-fg ansi-yellow-bg\">'</span><span class=\"ansi-yellow-bg\">,</span><span class=\"ansi-yellow-bg\"> </span><span class=\"ansi-yellow-bg\">fontsize</span><span class=\"ansi-yellow-bg\">=</span><span class=\"ansi-green-fg ansi-yellow-bg\">8</span><span class=\"ansi-yellow-bg\">)</span>\n\n<span class=\"ansi-cyan-fg\">File </span><span class=\"ansi-green-fg\">~/Documents/IFEBY310/.nlp-venv/lib/python3.12/site-packages/matplotlib/pyplot.py:2246</span>, in <span class=\"ansi-cyan-fg\">xticks</span><span class=\"ansi-blue-fg\">(ticks, labels, minor, **kwargs)</span>\n<span class=\"ansi-green-fg\">   2244</span>         l._internal_update(kwargs)\n<span class=\"ansi-green-fg\">   2245</span> <span style=\"font-weight:bold;color:rgb(0,135,0)\">else</span>:\n<span class=\"ansi-green-fg\">-&gt; </span><span class=\"ansi-green-fg\">2246</span>     labels_out = <span class=\"ansi-yellow-bg\">ax</span><span class=\"ansi-yellow-bg\">.</span><span class=\"ansi-yellow-bg\">set_xticklabels</span><span class=\"ansi-yellow-bg\">(</span><span class=\"ansi-yellow-bg\">labels</span><span class=\"ansi-yellow-bg\">,</span><span class=\"ansi-yellow-bg\"> </span><span class=\"ansi-yellow-bg\">minor</span><span class=\"ansi-yellow-bg\">=</span><span class=\"ansi-yellow-bg\">minor</span><span class=\"ansi-yellow-bg\">,</span><span class=\"ansi-yellow-bg\"> </span><span class=\"ansi-yellow-bg\">*</span><span class=\"ansi-yellow-bg\">*</span><span class=\"ansi-yellow-bg\">kwargs</span><span class=\"ansi-yellow-bg\">)</span>\n<span class=\"ansi-green-fg\">   2248</span> <span style=\"font-weight:bold;color:rgb(0,135,0)\">return</span> locs, labels_out\n\n<span class=\"ansi-cyan-fg\">File </span><span class=\"ansi-green-fg\">~/Documents/IFEBY310/.nlp-venv/lib/python3.12/site-packages/matplotlib/axes/_base.py:74</span>, in <span class=\"ansi-cyan-fg\">_axis_method_wrapper.__set_name__.&lt;locals&gt;.wrapper</span><span class=\"ansi-blue-fg\">(self, *args, **kwargs)</span>\n<span class=\"ansi-green-fg\">     73</span> <span style=\"font-weight:bold;color:rgb(0,135,0)\">def</span><span style=\"color:rgb(188,188,188)\"> </span><span class=\"ansi-blue-fg\">wrapper</span>(<span style=\"color:rgb(0,135,0)\">self</span>, *args, **kwargs):\n<span class=\"ansi-green-fg\">---&gt; </span><span class=\"ansi-green-fg\">74</span>     <span style=\"font-weight:bold;color:rgb(0,135,0)\">return</span> <span class=\"ansi-yellow-bg\">get_method</span><span class=\"ansi-yellow-bg\">(</span><span style=\"color:rgb(0,135,0)\" class=\"ansi-yellow-bg\">self</span><span class=\"ansi-yellow-bg\">)</span><span class=\"ansi-yellow-bg\">(</span><span class=\"ansi-yellow-bg\">*</span><span class=\"ansi-yellow-bg\">args</span><span class=\"ansi-yellow-bg\">,</span><span class=\"ansi-yellow-bg\"> </span><span class=\"ansi-yellow-bg\">*</span><span class=\"ansi-yellow-bg\">*</span><span class=\"ansi-yellow-bg\">kwargs</span><span class=\"ansi-yellow-bg\">)</span>\n\n<span class=\"ansi-cyan-fg\">File </span><span class=\"ansi-green-fg\">~/Documents/IFEBY310/.nlp-venv/lib/python3.12/site-packages/matplotlib/axis.py:2106</span>, in <span class=\"ansi-cyan-fg\">Axis.set_ticklabels</span><span class=\"ansi-blue-fg\">(self, labels, minor, fontdict, **kwargs)</span>\n<span class=\"ansi-green-fg\">   2102</span> <span style=\"font-weight:bold;color:rgb(0,135,0)\">elif</span> <span style=\"color:rgb(0,135,0)\">isinstance</span>(locator, mticker.FixedLocator):\n<span class=\"ansi-green-fg\">   2103</span>     <span style=\"font-style:italic;color:rgb(95,135,135)\"># Passing [] as a list of labels is often used as a way to</span>\n<span class=\"ansi-green-fg\">   2104</span>     <span style=\"font-style:italic;color:rgb(95,135,135)\"># remove all tick labels, so only error for &gt; 0 labels</span>\n<span class=\"ansi-green-fg\">   2105</span>     <span style=\"font-weight:bold;color:rgb(0,135,0)\">if</span> <span style=\"color:rgb(0,135,0)\">len</span>(locator.locs) != <span style=\"color:rgb(0,135,0)\">len</span>(labels) <span style=\"font-weight:bold;color:rgb(175,0,255)\">and</span> <span style=\"color:rgb(0,135,0)\">len</span>(labels) != <span class=\"ansi-green-fg\">0</span>:\n<span class=\"ansi-green-fg\">-&gt; </span><span class=\"ansi-green-fg\">2106</span>         <span style=\"font-weight:bold;color:rgb(0,135,0)\">raise</span> <span style=\"font-weight:bold;color:rgb(215,95,95)\">ValueError</span>(\n<span class=\"ansi-green-fg\">   2107</span>             <span class=\"ansi-yellow-fg\">\"</span><span class=\"ansi-yellow-fg\">The number of FixedLocator locations</span><span class=\"ansi-yellow-fg\">\"</span>\n<span class=\"ansi-green-fg\">   2108</span>             <span class=\"ansi-yellow-fg\">f</span><span class=\"ansi-yellow-fg\">\"</span><span class=\"ansi-yellow-fg\"> (</span><span style=\"font-weight:bold;color:rgb(175,95,135)\">{</span><span style=\"color:rgb(0,135,0)\">len</span>(locator.locs)<span style=\"font-weight:bold;color:rgb(175,95,135)\">}</span><span class=\"ansi-yellow-fg\">), usually from a call to</span><span class=\"ansi-yellow-fg\">\"</span>\n<span class=\"ansi-green-fg\">   2109</span>             <span class=\"ansi-yellow-fg\">\"</span><span class=\"ansi-yellow-fg\"> set_ticks, does not match</span><span class=\"ansi-yellow-fg\">\"</span>\n<span class=\"ansi-green-fg\">   2110</span>             <span class=\"ansi-yellow-fg\">f</span><span class=\"ansi-yellow-fg\">\"</span><span class=\"ansi-yellow-fg\"> the number of labels (</span><span style=\"font-weight:bold;color:rgb(175,95,135)\">{</span><span style=\"color:rgb(0,135,0)\">len</span>(labels)<span style=\"font-weight:bold;color:rgb(175,95,135)\">}</span><span class=\"ansi-yellow-fg\">).</span><span class=\"ansi-yellow-fg\">\"</span>)\n<span class=\"ansi-green-fg\">   2111</span>     tickd = {loc: lab <span style=\"font-weight:bold;color:rgb(0,135,0)\">for</span> loc, lab <span style=\"font-weight:bold;color:rgb(175,0,255)\">in</span> <span style=\"color:rgb(0,135,0)\">zip</span>(locator.locs, labels)}\n<span class=\"ansi-green-fg\">   2112</span>     func = functools.partial(<span style=\"color:rgb(0,135,0)\">self</span>._format_with_dict, tickd)\n\n<span class=\"ansi-red-fg\">ValueError</span>: The number of FixedLocator locations (90), usually from a call to set_ticks, does not match the number of labels (92).</pre>\n```\n:::\n\n:::\n\n::: {.cell-output .cell-output-display}\n![](notebook08_webdata-III_files/figure-html/cell-102-output-2.png){width=579 height=416}\n:::\n:::\n\n\n::: {#06562263 .cell execution_count=102}\n``` {.python .cell-code}\n_ = plt.yticks(fontsize=14)\n```\n\n::: {.cell-output .cell-output-display}\n![](notebook08_webdata-III_files/figure-html/cell-103-output-1.png){width=590 height=418}\n:::\n:::\n\n\n::: {#a5e10fb0 .cell execution_count=103}\n``` {.python .cell-code}\nfrom sklearn.metrics import precision_recall_curve, f1_score\n\nprecision, recall, _ = precision_recall_curve(y_test, clf.predict_proba(X_test)[:, 1])\n    \nplt.figure(figsize=(8, 6))\nplt.plot(recall, precision, label='LR (F1=%.2f)' % f1_score(y_test, clf.predict(X_test)), lw=2)\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('Recall', fontsize=16)\nplt.ylabel('Precision', fontsize=16)\nplt.title('Precision/recall curve', fontsize=18)\nplt.legend(loc=\"upper right\", fontsize=14)\n```\n\n::: {.cell-output .cell-output-display}\n![](notebook08_webdata-III_files/figure-html/cell-104-output-1.png){width=682 height=538}\n:::\n:::\n\n\n# Analyse the tables \n\n::: {#ef45ff2f .cell execution_count=104}\n``` {.python .cell-code}\nquery = \"\"\"\n    ANALYZE TABLE db_table \n    COMPUTE STATISTICS\n    FOR COLUMNS xid\n\"\"\"\n```\n:::\n\n\n::: {#4b2550e7 .cell execution_count=105}\n``` {.python .cell-code}\ndf.createOrReplaceTempView(\"db_table\")\n```\n:::\n\n\n::: {#108fd467 .cell execution_count=106}\n``` {.python .cell-code}\ndf.columns\n```\n\n::: {.cell-output .cell-output-display execution_count=99}\n```\n['feature_name',\n 'xid',\n 'value',\n 'col',\n 'row',\n 'keep',\n 'sum_keep',\n 'row_new',\n 'col_new']\n```\n:::\n:::\n\n\n::: {#c525d761 .cell execution_count=107}\n``` {.python .cell-code}\nspark.sql(\"cache table db_table\").show()\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n26/02/16 10:03:55 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n26/02/16 10:03:55 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n26/02/16 10:03:55 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n26/02/16 10:03:55 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n26/02/16 10:03:55 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n\r[Stage 124:==================================================>     (9 + 1) / 10]\r26/02/16 10:03:57 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n26/02/16 10:03:57 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n26/02/16 10:03:57 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n26/02/16 10:03:57 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n26/02/16 10:03:58 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n26/02/16 10:03:58 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n26/02/16 10:03:58 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n26/02/16 10:03:58 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n\r[Stage 129:>                                                        (0 + 1) / 1]\r\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n++\n||\n++\n++\n\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n\r                                                                                \r\n```\n:::\n:::\n\n\n::: {#ce3ddabd .cell execution_count=108}\n``` {.python .cell-code}\nspark.sql(query).show()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n++\n||\n++\n++\n\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n\r[Stage 135:>                                                        (0 + 1) / 1]\r\r                                                                                \r\n```\n:::\n:::\n\n\n::: {#db0a8924 .cell execution_count=109}\n``` {.python .cell-code}\nspark.sql(\"show tables\").show()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n+---------+---------+-----------+\n|namespace|tableName|isTemporary|\n+---------+---------+-----------+\n|         | db_table|       true|\n|         |  webdata|       true|\n+---------+---------+-----------+\n\n```\n:::\n:::\n\n\n::: {#b9750788 .cell execution_count=110}\n``` {.python .cell-code}\nspark.stop()\n```\n:::\n\n\n",
    "supporting": [
      "notebook08_webdata-III_files"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdn.jsdelivr.net/npm/requirejs@2.3.6/require.min.js\" integrity=\"sha384-c9c+LnTbwQ3aujuU7ULEPVvgLs+Fn6fJUvIGTsuu1ZcCf11fiEubah0ttpca4ntM sha384-6V1/AdqZRWk1KAlWbKBlGhN7VG4iE/yAZcO6NZPMF8od0vukrvr0tg4qY6NSrItx\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdn.jsdelivr.net/npm/jquery@3.5.1/dist/jquery.min.js\" integrity=\"sha384-ZvpUoO/+PpLXR1lu4jmpXWu80pZlYUAfxl5NsBMWOEPSjUn/6Z/hRTt8+pR6L4N2\" crossorigin=\"anonymous\" data-relocate-top=\"true\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}