{
  "hash": "2ab0930ea05e3ab935bd654847953767",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: '`DataFrame`'\njupyter: python3\n---\n\n::: {#1c419e08 .cell execution_count=1}\n``` {.python .cell-code}\nimport os\nimport sys\n\nos.environ['PYSPARK_PYTHON'] = sys.executable\nos.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable\n```\n:::\n\n\n::: {#context_session .cell execution_count=2}\n``` {.python .cell-code}\nfrom pyspark import SparkConf, SparkContext\nfrom pyspark.sql import SparkSession\n\nconf = (\n    SparkConf()\n        .setAppName(\"Spark SQL Course\")\n)\n\nsc = SparkContext(conf=conf)  # no need for Spark 3...\n\nspark = (\n    SparkSession\n        .builder\n        .appName(\"Spark SQL Course\")\n        .getOrCreate()\n)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWARNING: Using incubator modules: jdk.incubator.vector\nUsing Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n26/02/02 10:04:31 WARN Utils: Your hostname, boucheron-Precision-5480, resolves to a loopback address: 127.0.1.1; using 172.23.32.10 instead (on interface eth0)\n26/02/02 10:04:31 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\nUsing Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\nSetting default log level to \"WARN\".\nTo adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n26/02/02 10:04:31 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n26/02/02 10:04:32 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n```\n:::\n:::\n\n\n::: {#78bae724 .cell execution_count=3}\n``` {.python .cell-code}\nfrom pyspark.sql import Row\n\nrow1 = Row(name=\"John\", age=21)\nrow2 = Row(name=\"James\", age=32)\nrow3 = Row(name=\"Jane\", age=18)\nrow1['name']\n```\n\n::: {.cell-output .cell-output-display execution_count=3}\n```\n'John'\n```\n:::\n:::\n\n\n::: {#e44e4bd3 .cell quarto-private-1='{\"key\":\"ExecuteTime\",\"value\":{\"end_time\":\"2022-01-26T10:58:57.185741Z\",\"start_time\":\"2022-01-26T10:58:57.155181Z\"}}' execution_count=4}\n``` {.python .cell-code}\ndf = spark.createDataFrame([row1, row2, row3])\n```\n:::\n\n\n::: {#d6535775 .cell quarto-private-1='{\"key\":\"ExecuteTime\",\"value\":{\"end_time\":\"2022-01-26T10:59:13.491438Z\",\"start_time\":\"2022-01-26T10:59:13.486119Z\"}}' execution_count=5}\n``` {.python .cell-code}\ndf.printSchema()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nroot\n |-- name: string (nullable = true)\n |-- age: long (nullable = true)\n\n```\n:::\n:::\n\n\nHow does `printSchema` compare with Pandas `info()`?\n\n::: {#714443c9 .cell quarto-private-1='{\"key\":\"ExecuteTime\",\"value\":{\"end_time\":\"2022-01-26T10:59:17.760344Z\",\"start_time\":\"2022-01-26T10:59:17.597166Z\"}}' execution_count=6}\n``` {.python .cell-code}\ndf.show()\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n\r[Stage 0:>                                                          (0 + 1) / 1]\r\r                                                                                \r\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n+-----+---+\n| name|age|\n+-----+---+\n| John| 21|\n|James| 32|\n| Jane| 18|\n+-----+---+\n\n```\n:::\n:::\n\n\nHow does `show` compare with Pandas `head()`\n\n::: {#207dd5ed .cell quarto-private-1='{\"key\":\"ExecuteTime\",\"value\":{\"end_time\":\"2022-01-26T10:59:25.889372Z\",\"start_time\":\"2022-01-26T10:59:25.866666Z\"}}' execution_count=7}\n``` {.python .cell-code}\nprint(df.rdd.toDebugString().decode(\"utf-8\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n(20) MapPartitionsRDD[10] at javaToPython at NativeMethodAccessorImpl.java:0 []\n |   MapPartitionsRDD[9] at javaToPython at NativeMethodAccessorImpl.java:0 []\n |   SQLExecutionRDD[8] at javaToPython at NativeMethodAccessorImpl.java:0 []\n |   MapPartitionsRDD[7] at javaToPython at NativeMethodAccessorImpl.java:0 []\n |   MapPartitionsRDD[4] at applySchemaToPythonRDD at NativeMethodAccessorImpl.java:0 []\n |   MapPartitionsRDD[3] at map at SerDeUtil.scala:71 []\n |   MapPartitionsRDD[2] at mapPartitions at SerDeUtil.scala:119 []\n |   PythonRDD[1] at RDD at PythonRDD.scala:58 []\n |   ParallelCollectionRDD[0] at readRDDFromFile at PythonRDD.scala:299 []\n```\n:::\n:::\n\n\n::: {#dd9a55a0 .cell quarto-private-1='{\"key\":\"ExecuteTime\",\"value\":{\"end_time\":\"2022-01-26T10:59:45.432264Z\",\"start_time\":\"2022-01-26T10:59:45.426727Z\"}}' execution_count=8}\n``` {.python .cell-code}\ndf.rdd.getNumPartitions()\n```\n\n::: {.cell-output .cell-output-display execution_count=8}\n```\n20\n```\n:::\n:::\n\n\n## Creating dataframes\n\n::: {#c8f9ecb5 .cell quarto-private-1='{\"key\":\"ExecuteTime\",\"value\":{\"end_time\":\"2022-01-26T10:58:18.707591Z\",\"start_time\":\"2022-01-26T10:58:18.220608Z\"}}' execution_count=9}\n``` {.python .cell-code}\nrows = [\n    Row(name=\"John\", age=21, gender=\"male\"),\n    Row(name=\"James\", age=25, gender=\"female\"),\n    Row(name=\"Albert\", age=46, gender=\"male\")\n]\n\ndf = spark.createDataFrame(rows)\n```\n:::\n\n\n::: {#c3734e63 .cell quarto-private-1='{\"key\":\"ExecuteTime\",\"value\":{\"end_time\":\"2022-01-26T10:58:18.707591Z\",\"start_time\":\"2022-01-26T10:58:18.220608Z\"}}' execution_count=10}\n``` {.python .cell-code}\ndf.show()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n+------+---+------+\n|  name|age|gender|\n+------+---+------+\n|  John| 21|  male|\n| James| 25|female|\n|Albert| 46|  male|\n+------+---+------+\n\n```\n:::\n:::\n\n\n::: {#01b7ebb1 .cell execution_count=11}\n``` {.python .cell-code}\nhelp(Row)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nHelp on class Row in module pyspark.sql.types:\n\nclass Row(builtins.tuple)\n |  Row(*args: Optional[str], **kwargs: Optional[Any]) -> 'Row'\n |\n |  A row in :class:`DataFrame`.\n |  The fields in it can be accessed:\n |\n |  * like attributes (``row.key``)\n |  * like dictionary values (``row[key]``)\n |\n |  ``key in row`` will search through row keys.\n |\n |  Row can be used to create a row object by using named arguments.\n |  It is not allowed to omit a named argument to represent that the value is\n |  None or missing. This should be explicitly set to None in this case.\n |\n |  .. versionchanged:: 3.0.0\n |      Rows created from named arguments no longer have\n |      field names sorted alphabetically and will be ordered in the position as\n |      entered.\n |\n |  Examples\n |  --------\n |  >>> from pyspark.sql import Row\n |  >>> row = Row(name=\"Alice\", age=11)\n |  >>> row\n |  Row(name='Alice', age=11)\n |  >>> row['name'], row['age']\n |  ('Alice', 11)\n |  >>> row.name, row.age\n |  ('Alice', 11)\n |  >>> 'name' in row\n |  True\n |  >>> 'wrong_key' in row\n |  False\n |\n |  Row also can be used to create another Row like class, then it\n |  could be used to create Row objects, such as\n |\n |  >>> Person = Row(\"name\", \"age\")\n |  >>> Person\n |  <Row('name', 'age')>\n |  >>> 'name' in Person\n |  True\n |  >>> 'wrong_key' in Person\n |  False\n |  >>> Person(\"Alice\", 11)\n |  Row(name='Alice', age=11)\n |\n |  This form can also be used to create rows as tuple values, i.e. with unnamed\n |  fields.\n |\n |  >>> row1 = Row(\"Alice\", 11)\n |  >>> row2 = Row(name=\"Alice\", age=11)\n |  >>> row1 == row2\n |  True\n |\n |  Method resolution order:\n |      Row\n |      builtins.tuple\n |      builtins.object\n |\n |  Methods defined here:\n |\n |  __call__(self, *args: Any) -> 'Row'\n |      create new Row object\n |\n |  __contains__(self, item: Any) -> bool\n |      Return bool(key in self).\n |\n |  __getattr__(self, item: str) -> Any\n |\n |  __getitem__(self, item: Any) -> Any\n |      Return self[key].\n |\n |  __reduce__(self) -> Union[str, Tuple[Any, ...]]\n |      Returns a tuple so Python knows how to pickle Row.\n |\n |  __repr__(self) -> str\n |      Printable representation of Row used in Python REPL.\n |\n |  __setattr__(self, key: Any, value: Any) -> None\n |      Implement setattr(self, name, value).\n |\n |  asDict(self, recursive: bool = False) -> Dict[str, Any]\n |      Return as a dict\n |\n |      Parameters\n |      ----------\n |      recursive : bool, optional\n |          turns the nested Rows to dict (default: False).\n |\n |      Notes\n |      -----\n |      If a row contains duplicate field names, e.g., the rows of a join\n |      between two :class:`DataFrame` that both have the fields of same names,\n |      one of the duplicate fields will be selected by ``asDict``. ``__getitem__``\n |      will also return one of the duplicate fields, however returned value might\n |      be different to ``asDict``.\n |\n |      Examples\n |      --------\n |      >>> from pyspark.sql import Row\n |      >>> Row(name=\"Alice\", age=11).asDict() == {'name': 'Alice', 'age': 11}\n |      True\n |      >>> row = Row(key=1, value=Row(name='a', age=2))\n |      >>> row.asDict() == {'key': 1, 'value': Row(name='a', age=2)}\n |      True\n |      >>> row.asDict(True) == {'key': 1, 'value': {'name': 'a', 'age': 2}}\n |      True\n |\n |  ----------------------------------------------------------------------\n |  Static methods defined here:\n |\n |  __new__(cls, *args: Optional[str], **kwargs: Optional[Any]) -> 'Row'\n |      Create and return a new object.  See help(type) for accurate signature.\n |\n |  ----------------------------------------------------------------------\n |  Data descriptors defined here:\n |\n |  __dict__\n |      dictionary for instance variables\n |\n |  ----------------------------------------------------------------------\n |  Methods inherited from builtins.tuple:\n |\n |  __add__(self, value, /)\n |      Return self+value.\n |\n |  __eq__(self, value, /)\n |      Return self==value.\n |\n |  __ge__(self, value, /)\n |      Return self>=value.\n |\n |  __getattribute__(self, name, /)\n |      Return getattr(self, name).\n |\n |  __getnewargs__(self, /)\n |\n |  __gt__(self, value, /)\n |      Return self>value.\n |\n |  __hash__(self, /)\n |      Return hash(self).\n |\n |  __iter__(self, /)\n |      Implement iter(self).\n |\n |  __le__(self, value, /)\n |      Return self<=value.\n |\n |  __len__(self, /)\n |      Return len(self).\n |\n |  __lt__(self, value, /)\n |      Return self<value.\n |\n |  __mul__(self, value, /)\n |      Return self*value.\n |\n |  __ne__(self, value, /)\n |      Return self!=value.\n |\n |  __rmul__(self, value, /)\n |      Return value*self.\n |\n |  count(self, value, /)\n |      Return number of occurrences of value.\n |\n |  index(self, value, start=0, stop=9223372036854775807, /)\n |      Return first index of value.\n |\n |      Raises ValueError if the value is not present.\n |\n |  ----------------------------------------------------------------------\n |  Class methods inherited from builtins.tuple:\n |\n |  __class_getitem__(...)\n |      See PEP 585\n\n```\n:::\n:::\n\n\n::: {#e5f77170 .cell quarto-private-1='{\"key\":\"ExecuteTime\",\"value\":{\"end_time\":\"2022-01-26T10:58:19.065539Z\",\"start_time\":\"2022-01-26T10:58:18.710711Z\"}}' execution_count=12}\n``` {.python .cell-code}\ncolumn_names = [\"name\", \"age\", \"gender\"]\nrows = [\n    [\"John\", 21, \"male\"],\n    [\"James\", 25, \"female\"],\n    [\"Albert\", 46, \"male\"]\n]\n\ndf = spark.createDataFrame(\n    rows, \n    column_names\n)\n\ndf.show()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n+------+---+------+\n|  name|age|gender|\n+------+---+------+\n|  John| 21|  male|\n| James| 25|female|\n|Albert| 46|  male|\n+------+---+------+\n\n```\n:::\n:::\n\n\n::: {#7d8d9376 .cell quarto-private-1='{\"key\":\"ExecuteTime\",\"value\":{\"end_time\":\"2022-01-26T10:58:19.074335Z\",\"start_time\":\"2022-01-26T10:58:19.068088Z\"}}' execution_count=13}\n``` {.python .cell-code}\ndf.printSchema()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nroot\n |-- name: string (nullable = true)\n |-- age: long (nullable = true)\n |-- gender: string (nullable = true)\n\n```\n:::\n:::\n\n\n::: {#ac88bb9b .cell quarto-private-1='{\"key\":\"ExecuteTime\",\"value\":{\"end_time\":\"2022-01-26T10:58:19.840178Z\",\"start_time\":\"2022-01-26T10:58:19.077057Z\"}}' execution_count=14}\n``` {.python .cell-code}\n# sc = SparkContext(conf=conf)  # no need for Spark 3...\n\ncolumn_names = [\"name\", \"age\", \"gender\"]\nrdd = sc.parallelize([\n    (\"John\", 21, \"male\"),\n    (\"James\", 25, \"female\"),\n    (\"Albert\", 46, \"male\")\n])\ndf = spark.createDataFrame(rdd, column_names)\ndf.show()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n+------+---+------+\n|  name|age|gender|\n+------+---+------+\n|  John| 21|  male|\n| James| 25|female|\n|Albert| 46|  male|\n+------+---+------+\n\n```\n:::\n:::\n\n\n## Schema\n\nThere is special type schemata. A object of class `StructType` is made of a list of objects of type `StructField`. \n\n::: {#5a0f14c9 .cell quarto-private-1='{\"key\":\"ExecuteTime\",\"value\":{\"end_time\":\"2022-01-26T10:58:19.850578Z\",\"start_time\":\"2022-01-26T10:58:19.843835Z\"}}' execution_count=15}\n``` {.python .cell-code}\ndf.schema\n```\n\n::: {.cell-output .cell-output-display execution_count=15}\n```\nStructType([StructField('name', StringType(), True), StructField('age', LongType(), True), StructField('gender', StringType(), True)])\n```\n:::\n:::\n\n\nHow does `df.schema` relate to `df.printSchema()`? Where would you use the outputs of `df.schema` and `df.printSchema()`.\n\n::: {#290ca42f .cell quarto-private-1='{\"key\":\"ExecuteTime\",\"value\":{\"end_time\":\"2022-01-26T10:58:19.860631Z\",\"start_time\":\"2022-01-26T10:58:19.854012Z\"}}' execution_count=16}\n``` {.python .cell-code}\ntype(df.schema)\n```\n\n::: {.cell-output .cell-output-display execution_count=16}\n```\npyspark.sql.types.StructType\n```\n:::\n:::\n\n\nA object of type `StructField` has a *name* like `gender`, a PySpark *type* like `StringType()`, an d a boolean parameter.\n\nWhat does the boolean parameter stand for?\n\n::: {#52919fd5 .cell quarto-private-1='{\"key\":\"ExecuteTime\",\"value\":{\"end_time\":\"2022-01-26T10:58:20.199419Z\",\"start_time\":\"2022-01-26T10:58:19.863528Z\"}}' execution_count=17}\n``` {.python .cell-code}\nfrom pyspark.sql.types import *\n\nschema = StructType(\n    [\n        StructField(\"name\", StringType(), False),\n        StructField(\"age\", IntegerType(), True),\n        StructField(\"gender\", StringType(), True)\n    ]\n)\n\nrows = [(\"Jane\", 21, \"female\")]\ndf = spark.createDataFrame(rows, schema)\ndf.printSchema()\ndf.show()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nroot\n |-- name: string (nullable = false)\n |-- age: integer (nullable = true)\n |-- gender: string (nullable = true)\n\n+----+---+------+\n|name|age|gender|\n+----+---+------+\n|Jane| 21|female|\n+----+---+------+\n\n```\n:::\n:::\n\n\n# Queries  (single table $σ$, $π$)\n\nPySpark offers two ways to query a datafrane:\n\n- An ad hoc API with methods for the DataFrame class.\n- The possibility to post SQL queries (provided a temporary view has been created).\n\n::: {#play-it-like-sql .cell quarto-private-1='{\"key\":\"ExecuteTime\",\"value\":{\"end_time\":\"2022-01-26T10:58:20.882311Z\",\"start_time\":\"2022-01-26T10:58:20.201993Z\"}}' execution_count=18}\n``` {.python .cell-code}\ncolumn_names = [\"name\", \"age\", \"gender\"]\nrows = [\n    [\"John\", 21, \"male\"],\n    [\"Jane\", 25, \"female\"]\n]\n# \ndf = spark.createDataFrame(rows, column_names)\n\n# Create a temporary view from the DataFrame\ndf.createOrReplaceTempView(\"new_view\")\n\n# Apply the query\nquery = \"\"\"\n    SELECT \n        name, age \n    FROM \n        new_view \n    WHERE \n        gender='male'\n\"\"\"\n\nmen_df = spark.sql(query)\nmen_df.show()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n+----+---+\n|name|age|\n+----+---+\n|John| 21|\n+----+---+\n\n```\n:::\n:::\n\n\n::: {.callout-important title=\"New! (with Spark 4)\"}\n\nWe can now write SQL queries the way we have been writing in the `tidyverse` (`R`) using the SQL pipe `|>`.\n\n:::\n\n::: {#play-it-like-dplyr .cell execution_count=19}\n``` {.python .cell-code}\nnew_age_query = \"\"\"\n    FROM new_view\n    |> WHERE gender = 'male'\n    |> SELECT name, age\n\"\"\"\n```\n:::\n\n\nto be compared with \n\n```{.r}\nnew_df |> \n    dplyr::filter(gender=='male') |>\n    dplyr::select(name, age)\n```\n\n::: {#play-it-like-dplyr-2 .cell execution_count=20}\n``` {.python .cell-code}\nmen_df = (\n    spark\n        .sql(new_age_query)\n        .show()\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n+----+---+\n|name|age|\n+----+---+\n|John| 21|\n+----+---+\n\n```\n:::\n:::\n\n\n::: {.callout-tip  title=\"Have a look at \"}\n\n[https://www.databricks.com/blog/sql-gets-easier-announcing-new-pipe-syntax](https://www.databricks.com/blog/sql-gets-easier-announcing-new-pipe-syntax)\n\n:::\n\n\n## `SELECT`  (projection $π$)\n\n::: {#cd173ece .cell quarto-private-1='{\"key\":\"ExecuteTime\",\"value\":{\"end_time\":\"2022-01-26T10:58:21.162623Z\",\"start_time\":\"2022-01-26T10:58:20.884802Z\"}}' execution_count=21}\n``` {.python .cell-code}\ndf.createOrReplaceTempView(\"table\")    \n\nquery = \"\"\"\n    SELECT \n        name, age \n    FROM \n        table\n\"\"\"\n\nspark.sql(query).show()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n+----+---+\n|name|age|\n+----+---+\n|John| 21|\n|Jane| 25|\n+----+---+\n\n```\n:::\n:::\n\n\nUsing the API:\n\n::: {#03d604b9 .cell quarto-private-1='{\"key\":\"ExecuteTime\",\"value\":{\"end_time\":\"2022-01-26T10:58:21.388097Z\",\"start_time\":\"2022-01-26T10:58:21.164840Z\"}}' execution_count=22}\n``` {.python .cell-code}\n(\n    df\n        .select(\"name\", \"age\")\n        .show()\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n+----+---+\n|name|age|\n+----+---+\n|John| 21|\n|Jane| 25|\n+----+---+\n\n```\n:::\n:::\n\n\n`π(df, \"name\", \"age\")`\n\n## `WHERE`  (filter, selection, $σ$)\n\n::: {#59b1c2b9 .cell quarto-private-1='{\"key\":\"ExecuteTime\",\"value\":{\"end_time\":\"2022-01-26T10:58:21.704402Z\",\"start_time\":\"2022-01-26T10:58:21.402155Z\"}}' execution_count=23}\n``` {.python .cell-code}\ndf.createOrReplaceTempView(\"table\")\n\nquery = \"\"\"\n    SELECT \n        * \n    FROM \n        table\n    WHERE \n        age > 21\n\"\"\"\n\nquery = \"\"\"\n    FROM table \n    |> WHERE age > 21 \n    |> SELECT *   \n\"\"\"\n\nspark.sql(query).show()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n+----+---+------+\n|name|age|gender|\n+----+---+------+\n|Jane| 25|female|\n+----+---+------+\n\n```\n:::\n:::\n\n\nNote that you can get rid of `|> SELECT *` \n\nUsing the API \n\n::: {#02fe0eec .cell quarto-private-1='{\"key\":\"ExecuteTime\",\"value\":{\"end_time\":\"2022-01-26T10:58:21.924501Z\",\"start_time\":\"2022-01-26T10:58:21.706741Z\"}}' execution_count=24}\n``` {.python .cell-code}\n( \n    df\n        .where(\"age > 21\")\n        .show()\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n+----+---+------+\n|name|age|gender|\n+----+---+------+\n|Jane| 25|female|\n+----+---+------+\n\n```\n:::\n:::\n\n\nThis implements `σ(df, \"age > 21\")`\n\nThe `where()` method takes different types of inputs as argument: strings that can be interpreted as SQL conditions, but also boolean masks.\n\n::: {#a0f7958f .cell quarto-private-1='{\"key\":\"ExecuteTime\",\"value\":{\"end_time\":\"2022-01-26T10:58:22.377417Z\",\"start_time\":\"2022-01-26T10:58:21.926708Z\"}}' execution_count=25}\n``` {.python .cell-code}\n# Alternatively:\n( \n    df\n      .where(df['age'] > 21)\n      .show()\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n+----+---+------+\n|name|age|gender|\n+----+---+------+\n|Jane| 25|female|\n+----+---+------+\n\n```\n:::\n:::\n\n\nor \n\n::: {#428f5b98 .cell quarto-private-1='{\"key\":\"ExecuteTime\",\"value\":{\"end_time\":\"2022-01-26T10:58:22.566385Z\",\"start_time\":\"2022-01-26T10:58:22.380036Z\"}}' execution_count=26}\n``` {.python .cell-code}\n( \n    df\n      .where(df.age > 21)\n      .show()\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n+----+---+------+\n|name|age|gender|\n+----+---+------+\n|Jane| 25|female|\n+----+---+------+\n\n```\n:::\n:::\n\n\nWhere (and how) is the boolean mask built?\n\n\nMethod chaining allows to construct complex queries \n\n::: {#a8734789 .cell quarto-private-1='{\"key\":\"ExecuteTime\",\"value\":{\"end_time\":\"2022-01-26T10:58:22.837136Z\",\"start_time\":\"2022-01-26T10:58:22.569324Z\"}}' execution_count=27}\n``` {.python .cell-code}\n( \n    df\n      .where(\"age > 21\")\n      .select([\"name\", \"age\"])\n      .show()\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n+----+---+\n|name|age|\n+----+---+\n|Jane| 25|\n+----+---+\n\n```\n:::\n:::\n\n\nThis implements \n\n```\n    σ(df, \"age > 21\") |>\n    π([\"name\", \"age\"])\n```\n\n## `LIMIT`  \n\n::: {#6a001de2 .cell quarto-private-1='{\"key\":\"ExecuteTime\",\"value\":{\"end_time\":\"2022-01-26T10:58:23.315363Z\",\"start_time\":\"2022-01-26T10:58:22.842106Z\"}}' execution_count=28}\n``` {.python .cell-code}\ndf.createOrReplaceTempView(\"table\")\n\nquery = \"\"\"\n    SELECT \n        * \n    FROM \n        table \n    LIMIT 1\n\"\"\"\n\nquery = \"\"\"\n    FROM table\n    |> LIMIT 1\n\"\"\"\n\nspark.sql(query).show()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n+----+---+------+\n|name|age|gender|\n+----+---+------+\n|John| 21|  male|\n+----+---+------+\n\n```\n:::\n:::\n\n\n::: {#38820586 .cell quarto-private-1='{\"key\":\"ExecuteTime\",\"value\":{\"end_time\":\"2022-01-26T10:58:23.522646Z\",\"start_time\":\"2022-01-26T10:58:23.318694Z\"}}' execution_count=29}\n``` {.python .cell-code}\ndf.limit(1).show()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n+----+---+------+\n|name|age|gender|\n+----+---+------+\n|John| 21|  male|\n+----+---+------+\n\n```\n:::\n:::\n\n\n::: {#bb3b4ffe .cell quarto-private-1='{\"key\":\"ExecuteTime\",\"value\":{\"end_time\":\"2022-01-26T10:58:23.778517Z\",\"start_time\":\"2022-01-26T10:58:23.525281Z\"}}' execution_count=30}\n``` {.python .cell-code}\ndf.select(\"*\").limit(1).show()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n+----+---+------+\n|name|age|gender|\n+----+---+------+\n|John| 21|  male|\n+----+---+------+\n\n```\n:::\n:::\n\n\n## `ORDER BY`\n\n::: {#2bfe3de5 .cell quarto-private-1='{\"key\":\"ExecuteTime\",\"value\":{\"end_time\":\"2022-01-26T10:58:24.190838Z\",\"start_time\":\"2022-01-26T10:58:23.781166Z\"}}' execution_count=31}\n``` {.python .cell-code}\ndf.createOrReplaceTempView(\"table\")\n\nquery = \"\"\"\n    SELECT \n        * \n    FROM \n        table\n    ORDER BY \n        name ASC\n\"\"\"\n\nquery = \"\"\"\n    FROM table\n    |> ORDER BY name ASC\n\"\"\"\n\nspark.sql(query).show()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n+----+---+------+\n|name|age|gender|\n+----+---+------+\n|Jane| 25|female|\n|John| 21|  male|\n+----+---+------+\n\n```\n:::\n:::\n\n\nWith  the API \n\n::: {#ce3887ad .cell quarto-private-1='{\"key\":\"ExecuteTime\",\"value\":{\"end_time\":\"2022-01-26T10:58:24.368069Z\",\"start_time\":\"2022-01-26T10:58:24.193899Z\"}}' execution_count=32}\n``` {.python .cell-code}\ndf.orderBy(df.name.asc()).show()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n+----+---+------+\n|name|age|gender|\n+----+---+------+\n|Jane| 25|female|\n|John| 21|  male|\n+----+---+------+\n\n```\n:::\n:::\n\n\n## `ALIAS`  (rename)\n\n::: {#6a772c69 .cell quarto-private-1='{\"key\":\"ExecuteTime\",\"value\":{\"end_time\":\"2022-01-26T10:58:24.643668Z\",\"start_time\":\"2022-01-26T10:58:24.370758Z\"}}' execution_count=33}\n``` {.python .cell-code}\ndf.createOrReplaceTempView(\"table\")\n\nquery = \"\"\"\n    SELECT \n        name, age, gender  AS sex \n    FROM \n        table\n\"\"\"\n\nquery = \"\"\"\n    FROM table\n    |> SELECT name, age, gender  AS sex\n\"\"\"\n\nspark.sql(query).show()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n+----+---+------+\n|name|age|   sex|\n+----+---+------+\n|John| 21|  male|\n|Jane| 25|female|\n+----+---+------+\n\n```\n:::\n:::\n\n\n::: {#de602a7f .cell execution_count=34}\n``` {.python .cell-code}\ntype(df.age)\n```\n\n::: {.cell-output .cell-output-display execution_count=34}\n```\npyspark.sql.classic.column.Column\n```\n:::\n:::\n\n\n::: {#55055600 .cell quarto-private-1='{\"key\":\"ExecuteTime\",\"value\":{\"end_time\":\"2022-01-26T10:58:24.858104Z\",\"start_time\":\"2022-01-26T10:58:24.646119Z\"}}' execution_count=35}\n``` {.python .cell-code}\n(\n    df.select(\n        df.name, \n        df.age, \n        df.gender.alias('sex'))\n      .show()\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n+----+---+------+\n|name|age|   sex|\n+----+---+------+\n|John| 21|  male|\n|Jane| 25|female|\n+----+---+------+\n\n```\n:::\n:::\n\n\n## `CAST`\n\n::: {#335c47fc .cell quarto-private-1='{\"key\":\"ExecuteTime\",\"value\":{\"end_time\":\"2022-01-26T10:58:25.072286Z\",\"start_time\":\"2022-01-26T10:58:24.860474Z\"}}' execution_count=36}\n``` {.python .cell-code}\ndf.createOrReplaceTempView(\"table\")\n\nquery = \"\"\"\n    SELECT \n        name, \n        cast(age AS float) AS age_f \n    FROM \n        table\n\"\"\"\n\nquery = \"\"\"\n    FROM table |>  \n    SELECT \n        name, \n        cast(age AS float) AS age_f \n\"\"\"\n\nspark.sql(query).show()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n+----+-----+\n|name|age_f|\n+----+-----+\n|John| 21.0|\n|Jane| 25.0|\n+----+-----+\n\n```\n:::\n:::\n\n\n::: {#4c398802 .cell quarto-private-1='{\"key\":\"ExecuteTime\",\"value\":{\"end_time\":\"2022-01-26T10:58:25.384433Z\",\"start_time\":\"2022-01-26T10:58:25.074523Z\"}}' execution_count=37}\n``` {.python .cell-code}\n(\n    df\n        .select(\n            df.name, \n            df.age\n                .cast(\"float\")\n                .alias(\"age_f\"))\n        .show()\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n+----+-----+\n|name|age_f|\n+----+-----+\n|John| 21.0|\n|Jane| 25.0|\n+----+-----+\n\n```\n:::\n:::\n\n\n::: {#7ba0a8eb .cell quarto-private-1='{\"key\":\"ExecuteTime\",\"value\":{\"end_time\":\"2022-01-26T10:58:25.648155Z\",\"start_time\":\"2022-01-26T10:58:25.386952Z\"}}' execution_count=38}\n``` {.python .cell-code}\nnew_age_col = df.age.cast(\"float\").alias(\"age_f\")\ntype(new_age_col), type(df.age)\n```\n\n::: {.cell-output .cell-output-display execution_count=38}\n```\n(pyspark.sql.classic.column.Column, pyspark.sql.classic.column.Column)\n```\n:::\n:::\n\n\n::: {#a7326c78 .cell execution_count=39}\n``` {.python .cell-code}\ndf.select(df.name, new_age_col).show()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n+----+-----+\n|name|age_f|\n+----+-----+\n|John| 21.0|\n|Jane| 25.0|\n+----+-----+\n\n```\n:::\n:::\n\n\n## Adding new columns\n\n::: {#7515e340 .cell quarto-private-1='{\"key\":\"ExecuteTime\",\"value\":{\"end_time\":\"2022-01-26T10:58:25.931495Z\",\"start_time\":\"2022-01-26T10:58:25.651283Z\"}}' execution_count=40}\n``` {.python .cell-code}\ndf.createOrReplaceTempView(\"table\")\n\nquery = \"\"\"\n    FROM table |>\n    SELECT \n        *, \n        12*age AS age_months \n\"\"\"\n\nspark.sql(query).show()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n+----+---+------+----------+\n|name|age|gender|age_months|\n+----+---+------+----------+\n|John| 21|  male|       252|\n|Jane| 25|female|       300|\n+----+---+------+----------+\n\n```\n:::\n:::\n\n\n::: {#410b44e7 .cell quarto-private-1='{\"key\":\"ExecuteTime\",\"value\":{\"end_time\":\"2022-01-26T10:58:26.195480Z\",\"start_time\":\"2022-01-26T10:58:25.933620Z\"}}' execution_count=41}\n``` {.python .cell-code}\n( \n    df\n        .withColumn(\"age_months\", df.age * 12)\n        .show()\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n+----+---+------+----------+\n|name|age|gender|age_months|\n+----+---+------+----------+\n|John| 21|  male|       252|\n|Jane| 25|female|       300|\n+----+---+------+----------+\n\n```\n:::\n:::\n\n\n::: {#e864f8ed .cell quarto-private-1='{\"key\":\"ExecuteTime\",\"value\":{\"end_time\":\"2022-01-26T10:58:26.422122Z\",\"start_time\":\"2022-01-26T10:58:26.197759Z\"}}' execution_count=42}\n``` {.python .cell-code}\n(\n    df\n        .select(\"*\", \n                (df.age * 12).alias(\"age_months\"))\n        .show()\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n+----+---+------+----------+\n|name|age|gender|age_months|\n+----+---+------+----------+\n|John| 21|  male|       252|\n|Jane| 25|female|       300|\n+----+---+------+----------+\n\n```\n:::\n:::\n\n\n::: {#6e7f3236 .cell execution_count=43}\n``` {.python .cell-code}\nimport datetime\n\nhui = datetime.date.today()\n\nstr(hui)\n```\n\n::: {.cell-output .cell-output-display execution_count=43}\n```\n'2026-02-02'\n```\n:::\n:::\n\n\n::: {#239f8993 .cell execution_count=44}\n``` {.python .cell-code}\n( \n    df\n     .withColumn(\"yob\", datetime.date.today().year - df.age)\n     .show()\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n+----+---+------+----+\n|name|age|gender| yob|\n+----+---+------+----+\n|John| 21|  male|2005|\n|Jane| 25|female|2001|\n+----+---+------+----+\n\n```\n:::\n:::\n\n\n# Column functions\n\n## Numeric functions examples\n\n::: {#f24e3a23 .cell quarto-private-1='{\"key\":\"ExecuteTime\",\"value\":{\"end_time\":\"2022-01-26T10:58:26.748718Z\",\"start_time\":\"2022-01-26T10:58:26.425451Z\"}}' execution_count=45}\n``` {.python .cell-code}\nfrom pyspark.sql import functions as fn\n\ncolumns = [\"brand\", \"cost\"]\n\ndf = spark.createDataFrame(\n    [(\"garnier\", 3.49),\n     (\"elseve\", 2.71)], \n    columns\n)\n\nround_cost = fn.round(df.cost, 1)\nfloor_cost = fn.floor(df.cost)\nceil_cost = fn.ceil(df.cost)\n\n(\n    df\n    .withColumn('round', round_cost)\n    .withColumn('floor', floor_cost)\n    .withColumn('ceil', ceil_cost)\n    .show()\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n+-------+----+-----+-----+----+\n|  brand|cost|round|floor|ceil|\n+-------+----+-----+-----+----+\n|garnier|3.49|  3.5|    3|   4|\n| elseve|2.71|  2.7|    2|   3|\n+-------+----+-----+-----+----+\n\n```\n:::\n:::\n\n\n## String functions examples\n\n::: {#45ef8dd4 .cell quarto-private-1='{\"key\":\"ExecuteTime\",\"value\":{\"end_time\":\"2022-01-26T10:58:27.055563Z\",\"start_time\":\"2022-01-26T10:58:26.751235Z\"}}' execution_count=46}\n``` {.python .cell-code}\nfrom pyspark.sql import functions as fn\n\ncolumns = [\"first_name\", \"last_name\"]\n\ndf = spark.createDataFrame([\n    (\"John\", \"Doe\"),\n    (\"Mary\", \"Jane\")\n], columns)\n\nlast_name_initial = fn.substring(df.last_name, 0, 1)\n# last_name_initial_dotted = fn.concat(last_name_initial, \".\")\n\nname = fn.concat_ws(\" \", df.first_name, last_name_initial)\n\ndf.withColumn(\"name\", name).show()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n+----------+---------+------+\n|first_name|last_name|  name|\n+----------+---------+------+\n|      John|      Doe|John D|\n|      Mary|     Jane|Mary J|\n+----------+---------+------+\n\n```\n:::\n:::\n\n\n::: {#9d7e1924 .cell execution_count=47}\n``` {.python .cell-code}\n( \n    df.selectExpr(\"*\", \"substring(last_name, 0, 1) as lni\")\n      .selectExpr(\"first_name\", \"last_name\", \"concat(first_name, ' ', lni, '.') as nname\")\n      .show()\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n+----------+---------+-------+\n|first_name|last_name|  nname|\n+----------+---------+-------+\n|      John|      Doe|John D.|\n|      Mary|     Jane|Mary J.|\n+----------+---------+-------+\n\n```\n:::\n:::\n\n\nAs an SQL query\n\n::: {#385dadcb .cell execution_count=48}\n``` {.python .cell-code}\ndf.createOrReplaceTempView(\"table\")\n\nquery = \"\"\"\n    FROM table |>\n    SELECT *, substring(last_name, 0, 1) AS lni |>\n    SELECT first_name, last_name, concat(first_name, ' ', lni, '.') AS nname\n\"\"\"\nspark.sql(query).show()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n+----------+---------+-------+\n|first_name|last_name|  nname|\n+----------+---------+-------+\n|      John|      Doe|John D.|\n|      Mary|     Jane|Mary J.|\n+----------+---------+-------+\n\n```\n:::\n:::\n\n\nSpark SQL offers a large  subsets of [SQL functions](https://www.postgresql.org/docs/current/functions.html)\n\n## Date functions examples\n\n::: {#1ebf240d .cell quarto-private-1='{\"key\":\"ExecuteTime\",\"value\":{\"end_time\":\"2022-01-26T10:58:27.373396Z\",\"start_time\":\"2022-01-26T10:58:27.057938Z\"}}' execution_count=49}\n``` {.python .cell-code}\nfrom datetime import date\nfrom pyspark.sql import functions as fn\n\ndf = spark.createDataFrame([\n    (date(2015, 1, 1), date(2015, 1, 15)),\n    (date(2015, 2, 21), date(2015, 3, 8)),\n], [\"start_date\", \"end_date\"])\n\ndays_between = fn.datediff(df.end_date, df.start_date)\nstart_month = fn.month(df.start_date)\n\n(\n    df\n        .withColumn('days_between', days_between)\n        .withColumn('start_month', start_month)\n        .show()\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n+----------+----------+------------+-----------+\n|start_date|  end_date|days_between|start_month|\n+----------+----------+------------+-----------+\n|2015-01-01|2015-01-15|          14|          1|\n|2015-02-21|2015-03-08|          15|          2|\n+----------+----------+------------+-----------+\n\n```\n:::\n:::\n\n\nNote that `days_between` is an instance of `Column`, the Spark type for columns in Spark dataframes.\n\n::: {#4fa0faf4 .cell execution_count=50}\n``` {.python .cell-code}\n# %%\ntype(days_between)\n```\n\n::: {.cell-output .cell-output-display execution_count=50}\n```\npyspark.sql.classic.column.Column\n```\n:::\n:::\n\n\nRecall the datetime calculus available in Database systems and in many programming framework.\n\n::: {#564395ca .cell execution_count=51}\n``` {.python .cell-code}\nstr(date(2015, 1, 1) - date(2015, 1, 15))\n```\n\n::: {.cell-output .cell-output-display execution_count=51}\n```\n'-14 days, 0:00:00'\n```\n:::\n:::\n\n\n::: {#ef2ce29e .cell execution_count=52}\n``` {.python .cell-code}\nfrom datetime import timedelta\n\ndate(2023, 2 , 14) + timedelta(days=3)\n```\n\n::: {.cell-output .cell-output-display execution_count=52}\n```\ndatetime.date(2023, 2, 17)\n```\n:::\n:::\n\n\n## Conditional transformations\n\n::: {#8659e39a .cell quarto-private-1='{\"key\":\"ExecuteTime\",\"value\":{\"end_time\":\"2022-01-26T10:58:27.630822Z\",\"start_time\":\"2022-01-26T10:58:27.375855Z\"}}' execution_count=53}\n``` {.python .cell-code}\ndf = spark.createDataFrame([\n    (\"John\", 21, \"male\"),\n    (\"Jane\", 25, \"female\"),\n    (\"Albert\", 46, \"male\"),\n    (\"Brad\", 49, \"super-hero\")\n], [\"name\", \"age\", \"gender\"])\n```\n:::\n\n\n::: {#c35d404b .cell quarto-private-1='{\"key\":\"ExecuteTime\",\"value\":{\"end_time\":\"2022-01-26T10:58:27.630822Z\",\"start_time\":\"2022-01-26T10:58:27.375855Z\"}}' execution_count=54}\n``` {.python .cell-code}\nsupervisor = ( \n    fn.when(df.gender == 'male', 'Mr. Smith')\n      .when(df.gender == 'female', 'Miss Jones')\n      .otherwise('NA')\n)\n\ntype(supervisor), type(fn.when)\n```\n\n::: {.cell-output .cell-output-display execution_count=54}\n```\n(pyspark.sql.classic.column.Column, function)\n```\n:::\n:::\n\n\n::: {#3d67e9de .cell quarto-private-1='{\"key\":\"ExecuteTime\",\"value\":{\"end_time\":\"2022-01-26T10:58:27.630822Z\",\"start_time\":\"2022-01-26T10:58:27.375855Z\"}}' execution_count=55}\n``` {.python .cell-code}\ndf.withColumn(\"supervisor\", supervisor).show()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n+------+---+----------+----------+\n|  name|age|    gender|supervisor|\n+------+---+----------+----------+\n|  John| 21|      male| Mr. Smith|\n|  Jane| 25|    female|Miss Jones|\n|Albert| 46|      male| Mr. Smith|\n|  Brad| 49|super-hero|        NA|\n+------+---+----------+----------+\n\n```\n:::\n:::\n\n\n## User-defined functions\n\n::: {#8ed72d93 .cell quarto-private-1='{\"key\":\"ExecuteTime\",\"value\":{\"end_time\":\"2022-01-26T10:58:28.037428Z\",\"start_time\":\"2022-01-26T10:58:27.633093Z\"}}' execution_count=56}\n``` {.python .cell-code}\nfrom pyspark.sql import functions as fn\nfrom pyspark.sql.types import StringType\n\ndf = spark.createDataFrame([(1, 3), (4, 2)], [\"first\", \"second\"])\n\ndef my_func(col_1, col_2):\n    if (col_1 > col_2):\n        return \"{} is bigger than {}\".format(col_1, col_2)\n    else:\n        return \"{} is bigger than {}\".format(col_2, col_1)\n\n# registration\nmy_udf = fn.udf(my_func, StringType())\n\n# fn.udf is a decorator\n# it can be used in a a more explicit way\n\n@fn.udf(returnType=StringType())\ndef the_same_func(col_1, col_2):\n    if (col_1 > col_2):\n        return \"{} is bigger than {}\".format(col_1, col_2)\n    else:\n        return \"{} is bigger than {}\".format(col_2, col_1)\n\n# at work        \n(\n    df\n        .withColumn(\"udf\", my_udf(df['first'], df['second']))\n        .withColumn(\"udf_too\", the_same_func(df['first'], df['second']))\n        .show()\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n+-----+------+------------------+------------------+\n|first|second|               udf|           udf_too|\n+-----+------+------------------+------------------+\n|    1|     3|3 is bigger than 1|3 is bigger than 1|\n|    4|     2|4 is bigger than 2|4 is bigger than 2|\n+-----+------+------------------+------------------+\n\n```\n:::\n:::\n\n\n::: {.callout-tip title=\"Using UDF in SQL queries\"}\n\nThe user-defined-function (UDF) can also be used on SQL queries\nprovided the decorated function is registered. \n\n::: {#c38f52ba .cell execution_count=57}\n``` {.python .cell-code}\ndf.createOrReplaceTempView(\"table\")\n\nspark.udf.register(\"the_same_func\", the_same_func)  # the_same_func from @udf above\nspark.sql(\"SELECT *, the_same_func(first, second) AS udf FROM table\").show()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n+-----+------+------------------+\n|first|second|               udf|\n+-----+------+------------------+\n|    1|     3|3 is bigger than 1|\n|    4|     2|4 is bigger than 2|\n+-----+------+------------------+\n\n```\n:::\n:::\n\n\nBeware. `spark.udf.register` is not a decorator.\n\n:::\n\n\n::: {.callout-important title=\"More on UDF in Spark 4\"}\n\nSee [User-Defined Table Functions (UDTFs) in Python](https://docs.databricks.com/gcp/en/udf/python-udtf)\n\n:::\n\n# Joins  ($⋈$)\n\n## Using the `spark.sql` API\n\n::: {#23145e35 .cell quarto-private-1='{\"key\":\"ExecuteTime\",\"value\":{\"end_time\":\"2022-01-26T10:58:29.098957Z\",\"start_time\":\"2022-01-26T10:58:28.042691Z\"}}' execution_count=58}\n``` {.python .cell-code}\nfrom datetime import date\n\nproducts = spark.createDataFrame(\n    [\n        ('1', 'mouse', 'microsoft', 39.99),\n        ('2', 'keyboard', 'logitech', 59.99),\n    ], \n    ['prod_id', 'prod_cat', 'prod_brand', 'prod_value']\n)\n\npurchases = spark.createDataFrame([\n    (date(2017, 11, 1), 2, '1'),\n    (date(2017, 11, 2), 1, '1'),\n    (date(2017, 11, 5), 1, '2'),\n], ['date', 'quantity', 'prod_id'])\n\n# The default join type is the \"INNER\" join\npurchases.join(products, 'prod_id').show()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n+-------+----------+--------+--------+----------+----------+\n|prod_id|      date|quantity|prod_cat|prod_brand|prod_value|\n+-------+----------+--------+--------+----------+----------+\n|      1|2017-11-01|       2|   mouse| microsoft|     39.99|\n|      1|2017-11-02|       1|   mouse| microsoft|     39.99|\n|      2|2017-11-05|       1|keyboard|  logitech|     59.99|\n+-------+----------+--------+--------+----------+----------+\n\n```\n:::\n:::\n\n\nJust as in  RDBMs, we can ask for explanations:\n\n::: {#7e018f58 .cell execution_count=59}\n``` {.python .cell-code}\npurchases.join(products, 'prod_id').explain()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n== Physical Plan ==\nAdaptiveSparkPlan isFinalPlan=false\n+- Project [prod_id#423, date#421, quantity#422L, prod_cat#418, prod_brand#419, prod_value#420]\n   +- SortMergeJoin [prod_id#423], [prod_id#417], Inner\n      :- Sort [prod_id#423 ASC NULLS FIRST], false, 0\n      :  +- Exchange hashpartitioning(prod_id#423, 200), ENSURE_REQUIREMENTS, [plan_id=648]\n      :     +- Filter isnotnull(prod_id#423)\n      :        +- Scan ExistingRDD[date#421,quantity#422L,prod_id#423]\n      +- Sort [prod_id#417 ASC NULLS FIRST], false, 0\n         +- Exchange hashpartitioning(prod_id#417, 200), ENSURE_REQUIREMENTS, [plan_id=649]\n            +- Filter isnotnull(prod_id#417)\n               +- Scan ExistingRDD[prod_id#417,prod_cat#418,prod_brand#419,prod_value#420]\n\n\n```\n:::\n:::\n\n\nHave a look at the PostgreSQL documentation. What are the different join methods? When is one favored?\n\n\n## Using a `SQL` query\n\n::: {#83b8bc92 .cell quarto-private-1='{\"key\":\"ExecuteTime\",\"value\":{\"end_time\":\"2022-01-26T10:58:29.731271Z\",\"start_time\":\"2022-01-26T10:58:29.101559Z\"}}' execution_count=60}\n``` {.python .cell-code}\nproducts.createOrReplaceTempView(\"products\")\npurchases.createOrReplaceTempView(\"purchases\")\n\nquery = \"\"\"\n    SELECT * \n    FROM purchases AS prc INNER JOIN \n        products AS prd \n    ON prc.prod_id = prd.prod_id\n\"\"\"\n\nquery = \"\"\"\n    FROM purchases |>\n    INNER JOIN products USING(prod_id)    \n\"\"\"\n\nspark.sql(query).show()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n+-------+----------+--------+--------+----------+----------+\n|prod_id|      date|quantity|prod_cat|prod_brand|prod_value|\n+-------+----------+--------+--------+----------+----------+\n|      1|2017-11-01|       2|   mouse| microsoft|     39.99|\n|      1|2017-11-02|       1|   mouse| microsoft|     39.99|\n|      2|2017-11-05|       1|keyboard|  logitech|     59.99|\n+-------+----------+--------+--------+----------+----------+\n\n```\n:::\n:::\n\n\n::: {#70718d81 .cell execution_count=61}\n``` {.python .cell-code}\nspark.sql(query).explain()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n== Physical Plan ==\nAdaptiveSparkPlan isFinalPlan=false\n+- Project [prod_id#423, date#421, quantity#422L, prod_cat#418, prod_brand#419, prod_value#420]\n   +- SortMergeJoin [prod_id#423], [prod_id#417], Inner\n      :- Sort [prod_id#423 ASC NULLS FIRST], false, 0\n      :  +- Exchange hashpartitioning(prod_id#423, 200), ENSURE_REQUIREMENTS, [plan_id=798]\n      :     +- Filter isnotnull(prod_id#423)\n      :        +- Scan ExistingRDD[date#421,quantity#422L,prod_id#423]\n      +- Sort [prod_id#417 ASC NULLS FIRST], false, 0\n         +- Exchange hashpartitioning(prod_id#417, 200), ENSURE_REQUIREMENTS, [plan_id=799]\n            +- Filter isnotnull(prod_id#417)\n               +- Scan ExistingRDD[prod_id#417,prod_cat#418,prod_brand#419,prod_value#420]\n\n\n```\n:::\n:::\n\n\n::: {#66fc0983 .cell quarto-private-1='{\"key\":\"ExecuteTime\",\"value\":{\"end_time\":\"2022-01-26T10:58:30.660419Z\",\"start_time\":\"2022-01-26T10:58:29.734282Z\"}}' execution_count=62}\n``` {.python .cell-code}\nnew_purchases = spark.createDataFrame([\n    (date(2017, 11, 1), 2, '1'),\n    (date(2017, 11, 2), 1, '3'),\n], ['date', 'quantity', 'prod_id_x'])\n\n# The default join type is the \"INNER\" join\njoin_rule = new_purchases.prod_id_x == products.prod_id\n\nprint(type(join_rule))\n\nnew_purchases.join(products, join_rule, 'left').show()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<class 'pyspark.sql.classic.column.Column'>\n+----------+--------+---------+-------+--------+----------+----------+\n|      date|quantity|prod_id_x|prod_id|prod_cat|prod_brand|prod_value|\n+----------+--------+---------+-------+--------+----------+----------+\n|2017-11-01|       2|        1|      1|   mouse| microsoft|     39.99|\n|2017-11-02|       1|        3|   NULL|    NULL|      NULL|      NULL|\n+----------+--------+---------+-------+--------+----------+----------+\n\n```\n:::\n:::\n\n\nWhat is the type of `join_rule.info`?\n\n::: {#4c8a5d8c .cell execution_count=63}\n``` {.python .cell-code}\njoin_rule.info\n```\n\n::: {.cell-output .cell-output-display execution_count=63}\n```\nColumn<'=(prod_id_x, prod_id)['info']'>\n```\n:::\n:::\n\n\n::: {#dadb459c .cell quarto-private-1='{\"key\":\"ExecuteTime\",\"value\":{\"end_time\":\"2022-01-26T10:58:31.319336Z\",\"start_time\":\"2022-01-26T10:58:30.663809Z\"}}' execution_count=64}\n``` {.python .cell-code}\nnew_purchases = spark.createDataFrame([\n    (date(2017, 11, 1), 2, '1'),\n    (date(2017, 11, 2), 1, '3'),\n], ['date', 'quantity', 'prod_id_x'])\n\n# The default join type is the \"INNER\" join\njoin_rule = new_purchases.prod_id_x == products.prod_id\n\nnew_purchases.join(products, join_rule, 'left').show()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n+----------+--------+---------+-------+--------+----------+----------+\n|      date|quantity|prod_id_x|prod_id|prod_cat|prod_brand|prod_value|\n+----------+--------+---------+-------+--------+----------+----------+\n|2017-11-01|       2|        1|      1|   mouse| microsoft|     39.99|\n|2017-11-02|       1|        3|   NULL|    NULL|      NULL|      NULL|\n+----------+--------+---------+-------+--------+----------+----------+\n\n```\n:::\n:::\n\n\n::: {.callout-note title=\"Different kinds of joins\"}\n\nJust as in RDBMS, there are different kinds of joins. \n\nThey differ in the way tuples from left and the right tables are matched (is it a natural join, an equi-join, a $\\theta$ join?). They also differ\nin the way they handle non-matching tuples (inner or outer joins). \n\nThe `join` method has three parameters:\n\n- `other`: the right table\n- `on`: the join rule that defines how tuples are matched. \n- `how`: defines the way non-matching tuples are handled.\n\n:::\n\n## Various types of joins\n\n::: {#9c3a960e .cell quarto-private-1='{\"key\":\"ExecuteTime\",\"value\":{\"end_time\":\"2022-01-26T10:58:31.376310Z\",\"start_time\":\"2022-01-26T10:58:31.323600Z\"}}' execution_count=65}\n``` {.python .cell-code}\nleft = spark.createDataFrame([\n    (1, \"A1\"), (2, \"A2\"), (3, \"A3\"), (4, \"A4\")], \n    [\"id\", \"value\"])\n\nright = spark.createDataFrame([\n    (3, \"A3\"), (4, \"A4\"), (4, \"A4_1\"), (5, \"A5\"), (6, \"A6\")], \n    [\"id\", \"value\"])\n\njoin_types = [\n    \"inner\", \"outer\", \"left\", \"right\",\n    \"leftsemi\", \"leftanti\"\n]\n```\n:::\n\n\n::: {#b91b0f44 .cell quarto-private-1='{\"key\":\"ExecuteTime\",\"value\":{\"end_time\":\"2022-01-26T10:58:34.708236Z\",\"start_time\":\"2022-01-26T10:58:31.380091Z\"}}' execution_count=66}\n``` {.python .cell-code}\nfor join_type in join_types:\n    print(join_type)\n    left.join(right, on=\"id\", how=join_type)\\\n        .orderBy(\"id\")\\\n        .show()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ninner\n+---+-----+-----+\n| id|value|value|\n+---+-----+-----+\n|  3|   A3|   A3|\n|  4|   A4|   A4|\n|  4|   A4| A4_1|\n+---+-----+-----+\n\nouter\n+---+-----+-----+\n| id|value|value|\n+---+-----+-----+\n|  1|   A1| NULL|\n|  2|   A2| NULL|\n|  3|   A3|   A3|\n|  4|   A4|   A4|\n|  4|   A4| A4_1|\n|  5| NULL|   A5|\n|  6| NULL|   A6|\n+---+-----+-----+\n\nleft\n+---+-----+-----+\n| id|value|value|\n+---+-----+-----+\n|  1|   A1| NULL|\n|  2|   A2| NULL|\n|  3|   A3|   A3|\n|  4|   A4|   A4|\n|  4|   A4| A4_1|\n+---+-----+-----+\n\nright\n+---+-----+-----+\n| id|value|value|\n+---+-----+-----+\n|  3|   A3|   A3|\n|  4|   A4|   A4|\n|  4|   A4| A4_1|\n|  5| NULL|   A5|\n|  6| NULL|   A6|\n+---+-----+-----+\n\nleftsemi\n+---+-----+\n| id|value|\n+---+-----+\n|  3|   A3|\n|  4|   A4|\n+---+-----+\n\nleftanti\n+---+-----+\n| id|value|\n+---+-----+\n|  1|   A1|\n|  2|   A2|\n+---+-----+\n\n```\n:::\n:::\n\n\n# Agregations    (summarize)\n\n## Examples using the API\n\n::: {#220a6540 .cell quarto-private-1='{\"key\":\"ExecuteTime\",\"value\":{\"end_time\":\"2022-01-26T10:58:35.398306Z\",\"start_time\":\"2022-01-26T10:58:34.710552Z\"}}' execution_count=67}\n``` {.python .cell-code}\nfrom pyspark.sql import functions as fn\n\nproducts = spark.createDataFrame([\n    ('1', 'mouse', 'microsoft', 39.99),\n    ('2', 'mouse', 'microsoft', 59.99),\n    ('3', 'keyboard', 'microsoft', 59.99),\n    ('4', 'keyboard', 'logitech', 59.99),\n    ('5', 'mouse', 'logitech', 29.99),\n], ['prod_id', 'prod_cat', 'prod_brand', 'prod_value'])\n\n( \n    products\n        .groupBy('prod_cat')\n        .avg('prod_value')\n        .show()\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n+--------+-----------------+\n|prod_cat|  avg(prod_value)|\n+--------+-----------------+\n|   mouse|43.32333333333333|\n|keyboard|            59.99|\n+--------+-----------------+\n\n```\n:::\n:::\n\n\nWhat is the type of `products\n        .groupBy('prod_cat')`? \n\n::: {#6161c8ce .cell quarto-private-1='{\"key\":\"ExecuteTime\",\"value\":{\"end_time\":\"2022-01-26T10:58:35.782623Z\",\"start_time\":\"2022-01-26T10:58:35.400724Z\"}}' execution_count=68}\n``` {.python .cell-code}\n(\n    products\n        .groupBy('prod_cat')\n        .agg(fn.avg('prod_value'))\n        .show()\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n+--------+-----------------+\n|prod_cat|  avg(prod_value)|\n+--------+-----------------+\n|   mouse|43.32333333333333|\n|keyboard|            59.99|\n+--------+-----------------+\n\n```\n:::\n:::\n\n\n::: {#e8176c37 .cell execution_count=69}\n``` {.python .cell-code}\n(\n    products\n        .groupBy('prod_cat')\n        .agg(\n            fn.mean('prod_value'), \n            fn.stddev('prod_value')\n        )\n        .show()\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n+--------+-----------------+------------------+\n|prod_cat|  avg(prod_value)|stddev(prod_value)|\n+--------+-----------------+------------------+\n|   mouse|43.32333333333333|15.275252316519468|\n|keyboard|            59.99|               0.0|\n+--------+-----------------+------------------+\n\n```\n:::\n:::\n\n\n::: {#034fa5f0 .cell quarto-private-1='{\"key\":\"ExecuteTime\",\"value\":{\"end_time\":\"2022-01-26T10:58:36.195471Z\",\"start_time\":\"2022-01-26T10:58:35.784780Z\"}}' execution_count=70}\n``` {.python .cell-code}\nfrom pyspark.sql import functions as fn\n\n(\n    products\n        .groupBy('prod_brand', 'prod_cat')\\\n        .agg(\n            fn.avg('prod_value')\n        )\n        .show()\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n+----------+--------+---------------+\n|prod_brand|prod_cat|avg(prod_value)|\n+----------+--------+---------------+\n| microsoft|   mouse|          49.99|\n| microsoft|keyboard|          59.99|\n|  logitech|keyboard|          59.99|\n|  logitech|   mouse|          29.99|\n+----------+--------+---------------+\n\n```\n:::\n:::\n\n\n::: {#01774856 .cell quarto-private-1='{\"key\":\"ExecuteTime\",\"value\":{\"end_time\":\"2022-01-26T10:58:36.650354Z\",\"start_time\":\"2022-01-26T10:58:36.207985Z\"}}' execution_count=71}\n``` {.python .cell-code}\nfrom pyspark.sql import functions as fn\n\n(\n    products\n        .groupBy('prod_brand')\n        .agg(\n            fn.round(\n                fn.avg('prod_value'), 1)\n                .alias('average'),\n            fn.ceil(\n                fn.sum('prod_value'))\n                .alias('sum'),\n            fn.min('prod_value')\n                .alias('min')\n        )\n        .show()\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n+----------+-------+---+-----+\n|prod_brand|average|sum|  min|\n+----------+-------+---+-----+\n| microsoft|   53.3|160|39.99|\n|  logitech|   45.0| 90|29.99|\n+----------+-------+---+-----+\n\n```\n:::\n:::\n\n\n## Example using a query\n\n::: {#8d7c691e .cell quarto-private-1='{\"key\":\"ExecuteTime\",\"value\":{\"end_time\":\"2022-01-26T10:58:37.089099Z\",\"start_time\":\"2022-01-26T10:58:36.652842Z\"}}' execution_count=72}\n``` {.python .cell-code}\nproducts.createOrReplaceTempView(\"products\")\n```\n:::\n\n\n::: {#bef20ee9 .cell execution_count=73}\n``` {.python .cell-code}\nquery = \"\"\"\nSELECT\n    prod_brand,\n    round(avg(prod_value), 1) AS average,\n    min(prod_value) AS min\nFROM \n    products\nGROUP BY \n    prod_brand\n\"\"\"\n\nquery = \"\"\"\n    FROM products \n    |> AGGREGATE \n        round(avg(prod_value), 1) AS average,\n        min(prod_value) AS min GROUP BY prod_brand \n\"\"\"\n\nspark.sql(query).show()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n+----------+-------+-----+\n|prod_brand|average|  min|\n+----------+-------+-----+\n| microsoft|   53.3|39.99|\n|  logitech|   45.0|29.99|\n+----------+-------+-----+\n\n```\n:::\n:::\n\n\n::: {#64bfd241 .cell execution_count=74}\n``` {.python .cell-code}\nspark.sql(query).explain()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n== Physical Plan ==\nAdaptiveSparkPlan isFinalPlan=false\n+- HashAggregate(keys=[prod_brand#573], functions=[avg(prod_value#574), min(prod_value#574)])\n   +- Exchange hashpartitioning(prod_brand#573, 200), ENSURE_REQUIREMENTS, [plan_id=2044]\n      +- HashAggregate(keys=[prod_brand#573], functions=[partial_avg(prod_value#574), partial_min(prod_value#574)])\n         +- Project [prod_brand#573, prod_value#574]\n            +- Scan ExistingRDD[prod_id#571,prod_cat#572,prod_brand#573,prod_value#574]\n\n\n```\n:::\n:::\n\n\n# Window functions\n\n## Numerical window functions\n\n::: {#e1dfcbf8 .cell quarto-private-1='{\"key\":\"ExecuteTime\",\"value\":{\"end_time\":\"2022-01-26T10:58:37.751296Z\",\"start_time\":\"2022-01-26T10:58:37.092075Z\"}}' execution_count=75}\n``` {.python .cell-code}\nfrom pyspark.sql import Window\nfrom pyspark.sql import functions as fn\n\n# First, we create the Window definition\nwindow = Window.partitionBy('prod_brand')\n\nprint(type(window))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<class 'pyspark.sql.classic.window.WindowSpec'>\n```\n:::\n:::\n\n\nThen, we can use `over` to aggregate on this window\n\n::: {#da81a399 .cell execution_count=76}\n``` {.python .cell-code}\navg = fn.avg('prod_value').over(window)\n\n# Finally, we can it as a classical column\n(\n    products\n        .withColumn('avg_brand_value', fn.round(avg, 2))\n        .show()\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n+-------+--------+----------+----------+---------------+\n|prod_id|prod_cat|prod_brand|prod_value|avg_brand_value|\n+-------+--------+----------+----------+---------------+\n|      4|keyboard|  logitech|     59.99|          44.99|\n|      5|   mouse|  logitech|     29.99|          44.99|\n|      1|   mouse| microsoft|     39.99|          53.32|\n|      2|   mouse| microsoft|     59.99|          53.32|\n|      3|keyboard| microsoft|     59.99|          53.32|\n+-------+--------+----------+----------+---------------+\n\n```\n:::\n:::\n\n\nWith SQL queries, using windows ?\n\n::: {#f63ef53f .cell execution_count=77}\n``` {.python .cell-code}\nquery = \"\"\"\n    SELECT \n        *, \n        ROUND(AVG(prod_value) OVER w1, 2)  AS avg_brand_value,\n        ROUND(AVG(prod_value) OVER w2, 1)  AS avg_prod_value\n    FROM \n        products\n    WINDOW \n        w1 AS (PARTITION BY prod_brand),\n        w2 AS (PARTITION BY prod_cat)\n\"\"\"\n\n\nquery2 = \"\"\"\n    FROM products |>\n    SELECT\n        *,  \n        ROUND(AVG(prod_value) OVER w1, 2)  AS avg_brand_value,\n        ROUND(AVG(prod_value) OVER w2, 1)  AS avg_prod_value\n        WINDOW \n            w1 AS (PARTITION BY prod_brand),\n            w2 AS (PARTITION BY prod_cat)\n\"\"\"\n\nspark.sql(query2).show()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n+-------+--------+----------+----------+---------------+--------------+\n|prod_id|prod_cat|prod_brand|prod_value|avg_brand_value|avg_prod_value|\n+-------+--------+----------+----------+---------------+--------------+\n|      4|keyboard|  logitech|     59.99|          44.99|          60.0|\n|      3|keyboard| microsoft|     59.99|          53.32|          60.0|\n|      5|   mouse|  logitech|     29.99|          44.99|          43.3|\n|      1|   mouse| microsoft|     39.99|          53.32|          43.3|\n|      2|   mouse| microsoft|     59.99|          53.32|          43.3|\n+-------+--------+----------+----------+---------------+--------------+\n\n```\n:::\n:::\n\n\n::: {#1587e1a6 .cell execution_count=78}\n``` {.python .cell-code}\nwindow2 = Window.partitionBy('prod_cat')\n\navg2 = fn.avg('prod_value').over(window2)\n\n# Finally, we can do it as a classical column\n( \n    products\n        .withColumn('avg_brand_value', fn.round(avg, 2))\n        .withColumn('avg_prod_value', fn.round(avg2, 1))\n        .show()\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n+-------+--------+----------+----------+---------------+--------------+\n|prod_id|prod_cat|prod_brand|prod_value|avg_brand_value|avg_prod_value|\n+-------+--------+----------+----------+---------------+--------------+\n|      4|keyboard|  logitech|     59.99|          44.99|          60.0|\n|      3|keyboard| microsoft|     59.99|          53.32|          60.0|\n|      5|   mouse|  logitech|     29.99|          44.99|          43.3|\n|      1|   mouse| microsoft|     39.99|          53.32|          43.3|\n|      2|   mouse| microsoft|     59.99|          53.32|          43.3|\n+-------+--------+----------+----------+---------------+--------------+\n\n```\n:::\n:::\n\n\nNow we can compare the physical plans associated with the  two jobs.\n\n::: {#92b47b3d .cell execution_count=79}\n``` {.python .cell-code}\n( \n    products\n        .withColumn('avg_brand_value', fn.round(avg, 2))\n        .withColumn('avg_prod_value', fn.round(avg2, 1))\n        .explain()\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n== Physical Plan ==\nAdaptiveSparkPlan isFinalPlan=false\n+- Project [prod_id#571, prod_cat#572, prod_brand#573, prod_value#574, avg_brand_value#846, round(_we0#851, 1) AS avg_prod_value#849]\n   +- Window [avg(prod_value#574) windowspecdefinition(prod_cat#572, specifiedwindowframe(RowFrame, unboundedpreceding$(), unboundedfollowing$())) AS _we0#851], [prod_cat#572]\n      +- Sort [prod_cat#572 ASC NULLS FIRST], false, 0\n         +- Exchange hashpartitioning(prod_cat#572, 200), ENSURE_REQUIREMENTS, [plan_id=2385]\n            +- Project [prod_id#571, prod_cat#572, prod_brand#573, prod_value#574, round(_we0#848, 2) AS avg_brand_value#846]\n               +- Window [avg(prod_value#574) windowspecdefinition(prod_brand#573, specifiedwindowframe(RowFrame, unboundedpreceding$(), unboundedfollowing$())) AS _we0#848], [prod_brand#573]\n                  +- Sort [prod_brand#573 ASC NULLS FIRST], false, 0\n                     +- Exchange hashpartitioning(prod_brand#573, 200), ENSURE_REQUIREMENTS, [plan_id=2380]\n                        +- Scan ExistingRDD[prod_id#571,prod_cat#572,prod_brand#573,prod_value#574]\n\n\n```\n:::\n:::\n\n\n::: {#89ac4d53 .cell execution_count=80}\n``` {.python .cell-code}\nspark.sql(query).explain()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n== Physical Plan ==\nAdaptiveSparkPlan isFinalPlan=false\n+- Project [prod_id#571, prod_cat#572, prod_brand#573, prod_value#574, round(_we0#856, 2) AS avg_brand_value#852, round(_we1#857, 1) AS avg_prod_value#853]\n   +- Window [avg(prod_value#574) windowspecdefinition(prod_cat#572, specifiedwindowframe(RowFrame, unboundedpreceding$(), unboundedfollowing$())) AS _we1#857], [prod_cat#572]\n      +- Sort [prod_cat#572 ASC NULLS FIRST], false, 0\n         +- Exchange hashpartitioning(prod_cat#572, 200), ENSURE_REQUIREMENTS, [plan_id=2409]\n            +- Window [avg(prod_value#574) windowspecdefinition(prod_brand#573, specifiedwindowframe(RowFrame, unboundedpreceding$(), unboundedfollowing$())) AS _we0#856], [prod_brand#573]\n               +- Sort [prod_brand#573 ASC NULLS FIRST], false, 0\n                  +- Exchange hashpartitioning(prod_brand#573, 200), ENSURE_REQUIREMENTS, [plan_id=2405]\n                     +- Scan ExistingRDD[prod_id#571,prod_cat#572,prod_brand#573,prod_value#574]\n\n\n```\n:::\n:::\n\n\n# Windows can be defined on multiple columns\n\n::: {#acbda84c .cell quarto-private-1='{\"key\":\"ExecuteTime\",\"value\":{\"end_time\":\"2022-01-26T10:58:38.261379Z\",\"start_time\":\"2022-01-26T10:58:37.753256Z\"}}' execution_count=81}\n``` {.python .cell-code}\nfrom pyspark.sql import Window\nfrom pyspark.sql import functions as fn\n\nwindow = Window.partitionBy('prod_brand', 'prod_cat')\n\navg = fn.avg('prod_value').over(window)\n\n\n(\n    products    \n        .withColumn('avg_value', fn.round(avg, 2))\n        .show()\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n+-------+--------+----------+----------+---------+\n|prod_id|prod_cat|prod_brand|prod_value|avg_value|\n+-------+--------+----------+----------+---------+\n|      4|keyboard|  logitech|     59.99|    59.99|\n|      5|   mouse|  logitech|     29.99|    29.99|\n|      3|keyboard| microsoft|     59.99|    59.99|\n|      1|   mouse| microsoft|     39.99|    49.99|\n|      2|   mouse| microsoft|     59.99|    49.99|\n+-------+--------+----------+----------+---------+\n\n```\n:::\n:::\n\n\n## Lag and Lead\n\n::: {#499ad85d .cell quarto-private-1='{\"key\":\"ExecuteTime\",\"value\":{\"end_time\":\"2022-01-26T10:58:39.785452Z\",\"start_time\":\"2022-01-26T10:58:39.084502Z\"}}' execution_count=82}\n``` {.python .cell-code}\npurchases = spark.createDataFrame(\n    [\n        (date(2017, 11, 1), 'mouse'),\n        (date(2017, 11, 2), 'mouse'),\n        (date(2017, 11, 4), 'keyboard'),\n        (date(2017, 11, 6), 'keyboard'),\n        (date(2017, 11, 9), 'keyboard'),\n        (date(2017, 11, 12), 'mouse'),\n        (date(2017, 11, 18), 'keyboard')\n    ], \n    ['date', 'prod_cat']\n)\n\npurchases.show()\n\nwindow = Window.partitionBy('prod_cat').orderBy('date')\n\nprev_purch = fn.lag('date', 1).over(window)\nnext_purch = fn.lead('date', 1).over(window)\n\npurchases\\\n    .withColumn('prev', prev_purch)\\\n    .withColumn('next', next_purch)\\\n    .orderBy('prod_cat', 'date')\\\n    .show()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n+----------+--------+\n|      date|prod_cat|\n+----------+--------+\n|2017-11-01|   mouse|\n|2017-11-02|   mouse|\n|2017-11-04|keyboard|\n|2017-11-06|keyboard|\n|2017-11-09|keyboard|\n|2017-11-12|   mouse|\n|2017-11-18|keyboard|\n+----------+--------+\n\n+----------+--------+----------+----------+\n|      date|prod_cat|      prev|      next|\n+----------+--------+----------+----------+\n|2017-11-04|keyboard|      NULL|2017-11-06|\n|2017-11-06|keyboard|2017-11-04|2017-11-09|\n|2017-11-09|keyboard|2017-11-06|2017-11-18|\n|2017-11-18|keyboard|2017-11-09|      NULL|\n|2017-11-01|   mouse|      NULL|2017-11-02|\n|2017-11-02|   mouse|2017-11-01|2017-11-12|\n|2017-11-12|   mouse|2017-11-02|      NULL|\n+----------+--------+----------+----------+\n\n```\n:::\n:::\n\n\n## Rank, DenseRank and RowNumber\n\n::: {#d86884cf .cell quarto-private-1='{\"key\":\"ExecuteTime\",\"value\":{\"end_time\":\"2022-01-26T10:58:40.005845Z\",\"start_time\":\"2022-01-26T10:58:39.787433Z\"}}' execution_count=83}\n``` {.python .cell-code}\ncontestants = spark.createDataFrame(\n    [   \n        ('veterans', 'John', 3000),\n        ('veterans', 'Bob', 3200),\n        ('veterans', 'Mary', 4000),\n        ('young', 'Jane', 4000),\n        ('young', 'April', 3100),\n        ('young', 'Alice', 3700),\n        ('young', 'Micheal', 4000),\n    ], \n    ['category', 'name', 'points']\n)\n\ncontestants.show()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n+--------+-------+------+\n|category|   name|points|\n+--------+-------+------+\n|veterans|   John|  3000|\n|veterans|    Bob|  3200|\n|veterans|   Mary|  4000|\n|   young|   Jane|  4000|\n|   young|  April|  3100|\n|   young|  Alice|  3700|\n|   young|Micheal|  4000|\n+--------+-------+------+\n\n```\n:::\n:::\n\n\n::: {#7aaa0ec4 .cell quarto-private-1='{\"key\":\"ExecuteTime\",\"value\":{\"end_time\":\"2022-01-26T10:58:40.653650Z\",\"start_time\":\"2022-01-26T10:58:40.009618Z\"}}' execution_count=84}\n``` {.python .cell-code}\nwindow = (\n    Window\n        .partitionBy('category')\n        .orderBy(contestants.points.desc())\n)\n\nrank = fn.rank().over(window)\ndense_rank = fn.dense_rank().over(window)\nrow_number = fn.row_number().over(window)\n\n(\ncontestants\n    .withColumn('rank', rank)\n    .withColumn('dense_rank', dense_rank)\n    .withColumn('row_number', row_number)\n    .orderBy('category', fn.col('points').desc())\n    .show()\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n+--------+-------+------+----+----------+----------+\n|category|   name|points|rank|dense_rank|row_number|\n+--------+-------+------+----+----------+----------+\n|veterans|   Mary|  4000|   1|         1|         1|\n|veterans|    Bob|  3200|   2|         2|         2|\n|veterans|   John|  3000|   3|         3|         3|\n|   young|   Jane|  4000|   1|         1|         1|\n|   young|Micheal|  4000|   1|         1|         2|\n|   young|  Alice|  3700|   3|         2|         3|\n|   young|  April|  3100|   4|         3|         4|\n+--------+-------+------+----+----------+----------+\n\n```\n:::\n:::\n\n\n::: {.content-hidden unless-profile=\"dorenavant\"}\n\n\n# Connection to a database {{< fa database >}}\n\nThe postgres server runs locally on my laptop, it is equiped with a\nnumber of training schemata, including `nycflights` (see [https://s-v-b.github.io/MA15Y030/schemas/schema-nycflights.html](https://s-v-b.github.io/MA15Y030/schemas/schema-nycflights.html))\n\n```{.python}\ndf_flights = spark.read \\\n    .format(\"jdbc\") \\\n    .option(\"url\", \"jdbc:postgresql://localhost:5436/bd_2023-24\") \\\n    .option(\"dbschema\", \"nycflights\")\\\n    .option(\"dbtable\", \"flights\") \\\n    .option(\"user\", \"postgres\") \\\n    .option(\"password\", \"postgres\") \\\n    .option(\"driver\", \"org.postgresql.Driver\") \\\n    .load()\n\ndf_flights.printSchema()\n```\n\nTo load the five tables, we avoid cut and paste, and abide to the DRY principle.\n\nWe package the options in a dictionnary\n\n::: {#2c3829f7 .cell execution_count=85}\n``` {.python .cell-code}\ndb_con_settings = {\n    'url': \"jdbc:postgresql://localhost:5436/bd_2023-24\",\n    'dbschema':  \"nycflights\",\n    'user':  \"postgres\",\n    'password':  \"postgres\",\n    'driver':  \"org.postgresql.Driver\"\n}\n```\n:::\n\n\nWe prepare a Python object using dictionnary unpacking. \n\n::: {#69a07725 .cell execution_count=86}\n``` {.python .cell-code}\no  = spark.read \\\n    .format(\"jdbc\")\\\n    .options(**db_con_settings)\n```\n:::\n\n\nWe use the object to load the different tables in a `for` loop.\n\n::: {#23c8c6ec .cell execution_count=87}\n``` {.python .cell-code}\ntbl_names = ['flights', 'airports', 'airlines', 'planes', 'weather']\n\ndic_df = {}\n\nfor tn in tbl_names:\n    dic_df[tn] = o.option('dbtable', tn).load()\n```\n\n::: {.cell-output .cell-output-error}\n\n::: {.ansi-escaped-output}\n```{=html}\n<pre><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">Py4JJavaError</span>                             Traceback (most recent call last)\nCell <span class=\"ansi-green-fg\">In[87], line 6</span>\n<span class=\"ansi-green-fg ansi-bold\">      3</span> dic_df <span style=\"color:rgb(98,98,98)\">=</span> {}\n<span class=\"ansi-green-fg ansi-bold\">      5</span> <span style=\"font-weight:bold;color:rgb(0,135,0)\">for</span> tn <span style=\"font-weight:bold;color:rgb(175,0,255)\">in</span> tbl_names:\n<span class=\"ansi-green-fg\">----&gt; 6</span>     dic_df[tn] <span style=\"color:rgb(98,98,98)\">=</span> <span class=\"ansi-yellow-bg\">o</span><span style=\"color:rgb(98,98,98)\" class=\"ansi-yellow-bg\">.</span><span class=\"ansi-yellow-bg\">option</span><span class=\"ansi-yellow-bg\">(</span><span style=\"color:rgb(175,0,0)\" class=\"ansi-yellow-bg\">'</span><span style=\"color:rgb(175,0,0)\" class=\"ansi-yellow-bg\">dbtable</span><span style=\"color:rgb(175,0,0)\" class=\"ansi-yellow-bg\">'</span><span class=\"ansi-yellow-bg\">,</span><span class=\"ansi-yellow-bg\"> </span><span class=\"ansi-yellow-bg\">tn</span><span class=\"ansi-yellow-bg\">)</span><span style=\"color:rgb(98,98,98)\" class=\"ansi-yellow-bg\">.</span><span class=\"ansi-yellow-bg\">load</span><span class=\"ansi-yellow-bg\">(</span><span class=\"ansi-yellow-bg\">)</span>\n\nFile <span class=\"ansi-green-fg\">~/Documents/IFEBY310/.venv/lib/python3.12/site-packages/pyspark/sql/readwriter.py:318</span>, in <span class=\"ansi-cyan-fg\">DataFrameReader.load</span><span class=\"ansi-blue-fg\">(self, path, format, schema, **options)</span>\n<span class=\"ansi-green-fg ansi-bold\">    316</span>     <span style=\"font-weight:bold;color:rgb(0,135,0)\">return</span> <span style=\"color:rgb(0,135,0)\">self</span><span style=\"color:rgb(98,98,98)\">.</span>_df(<span style=\"color:rgb(0,135,0)\">self</span><span style=\"color:rgb(98,98,98)\">.</span>_jreader<span style=\"color:rgb(98,98,98)\">.</span>load(<span style=\"color:rgb(0,135,0)\">self</span><span style=\"color:rgb(98,98,98)\">.</span>_spark<span style=\"color:rgb(98,98,98)\">.</span>_sc<span style=\"color:rgb(98,98,98)\">.</span>_jvm<span style=\"color:rgb(98,98,98)\">.</span>PythonUtils<span style=\"color:rgb(98,98,98)\">.</span>toSeq(path)))\n<span class=\"ansi-green-fg ansi-bold\">    317</span> <span style=\"font-weight:bold;color:rgb(0,135,0)\">else</span>:\n<span class=\"ansi-green-fg\">--&gt; 318</span>     <span style=\"font-weight:bold;color:rgb(0,135,0)\">return</span> <span style=\"color:rgb(0,135,0)\">self</span><span style=\"color:rgb(98,98,98)\">.</span>_df(<span style=\"color:rgb(0,135,0)\" class=\"ansi-yellow-bg\">self</span><span style=\"color:rgb(98,98,98)\" class=\"ansi-yellow-bg\">.</span><span class=\"ansi-yellow-bg\">_jreader</span><span style=\"color:rgb(98,98,98)\" class=\"ansi-yellow-bg\">.</span><span class=\"ansi-yellow-bg\">load</span><span class=\"ansi-yellow-bg\">(</span><span class=\"ansi-yellow-bg\">)</span>)\n\nFile <span class=\"ansi-green-fg\">~/Documents/IFEBY310/.venv/lib/python3.12/site-packages/py4j/java_gateway.py:1322</span>, in <span class=\"ansi-cyan-fg\">JavaMember.__call__</span><span class=\"ansi-blue-fg\">(self, *args)</span>\n<span class=\"ansi-green-fg ansi-bold\">   1316</span> command <span style=\"color:rgb(98,98,98)\">=</span> proto<span style=\"color:rgb(98,98,98)\">.</span>CALL_COMMAND_NAME <span style=\"color:rgb(98,98,98)\">+</span>\\\n<span class=\"ansi-green-fg ansi-bold\">   1317</span>     <span style=\"color:rgb(0,135,0)\">self</span><span style=\"color:rgb(98,98,98)\">.</span>command_header <span style=\"color:rgb(98,98,98)\">+</span>\\\n<span class=\"ansi-green-fg ansi-bold\">   1318</span>     args_command <span style=\"color:rgb(98,98,98)\">+</span>\\\n<span class=\"ansi-green-fg ansi-bold\">   1319</span>     proto<span style=\"color:rgb(98,98,98)\">.</span>END_COMMAND_PART\n<span class=\"ansi-green-fg ansi-bold\">   1321</span> answer <span style=\"color:rgb(98,98,98)\">=</span> <span style=\"color:rgb(0,135,0)\">self</span><span style=\"color:rgb(98,98,98)\">.</span>gateway_client<span style=\"color:rgb(98,98,98)\">.</span>send_command(command)\n<span class=\"ansi-green-fg\">-&gt; 1322</span> return_value <span style=\"color:rgb(98,98,98)\">=</span> <span class=\"ansi-yellow-bg\">get_return_value</span><span class=\"ansi-yellow-bg\">(</span>\n<span class=\"ansi-green-fg ansi-bold\">   1323</span> <span class=\"ansi-yellow-bg\">    </span><span class=\"ansi-yellow-bg\">answer</span><span class=\"ansi-yellow-bg\">,</span><span class=\"ansi-yellow-bg\"> </span><span style=\"color:rgb(0,135,0)\" class=\"ansi-yellow-bg\">self</span><span style=\"color:rgb(98,98,98)\" class=\"ansi-yellow-bg\">.</span><span class=\"ansi-yellow-bg\">gateway_client</span><span class=\"ansi-yellow-bg\">,</span><span class=\"ansi-yellow-bg\"> </span><span style=\"color:rgb(0,135,0)\" class=\"ansi-yellow-bg\">self</span><span style=\"color:rgb(98,98,98)\" class=\"ansi-yellow-bg\">.</span><span class=\"ansi-yellow-bg\">target_id</span><span class=\"ansi-yellow-bg\">,</span><span class=\"ansi-yellow-bg\"> </span><span style=\"color:rgb(0,135,0)\" class=\"ansi-yellow-bg\">self</span><span style=\"color:rgb(98,98,98)\" class=\"ansi-yellow-bg\">.</span><span class=\"ansi-yellow-bg\">name</span><span class=\"ansi-yellow-bg\">)</span>\n<span class=\"ansi-green-fg ansi-bold\">   1325</span> <span style=\"font-weight:bold;color:rgb(0,135,0)\">for</span> temp_arg <span style=\"font-weight:bold;color:rgb(175,0,255)\">in</span> temp_args:\n<span class=\"ansi-green-fg ansi-bold\">   1326</span>     <span style=\"font-weight:bold;color:rgb(0,135,0)\">if</span> <span style=\"color:rgb(0,135,0)\">hasattr</span>(temp_arg, <span style=\"color:rgb(175,0,0)\">\"</span><span style=\"color:rgb(175,0,0)\">_detach</span><span style=\"color:rgb(175,0,0)\">\"</span>):\n\nFile <span class=\"ansi-green-fg\">~/Documents/IFEBY310/.venv/lib/python3.12/site-packages/pyspark/errors/exceptions/captured.py:263</span>, in <span class=\"ansi-cyan-fg\">capture_sql_exception.&lt;locals&gt;.deco</span><span class=\"ansi-blue-fg\">(*a, **kw)</span>\n<span class=\"ansi-green-fg ansi-bold\">    260</span> <span style=\"font-weight:bold;color:rgb(0,135,0)\">from</span> <span style=\"font-weight:bold;color:rgb(0,0,255)\">py4j</span><span style=\"font-weight:bold;color:rgb(0,0,255)\">.</span><span style=\"font-weight:bold;color:rgb(0,0,255)\">protocol</span> <span style=\"font-weight:bold;color:rgb(0,135,0)\">import</span> Py4JJavaError\n<span class=\"ansi-green-fg ansi-bold\">    262</span> <span style=\"font-weight:bold;color:rgb(0,135,0)\">try</span>:\n<span class=\"ansi-green-fg\">--&gt; 263</span>     <span style=\"font-weight:bold;color:rgb(0,135,0)\">return</span> <span class=\"ansi-yellow-bg\">f</span><span class=\"ansi-yellow-bg\">(</span><span style=\"color:rgb(98,98,98)\" class=\"ansi-yellow-bg\">*</span><span class=\"ansi-yellow-bg\">a</span><span class=\"ansi-yellow-bg\">,</span><span class=\"ansi-yellow-bg\"> </span><span style=\"color:rgb(98,98,98)\" class=\"ansi-yellow-bg\">*</span><span style=\"color:rgb(98,98,98)\" class=\"ansi-yellow-bg\">*</span><span class=\"ansi-yellow-bg\">kw</span><span class=\"ansi-yellow-bg\">)</span>\n<span class=\"ansi-green-fg ansi-bold\">    264</span> <span style=\"font-weight:bold;color:rgb(0,135,0)\">except</span> Py4JJavaError <span style=\"font-weight:bold;color:rgb(0,135,0)\">as</span> e:\n<span class=\"ansi-green-fg ansi-bold\">    265</span>     converted <span style=\"color:rgb(98,98,98)\">=</span> convert_exception(e<span style=\"color:rgb(98,98,98)\">.</span>java_exception)\n\nFile <span class=\"ansi-green-fg\">~/Documents/IFEBY310/.venv/lib/python3.12/site-packages/py4j/protocol.py:326</span>, in <span class=\"ansi-cyan-fg\">get_return_value</span><span class=\"ansi-blue-fg\">(answer, gateway_client, target_id, name)</span>\n<span class=\"ansi-green-fg ansi-bold\">    324</span> value <span style=\"color:rgb(98,98,98)\">=</span> OUTPUT_CONVERTER[<span style=\"color:rgb(0,135,0)\">type</span>](answer[<span style=\"color:rgb(98,98,98)\">2</span>:], gateway_client)\n<span class=\"ansi-green-fg ansi-bold\">    325</span> <span style=\"font-weight:bold;color:rgb(0,135,0)\">if</span> answer[<span style=\"color:rgb(98,98,98)\">1</span>] <span style=\"color:rgb(98,98,98)\">==</span> REFERENCE_TYPE:\n<span class=\"ansi-green-fg\">--&gt; 326</span>     <span style=\"font-weight:bold;color:rgb(0,135,0)\">raise</span> Py4JJavaError(\n<span class=\"ansi-green-fg ansi-bold\">    327</span>         <span style=\"color:rgb(175,0,0)\">\"</span><span style=\"color:rgb(175,0,0)\">An error occurred while calling </span><span style=\"font-weight:bold;color:rgb(175,95,135)\">{0}</span><span style=\"font-weight:bold;color:rgb(175,95,135)\">{1}</span><span style=\"font-weight:bold;color:rgb(175,95,135)\">{2}</span><span style=\"color:rgb(175,0,0)\">.</span><span style=\"font-weight:bold;color:rgb(175,95,0)\">\\n</span><span style=\"color:rgb(175,0,0)\">\"</span><span style=\"color:rgb(98,98,98)\">.</span>\n<span class=\"ansi-green-fg ansi-bold\">    328</span>         <span style=\"color:rgb(0,135,0)\">format</span>(target_id, <span style=\"color:rgb(175,0,0)\">\"</span><span style=\"color:rgb(175,0,0)\">.</span><span style=\"color:rgb(175,0,0)\">\"</span>, name), value)\n<span class=\"ansi-green-fg ansi-bold\">    329</span> <span style=\"font-weight:bold;color:rgb(0,135,0)\">else</span>:\n<span class=\"ansi-green-fg ansi-bold\">    330</span>     <span style=\"font-weight:bold;color:rgb(0,135,0)\">raise</span> Py4JError(\n<span class=\"ansi-green-fg ansi-bold\">    331</span>         <span style=\"color:rgb(175,0,0)\">\"</span><span style=\"color:rgb(175,0,0)\">An error occurred while calling </span><span style=\"font-weight:bold;color:rgb(175,95,135)\">{0}</span><span style=\"font-weight:bold;color:rgb(175,95,135)\">{1}</span><span style=\"font-weight:bold;color:rgb(175,95,135)\">{2}</span><span style=\"color:rgb(175,0,0)\">. Trace:</span><span style=\"font-weight:bold;color:rgb(175,95,0)\">\\n</span><span style=\"font-weight:bold;color:rgb(175,95,135)\">{3}</span><span style=\"font-weight:bold;color:rgb(175,95,0)\">\\n</span><span style=\"color:rgb(175,0,0)\">\"</span><span style=\"color:rgb(98,98,98)\">.</span>\n<span class=\"ansi-green-fg ansi-bold\">    332</span>         <span style=\"color:rgb(0,135,0)\">format</span>(target_id, <span style=\"color:rgb(175,0,0)\">\"</span><span style=\"color:rgb(175,0,0)\">.</span><span style=\"color:rgb(175,0,0)\">\"</span>, name, value))\n\n<span class=\"ansi-red-fg\">Py4JJavaError</span>: An error occurred while calling o949.load.\n: java.lang.ClassNotFoundException: org.postgresql.Driver\n\tat java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:445)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:593)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:526)\n\tat org.apache.spark.sql.execution.datasources.jdbc.DriverRegistry$.register(DriverRegistry.scala:47)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.$anonfun$driverClass$1(JDBCOptions.scala:112)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.$anonfun$driverClass$1$adapted(JDBCOptions.scala:112)\n\tat scala.Option.foreach(Option.scala:437)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.&lt;init&gt;(JDBCOptions.scala:112)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.&lt;init&gt;(JDBCOptions.scala:42)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:35)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:364)\n\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.org$apache$spark$sql$catalyst$analysis$ResolveDataSource$$loadV1BatchSource(ResolveDataSource.scala:143)\n\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.$anonfun$applyOrElse$2(ResolveDataSource.scala:61)\n\tat scala.Option.getOrElse(Option.scala:201)\n\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:61)\n\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:45)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:139)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:107)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:139)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:416)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:135)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:131)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:37)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:112)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:111)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:37)\n\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:45)\n\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:43)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:248)\n\tat scala.collection.LinearSeqOps.foldLeft(LinearSeq.scala:183)\n\tat scala.collection.LinearSeqOps.foldLeft$(LinearSeq.scala:179)\n\tat scala.collection.immutable.List.foldLeft(List.scala:79)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:245)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:237)\n\tat scala.collection.immutable.List.foreach(List.scala:323)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:237)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:343)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:339)\n\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:224)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:339)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:289)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:207)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:207)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:236)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:91)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:122)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:84)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:322)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:423)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:322)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:139)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:148)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:330)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:717)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:330)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:329)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:139)\n\tat scala.util.Try$.apply(Try.scala:217)\n\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1392)\n\tat org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1453)\n\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:150)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:90)\n\tat org.apache.spark.sql.classic.Dataset$.$anonfun$ofRows$1(Dataset.scala:114)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\n\tat org.apache.spark.sql.classic.Dataset$.ofRows(Dataset.scala:112)\n\tat org.apache.spark.sql.classic.DataFrameReader.load(DataFrameReader.scala:108)\n\tat org.apache.spark.sql.classic.DataFrameReader.load(DataFrameReader.scala:91)\n\tat org.apache.spark.sql.classic.DataFrameReader.load(DataFrameReader.scala:57)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:75)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:580)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\n\tat java.base/java.lang.Thread.run(Thread.java:1583)\n\tSuppressed: org.apache.spark.util.Utils$OriginalTryStackTraceException: Full stacktrace of original doTryWithCallerStacktrace caller\n\t\tat java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:445)\n\t\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:593)\n\t\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:526)\n\t\tat org.apache.spark.sql.execution.datasources.jdbc.DriverRegistry$.register(DriverRegistry.scala:47)\n\t\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.$anonfun$driverClass$1(JDBCOptions.scala:112)\n\t\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.$anonfun$driverClass$1$adapted(JDBCOptions.scala:112)\n\t\tat scala.Option.foreach(Option.scala:437)\n\t\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.&lt;init&gt;(JDBCOptions.scala:112)\n\t\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.&lt;init&gt;(JDBCOptions.scala:42)\n\t\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:35)\n\t\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:364)\n\t\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.org$apache$spark$sql$catalyst$analysis$ResolveDataSource$$loadV1BatchSource(ResolveDataSource.scala:143)\n\t\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.$anonfun$applyOrElse$2(ResolveDataSource.scala:61)\n\t\tat scala.Option.getOrElse(Option.scala:201)\n\t\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:61)\n\t\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:45)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:139)\n\t\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:107)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:139)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:416)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:135)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:131)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:37)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:112)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:111)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:37)\n\t\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:45)\n\t\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:43)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:248)\n\t\tat scala.collection.LinearSeqOps.foldLeft(LinearSeq.scala:183)\n\t\tat scala.collection.LinearSeqOps.foldLeft$(LinearSeq.scala:179)\n\t\tat scala.collection.immutable.List.foldLeft(List.scala:79)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:245)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:237)\n\t\tat scala.collection.immutable.List.foreach(List.scala:323)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:237)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:343)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:339)\n\t\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:224)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:339)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:289)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:207)\n\t\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)\n\t\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:207)\n\t\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:236)\n\t\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:91)\n\t\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:122)\n\t\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:84)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:322)\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:423)\n\t\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:322)\n\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:139)\n\t\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:148)\n\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:330)\n\t\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:717)\n\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:330)\n\t\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\n\t\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:329)\n\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:139)\n\t\tat scala.util.Try$.apply(Try.scala:217)\n\t\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1392)\n\t\tat org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)\n\t\tat org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)\n\t\t... 21 more\n</pre>\n```\n:::\n\n:::\n:::\n\n\n::: {#dbbadf8c .cell execution_count=88}\n``` {.python .cell-code}\nfor k, v in dic_df.items():\n    v.printSchema()\n```\n:::\n\n\nWe can now query the tables. \n\n:::\n\n",
    "supporting": [
      "notebook06_sparksql_files"
    ],
    "filters": [],
    "includes": {}
  }
}