{
  "hash": "5b1292cdbd363e64816be1fda4cf11d5",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: Diving deeer\njupyter: python3\n---\n\n::: {#67050681 .cell execution_count=1}\n``` {.python .cell-code}\nimport os\nimport sys\n\nos.environ['PYSPARK_PYTHON'] = sys.executable\nos.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable\n```\n:::\n\n\n::: {#6adc399a .cell execution_count=2}\n``` {.python .cell-code}\nfrom pyspark import SparkConf, SparkContext\nfrom pyspark.sql import SparkSession\n\nconf = SparkConf().setAppName(\"Spark SQL Course\")\nsc = SparkContext(conf=conf)  # no need for Spark 3...\n\nspark = (SparkSession\n    .builder\n    .appName(\"Spark SQL Course\")\n    .getOrCreate()\n)\n```\n:::\n\n\n::: {#e5f64270 .cell execution_count=3}\n``` {.python .cell-code}\nsc = spark._sc\n```\n:::\n\n\n::: {#a82d39bb .cell execution_count=4}\n``` {.python .cell-code}\nrdd = sc.parallelize([(\"a\", 1), (\"b\", 1), (\"a\", 1)])\n```\n:::\n\n\n::: {#a81b27c3 .cell execution_count=5}\n``` {.python .cell-code}\nrdd.reduceByKey(lambda a, b: a + b).collect()\n```\n:::\n\n\n::: {#dd49a238 .cell execution_count=6}\n``` {.python .cell-code}\nimport requests, zipfile, io\nfrom pathlib import Path\n\npath = Path('webdata.parquet')\nif not path.exists():\n    url = \"https://stephanegaiffas.github.io/big_data_course/data/webdata.parquet.zip\"\n    r = requests.get(url)\n    z = zipfile.ZipFile(io.BytesIO(r.content))\n    z.extractall(path='./')\n```\n:::\n\n\n::: {#8b81d07f .cell execution_count=7}\n``` {.python .cell-code}\ninput_path = './'\n\ninput_file = os.path.join(input_path, 'webdata.parquet')\ndf = spark.read.parquet(input_file)\n```\n:::\n\n\n::: {#5546ab33 .cell execution_count=8}\n``` {.python .cell-code}\ndf.head(6)\n```\n:::\n\n\n::: {#a3a0a431 .cell execution_count=9}\n``` {.python .cell-code}\ndf.describe()\n```\n:::\n\n\n::: {#4d5236fd .cell execution_count=10}\n``` {.python .cell-code}\ndf.count()\n```\n:::\n\n\n# Basic statistics\n\nFirst we need to import some things\n\n::: {#7ceff9fa .cell execution_count=11}\n``` {.python .cell-code}\nfrom pyspark.sql import Window\nimport pyspark.sql.functions as func\nfrom pyspark.sql.types import *\nfrom pyspark.sql.functions import col, lit\n```\n:::\n\n\n## Compute the total number of unique users\n\n::: {#695d377d .cell execution_count=12}\n``` {.python .cell-code}\ndf.select('xid').distinct().count()\n```\n:::\n\n\n## Construct a column containing the total number of actions per user\n\n::: {#2d909cb6 .cell execution_count=13}\n``` {.python .cell-code}\nxid_partition = Window.partitionBy('xid')\nn_events = func.count(col('action')).over(xid_partition)\ndf = df.withColumn('n_events', n_events)\ndf.head(n=2)\n```\n:::\n\n\n::: {#7856172f .cell execution_count=14}\n``` {.python .cell-code}\ndf.groupBy('xid').agg(func.count('action')).head(5)\n```\n:::\n\n\n## Construct a column containing the number of days since the last action of the user\n\n::: {#6fac33e0 .cell execution_count=15}\n``` {.python .cell-code}\nxid_partition = Window.partitionBy('xid')\nmax_date = func.max(col('date')).over(xid_partition)\nn_days_since_last_event = func.datediff(func.current_date(), max_date)\ndf = df.withColumn('n_days_since_last_event',\n                   n_days_since_last_event)\ndf.head(n=2)\n```\n:::\n\n\n## Construct a column containing the number of actions of each user for each modality of device\n\n::: {#465c73a6 .cell execution_count=16}\n``` {.python .cell-code}\nxid_device_partition = Window.partitionBy('xid', 'device')\nn_events_per_device = func.count(col('action')).over(xid_device_partition)\ndf = df.withColumn('n_events_per_device', n_events_per_device)\ndf.head(n=2)\n```\n:::\n\n\n## Number of device per user: some mental gymnastics\n\n::: {#106b3cb4 .cell execution_count=17}\n``` {.python .cell-code}\nxid_partition = Window.partitionBy('xid')\nrank_device = func.dense_rank().over(xid_partition.orderBy('device'))\nn_unique_device = func.last(rank_device).over(xid_partition)\ndf = df.withColumn('n_device', n_unique_device)\ndf.head(n=2)\n```\n:::\n\n\n::: {#b1b2d39a .cell execution_count=18}\n``` {.python .cell-code}\ndf\\\n    .where(col('n_device') > 1)\\\n    .select('xid', 'device', 'n_events',  'n_device', 'n_events_per_device')\\\n    .head(n=8)\n```\n:::\n\n\n# Let's select the correct users and build a training dataset\n\nWe construct a ETL (Extract Transform Load) process on this data using the `pyspark.sql` API.\n\n## Extraction\n\nExtraction is easy here, it's just about reading the data\n\n::: {#13937004 .cell execution_count=19}\n``` {.python .cell-code}\ndf = spark.read.parquet(input_file)\ndf.head(n=3)\n```\n:::\n\n\n## Transformation of the data\n\nAt this step we compute a lot of extra things from the data. The aim is to build features that describe users.\n\n::: {#17281fd2 .cell execution_count=20}\n``` {.python .cell-code}\ndef n_events_transformer(df):\n    xid_partition = Window.partitionBy('xid')\n    n_events = func.count(col('action')).over(xid_partition)\n    df = df.withColumn('n_events', n_events)\n    return df\n\ndef n_events_per_action_transformer(df):\n    xid_action_partition = Window.partitionBy('xid', 'action')\n    n_events_per_action = func.count(col('action')).over(xid_action_partition)\n    df = df.withColumn('n_events_per_action', n_events_per_action)\n    return df\n\ndef hour_transformer(df):\n    hour = func.hour(col('date'))\n    df = df.withColumn('hour', hour)\n    return df\n\ndef weekday_transformer(df):\n    weekday = func.date_format(col('date'), 'EEEE')\n    df = df.withColumn('weekday', weekday)\n    return df\n\ndef n_events_per_hour_transformer(df):\n    xid_hour_partition = Window.partitionBy('xid', 'hour')\n    n_events_per_hour = func.count(col('action')).over(xid_hour_partition)\n    df = df.withColumn('n_events_per_hour', n_events_per_hour)\n    return df\n\ndef n_events_per_weekday_transformer(df):\n    xid_weekday_partition = Window.partitionBy('xid', 'weekday')\n    n_events_per_weekday = func.count(col('action')).over(xid_weekday_partition)\n    df = df.withColumn('n_events_per_weekday', n_events_per_weekday)\n    return df\n\ndef n_days_since_last_event_transformer(df):\n    xid_partition = Window.partitionBy('xid')\n    max_date = func.max(col('date')).over(xid_partition)\n    n_days_since_last_event = func.datediff(func.current_date(), max_date)\n    df = df.withColumn('n_days_since_last_event',\n                       n_days_since_last_event + lit(0.1))\n    return df\n\ndef n_days_since_last_action_transformer(df):\n    xid_partition_action = Window.partitionBy('xid', 'action')\n    max_date = func.max(col('date')).over(xid_partition_action)\n    n_days_since_last_action = func.datediff(func.current_date(),\n                                                        max_date)\n    df = df.withColumn('n_days_since_last_action',\n                       n_days_since_last_action + lit(0.1))\n    return df\n\ndef n_unique_day_transformer(df):\n    xid_partition = Window.partitionBy('xid')\n    dayofyear = func.dayofyear(col('date'))\n    rank_day = func.dense_rank().over(xid_partition.orderBy(dayofyear))\n    n_unique_day = func.last(rank_day).over(xid_partition)\n    df = df.withColumn('n_unique_day', n_unique_day)\n    return df\n\ndef n_unique_hour_transformer(df):\n    xid_partition = Window.partitionBy('xid')\n    rank_hour = func.dense_rank().over(xid_partition.orderBy('hour'))\n    n_unique_hour = func.last(rank_hour).over(xid_partition)\n    df = df.withColumn('n_unique_hour', n_unique_hour)\n    return df\n\ndef n_events_per_device_transformer(df):\n    xid_device_partition = Window.partitionBy('xid', 'device')\n    n_events_per_device = func.count(func.col('device')) \\\n        .over(xid_device_partition)\n    df = df.withColumn('n_events_per_device', n_events_per_device)\n    return df\n\ndef n_unique_device_transformer(df):\n    xid_partition = Window.partitionBy('xid')\n    rank_device = func.dense_rank().over(xid_partition.orderBy('device'))\n    n_unique_device = func.last(rank_device).over(xid_partition)\n    df = df.withColumn('n_device', n_unique_device)\n    return df\n\ndef n_actions_per_category_id_transformer(df):\n    xid_category_id_partition = Window.partitionBy('xid', 'category_id',\n                                                   'action')\n    n_actions_per_category_id = func.count(func.col('action')) \\\n        .over(xid_category_id_partition)\n    df = df.withColumn('n_actions_per_category_id', n_actions_per_category_id)\n    return df\n\ndef n_unique_category_id_transformer(df):\n    xid_partition = Window.partitionBy('xid')\n    rank_category_id = func.dense_rank().over(xid_partition\\\n                                              .orderBy('category_id'))\n    n_unique_category_id = func.last(rank_category_id).over(xid_partition)\n    df = df.withColumn('n_unique_category_id', n_unique_category_id)\n    return df\n\ndef n_events_per_category_id_transformer(df):\n    xid_category_id_partition = Window.partitionBy('xid', 'category_id')\n    n_events_per_category_id = func.count(func.col('action')) \\\n        .over(xid_category_id_partition)\n    df = df.withColumn('n_events_per_category_id', n_events_per_category_id)\n    return df\n\ndef n_events_per_website_id_transformer(df):\n    xid_website_id_partition = Window.partitionBy('xid', 'website_id')\n    n_events_per_website_id = func.count(col('action'))\\\n        .over(xid_website_id_partition)\n    df = df.withColumn('n_events_per_website_id', n_events_per_website_id)\n    return df\n```\n:::\n\n\n::: {#09a33774 .cell execution_count=21}\n``` {.python .cell-code}\ntransformers = [\n    hour_transformer,\n    weekday_transformer,\n    n_events_per_hour_transformer,\n    n_events_per_weekday_transformer,\n    n_days_since_last_event_transformer,\n    n_days_since_last_action_transformer,\n    n_unique_day_transformer,\n    n_unique_hour_transformer,\n    n_events_per_device_transformer,\n    n_unique_device_transformer,\n    n_actions_per_category_id_transformer,\n    n_events_per_category_id_transformer,\n    n_events_per_website_id_transformer,\n]\n\nfor transformer in transformers:\n    df = transformer(df)\n\ndf.head(n=1)\n```\n:::\n\n\n::: {#881402ac .cell execution_count=22}\n``` {.python .cell-code}\nsorted(df.columns)\n```\n:::\n\n\n## Load step\n\nHere, we use all the previous computations (saved in the columns of the dataframe) \nto compute aggregated informations about each user.\n\n::: {#0fa9d0e2 .cell execution_count=23}\n``` {.python .cell-code}\ndef n_events_per_hour_loader(df):\n    csr = df\\\n        .select('xid', 'hour', 'n_events_per_hour')\\\n        .withColumnRenamed('n_events_per_hour', 'value')\\\n        .distinct()\n    feature_name = func.concat(lit('n_events_per_hour#'), col('hour'))\n    csr = csr\\\n        .withColumn('feature_name', feature_name)\\\n        .drop('hour')\n    return csr\n\ndef n_events_per_website_id_loader(df):\n    csr = df.select('xid', 'website_id', 'n_events_per_website_id')\\\n        .withColumnRenamed('n_events_per_hour', 'value')\\\n        .distinct()\n    feature_name = func.concat(lit('n_events_per_website_id#'),\n                               col('website_id'))\n    csr = csr\\\n        .withColumn('feature_name', feature_name)\\\n        .drop('website_id')\n    return csr\n\ndef n_events_per_hour_loader(df):\n    csr = df\\\n        .select('xid', 'hour', 'n_events_per_hour')\\\n        .withColumnRenamed('n_events_per_hour', 'value')\\\n        .distinct()\n    feature_name = func.concat(lit('n_events_per_hour#'), col('hour'))\n    csr = csr\\\n        .withColumn('feature_name', feature_name)\\\n        .drop('hour')\n    return csr\n\ndef n_events_per_weekday_loader(df):\n    csr = df\\\n        .select('xid', 'weekday', 'n_events_per_weekday')\\\n        .withColumnRenamed('n_events_per_weekday', 'value')\\\n        .distinct()\n    feature_name = func.concat(lit('n_events_per_weekday#'), col('weekday'))\n    csr = csr\\\n        .withColumn('feature_name', feature_name)\\\n        .drop('weekday')\n    return csr\n\ndef n_days_since_last_event_loader(df):\n    csr = df.select('xid',  'n_days_since_last_event')\\\n        .withColumnRenamed('n_days_since_last_event#', 'value')\\\n        .distinct()\n    feature_name = lit('n_days_since_last_event')\n    csr = csr\\\n        .withColumn('feature_name', feature_name)\n    return csr\n\ndef n_days_since_last_action_loader(df):\n    csr = df.select('xid', 'action', 'n_days_since_last_action')\\\n        .withColumnRenamed('n_days_since_last_action', 'value')\\\n        .distinct()\n    feature_name = func.concat(lit('n_days_since_last_action#'), col('action'))\n    csr = csr\\\n        .withColumn('feature_name', feature_name)\\\n        .drop('action')\n    return csr\n\ndef n_unique_day_loader(df):\n    csr = df.select('xid', 'n_unique_day')\\\n        .withColumnRenamed('n_unique_day', 'value')\\\n        .distinct()\n    feature_name = lit('n_unique_day')\n    csr = csr\\\n        .withColumn('feature_name', feature_name)\n    return csr\n\ndef n_unique_hour_loader(df):\n    csr = df.select('xid', 'n_unique_hour')\\\n        .withColumnRenamed('n_unique_hour', 'value')\\\n        .distinct()\n    feature_name = lit('n_unique_hour')\n    csr = csr\\\n        .withColumn('feature_name', feature_name)\n    return csr\n\ndef n_events_per_device_loader(df):\n    csr = df\\\n        .select('xid', 'device', 'n_events_per_device')\\\n        .withColumnRenamed('n_events_per_device', 'value')\\\n        .distinct()\n    feature_name = func.concat(lit('n_events_per_device#'), col('device'))\n    csr = csr\\\n        .withColumn('feature_name', feature_name)\\\n        .drop('device')\n    return csr\n\ndef n_unique_device_loader(df):\n    csr = df.select('xid', 'n_device')\\\n        .withColumnRenamed('n_device', 'value')\\\n        .distinct()\n    feature_name = lit('n_device')\n    csr = csr\\\n        .withColumn('feature_name', feature_name)\n    return csr\n\ndef n_events_per_category_id_loader(df):\n    csr = df.select('xid', 'category_id', 'n_events_per_category_id')\\\n        .withColumnRenamed('n_events_per_category_id', 'value')\\\n        .distinct()\n    feature_name = func.concat(lit('n_events_per_category_id#'),\n                               col('category_id'))\n    csr = csr\\\n        .withColumn('feature_name', feature_name)\\\n        .drop('category_id')\n    return csr\n\ndef n_actions_per_category_id_loader(df):\n    csr = df.select('xid', 'category_id', 'action', 'n_actions_per_category_id')\\\n        .withColumnRenamed('n_actions_per_category_id', 'value')\\\n        .distinct()\n    feature_name = func.concat(lit('n_actions_per_category_id#'),\n                               col('action'), lit('#'), \n                               col('category_id'))\n    csr = csr\\\n        .withColumn('feature_name', feature_name)\\\n        .drop('category_id')\\\n        .drop('action')\n    return csr\n\ndef n_events_per_website_id_loader(df):\n    csr = df.select('xid', 'website_id', 'n_events_per_website_id')\\\n        .withColumnRenamed('n_events_per_website_id', 'value')\\\n        .distinct()\n    feature_name = func.concat(lit('n_events_per_website_id#'),\n                               col('website_id'))\n    csr = csr\\\n        .withColumn('feature_name', feature_name)\\\n        .drop('website_id')\n    return csr\n```\n:::\n\n\n::: {#bde3dede .cell execution_count=24}\n``` {.python .cell-code}\nfrom functools import reduce\n\nloaders = [\n    n_events_per_hour_loader,\n    n_events_per_website_id_loader,\n    n_events_per_hour_loader,\n    n_events_per_weekday_loader,\n    n_days_since_last_event_loader,\n    n_days_since_last_action_loader,\n    n_unique_day_loader,\n    n_unique_hour_loader,\n    n_events_per_device_loader,\n    n_unique_device_loader,\n    n_events_per_category_id_loader,\n    n_actions_per_category_id_loader,\n    n_events_per_website_id_loader,\n]\n\ndef union(df, other):\n    return df.union(other)\n\ncsr = reduce(\n    lambda df1, df2: df1.union(df2),\n    [loader(df) for loader in loaders]\n)\n\ncsr.head(n=3)\n```\n:::\n\n\n::: {#8c8c532c .cell execution_count=25}\n``` {.python .cell-code}\ncsr.columns\n```\n:::\n\n\n::: {#b50b778a .cell execution_count=26}\n``` {.python .cell-code}\ncsr.count()\n```\n:::\n\n\n::: {#d89f0719 .cell execution_count=27}\n``` {.python .cell-code}\n# Replace features names and xid by a unique number\nfeature_name_partition = Window().orderBy('feature_name')\nxid_partition = Window().orderBy('xid')\n\ncol_idx = func.dense_rank().over(feature_name_partition)\nrow_idx = func.dense_rank().over(xid_partition)\n\ncsr = csr.withColumn('col', col_idx)\\\n    .withColumn('row', row_idx)\n\ncsr = csr.na.drop('any')\n\ncsr.head(n=5)\n```\n:::\n\n\n::: {#e4ea40d6 .cell execution_count=28}\n``` {.python .cell-code}\n# Let's save the result of our hard work into a new parquet file\noutput_path = './'\noutput_file = os.path.join(output_path, 'csr.parquet')\ncsr.write.parquet(output_file, mode='overwrite')\n```\n:::\n\n\n# Preparation of the training dataset\n\n::: {#64df2c98 .cell execution_count=29}\n``` {.python .cell-code}\ncsr_path = './'\ncsr_file = os.path.join(csr_path, 'csr.parquet')\n\ndf = spark.read.parquet(csr_file)\ndf.head(n=5)\n```\n:::\n\n\n::: {#e36d514c .cell execution_count=30}\n``` {.python .cell-code}\ndf.count()\n```\n:::\n\n\n::: {#11ce228c .cell execution_count=31}\n``` {.python .cell-code}\n# What are the features related to campaign_id 1204 ?\nfeatures_names = \\\n    df.select('feature_name')\\\n    .distinct()\\\n    .toPandas()['feature_name']\n```\n:::\n\n\n::: {#e3647d64 .cell execution_count=32}\n``` {.python .cell-code}\nfeatures_names\n```\n:::\n\n\n::: {#2da9763b .cell execution_count=33}\n``` {.python .cell-code}\n[feature_name for feature_name in features_names if '1204' in feature_name]\n```\n:::\n\n\n::: {#2b49cda6 .cell execution_count=34}\n``` {.python .cell-code}\n# Look for the xid that have at least one exposure to campaign 1204\nkeep = func.when(\n    (col('feature_name') == 'n_actions_per_category_id#C#1204.0') |\n    (col('feature_name') == 'n_actions_per_category_id#O#1204.0'),\n    1).otherwise(0)\ndf = df.withColumn('keep', keep)\n\ndf.where(col('keep') > 0).count()\n```\n:::\n\n\n::: {#94d08cc0 .cell execution_count=35}\n``` {.python .cell-code}\n# Sum of the keeps :)\nxid_partition = Window.partitionBy('xid')\nsum_keep = func.sum(col('keep')).over(xid_partition)\ndf = df.withColumn('sum_keep', sum_keep)\n```\n:::\n\n\n::: {#1a14730d .cell execution_count=36}\n``` {.python .cell-code}\n# Let's keep the xid exposed to 1204\ndf = df.where(col('sum_keep') > 0)\n```\n:::\n\n\n::: {#35eda6cd .cell execution_count=37}\n``` {.python .cell-code}\ndf.count()\n```\n:::\n\n\n::: {#96b8b970 .cell execution_count=38}\n``` {.python .cell-code}\ndf.select('xid').distinct().count()\n```\n:::\n\n\n::: {#5d12f76f .cell execution_count=39}\n``` {.python .cell-code}\nrow_partition = Window().orderBy('row')\ncol_partition = Window().orderBy('col')\nrow_new = func.dense_rank().over(row_partition)\ncol_new = func.dense_rank().over(col_partition)\ndf = df.withColumn('row_new', row_new)\ndf = df.withColumn('col_new', col_new)\ncsr_data = df.select('row_new', 'col_new', 'value').toPandas()\n```\n:::\n\n\n::: {#24f81387 .cell execution_count=40}\n``` {.python .cell-code}\ncsr_data.head()\n```\n:::\n\n\n::: {#d7ec2bad .cell execution_count=41}\n``` {.python .cell-code}\nfeatures_names = df.select('feature_name', 'col_new').distinct()\nfeatures_names.where(col('feature_name') == 'n_actions_per_category_id#C#1204.0').head()\n```\n:::\n\n\n::: {#dba73f68 .cell execution_count=42}\n``` {.python .cell-code}\nfeatures_names.where(col('feature_name') == 'n_actions_per_category_id#O#1204.0').head()\n```\n:::\n\n\n::: {#10126386 .cell execution_count=43}\n``` {.python .cell-code}\nfrom scipy.sparse import csr_matrix\nimport numpy as np\n\nrows = csr_data['row_new'].values - 1\ncols = csr_data['col_new'].values - 1\nvals = csr_data['value'].values\n\nX_csr = csr_matrix((vals, (rows, cols)))\n```\n:::\n\n\n::: {#5bfb7272 .cell execution_count=44}\n``` {.python .cell-code}\nX_csr.shape\n```\n:::\n\n\n::: {#74e2dd38 .cell execution_count=45}\n``` {.python .cell-code}\nX_csr.shape, X_csr.nnz\n```\n:::\n\n\n::: {#d1932b32 .cell execution_count=46}\n``` {.python .cell-code}\nX_csr.nnz / (152347 * 92)\n```\n:::\n\n\n::: {#3d278b1e .cell execution_count=47}\n``` {.python .cell-code}\n# The label vector. Let's make it dense, flat and binary\ny = np.array(X_csr[:, 1].todense()).ravel()\ny = np.array(y > 0, dtype=np.int64)\n```\n:::\n\n\n::: {#67045821 .cell execution_count=48}\n``` {.python .cell-code}\nX_csr.shape\n```\n:::\n\n\n::: {#466d70bc .cell execution_count=49}\n``` {.python .cell-code}\n# We remove the second and fourth column. \n# It actually contain the label we'll want to predict.\nkept_cols = list(range(92))\nkept_cols.pop(1)\nkept_cols.pop(2)\nX = X_csr[:, kept_cols]\n```\n:::\n\n\n::: {#a64404ca .cell execution_count=50}\n``` {.python .cell-code}\nX_csr.shape\n```\n:::\n\n\n## Finally !!\n\nWow ! That was a lot of work. Now we have a features matrix $X$ and a vector of labels $y$.\n\n::: {#d0d5e084 .cell execution_count=51}\n``` {.python .cell-code}\nX.indices\n```\n:::\n\n\n::: {#6bdb0151 .cell execution_count=52}\n``` {.python .cell-code}\nX.indptr\n```\n:::\n\n\n::: {#b8df722b .cell execution_count=53}\n``` {.python .cell-code}\nX.shape, X.nnz\n```\n:::\n\n\n::: {#2d7871f4 .cell execution_count=54}\n``` {.python .cell-code}\ny.shape, y.sum()\n```\n:::\n\n\n# Some learning for this data\n\n::: {#432b408b .cell execution_count=55}\n``` {.python .cell-code}\nfrom sklearn.preprocessing import MaxAbsScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\n\n# Normalize the features\nX = MaxAbsScaler().fit_transform(X)\nX_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.3)\n\nclf = LogisticRegression(\n    penalty='l2',\n    C=1e3,\n    solver='lbfgs',\n    class_weight='balanced'\n)\nclf.fit(X_train, y_train)\n```\n:::\n\n\n::: {#e9516e42 .cell execution_count=56}\n``` {.python .cell-code}\nfeatures_names = features_names.toPandas()['feature_name']\n```\n:::\n\n\n::: {#4ad8e387 .cell execution_count=57}\n``` {.python .cell-code}\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nplt.figure(figsize=(16, 5))\nplt.stem(clf.coef_[0], use_line_collection=True)\nplt.title('Logistic regression coefficients', fontsize=18)\n# We change the fontsize of minor ticks label\n_ = plt.xticks(np.arange(clf.coef_[0].shape[0]), features_names, \n           rotation='vertical', fontsize=8)\n_ = plt.yticks(fontsize=14)\n```\n:::\n\n\n::: {#255eef79 .cell execution_count=58}\n``` {.python .cell-code}\nfrom sklearn.metrics import precision_recall_curve, f1_score\n\nprecision, recall, _ = precision_recall_curve(y_test, clf.predict_proba(X_test)[:, 1])\n    \nplt.figure(figsize=(8, 6))\nplt.plot(recall, precision, label='LR (F1=%.2f)' % f1_score(y_test, clf.predict(X_test)), lw=2)\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('Recall', fontsize=16)\nplt.ylabel('Precision', fontsize=16)\nplt.title('Precision/recall curve', fontsize=18)\nplt.legend(loc=\"upper right\", fontsize=14)\n```\n:::\n\n\n---\njupyter:\n  kernelspec:\n    display_name: Python 3 (ipykernel)\n    language: python\n    name: python3\n    path: /usr/share/jupyter/kernels/python3\n  language_info:\n    codemirror_mode:\n      name: ipython\n      version: 3\n    file_extension: .py\n    mimetype: text/x-python\n    name: python\n    nbconvert_exporter: python\n    pygments_lexer: ipython3\n    version: 3.12.3\n---\n",
    "supporting": [
      "notebook11_dive_files"
    ],
    "filters": []
  }
}