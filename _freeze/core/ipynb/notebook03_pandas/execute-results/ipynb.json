{
  "hash": "0d3923b92ce939762392d4327f8d4643",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: Introduction to `pandas`\njupyter: python3\n---\n\n\nThe `pandas` library (https://pandas.pydata.org) is one of the most used tool at the disposal of people working with data in `python` today.\n\n- It allows to **crunch data** easily\n- It mainly provides a `DataFrame` object (a **table of data**) with a huge set of functionalities\n\n\n## Why ?\n\nThrough `pandas`, you get acquainted with your data by **analyzing** it \n\n- What's the average, median, max, or min of each column?\n- Does column A correlate with column B?\n- What does the distribution of data in column C look like?\n\n## Why  (con't) ?\n\nyou get acquainted with your data by **cleaning** and  **transforming** it \n\n- Removing missing values, filter rows or columns using some criteria\n- Store the cleaned, transformed data back into virtually any format or database\n- Data visualization (when combined `matplotlib`, `seaborn`, `plotly` or others)\n\n## Where ?\n\n`pandas` is a central component of the `python` stack for data science\n\n- `Pandas` is built on top of `NumPy`\n- often used in conjunction with other libraries\n- a `DataFrame` is often fed to plotting functions or machine learning algorithms (such as `scikit-learn`)\n- Well-interfaced with `jupyter`, leading to a nice interactive environment for data exploration and modeling\n\n## Core components of pandas\n\nThe two primary components of `Pandas` are the `Series` and `DataFrame`.\n\n- A `Series` is essentially a column\n\n- A `DataFrame` is a multi-dimensional table made up of a collection of `Series` with equal length\n\n## Creating a `DataFrame` from scratch\n\n::: {#df_fruits .cell execution_count=1}\n``` {.python .cell-code}\nimport pandas as pd\n\nfruits = {\n    \"apples\": [3, 2, 0, 1],\n    \"oranges\": [0, 3, 7, 2]\n}\n\ndf_fruits = pd.DataFrame(fruits)\ndf_fruits\n```\n:::\n\n\n::: {#05214299 .cell lavel='type_df' execution_count=2}\n``` {.python .cell-code}\ntype(df_fruits)\n```\n:::\n\n\n::: {#display_series_apple .cell execution_count=3}\n``` {.python .cell-code}\ndf_fruits[\"apples\"]\n```\n:::\n\n\n::: {#type_apples .cell execution_count=4}\n``` {.python .cell-code}\ntype(df_fruits[\"apples\"])\n```\n:::\n\n\n## Indexing\n\n- By default, a `DataFrame` uses a contiguous index\n- But what if we want to say **who** buys the fruits ?\n\n::: {#setting_up_index .cell execution_count=5}\n``` {.python .cell-code}\ndf_fruits = pd.DataFrame(fruits, index=[\"Daniel\", \"Sean\", \"Pierce\", \"Roger\"])\ndf_fruits\n```\n:::\n\n\n## `.loc` versus `.iloc`\n\n- `.loc` **loc**ates by name\n- `.iloc` **loc**ates by numerical **i**ndex\n\n::: {#d80fe567 .cell execution_count=6}\n``` {.python .cell-code}\ndf_fruits\n```\n:::\n\n\nWe can pick rows \n\n::: {#locating_information_by_name .cell execution_count=7}\n``` {.python .cell-code}\n# What's in Sean's basket ?\ndf_fruits.loc['Sean']\n```\n:::\n\n\nNote that this returns a `Series`\n\nWe can pick slices of rows and columns\n\n::: {#7e9c2841 .cell execution_count=8}\n``` {.python .cell-code}\n# Who has oranges ?\ndf_fruits.loc[:, 'oranges']\n```\n:::\n\n\n::: {#a00eb4f9 .cell execution_count=9}\n``` {.python .cell-code}\n# How many apples in Pierce's basket ?\ndf_fruits.loc['Pierce', 'apples']\n```\n:::\n\n\nNote that the type of the result depends on the indexing information.\n\n::: {#6343a7ca .cell execution_count=10}\n``` {.python .cell-code}\ndf_fruits\n```\n:::\n\n\nWe may also pick information through positions using `iloc`. \n\n::: {#126149cf .cell execution_count=11}\n``` {.python .cell-code}\ndf_fruits.iloc[2, 1]\n```\n:::\n\n\nNote that the DataFrame has two index:\n\n::: {#two-index-in-df .cell execution_count=12}\n``` {.python .cell-code}\ndf_fruits.index \ndf_fruits.columns\n```\n:::\n\n\n## Main attributes and methods of a `DataFrame`\n\nA `DataFrame` has many **attributes**\n\n::: {#column-attribute .cell execution_count=13}\n``` {.python .cell-code}\ndf_fruits.columns\n```\n:::\n\n\n::: {#ze-index-attribute .cell execution_count=14}\n``` {.python .cell-code}\ndf_fruits.index\n```\n:::\n\n\n::: {#dtypes-attribute .cell execution_count=15}\n``` {.python .cell-code}\ndf_fruits.dtypes\n```\n:::\n\n\nA `DataFrame` has many **methods**\n\n\nMethod `info()` provides information on the table schema, name and type columns, whether the cells can contain missing values.\n\n::: {#info-method .cell execution_count=16}\n``` {.python .cell-code}\ndf_fruits.info()\n```\n:::\n\n\nMethod `describe()` provides with statistical summaries for columns  \n\n::: {#describe-method .cell execution_count=17}\n``` {.python .cell-code}\ndf_fruits.describe()\n```\n:::\n\n\n## Missing values\n\nWhat if we don't know how many apples are in Sean's basket ?\n\n::: {#nulling-some-cells .cell execution_count=18}\n``` {.python .cell-code}\ndf_fruits.loc['Sean', 'apples'] = None\ndf_fruits\n```\n:::\n\n\n`None` is a Python keyword. `NaN` belongs to `Pandas`. \n\n::: {#describe-with-nulls .cell execution_count=19}\n``` {.python .cell-code}\ndf_fruits.describe()\n```\n:::\n\n\nNote that `count` is **3** for apples now, since we have 1 missing value among the 4\n\n\n::: {.callout-note}\n\nTo review the members of objects of class `pandas.DataFrame`, `dir()` and module `inspect` are convenient.\n\n:::\n\n::: {#list-non-dunder-attributes .cell execution_count=20}\n``` {.python .cell-code}\n[x for x in dir(df_fruits) if not x.startswith('_') and not callable(x)]\n```\n:::\n\n\n::: {#26b62bdb .cell execution_count=21}\n``` {.python .cell-code}\nimport inspect\n\n# Get a list of methods\nmembres = inspect.getmembers(df_fruits)\n\nmethod_names = [m[0] for m in membres \n    if callable(m[1]) and not m[0].startswith('_')]\n\nprint(method_names)\n```\n:::\n\n\nDataFrames have more than $400$ *members*. Among them, almost $200$ are what we call *methods*. \nSee [Dataframe documentation](https://pandas.pydata.org/docs/reference/frame.html)\n\nAmong non-callable members, we find genuine data attributes and properties. \n\n::: {#2521f10f .cell execution_count=22}\n``` {.python .cell-code}\nothers = [x for x in membres\n    if not callable(x[1])]\n\n[x[0] for x in others if not x[0].startswith('_')]\n```\n:::\n\n\n## Adding a column\n\nOoooops, we forgot about the bananas !\n\n::: {#27d469a0 .cell execution_count=23}\n``` {.python .cell-code}\ndf_fruits[\"bananas\"] = [0, 2, 1, 6]\ndf_fruits\n```\n:::\n\n\nThis amounts to add an entry in the *columns* index.\n\n::: {#fcfb119a .cell execution_count=24}\n``` {.python .cell-code}\ndf_fruits.columns\n```\n:::\n\n\n## Adding a column with the date\n\nAnd we forgot the dates!\n\n::: {#3ebe5396 .cell execution_count=25}\n``` {.python .cell-code}\ndf_fruits['time'] = [\n    \"2020/10/08 12:13\", \"2020/10/07 11:37\", \n    \"2020/10/10 14:07\", \"2020/10/09 10:51\"\n]\ndf_fruits\n```\n:::\n\n\n::: {#8c12b14e .cell execution_count=26}\n``` {.python .cell-code}\ndf_fruits.dtypes\n```\n:::\n\n\n::: {#b5177e4a .cell execution_count=27}\n``` {.python .cell-code}\ntype(df_fruits.loc[\"Roger\", \"time\"])\n```\n:::\n\n\nIt is not a date but a string (`str`) ! So we convert this column to something called `datetime`  \n\n::: {#fc059750 .cell execution_count=28}\n``` {.python .cell-code}\ndf_fruits[\"time\"] = pd.to_datetime(df_fruits[\"time\"])\ndf_fruits\n```\n:::\n\n\n::: {#cfdda475 .cell execution_count=29}\n``` {.python .cell-code}\ndf_fruits.dtypes\n```\n:::\n\n\n::: {.callout-note}\n\nEvery data science framework implements some `datetime` handling scheme. For Python see [Python official documentation on `datetime` module](https://docs.python.org/3/library/datetime.html#module-datetime)\n\nNote that `datetime64[ns]`  parallels NumPy `datetime64`.  \n\n\n:::\n\nWhat if we want to keep only the baskets after (including) October, 9th ?\n\n::: {#32a7aa02 .cell execution_count=30}\n``` {.python .cell-code}\ndf_fruits.loc[df_fruits[\"time\"] >= pd.Timestamp(\"2020/10/09\")]\n```\n:::\n\n\nWe can filter rows using a boolean mask and member `loc`. This does not work with `iloc`.\n\n## Casting a Series to another type\n\n\nIn many circumstances, we have to cast columns to a different type. \nTo convert a Pandas Series to a different data type, we may use \nthe `.astype()` method:\n\n::: {#f29df299 .cell execution_count=31}\n``` {.python .cell-code}\n# Create a sample Series\ns = pd.Series([1, 2, 3, 4, 5])\ns\n```\n:::\n\n\n::: {#376636f5 .cell execution_count=32}\n``` {.python .cell-code}\n# Check the current dtype\ns.dtype\n```\n:::\n\n\nIf we want to move to `float`:\n\n::: {#a135f81d .cell execution_count=33}\n``` {.python .cell-code}\n# Cast to float\ns_float = s.astype('float64')\ns_float\n```\n:::\n\n\nto strings:\n\n::: {#cccc93cf .cell execution_count=34}\n``` {.python .cell-code}\n# Cast to string\ns_str = s.astype('str')\ns_str\n```\n:::\n\n\nto a categorical type:\n\n::: {#b29dc658 .cell execution_count=35}\n``` {.python .cell-code}\n# Cast to category\ns_cat = s.astype('category')\ns_cat\n```\n:::\n\n\nThis also works for `bool`. \n\nSometimes, it may go wrong. \n\n### Handling errors during conversion\n\nWhen converting types, you may encounter errors if the conversion is not possible:\n\n::: {#7c96e57e .cell execution_count=36}\n``` {.python .cell-code}\n# This will raise an error if conversion fails\ntry:\n    pd.Series(['1', '2', 'abc']).astype('int64')\nexcept ValueError as e:\n    print(f\"Error: {e}\")\n```\n:::\n\n\nThen method `astype()` may not be the best choice. \nFor more robust conversion, use `pd.to_numeric()` with error handling:\n\n::: {#7b1ccc07 .cell execution_count=37}\n``` {.python .cell-code}\n# Convert with error handling - invalid values become NaN\npd.to_numeric(pd.Series(['1', '2', 'abc']), errors='coerce')\n```\n:::\n\n\nFor datetime conversions, `pd.to_datetime()` is usually preferred over `.astype('datetime64[ns]')` as it handles various date formats more robustly.\n\n\n\n\n## Slices and subsets of rows or columns\n\n::: {#c4beebe9 .cell execution_count=38}\n``` {.python .cell-code}\ndf_fruits\n```\n:::\n\n\n::: {#1f8e3882 .cell execution_count=39}\n``` {.python .cell-code}\ndf_fruits.loc[:, \"oranges\":\"time\"]\n```\n:::\n\n\n::: {#6de5c6d7 .cell execution_count=40}\n``` {.python .cell-code}\ndf_fruits.loc[\"Daniel\":\"Sean\", \"apples\":\"bananas\"]\n```\n:::\n\n\nIf we want to project over a collection of columns, \nwe have to  \n\n::: {#69b0e25a .cell execution_count=41}\n``` {.python .cell-code}\ndf_fruits[[\"apples\", \"time\"]]\n```\n:::\n\n\n::: {#47438962 .cell execution_count=42}\n``` {.python .cell-code}\ntropicals = (\"apples\", \"oranges\")\n\ndf_fruits[[*tropicals]]\n```\n:::\n\n\nWe cannot write: \n\n::: {#260555cf .cell execution_count=43}\n``` {.python .cell-code}\ndf_fruits[\"apples\", \"time\"]\n```\n:::\n\n\nWhy?\n\n## Write our data to a CSV file\n\nWhat if we want to write the file ?\n\n::: {#eeb2ef96 .cell execution_count=44}\n``` {.python .cell-code}\ndf_fruits\n```\n:::\n\n\n::: {#5b3a06b9 .cell execution_count=45}\n``` {.python .cell-code}\ndf_fruits.to_csv(\"fruits.csv\")\n```\n:::\n\n\n::: {#1162e9b7 .cell execution_count=46}\n``` {.python .cell-code}\n# Use !dir on windows\n!ls -alh | grep fru\n```\n:::\n\n\n::: {#af3065e6 .cell execution_count=47}\n``` {.python .cell-code}\n!head -n 5 fruits.csv\n```\n:::\n\n\n## Reading data and working with it\n\n\n\n::: {.callout-note}\n\nThe `tips` dataset comes through [Kaggle](https://www.kaggle.com/code/sanjanabasu/tips-dataset/input)\n\n> This dataset is a treasure trove of information from a collection of case studies for business statistics. Special thanks to Bryant and Smith for their diligent work:\n\n> Bryant, P. G. and Smith, M (1995) Practical Data Analysis: Case Studies in Business Statistics. Homewood, IL: Richard D. Irwin Publishing.\n\n> You can also access this dataset now through the Python package Seaborn.\n\n:::\n\nIt contains data about a restaurant: the bill, tip and some informations about the customers.\n\n::: {.callout-note}\n\n### A toy extraction pattern\n\nA data pipeline usually starts with Extraction, that is gathering data from some source, possibly in a galaxy far, far awy. Here follows a toy extraction pattern\n\n- obtain the data from some `URL` using package `requests`\n- save the data on the hard drive\n- load the data using Pandas \n\n::: {#24bbe6e1 .cell execution_count=48}\n``` {.python .cell-code}\nimport requests\nimport os\n\n# The path containing your notebook\npath_data = './'\n# The name of the file\nfilename = 'tips.csv'\n\nif os.path.exists(os.path.join(path_data, filename)):\n    print(f'The file {os.path.join(path_data, filename)} already exists.')\nelse:\n    url = 'https://raw.githubusercontent.com/mwaskom/seaborn-data/refs/heads/master/tips.csv'\n    r = requests.get(url)\n    with open(os.path.join(path_data, filename), 'wb') as f:\n        f.write(r.content)\n    print('Downloaded file %s.' % os.path.join(path_data, filename))\n```\n:::\n\n\n::: {#b417e589 .cell execution_count=49}\n``` {.python .cell-code}\ndf = pd.read_csv(\n    \"tips.csv\", \n    delimiter=\",\"\n)\n```\n:::\n\n\n:::\n\nThe data can be obtained from package `seaborn`.\n\n::: {#3641f279 .cell execution_count=50}\n``` {.python .cell-code}\nimport seaborn as sns\n\nsns_ds = sns.get_dataset_names()\n\n'tips' in sns_ds\n\ndf = sns.load_dataset('tips')\n```\n:::\n\n\nNote that the dataframe loaded from the `csv` file and the dataframe obtained from package `seaborn` differ. This can be checked by examining the representations and properties of column `smoker, sex` (check `df.sex.array` in both cases)\n\n::: {#a08287d5 .cell execution_count=51}\n``` {.python .cell-code}\n# `.head()` shows the first rows of the dataframe\ndf.head(n=10)\n```\n:::\n\n\n::: {#33d7f51c .cell execution_count=52}\n``` {.python .cell-code}\ndf.info()\n```\n:::\n\n\n::: {#a10d4671 .cell execution_count=53}\n``` {.python .cell-code}\ndf.loc[42, \"day\"]\n```\n:::\n\n\n::: {#427f8536 .cell execution_count=54}\n``` {.python .cell-code}\ntype(df.loc[42, \"day\"])\n```\n:::\n\n\nBy default, columns that are non-numerical contain strings (`str` type)\n\n## The `category` type\n\nAn important type in `pandas` is `category` for variables that are **non-numerical**\n\n**Pro tip.** It's always a good idea to tell `pandas` which columns should be imported as **categorical**\n\nSo, let's read again the file specifying some `dtype`s to the `read_csv` function\n\n::: {#3e76651c .cell execution_count=55}\n``` {.python .cell-code}\ndtypes = {\n    \"sex\": \"category\",\n    \"smoker\": \"category\",\n    \"day\": \"category\",\n    \"time\": \"category\"\n} \n\ndf = pd.read_csv(\"tips.csv\", dtype=dtypes)\n```\n:::\n\n\nSupplemented with this typing information, the dataframe loaded from the `csv` file is more like the dataframe obtained from `seaborn`. \n\n::: {#bfb54d2c .cell execution_count=56}\n``` {.python .cell-code}\ndf.dtypes\n```\n:::\n\n\n## Computing statistics\n\n::: {#1278b961 .cell execution_count=57}\n``` {.python .cell-code}\n# The describe method only shows statistics for the numerical columns by default\ndf.describe()\n```\n:::\n\n\n::: {#bac89ae7 .cell execution_count=58}\n``` {.python .cell-code}\n# We use the include=\"all\" option to see everything\ndf.describe(include=\"all\")\n```\n:::\n\n\n::: {#6f09b3a6 .cell execution_count=59}\n``` {.python .cell-code}\n# Correlation between the numerical columns\ndf.corr(numeric_only = True)\n```\n:::\n\n\nIn more general settings, to select only numerical columns from a DataFrame, use the `select_dtypes()` method:\n\n::: {#2bbc2512 .cell execution_count=60}\n``` {.python .cell-code}\n# Select only numerical columns (int, float, etc.)\n(\n    df\n        .select_dtypes(include=['number'])\n        .head()\n)\n```\n:::\n\n\n::: {.callout-note}\n\nThe `select_dtypes()` method is very flexible:\n- `include=['number']` selects all numeric types (int, float, etc.)\n- `include=['int64', 'float64']` selects specific dtypes\n- `exclude=['object']` excludes string columns\n- You can combine `include` and `exclude` parameters\n\n:::\n\n::: {#4dac8fd1 .cell execution_count=61}\n``` {.python .cell-code}\n(\n    df\n        .select_dtypes(include='float64')\n        .corr()\n)\n```\n:::\n\n\n# Plotting backends for Pandas DataFrames\n\nPandas DataFrames have built-in plotting capabilities through the `.plot` accessor. By default, Pandas uses `matplotlib` as the plotting backend.\n\n## Setting the plotting backend\n\nYou can set the plotting backend using `pd.options.plotting.backend`:\n\n::: {#90291ed9 .cell execution_count=62}\n``` {.python .cell-code}\n# Set the backend globally for all DataFrames\npd.options.plotting.backend = 'matplotlib'  # default\n```\n:::\n\n\nAvailable backends include:\n\n- `'matplotlib'` (default) \n- `'plotly'` (requires `plotly` package)\n- `'hvplot'` (requires `hvplot` package)\n\n::: {.callout-caution}\n\nSeaborn is **not** available as a plotting backend for `pd.options.plotting.backend`. Seaborn is a separate visualization library built on top of matplotlib that works directly with pandas DataFrames through its own API (e.g., `sns.scatterplot(data=df, ...)`). While seaborn integrates seamlessly with pandas DataFrames, it doesn't replace the `.plot` accessor's backend system.\n\n:::\n\n## Using different backends\n\n### Matplotlib (default)\n\n::: {#461f9b5f .cell execution_count=63}\n``` {.python .cell-code}\n# Default matplotlib backend\npd.options.plotting.backend = 'matplotlib'\ndf.plot.scatter(x='total_bill', y='tip')\n```\n\n::: {.cell-output .cell-output-error}\n\n::: {.ansi-escaped-output}\n```{=html}\n<pre><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">NameError</span>                                 Traceback (most recent call last)\nCell <span class=\"ansi-green-fg\">In[1], line 2</span>\n<span class=\"ansi-green-fg ansi-bold\">      1</span> <span style=\"font-style:italic;color:rgb(95,135,135)\"># Default matplotlib backend</span>\n<span class=\"ansi-green-fg\">----&gt; 2</span> <span class=\"ansi-yellow-bg\">pd</span><span style=\"color:rgb(98,98,98)\">.</span>options<span style=\"color:rgb(98,98,98)\">.</span>plotting<span style=\"color:rgb(98,98,98)\">.</span>backend <span style=\"color:rgb(98,98,98)\">=</span> <span style=\"color:rgb(175,0,0)\">'</span><span style=\"color:rgb(175,0,0)\">matplotlib</span><span style=\"color:rgb(175,0,0)\">'</span>\n<span class=\"ansi-green-fg ansi-bold\">      3</span> df<span style=\"color:rgb(98,98,98)\">.</span>plot<span style=\"color:rgb(98,98,98)\">.</span>scatter(x<span style=\"color:rgb(98,98,98)\">=</span><span style=\"color:rgb(175,0,0)\">'</span><span style=\"color:rgb(175,0,0)\">total_bill</span><span style=\"color:rgb(175,0,0)\">'</span>, y<span style=\"color:rgb(98,98,98)\">=</span><span style=\"color:rgb(175,0,0)\">'</span><span style=\"color:rgb(175,0,0)\">tip</span><span style=\"color:rgb(175,0,0)\">'</span>)\n\n<span class=\"ansi-red-fg\">NameError</span>: name 'pd' is not defined</pre>\n```\n:::\n\n:::\n:::\n\n\n### Plotly backend\n\n::: {#8b8120bf .cell execution_count=64}\n``` {.python .cell-code}\n# Switch to plotly for interactive plots\npd.options.plotting.backend = 'plotly'\ndf.plot.scatter(x='total_bill', y='tip')  # Now creates an interactive plotly plot\n```\n\n::: {.cell-output .cell-output-error}\n\n::: {.ansi-escaped-output}\n```{=html}\n<pre><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">NameError</span>                                 Traceback (most recent call last)\nCell <span class=\"ansi-green-fg\">In[2], line 2</span>\n<span class=\"ansi-green-fg ansi-bold\">      1</span> <span style=\"font-style:italic;color:rgb(95,135,135)\"># Switch to plotly for interactive plots</span>\n<span class=\"ansi-green-fg\">----&gt; 2</span> <span class=\"ansi-yellow-bg\">pd</span><span style=\"color:rgb(98,98,98)\">.</span>options<span style=\"color:rgb(98,98,98)\">.</span>plotting<span style=\"color:rgb(98,98,98)\">.</span>backend <span style=\"color:rgb(98,98,98)\">=</span> <span style=\"color:rgb(175,0,0)\">'</span><span style=\"color:rgb(175,0,0)\">plotly</span><span style=\"color:rgb(175,0,0)\">'</span>\n<span class=\"ansi-green-fg ansi-bold\">      3</span> df<span style=\"color:rgb(98,98,98)\">.</span>plot<span style=\"color:rgb(98,98,98)\">.</span>scatter(x<span style=\"color:rgb(98,98,98)\">=</span><span style=\"color:rgb(175,0,0)\">'</span><span style=\"color:rgb(175,0,0)\">total_bill</span><span style=\"color:rgb(175,0,0)\">'</span>, y<span style=\"color:rgb(98,98,98)\">=</span><span style=\"color:rgb(175,0,0)\">'</span><span style=\"color:rgb(175,0,0)\">tip</span><span style=\"color:rgb(175,0,0)\">'</span>)  <span style=\"font-style:italic;color:rgb(95,135,135)\"># Now creates an interactive plotly plot</span>\n\n<span class=\"ansi-red-fg\">NameError</span>: name 'pd' is not defined</pre>\n```\n:::\n\n:::\n:::\n\n\n### Per-plot backend selection\n\nYou can also specify the backend for a specific plot without changing the global setting:\n\n::: {#cb7479e5 .cell execution_count=65}\n``` {.python .cell-code}\n# Use a specific backend for one plot only\ndf.plot(backend='plotly', kind='scatter', x='total_bill', y='tip')\n```\n\n::: {.cell-output .cell-output-error}\n\n::: {.ansi-escaped-output}\n```{=html}\n<pre><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">NameError</span>                                 Traceback (most recent call last)\nCell <span class=\"ansi-green-fg\">In[3], line 2</span>\n<span class=\"ansi-green-fg ansi-bold\">      1</span> <span style=\"font-style:italic;color:rgb(95,135,135)\"># Use a specific backend for one plot only</span>\n<span class=\"ansi-green-fg\">----&gt; 2</span> <span class=\"ansi-yellow-bg\">df</span><span style=\"color:rgb(98,98,98)\">.</span>plot(backend<span style=\"color:rgb(98,98,98)\">=</span><span style=\"color:rgb(175,0,0)\">'</span><span style=\"color:rgb(175,0,0)\">plotly</span><span style=\"color:rgb(175,0,0)\">'</span>, kind<span style=\"color:rgb(98,98,98)\">=</span><span style=\"color:rgb(175,0,0)\">'</span><span style=\"color:rgb(175,0,0)\">scatter</span><span style=\"color:rgb(175,0,0)\">'</span>, x<span style=\"color:rgb(98,98,98)\">=</span><span style=\"color:rgb(175,0,0)\">'</span><span style=\"color:rgb(175,0,0)\">total_bill</span><span style=\"color:rgb(175,0,0)\">'</span>, y<span style=\"color:rgb(98,98,98)\">=</span><span style=\"color:rgb(175,0,0)\">'</span><span style=\"color:rgb(175,0,0)\">tip</span><span style=\"color:rgb(175,0,0)\">'</span>)\n\n<span class=\"ansi-red-fg\">NameError</span>: name 'df' is not defined</pre>\n```\n:::\n\n:::\n:::\n\n\n::: {.callout-important}\n\n- The backend must be installed separately (e.g., `pip install plotly` for plotly backend)\n- Different backends support different plot types and options\n- The `matplotlib` backend is always available and is the default\n- Backend settings are session-wide until changed\n\n:::\n\n# Data visualization with `matplotlib` and `seaborn`\n\nLet's show how we can use `matplotlib` and `seaborn` to visualize data contained in a `pandas` dataframe\n\n::: {#0601486d .cell execution_count=66}\n``` {.python .cell-code}\nimport matplotlib.pyplot as plt\n```\n:::\n\n\n## How do the tip depends on the total bill ?\n\n::: {#b978cfe0 .cell execution_count=67}\n``` {.python .cell-code}\nsns.jointplot(x=\"total_bill\", y=\"tip\", data=df)\n```\n:::\n\n\nA `jointplot` (as in seaborn) is an enriched `scatterplot` with histograms on both axes.\n\n## When do customers go to this restaurant ?\n\n::: {#a865a69b .cell execution_count=68}\n``` {.python .cell-code}\nsns.countplot(x='day', hue=\"time\", data=df)\n```\n:::\n\n\nThis is also called a `barplot`. \n\n## When do customers spend the most ?\n\n::: {#19f71fff .cell execution_count=69}\n``` {.python .cell-code}\nplt.figure(figsize=(7, 5))\nsns.boxplot(x='day', y='total_bill', hue='time', data=df)\nplt.legend(loc=\"upper left\")\n```\n:::\n\n\n`boxplot` (box and whiskers plot)  are used to sketch empirical distributions and to display summary statistics (median and quartiles).\n\n::: {#66bc98a9 .cell execution_count=70}\n``` {.python .cell-code}\nplt.figure(figsize=(7, 5))\nsns.violinplot(x='day', y='total_bill', hue='time', split=True, data=df)\nplt.legend(loc=\"upper left\")\n```\n:::\n\n\n`violinplot` are sketchy variants of kernel density estimates. \n\n\n## Who spends the most ?\n\n::: {#b3fc8849 .cell execution_count=71}\n``` {.python .cell-code}\nsns.boxplot(x='sex', y='total_bill', hue='smoker', data=df)\n```\n:::\n\n\n## When should waiters want to work ?\n\n::: {#9743677d .cell execution_count=72}\n``` {.python .cell-code}\nsns.boxplot(x='day', y='tip', hue='time', data=df)\n```\n:::\n\n\n::: {#596c6e99 .cell execution_count=73}\n``` {.python .cell-code}\nsns.violinplot(x='day', y='tip', hue='time', data=df)\n```\n:::\n\n\n# Data processing with `pandas`\n\nLet us read again the `tips.csv` file\n\n::: {#54142347 .cell execution_count=74}\n``` {.python .cell-code}\nimport pandas as pd\n\ndtypes = {\n    \"sex\": \"category\",\n    \"smoker\": \"category\",\n    \"day\": \"category\",\n    \"time\": \"category\"\n} \n\ndf = pd.read_csv(\"tips.csv\", dtype=dtypes)\ndf.head()\n```\n:::\n\n\n## Computations using `pandas` : broadcasting\n\nLet's add a column that contains the tip percentage. The content \nof this column is computed by performing elementwise operations between \nelements of two columns.  This works as if the columns were NumPy arrays (even though they are not). \n\n::: {#29deeeee .cell execution_count=75}\n``` {.python .cell-code}\ndf[\"tip_percentage\"] = df[\"tip\"] / df[\"total_bill\"]\ndf.head()\n```\n:::\n\n\nThe computation\n\n```{{python}}\ndf[\"tip\"] / df[\"total_bill\"]\n```\nuses a **broadcast** rule (see [NumPy notebook about broadcasting](/core/notebooks/notebook02-2_numpy.qmd)).\n\n- We can multiply, add, subtract, etc. together `numpy` arrays, `Series` or `pandas` dataframes when the computation **makes sense** in view of their respective **shape**\n\nThis principle is called **broadcast** or **broadcasting**.\n\n::: {.callout-note}\n\nBroadcasting is a key feature of `numpy` `ndarray`, see \n\n- [Numpy User's guide](https://numpy.org/doc/stable/user/basics.broadcasting.html)\n- [Pandas book](https://wesmckinney.com/book/advanced-numpy.html#numpy_broadcasting)\n- [A toy example with broadcasting](/core/notebooks/notebook02-2_numpy.qmd)\n\n:::\n\n::: {#e6fa75fd .cell execution_count=76}\n``` {.python .cell-code}\ndf[\"tip\"].shape, df[\"total_bill\"].shape\n```\n:::\n\n\nThe `tip` and `total_bill`columns have the same `shape`, so broadcasting performs **pairwise division**.\n\nThis corresponds to the following \"hand-crafted\" approach with a `for` loop:\n\n```{.python}\n#| \nfor i in range(df.shape[0]):\n    df.loc[i, \"tip_percentage\"] = df.loc[i, \"tip\"] / df.loc[i, \"total_bill\"]\n```\n\nBut using such a loop is: \n\n- longer to write\n- less readable \n- prone to mistakes\n- and *slower* :(\n\n*NEVER* use `Python` for-loops unless you need to !\n\n::: {#38b36e17 .cell execution_count=77}\n``` {.python .cell-code}\n%%timeit -n 10\nfor i in range(df.shape[0]):\n    df.loc[i, \"tip_percentage\"] = df.loc[i, \"tip\"] / df.loc[i, \"total_bill\"]\n```\n:::\n\n\n::: {#35456797 .cell execution_count=78}\n``` {.python .cell-code}\n%%timeit -n 10\ndf[\"tip_percentage\"] = df[\"tip\"] / df[\"total_bill\"]\n```\n:::\n\n\nThe `for` loop is $\\approx$ **100 times slower** ! (even worse on larger data)\n\n### Pitfall. Changing values in a `DataFrame`\n\nWhen you want to change a value in a `DataFrame`, never use\n\n```{.python}\ndf[\"tip_percentage\"].loc[i] = 42\n```\n\nbut use\n\n```{.python}\ndf.loc[i, \"tip_percentage\"] = 42\n```\n\n::: {.callout-caution}\n\nUse a **single** `loc` or `iloc` statement. The first version **might not work**: it might modify a copy of the column and not the dataframe itself !\n\n:::\n\nAnother example of broadcasting is:\n\n::: {#dd785266 .cell execution_count=79}\n``` {.python .cell-code}\n(100 * df[[\"tip_percentage\"]]).head()\n```\n:::\n\n\nwhere we multiplied **each entry** of the `tip_percentage` column by 100.\n\n::: {.callout-note}\n\n### Remark \n\nNote the difference between\n\n\n```{.python}\ndf[['tip_percentage']]\n```\n\nwhich returns a `DataFrame` containing only the `tip_percentage` column and\n\n```{.python}\ndf['tip_percentage']\n```\n\nwhich returns a `Series` containing the data of the `tip_percentage` column\n\n:::\n\n\n## Some more plots\n\n### How do the tip percentages relates to the total bill ?\n\n::: {#4da96540 .cell execution_count=80}\n``` {.python .cell-code}\nsns.jointplot(\n    x=\"total_bill\", \n    y=\"tip_percentage\", \n    data=df\n)\n```\n:::\n\n\n### Who tips best ?\n\n::: {#8630a8a4 .cell execution_count=81}\n``` {.python .cell-code}\nsns.boxplot(\n    x='sex', \n    y='tip_percentage', \n    hue='smoker', \n    data=df\n)\n```\n:::\n\n\n### Who tips best without the `tip_percentage` outliers ?\n\n::: {#1d37451a .cell execution_count=82}\n``` {.python .cell-code}\nsns.boxplot(\n    x='sex', \n    y='tip_percentage', \n    hue='smoker', \n    data=df.loc[df[\"tip_percentage\"] <= 0.3]\n)\n```\n:::\n\n\nObject identity\n\n::: {#c248f902 .cell execution_count=83}\n``` {.python .cell-code}\nid(df)\n```\n:::\n\n\n## The all-mighty `groupby` and `aggregate`\n\nMany computations can be formulated as a **groupby** followed by and **aggregation**.\n\n### What is the mean `tip` and `tip percentage` each day ?\n\n::: {#933f3498 .cell execution_count=84}\n``` {.python .cell-code}\ndf.head()\n```\n:::\n\n\n::: {#d8acc541 .cell execution_count=85}\n``` {.python .cell-code}\ntry:\n    (\n        df\n            .groupby(\"day\", observed=True)\n            .mean()\n    )\nexcept TypeError:\n    print('TypeError: category dtype does not support aggregation \"mean\"')\n```\n:::\n\n\nBut we do not care about the `size` column here, so we can use instead\n\n::: {#26533881 .cell execution_count=86}\n``` {.python .cell-code}\n(\n    df[[\"total_bill\", \"tip\", \"tip_percentage\", \"day\"]]\n        .groupby(\"day\")\n        .mean()\n)\n```\n:::\n\n\nIf we want to be more precise, we can `groupby` using several columns\n\n::: {#2ca5720f .cell execution_count=87}\n``` {.python .cell-code}\n(\n    df[[\"total_bill\", \"tip\", \"tip_percentage\", \"day\", \"time\"]]   # selection\n        .groupby([\"day\",\"time\"])                                # partition\n        .mean()                                                  # aggregation\n)\n```\n:::\n\n\n::: {.callout-note}\n\n### Remarks \n\n- We obtain a `DataFrame` with a two-level indexing: on the `day` and the `time`\n- Groups must be homogeneous: we have `NaN` values for empty groups (e.g. `Sat`, `Lunch`)\n\n:::\n\n\n### Pro tip\n\nSometimes, it is more convenient to get the groups as columns instead of a multi-level index.\n\nFor this, use `reset_index`:\n\n::: {#d407830e .cell execution_count=88}\n``` {.python .cell-code}\n(\n    df[[\"total_bill\", \"tip\", \"tip_percentage\", \"day\", \"time\"]]   # selection\n        .groupby([\"day\", \"time\"])                                # partition\n        .mean() # aggregation\n        .reset_index()   # ako ungroup\n)\n```\n:::\n\n\n### Another pro tip: care about code readers\n\nComputations with pandas can include many operations that are **pipelined** until the final computation.\n\nPipelining many operations is good practice and perfectly normal, but in order to make the code readable you can put it between parenthesis (`python` expression) as follows:\n\n::: {#fe5f335d .cell execution_count=89}\n``` {.python .cell-code}\n(\n    df[[\"total_bill\", \"tip\", \"tip_percentage\", \"day\", \"time\"]]\n    .groupby([\"day\", \"time\"])\n    .mean()\n    .reset_index()\n    # and on top of all this we sort the dataframe with respect \n    # to the tip_percentage\n    .sort_values(\"tip_percentage\")\n)\n```\n:::\n\n\n## Displaying a `DataFrame` with `style`\n\nNow, we can answer, with style, to the question: what are the average tip percentages along the week ?\n\n::: {#8c7e6738 .cell execution_count=90}\n``` {.python .cell-code}\n(\n    df[[\"tip_percentage\", \"day\", \"time\"]]\n    .groupby([\"day\", \"time\"])\n    .mean()\n    # At the end of the pipeline you can use .style\n    .style\n    # Print numerical values as percentages \n    .format(\"{:.2%}\")\n    .background_gradient()\n)\n```\n:::\n\n\n## Removing the `NaN` values\n\nBut the `NaN` values are somewhat annoying. Let's remove them\n\n::: {#b0b4039e .cell execution_count=91}\n``` {.python .cell-code}\n(\n    df[[\"tip_percentage\", \"day\", \"time\"]]\n    .groupby([\"day\", \"time\"])\n    .mean()\n    # We just add this from the previous pipeline\n    .dropna()\n    .style\n    .format(\"{:.2%}\")\n    .background_gradient()\n)\n```\n:::\n\n\nNow, we see when `tip_percentage` is maximal. But what about the standard deviation?\n\n- We used only `.mean()` for now, but we can use several aggregating function using `.agg()`\n\n::: {#44e25efc .cell execution_count=92}\n``` {.python .cell-code}\n(\n    df[[\"tip_percentage\", \"day\", \"time\"]]\n    .groupby([\"day\", \"time\"])\n    .agg([\"mean\", \"std\"])   # we feed `agg`  with a list of names of callables \n    .dropna()\n    .style\n    .format(\"{:.2%}\")\n    .background_gradient()\n)\n```\n:::\n\n\nAnd we can use also `.describe()` as aggregation function. Moreover we\n- use the `subset` option to specify which column we want to style\n- we use `(\"tip_percentage\", \"count\")` to access multi-level index\n\n::: {#d5094ba5 .cell execution_count=93}\n``` {.python .cell-code}\n(\n    df[[\"tip_percentage\", \"day\", \"time\"]]\n    .groupby([\"day\", \"time\"])\n    .describe()    # all-purpose summarising function\n)\n```\n:::\n\n\n::: {#dbdb7eb8 .cell execution_count=94}\n``` {.python .cell-code}\n(\n    df[[\"tip_percentage\", \"day\", \"time\"]]\n    .groupby([\"day\", \"time\"])\n    .describe()\n    .dropna()\n    .style\n    .bar(subset=[(\"tip_percentage\", \"count\")])\n    .background_gradient(subset=[(\"tip_percentage\", \"50%\")])\n)\n```\n:::\n\n\n## Supervised learning of `tip` based on the `total_bill` \n\nAs an example of very simple **machine-learning** problem, let us try to understand how we can predict `tip` based on `total_bill`.\n\n::: {#79222688 .cell execution_count=95}\n``` {.python .cell-code}\nimport numpy as np\n\nplt.scatter(df[\"total_bill\"], df[\"tip\"])\nplt.xlabel(\"total_bill\", fontsize=12)\nplt.ylabel(\"tip\", fontsize=12)\n```\n:::\n\n\nThere's a rough **linear** dependence between the two. Let us try to find it by hand!<br>\nNamely, we look for numbers $b$ and $w$ such that\n\n```\ntip ≈ b + w × total_bill\n```\n\nfor all the examples of pairs of `(tip, total_bill)` we observe in the data.\n\nIn **machine learning**, we say that this is a very simple example of a **supervised learning** problem (here it is a regression problem), where `tip` is the **label** and where `total_bill` is the (only) **feature**, for which we intend to use a **linear predictor**.\n\n::: {#a49e4f5d .cell execution_count=96}\n``` {.python .cell-code}\nplt.scatter(df[\"total_bill\"], df[\"tip\"])\nplt.xlabel(\"total_bill\", fontsize=12)\nplt.ylabel(\"tip\", fontsize=12)\n\nslope = 1.0\nintercept = 0.0\n\nx = np.linspace(0, 50, 1000)\nplt.plot(x, intercept + slope * x, color=\"red\")\n```\n:::\n\n\n### A more interactive way \n\nThis might require\n\n::: {#f39bcc0f .cell execution_count=97}\n``` {.python .cell-code}\n!pip install ipympl\n```\n:::\n\n\n::: {#8f8301b7 .cell execution_count=98}\n``` {.python .cell-code}\nimport ipywidgets as widgets\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n%matplotlib widget\n%matplotlib inline\n\nx = np.linspace(0, 50, 1000)\n\n@widgets.interact(intercept=(-5, 5, 1.), slope=(0, 1, .05))\ndef update(intercept=0.0, slope=0.5):\n    plt.scatter(df[\"total_bill\"], df[\"tip\"])\n    plt.plot(x, intercept + slope * x, color=\"red\")\n    plt.xlim((0, 50))\n    plt.ylim((0, 10))\n    plt.xlabel(\"total_bill\", fontsize=12)\n    plt.ylabel(\"tip\", fontsize=12)\n```\n:::\n\n\nThis is kind of tedious to do this by hand... it would be nice to come up with an **automated** way of doing this. Moreover:\n\n- We are using a **linear** function, while something more complicated (such as a polynomial) might be better\n- More importantly, we use **only** the `total_bill` column to predict the `tip`, while we know about many other things\n\n::: {#3336f67e .cell execution_count=99}\n``` {.python .cell-code}\ndf.head()\n```\n:::\n\n\n## One-hot encoding of categorical variables\n\nWe can't perform computations (products and sums) with columns containing **categorical** variables. So, we can't use them like this to predict the `tip`.\nWe need to **convert** them to numbers somehow.\n\nThe most classical approach for this is **one-hot encoding** (or \"create dummies\" or \"binarize\") of the categorical variables, which can be easily achieved with `pandas.get_dummies`\n\nWhy *one-hot* ? See [wikipedia](https://en.wikipedia.org/wiki/One-hot) for a plausible explanation\n\n::: {#4b5757c2 .cell execution_count=100}\n``` {.python .cell-code}\ndf_one_hot = pd.get_dummies(df, prefix_sep='#')\ndf_one_hot.head(5)\n```\n:::\n\n\nOnly the categorical columns have been one-hot encoded. For instance, the `\"day\"` column is replaced by 4 columns named `\"day#Thur\"`, `\"day#Fri\"`, `\"day#Sat\"`, `\"day#Sun\"`, since `\"day\"` has 4 modalities (see next line).\n\n::: {#4a6ab6fb .cell execution_count=101}\n``` {.python .cell-code}\ndf['day'].unique()\n```\n:::\n\n\n::: {#f7d415db .cell execution_count=102}\n``` {.python .cell-code}\ndf_one_hot.dtypes\n```\n:::\n\n\n## Pitfall. Colinearities with one-hot encoding\n\nSums over dummies for `sex`, `smoker`, `day`, `time` and `size` are all equal to one (by constrution of the one-hot encoded vectors).\n\n- Leads to **colinearities** in the matrix of features\n- It is **much harder** to train a linear regressor when the columns of the features matrix has colinearities\n\n::: {#fb2acbcb .cell execution_count=103}\n``` {.python .cell-code}\nday_cols = [col for col in df_one_hot.columns if col.startswith(\"day\")]\ndf_one_hot[day_cols].head()\ndf_one_hot[day_cols].sum(axis=1)\n```\n:::\n\n\n::: {#ed93022e .cell execution_count=104}\n``` {.python .cell-code}\nall(df_one_hot[day_cols].sum(axis=1) == 1)\n```\n:::\n\n\nThe most standard solution is to remove a modality (i.e. remove a one-hot encoding vector). Simply achieved by specifying `drop_first=True` in the `get_dummies` function.\n\n::: {#10f0330d .cell execution_count=105}\n``` {.python .cell-code}\ndf[\"day\"].unique()\n```\n:::\n\n\n::: {#28adf9b3 .cell execution_count=106}\n``` {.python .cell-code}\npd.get_dummies(df, prefix_sep='#', drop_first=True).head()\n```\n:::\n\n\nNow, if a categorical feature has $K$ modalities/levels, we use only $K-1$ dummies.\nFor instance, there is no more `sex#Female` binary column. \n\n**Question.** So, a linear regression won't fit a weight for `sex#Female`. But, where do the model weights of the dropped binary columns go ?\n\n**Answer.** They just \"go\" to the **intercept**: interpretation of the population bias depends on the \"dropped\" one-hot encodings.\n\nSo, we actually fit:\n\n$$\n\\begin{array}{rl} \\texttt{tip} \\approx b & + w_1 \\times \\texttt{total\\_bill} + w_2 \\times \\texttt{size} \\\\ & + w_3 \\times \\texttt{sex\\#Male} + w_4 \\times \\texttt{smoker\\#Yes} \\\\ & + w_5 \\times \\texttt{day\\#Sat} + w_6 \\times \\texttt{day\\#Sun} + w_7 \\times \\texttt{day\\#Thur} \\\\ & + w_8 \\times \\texttt{time\\#Lunch} \\end{array}\n$$\n\n---\njupyter:\n  kernelspec:\n    display_name: Python 3 (ipykernel)\n    language: python\n    name: python3\n    path: /usr/share/jupyter/kernels/python3\n  language_info:\n    codemirror_mode:\n      name: ipython\n      version: 3\n    file_extension: .py\n    mimetype: text/x-python\n    name: python\n    nbconvert_exporter: python\n    pygments_lexer: ipython3\n    version: 3.12.3\n---\n",
    "supporting": [
      "notebook03_pandas_files"
    ],
    "filters": []
  }
}