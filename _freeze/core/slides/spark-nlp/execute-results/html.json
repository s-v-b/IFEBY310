{
  "hash": "228936c3ffaabd8fee82935e7d6eece2",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Spark NLP\"\nengine: \"jupyter\"\ndate: \"2026-02-17\"\n\nexecute: \n  echo: true\n  eval: true\n\nformat:\n  revealjs: \n    scroll-view: true\n---\n\n# Spark-NLP in perspective {background-color=\"#1c191c\"}\n\n## Spark NLP\n\n[Spark NLP](https://sparknlp.org) provides an example of an application in the Apache Spark Ecosystem\n\n. . .\n\nSpark NLP relies on the Spark SQL Lib and Spark Dataframes (high level APIs) and also on the Spark ML Lib.\n\n. . .\n\nSpark NLP borrows ideas from existing NLP softwares and adapts the known techniques to the Spark principles\n\n\n...\n\nNLP deals with many applications of machine learning\n\n- Automatic translation  (see [deepl.com](https://www.deepl.com/translator))\n- Topic modeling (text clustering)\n- Sentiment Analysis\n- LLMs\n- ...\n\n\n\n# NLP Libraries {background-color=\"#1c191c\"}\n\n## Two flavors of NLP libraries\n\n-   *Functionality* Libraries  [nltk.org](https://www.nltk.org)\n\n-   *Annotation* Libraries [spaCy's site](https://spacy.io)\n\n\n## spaCy and Spark?\n\nA [databricks notebook](https://winf-hsos.github.io/databricks-notebooks/big-data-analytics/ss-2020/NLP%20with%20Python%20and%20spaCy%20-%20First%20Steps.html) discusses possible interactions between spaCy and Spark on a use case:\n\n. . .\n\n-   Get the tweets (the texts) into a Spark dataframe using `spark.sql()`\n-   Convert the Spark dataframe to a `numpy` array\n-   Stream all tweets in batches using `nlp.pipe()`\n-   Go through the processed tweets and take copy everything we need in a large array object\n-   Convert back the large array object into a Spark dataframe\n-   Save the dataframe as table, so we can query the whole thing withh SQL again\n\n. . .\n\n::: callout-warning\n\n### No hint at parallelizing spaCy's annotation process\n\n:::\n\n## spaCy v2 (current v3.7)\n\n> spaCy v2 now fully supports the `Pickle` protocol, making it easy to use spaCy with Apache Spark.\n\n[spaCy v2 documentation](https://spacy.io/usage/v2)\n\n# A short example (from [John Snow Labs](https://sparknlp.org)) {background-color=\"#1c191c\"}\n\n---\n\n-   Initializing a `sparknlp` session\n-   Building a toy NLP pipeline for detecting dates in a text\n\n## Imports sparknlp and others\n\n::: {#65aff0a3 .cell execution_count=1}\n``` {.python .cell-code}\n# Import Spark NLP\nfrom sparknlp.base import *\nfrom sparknlp.annotator import *\nfrom sparknlp.pretrained import PretrainedPipeline\nimport sparknlp\n```\n:::\n\n\n\n\n## Initiate Spark session\n\nAssuming `standalone` mode on a laptop. `master` runs on `localhost`\n\n::: {#b41b967b .cell execution_count=3}\n``` {.python .cell-code}\nspark = SparkSession.builder \\\n    .appName(\"Spark NLP\") \\\n    .master(\"local[*]\") \\\n    .config(\"spark.driver.memory\", \"16G\") \\\n    .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") \\\n    .config(\"spark.kryoserializer.buffer.max\", \"2000M\") \\\n    .config(\"spark.driver.maxResultSize\", \"0\") \\\n    .config(\"spark.jars.packages\", \"com.johnsnowlabs.nlp:spark-nlp_2.12:6.3.2\") \\\n    .getOrCreate()\n```\n:::\n\n\n. . .\n\n::: {#b117475c .cell execution_count=4}\n``` {.python .cell-code}\n# sparknlp.version()\n```\n:::\n\n\n...\n\n::: {#a504c8ac .cell execution_count=5}\n``` {.python .cell-code}\nspark\n```\n\n::: {.cell-output .cell-output-display execution_count=51}\n```{=html}\n\n            <div>\n                <p><b>SparkSession - in-memory</b></p>\n                \n        <div>\n            <p><b>SparkContext</b></p>\n\n            <p><a href=\"http://host-32-10.sg.lan:4040\">Spark UI</a></p>\n\n            <dl>\n              <dt>Version</dt>\n                <dd><code>v3.5.3</code></dd>\n              <dt>Master</dt>\n                <dd><code>local[*]</code></dd>\n              <dt>AppName</dt>\n                <dd><code>Spark NLP</code></dd>\n            </dl>\n        </div>\n        \n            </div>\n        \n```\n:::\n:::\n\n\n## Toy (big) data\n\n::: {#2179a383 .cell execution_count=6}\n``` {.python .cell-code}\nfr_articles = [\n  (\"Le dimanche 11 juillet 2021, Chiellini a utilisé le mot Kiricocho lorsque Saka s'est approché du ballon pour le penalty.\",),\n  (\"La prochaine Coupe du monde aura lieu en novembre 2022.\",),\n  (\"À Noël 800, Charlemagne se fit couronner empereur à Rome.\",),\n  (\"Le Marathon de Paris a lieu le premier dimanche d'avril 2024\",)\n]\n```\n:::\n\n\n. . .\n\n::: {#889f21fd .cell execution_count=7}\n``` {.python .cell-code}\narticles_cols = [\"text\"]\n\ndf = spark.createDataFrame(\n  data=fr_articles, \n  schema=articles_cols)\n\ndf.printSchema()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nroot\n |-- text: string (nullable = true)\n\n```\n:::\n:::\n\n\n## Pipelines\n\n::: {#b403477e .cell execution_count=8}\n``` {.python .cell-code}\ndocument_assembler = (\n  DocumentAssembler() \n            .setInputCol(\"text\") \n            .setOutputCol(\"document\")\n)\n```\n:::\n\n\n::: callout\nColumn `document` contains the 'text' to be annotated as well as some possible metadata.\n\nStarting point of any annotation process\n\nSpark NLP relies on Saprk SQL for storing, moving, data.\n:::\n\n. . .\n\n::: {#e60ed510 .cell execution_count=9}\n``` {.python .cell-code}\ndate_matcher = DateMatcher() \\\n            .setInputCols(['document']) \\\n            .setOutputCol(\"date\") \\\n            .setOutputFormat(\"MM/dd/yyyy\") \\\n            .setSourceLanguage(\"fr\")\n```\n:::\n\n\n::: callout\n-   Spark NLP adopts an original way of storing annotations\n-   Spark NLP creates columns for annotations\n-   Spark NLP stores annotation in Spark dataframes\n-   Annotators are\n    -   Tranformers\n    -   Estimators\n    -   Models\n:::\n\n## Transformation/Action\n\n::: {#40784d3b .cell execution_count=10}\n``` {.python .cell-code}\nassembled = ( \n  document_assembler.transform(df)\n)\n```\n:::\n\n\n::: {#b6a94192 .cell execution_count=11}\n``` {.python .cell-code}\n(\n date_matcher\n  .transform(assembled)\n  .select('date')\n  .show(10, False)\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n+-------------------------------------------------+\n|date                                             |\n+-------------------------------------------------+\n|[{date, 10, 21, 07/11/2021, {sentence -> 0}, []}]|\n|[{date, 41, 53, 11/01/2022, {sentence -> 0}, []}]|\n|[]                                               |\n|[{date, 3, 60, 01/01/2024, {sentence -> 0}, []}] |\n+-------------------------------------------------+\n\n```\n:::\n:::\n\n\n## More\n\n::: {#9d1e9cd4 .cell execution_count=12}\n``` {.python .cell-code}\nfr_articles.append((\"Nous nous sommes rencontrés le 13/05/2018 puis le 18/05/2020.\",))\n\nfr_articles.append((\"Nous nous sommes rencontrés il y a 2 jours et il m'a dit qu'il nous rendrait visite la semaine prochaine.\",))\n```\n:::\n\n\n::: {#c540f7ac .cell execution_count=13}\n``` {.python .cell-code}\ndf = spark.createDataFrame(\n  data=fr_articles, \n  schema=articles_cols)\n\ndf.printSchema()\ndf.show()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nroot\n |-- text: string (nullable = true)\n\n+--------------------+\n|                text|\n+--------------------+\n|Le dimanche 11 ju...|\n|La prochaine Coup...|\n|À Noël 800, Charl...|\n|Le Marathon de Pa...|\n|Nous nous sommes ...|\n|Nous nous sommes ...|\n+--------------------+\n\n```\n:::\n:::\n\n\n::: {#10281044 .cell execution_count=14}\n``` {.python .cell-code}\nassembled = ( \n  document_assembler.transform(df)\n)\n```\n:::\n\n\n::: {#282b3c26 .cell execution_count=15}\n``` {.python .cell-code}\n(\n date_matcher\n  .transform(assembled)\n  .select('date')\n  .show(10, False)\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n+-------------------------------------------------+\n|date                                             |\n+-------------------------------------------------+\n|[{date, 10, 21, 07/11/2021, {sentence -> 0}, []}]|\n|[{date, 41, 53, 11/01/2022, {sentence -> 0}, []}]|\n|[]                                               |\n|[{date, 3, 60, 01/01/2024, {sentence -> 0}, []}] |\n|[{date, 31, 40, 05/13/2018, {sentence -> 0}, []}]|\n|[{date, 84, 92, 02/24/2026, {sentence -> 0}, []}]|\n+-------------------------------------------------+\n\n```\n:::\n:::\n\n\n## Another annotator\n\n::: {#d4c765b4 .cell execution_count=16}\n``` {.python .cell-code}\ndate_matcher_bis = MultiDateMatcher() \\\n            .setInputCols(['document']) \\\n            .setOutputCol(\"date\") \\\n            .setOutputFormat(\"MM/dd/yyyy\") \\\n            .setSourceLanguage(\"fr\")\n```\n:::\n\n\n::: {#2a47e865 .cell execution_count=17}\n``` {.python .cell-code}\n(\n  date_matcher_bis\n    .transform(assembled)\n    .select(\"date\")\n    .show(10, False)\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n+--------------------------------------------------------------------------------------------------+\n|date                                                                                              |\n+--------------------------------------------------------------------------------------------------+\n|[{date, 11, 22, 07/11/2021, {sentence -> 0}, []}]                                                 |\n|[{date, 41, 53, 11/01/2022, {sentence -> 0}, []}]                                                 |\n|[]                                                                                                |\n|[{date, 53, 62, 04/01/2024, {sentence -> 0}, []}]                                                 |\n|[{date, 31, 40, 05/13/2018, {sentence -> 0}, []}, {date, 50, 59, 05/18/2020, {sentence -> 0}, []}]|\n|[{date, 28, 37, 02/15/2026, {sentence -> 0}, []}, {date, 80, 88, 02/24/2026, {sentence -> 0}, []}]|\n+--------------------------------------------------------------------------------------------------+\n\n```\n:::\n:::\n\n\n# Spark NLP Design {background-color=\"#1c191c\"}\n\n## SQL Lib and Dataframes\n\n## ML Lib, Transformers and Estimators\n\n# Spark NLP Pipelines {background-color=\"#1c191c\"}\n\n##  Getting a corpus : ETL\n\n::: {#5f312bd0 .cell execution_count=18}\n``` {.python .cell-code}\n# Warm up \npattern = 'URL: http://www.nytimes.com/(?P<zedate>[0-9]{4}/[0-9]{2}/[0-9]{2})/.*'\ntitle = 'URL: http://www.nytimes.com/[0-9]{4}/[0-9]{2}/[0-9]{2}/(.*)'\nreg_date = re.compile(pattern)\nreg_title = re.compile(title)\n```\n:::\n\n\n::: {#fa260449 .cell execution_count=19}\n``` {.python .cell-code}\n# Getting the data\nnypath = Path('../../data/nytimes_news_articles.txt')\ncorpus_list = list()\n```\n:::\n\n\n##  Building the corpus   {.scrollable}\n\n::: {#c6f777d7 .cell execution_count=20}\n``` {.python .cell-code}\n##\nwith open(nypath, encoding='UTF-8')  as fd:\n    doc, document = None, None\n    while l := fd.readline():        \n        if m := reg_date.match(l):\n            if doc is not None:\n                corpus_list.append((*document, doc))\n                doc, document = None, None\n            ymd = date(*[int(n) for n in m.groups()[0].split('/')])\n            title = (\n                reg_title.match(l)\n                  .groups()[0]\n                  .split('/')\n            )\n            document =  (ymd, title[-1], '/'.join(title[:-1]))\n            doc = ''\n        else: doc = doc + l\n    else:\n        if doc is not None:\n            corpus_list.append((*document, doc))\n```\n:::\n\n\n::: {#e3a9b764 .cell execution_count=21}\n``` {.python .cell-code}\ndf_texts = spark.createDataFrame(\n    corpus_list,\n    schema= StructType([\n      StructField('date', DateType(), False),\n      StructField('title', StringType(), False),\n      StructField('topic', StringType(), False),\n      StructField('text', StringType(), True)\n]))\n```\n:::\n\n\n## Checking the loaded data\n\n::: {#c4a87b6d .cell execution_count=22}\n``` {.python .cell-code}\ndf_texts.printSchema()\ndf_texts.count()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nroot\n |-- date: date (nullable = false)\n |-- title: string (nullable = false)\n |-- topic: string (nullable = false)\n |-- text: string (nullable = true)\n\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=68}\n```\n8888\n```\n:::\n:::\n\n\n## Saving \n\nLocally\n\n::: {#234d8216 .cell execution_count=23}\n``` {.python .cell-code}\ndf_texts.write.parquet('../../data/ny_corpus_pq', mode=\"overwrite\")\n```\n:::\n\n\n::: {#89364065 .cell execution_count=24}\n``` {.python .cell-code}\nspam = spark.read.parquet('../../data/ny_corpus_pq')\n\nspam.printSchema()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nroot\n |-- date: date (nullable = true)\n |-- title: string (nullable = true)\n |-- topic: string (nullable = true)\n |-- text: string (nullable = true)\n\n```\n:::\n:::\n\n\n::: {#1cbc15a3 .cell execution_count=25}\n``` {.python .cell-code}\nspam.rdd.getNumPartitions()\n```\n\n::: {.cell-output .cell-output-display execution_count=71}\n```\n20\n```\n:::\n:::\n\n\n## \n\n::: {#c3fb3c0f .cell execution_count=26}\n``` {.python .cell-code}\ncorpus_assembled = ( \n  document_assembler.transform(df_texts)\n)\n```\n:::\n\n\n::: {#f49f4d54 .cell execution_count=27}\n``` {.python .cell-code}\ncorpus_assembled.printSchema()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nroot\n |-- date: date (nullable = false)\n |-- title: string (nullable = false)\n |-- topic: string (nullable = false)\n |-- text: string (nullable = true)\n |-- document: array (nullable = true)\n |    |-- element: struct (containsNull = true)\n |    |    |-- annotatorType: string (nullable = true)\n |    |    |-- begin: integer (nullable = false)\n |    |    |-- end: integer (nullable = false)\n |    |    |-- result: string (nullable = true)\n |    |    |-- metadata: map (nullable = true)\n |    |    |    |-- key: string\n |    |    |    |-- value: string (valueContainsNull = true)\n |    |    |-- embeddings: array (nullable = true)\n |    |    |    |-- element: float (containsNull = false)\n\n```\n:::\n:::\n\n\n::: {#dd698583 .cell execution_count=28}\n``` {.python .cell-code}\n(\n  date_matcher_bis\n    .transform(corpus_assembled)\n    .select(\"title\", \"date\")\n    .show(10, False)\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n+-----------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------+\n|title                                                                        |date                                                                                                                                                          |\n+-----------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------+\n|washington-nationals-max-scherzer-baffles-mets-completing-a-sweep.html       |[{date, 478, 1690, 01/04/2026, {sentence -> 0}, []}]                                                                                                          |\n|mayor-de-blasios-counsel-to-leave-next-month-to-lead-police-review-board.html|[{date, 1, 670, 05/01/2014, {sentence -> 0}, []}, {date, 82, 91, 03/17/2026, {sentence -> 0}, []}]                                                            |\n|three-men-charged-in-killing-of-cuomo-administration-lawyer.html             |[{date, 81, 547, 08/08/2026, {sentence -> 0}, []}, {date, 42, 50, 02/17/2025, {sentence -> 0}, []}, {date, 1261, 1268, 02/16/2026, {sentence -> 0}, []}]      |\n|tekserve-precursor-to-the-apple-store-to-close-after-29-years.html           |[{date, 1699, 1710, 02/17/2011, {sentence -> 0}, []}, {date, 257, 1549, 07/23/1987, {sentence -> 0}, []}]                                                     |\n|once-at-michael-phelpss-feet-and-still-chasing-them.html                     |[{date, 1101, 1111, 02/17/2026, {sentence -> 0}, []}, {date, 204, 3658, 08/14/2004, {sentence -> 0}, []}, {date, 2985, 2993, 02/17/2025, {sentence -> 0}, []}]|\n|missy-franklin-breaks-through-in-trials-and-earns-a-return-to-olympics.html  |[{date, 369, 1462, 12/21/2012, {sentence -> 0}, []}]                                                                                                          |\n|lionsgate-is-said-to-be-near-deal-to-buy-starz.html                          |[{date, 290, 2569, 12/28/2026, {sentence -> 0}, []}, {date, 1802, 1810, 02/17/2025, {sentence -> 0}, []}]                                                     |\n|pool-rules-no-running-no-eating-or-drinking-no-men.html                      |[{date, 274, 1351, 12/20/1922, {sentence -> 0}, []}]                                                                                                          |\n|knicks-look-to-young-blood-and-free-agency-to-patch-porous-roster.html       |[{date, 473, 3111, 05/30/2014, {sentence -> 0}, []}, {date, 1090, 1098, 02/10/2026, {sentence -> 0}, []}]                                                     |\n|latest-sign-of-change-in-harlem-its-congressional-candidate.html             |[{date, 389, 1270, 12/01/2026, {sentence -> 0}, []}]                                                                                                          |\n+-----------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------+\nonly showing top 10 rows\n\n```\n:::\n:::\n\n\n::: {.callout-warning}\nExtracted dates should be taken with a grain of salt \n:::\n\n\n## Public pipelines\n\n::: {#34544e39 .cell execution_count=29}\n``` {.python .cell-code}\nfrom sparknlp.pretrained import PretrainedPipeline\nexplain_document_pipeline = PretrainedPipeline(\"explain_document_ml\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nexplain_document_ml download started this may take some time.\nApprox size to download 9 MB\n\r[ | ]\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nexplain_document_ml download started this may take some time.\nApproximate size to download 9 MB\nDownload done! Loading the resource.\n\r[ / ]\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n\r[ — ]\r[OK!]\n```\n:::\n:::\n\n\n## Chaining annotators  {.scrollable}\n\n::: {#688a69d6 .cell execution_count=30}\n``` {.python .cell-code}\nsentenceDetector = SentenceDetector() \\\n    .setInputCols([\"document\"]) \\\n    .setOutputCol(\"sentence\")\nregexTokenizer = Tokenizer() \\\n    .setInputCols([\"sentence\"]) \\\n    .setOutputCol(\"token\")\n```\n:::\n\n\n::: {#d007319f .cell execution_count=31}\n``` {.python .cell-code}\nfinisher = Finisher() \\\n    .setInputCols([\"token\"]) \\\n    .setIncludeMetadata(True)\n```\n:::\n\n\n::: {#749c6da5 .cell execution_count=32}\n``` {.python .cell-code}\npipeline = Pipeline().setStages([\n    document_assembler,\n    sentenceDetector,\n    regexTokenizer,\n    finisher\n])\n```\n:::\n\n\n## Fitting and transforming\n\n::: {#356f3c1c .cell execution_count=33}\n``` {.python .cell-code}\nspam = ( \n  pipeline.fit(df_texts)\n    .transform(df_texts)\n    .select(\"finished_token\")\n    .collect()\n)\n```\n:::\n\n\n## A customized pipeline\n\n::: {#ad0799d2 .cell execution_count=34}\n``` {.python .cell-code}\nstemmer = (\n  Stemmer()\n    .setInputCols(['token'])\n    .setOutputCol('stem')\n)\n```\n:::\n\n\n::: {#b09318b3 .cell execution_count=35}\n``` {.python .cell-code}\nlemmatizer = (\n  LemmatizerModel.pretrained()\n    .setInputCols(['token'])\n    .setOutputCol('lemma')\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nlemma_antbnc download started this may take some time.\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nApproximate size to download 907.6 KB\n\r[ | ]\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nlemma_antbnc download started this may take some time.\nApproximate size to download 907.6 KB\nDownload done! Loading the resource.\n\r[OK!]\n```\n:::\n:::\n\n\n::: {.callout-warning}\n\n```{.bash}\nlemma_antbnc download started this may take some time.\nApproximate size to download 907.6 KB\n[ / ]lemma_antbnc download started this may take some time.\nApproximate size to download 907.6 KB\n[ / ]Download done! Loading the resource.\n[ — ]\n\n[OK!]\n```\n:::\n\n::: {#e3690163 .cell execution_count=36}\n``` {.python .cell-code}\nposTagger = PerceptronModel.pretrained() \\\n    .setInputCols([\"document\", \"token\"]) \\\n    .setOutputCol(\"pos\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\npos_anc download started this may take some time.\nApproximate size to download 3.9 MB\n\r[ | ]\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\npos_anc download started this may take some time.\nApproximate size to download 3.9 MB\nDownload done! Loading the resource.\n\r[ / ]\r[OK!]\n```\n:::\n:::\n\n\n::: {.callout-warning}\n\n```{.bash}\npos_anc download started this may take some time.\nApproximate size to download 3.9 MB\n[ — ]pos_anc download started this may take some time.\nApproximate size to download 3.9 MB\n[ \\ ]Download done! Loading the resource.\n[Stage 34:===========================================>              (3 + 1) / 4]\n[ | ]\n                                                                \n[OK!]\n```\n\n:::\n\n::: {#82ce068b .cell execution_count=37}\n``` {.python .cell-code}\nfinisher = (\n  Finisher()\n    .setInputCols([\n      'token', \n#      'stem', \n#      'lemma', \n      'pos'])\n    .setIncludeMetadata(False)\n    .setOutputAsArray(True)\n)\n```\n:::\n\n\n## \n\n::: {#8a58839c .cell execution_count=38}\n``` {.python .cell-code}\npipeline = (\n  Pipeline()\n    .setStages([\n      document_assembler,\n      sentenceDetector,\n      regexTokenizer,\n      posTagger, \n      finisher\n    ])\n)\n```\n:::\n\n\n::: {#8b822781 .cell execution_count=39}\n``` {.python .cell-code}\nspam = ( \n  pipeline.fit(df_texts)\n    .transform(df_texts)\n    .selectExpr(\"*\")\n    .collect()\n)\n```\n:::\n\n\n# Spark NLP and feature engineering {background-color=\"#1c191c\"}\n\n## Topic modelling\n\n## TF-IDF\n\n## Latent Dirichlet Allocation\n\n# Distributed computations {background-color=\"#1c191c\"}\n\n## Execution modes\n\n-   standalone\n-   client\n-   cluster\n-   \n\n# Spark NLP and composite types in Spark Dataframes \n\n```{.python}\n\n>>> result = documentAssembler.transform(data)\n>>> result.select(\"document\").show(truncate=False)\n+----------------------------------------------------------------------------------------------+\n|document                                                                                      |\n+----------------------------------------------------------------------------------------------+\n|[[document, 0, 51, Spark NLP is an open-source text processing library., [sentence -> 0], []]]|\n+----------------------------------------------------------------------------------------------+\n>>> result.select(\"document\").printSchema()\nroot\n|-- document: array (nullable = True)\n|    |-- element: struct (containsNull = True)\n|    |    |-- annotatorType: string (nullable = True)\n|    |    |-- begin: integer (nullable = False)\n|    |    |-- end: integer (nullable = False)\n|    |    |-- result: string (nullable = True)\n|    |    |-- metadata: map (nullable = True)\n|    |    |    |-- key: string\n|    |    |    |-- value: string (valueContainsNull = True)\n|    |    |-- embeddings: array (nullable = True)\n|    |    |    |-- element: float (containsNull = False)\n```\n\n\nColumn `document` is of type `ArrayType()`. The basetype of `document` column is of `StructType()` (`element`), the `element` contains subfields of primitive type, but alo a field of type `map` (`MapType()`) and a field of type `StructType()`.  \n\n",
    "supporting": [
      "spark-nlp_files"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdn.jsdelivr.net/npm/requirejs@2.3.6/require.min.js\" integrity=\"sha384-c9c+LnTbwQ3aujuU7ULEPVvgLs+Fn6fJUvIGTsuu1ZcCf11fiEubah0ttpca4ntM sha384-6V1/AdqZRWk1KAlWbKBlGhN7VG4iE/yAZcO6NZPMF8od0vukrvr0tg4qY6NSrItx\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdn.jsdelivr.net/npm/jquery@3.5.1/dist/jquery.min.js\" integrity=\"sha384-ZvpUoO/+PpLXR1lu4jmpXWu80pZlYUAfxl5NsBMWOEPSjUn/6Z/hRTt8+pR6L4N2\" crossorigin=\"anonymous\" data-relocate-top=\"true\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}